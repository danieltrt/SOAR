file_path,api_count,code
Scripts_Dataset/main.py,32,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nGPU = False\ntorch.manual_seed(0)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1_1 = torch.nn.BatchNorm2d(32)\n        self.conv1_2 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn1_2 = torch.nn.BatchNorm2d(32)\n        self.conv2_1 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2_1 = torch.nn.BatchNorm2d(64)\n        self.conv2_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn2_2 = torch.nn.BatchNorm2d(64)\n        self.conv3_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3_1 = torch.nn.BatchNorm2d(128)\n        self.conv3_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn3_2 = torch.nn.BatchNorm2d(128)\n        self.conv4_1 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn4_1 = torch.nn.BatchNorm2d(256)\n        self.conv4_2 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn4_2 = torch.nn.BatchNorm2d(256)\n        self.fc1 = torch.nn.Linear(img_height//16 * img_width//16 * 256, 512)\n        #self.fc1_d = torch.nn.Dropout2d()\n        self.fc2 = torch.nn.Linear(512, 512)\n        self.fc_out = torch.nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1_1(self.conv1_1(x)))\n        x = F.relu(self.bn1_2(self.conv1_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn2_1(self.conv2_1(x)))\n        x = F.relu(self.bn2_2(self.conv2_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn3_1(self.conv3_1(x)))\n        x = F.relu(self.bn3_2(self.conv3_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn4_1(self.conv4_1(x)))\n        x = F.relu(self.bn4_2(self.conv4_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, img_height//16 * img_width // 16 * 256)\n        x = F.relu(self.fc1(x))\n        #x = self.fc1_d(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc_out(x)\n        return x\n\n\n# get train data\ndef data_load():\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0))\n    \n    for dir_path in glob(\'../../Dataset/train/images/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((1))\n            if \'akahara\' in path:\n                t = np.array((0))\n            elif \'madara\' in path:\n                t = np.array((1))\n            ts = np.r_[ts, t]\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts = data_load()\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    for dir_path in glob(\'../../Dataset/test/images/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x.transpose(2, 0, 1)\n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n            \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/load_mnist.py,0,"b'import os\nimport gzip\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32) / 255\n        train_x = train_x.reshape((-1, 28, 28))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32) / 255\n        test_x = test_x.reshape((-1, 28, 28))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n\n        """"""\n        with open(dir_path + \'/\' + f_name, \'rb\') as f:\n            #print(struct.unpack(""b"", f.read(1)))\n            a = f.readlines()\n\n        #print(struct.unpack(""b"", a[0]))\n        print(len(a))\n        for _a in a[:1]:\n            print(int.from_bytes(_a, \'little\'))\n            print(_a)\n        """"""\n        \n\n    return train_x, train_y ,test_x, test_y\n\n\nload_mnist()\n'"
Scripts_HowTo/main_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nGPU = -1\n\nclass Mynet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.conv1_1 = L.Convolution2D(None, 32, ksize=3, pad=1, nobias=False)\n            self.bn1_1 = L.BatchNormalization(32)\n            self.conv1_2 = L.Convolution2D(None, 32, ksize=3, pad=1, nobias=False)\n            self.bn1_2 = L.BatchNormalization(32)\n            self.conv2_1 = L.Convolution2D(None, 64, ksize=3, pad=1, nobias=False)\n            self.bn2_1 = L.BatchNormalization(64)\n            self.conv2_2 = L.Convolution2D(None, 64, ksize=3, pad=1, nobias=False)\n            self.bn2_2 = L.BatchNormalization(64)\n            self.conv3_1 = L.Convolution2D(None, 128, ksize=3, pad=1, nobias=False)\n            self.bn3_1 = L.BatchNormalization(128)\n            self.conv3_2 = L.Convolution2D(None, 128, ksize=3, pad=1, nobias=False)\n            self.bn3_2 = L.BatchNormalization(128)\n            self.conv4_1 = L.Convolution2D(None, 256, ksize=3, pad=1, nobias=False)\n            self.bn4_1 = L.BatchNormalization(256)\n            self.conv4_2 = L.Convolution2D(None, 256, ksize=3, pad=1, nobias=False)\n            self.bn4_2 = L.BatchNormalization(256)\n            self.fc1 = L.Linear(None, 512, nobias=False)\n            self.fc2 = L.Linear(None, 512, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = self.bn1_1(x)\n        x = F.relu(self.conv1_2(x))\n        x = self.bn1_2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv2_1(x))\n        x = self.bn2_1(x)\n        x = F.relu(self.conv2_2(x))\n        x = self.bn2_2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv3_1(x))\n        x = self.bn3_1(x)\n        x = F.relu(self.conv3_2(x))\n        x = self.bn3_2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv4_1(x))\n        x = self.bn4_1(x)\n        x = F.relu(self.conv4_2(x))\n        x = self.bn4_2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.fc1(x))\n        if self.train:\n            x = F.dropout(x, ratio=0.5)\n        x = F.relu(self.fc2(x))\n        if self.train:\n            x = F.dropout(x, ratio=0.5)\n        x = self.fc_out(x)\n        return x\n\n\n# get train data\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3), dtype=np.float32)\n    ts = np.ndarray((0), dtype=np.int32)\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((1))\n            if \'akahara\' in path:\n                t = np.array((0))\n            elif \'madara\' in path:\n                t = np.array((1))\n            ts = np.r_[ts, t]\n\n            paths.append(path)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.001)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_HowTo/main_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\ndef Mynet():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(32, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv1_1\')(inputs)\n    x = BatchNormalization()(x)\n    x = Conv2D(32, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv1_2\')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Conv2D(64, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv2_1\')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(64, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv2_2\')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Conv2D(128, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv3_1\')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(128, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv3_2\')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv4_1\')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv4_2\')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Flatten()(x)\n    x = Dense(1024, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0, num_classes))\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((num_classes))\n            if \'akahara\' in path:\n                t[0] = 1\n            elif \'madara\' in path:\n                t[1] = 1\n            ts = np.r_[ts, t[None, ...]]\n\n            paths.append(path)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_HowTo/main_pytorch.py,32,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nGPU = False\ntorch.manual_seed(0)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1_1 = torch.nn.BatchNorm2d(32)\n        self.conv1_2 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn1_2 = torch.nn.BatchNorm2d(32)\n        self.conv2_1 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2_1 = torch.nn.BatchNorm2d(64)\n        self.conv2_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn2_2 = torch.nn.BatchNorm2d(64)\n        self.conv3_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3_1 = torch.nn.BatchNorm2d(128)\n        self.conv3_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn3_2 = torch.nn.BatchNorm2d(128)\n        self.conv4_1 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn4_1 = torch.nn.BatchNorm2d(256)\n        self.conv4_2 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn4_2 = torch.nn.BatchNorm2d(256)\n        self.fc1 = torch.nn.Linear(img_height//16 * img_width//16 * 256, 512)\n        #self.fc1_d = torch.nn.Dropout2d()\n        self.fc2 = torch.nn.Linear(512, 512)\n        self.fc_out = torch.nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1_1(self.conv1_1(x)))\n        x = F.relu(self.bn1_2(self.conv1_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn2_1(self.conv2_1(x)))\n        x = F.relu(self.bn2_2(self.conv2_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn3_1(self.conv3_1(x)))\n        x = F.relu(self.bn3_2(self.conv3_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn4_1(self.conv4_1(x)))\n        x = F.relu(self.bn4_2(self.conv4_2(x)))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, img_height//16 * img_width // 16 * 256)\n        x = F.relu(self.fc1(x))\n        #x = self.fc1_d(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc_out(x)\n        return x\n\n\n# get train data\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0))\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((1))\n            if \'akahara\' in path:\n                t = np.array((0))\n            elif \'madara\' in path:\n                t = np.array((1))\n            ts = np.r_[ts, t]\n\n            paths += [path]\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        pred = model(x)\n        pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n    \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_HowTo/main_tensorflow_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\n\n\ndef Mynet(x, train=False):\n    x = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[3, 3], padding=\'same\', activation=tf.nn.relu, name=\'conv1_1\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[3, 3], padding=\'same\', activation=tf.nn.relu, name=\'conv2\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)   \n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=128, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.layers.dropout(inputs=x, rate=0.25, training=train)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=""fc_cls"")\n    return x\n\n\n# get train data\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0, num_classes))\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((num_classes))\n            if \'akahara\' in path:\n                t[0] = 1\n            elif \'madara\' in path:\n                t[1] = 1\n            ts = np.r_[ts, t[None, ...]]\n\n            paths += [path]\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, train=True)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(100):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_HowTo/main_tensorflow_raw.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\ndef conv2d(x, k=3, in_num=1, out_num=32, strides=1, activ=None, bias=True, name=\'conv\'):\n    w = tf.Variable(tf.random_normal([k, k, in_num, out_num]), name=name+\'_w\')\n    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=\'SAME\')\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.nn.bias_add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=\'SAME\')\n\ndef fc(x, in_num=100, out_num=100, bias=True, activ=None, name=\'fc\'):\n    w = tf.Variable(tf.random_normal([in_num, out_num]), name=name+\'_w\')\n    x = tf.matmul(x, w)\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\ndef Mynet(x, keep_prob):\n    x = conv2d(x, k=3, in_num=3, out_num=32, activ=tf.nn.relu, name=\'conv1_1\')\n    x = conv2d(x, k=3, in_num=32, out_num=32, activ=tf.nn.relu, name=\'conv1_2\')\n    x = maxpool2d(x, k=2)\n    x = conv2d(x, k=3, in_num=32, out_num=64, activ=tf.nn.relu, name=\'conv2_1\')\n    x = conv2d(x, k=3, in_num=64, out_num=64, activ=tf.nn.relu, name=\'conv2_2\')\n    x = maxpool2d(x, k=2)\n    x = conv2d(x, k=3, in_num=64, out_num=128, activ=tf.nn.relu, name=\'conv3_1\')\n    x = conv2d(x, k=3, in_num=128, out_num=128, activ=tf.nn.relu, name=\'conv3_2\')\n    x = maxpool2d(x, k=2)\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = fc(x, in_num=w*h*c, out_num=1024, activ=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.relu(x)\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = fc(x, in_num=1024, out_num=num_classes, name=\'fc_out\')\n    return x\n\n# get train data\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0, num_classes))\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((num_classes))\n            if \'akahara\' in path:\n                t[0] = 1\n            elif \'madara\' in path:\n                t[1] = 1\n            ts = np.r_[ts, t[None, ...]]\n\n            paths += [path]\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(100):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_HowTo/main_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\n\n\ndef Mynet(x, train=False):\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=tf.truncated_normal_initializer(0., 0.01)):\n        x = slim.conv2d(x, 64, [3,3], scope=\'conv1\')\n        x = slim.batch_norm(x, is_training=train, scope=\'bn1\')\n        x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n        x = slim.conv2d(x, 64, [3,3], scope=\'conv2\')\n        x = slim.batch_norm(x, is_training=train, scope=\'bn2\')\n        x = slim.max_pool2d(x, [2,2], scope=\'pool2\')\n        x = slim.flatten(x)\n        x = slim.fully_connected(x, 256, scope=\'fc1\')\n        x = slim.dropout(x, 0.25, scope=\'drop1\')\n        x = slim.fully_connected(x, num_classes, scope=\'fc_cls\')\n    \n    return x\n\n\n# get train data\ndef data_load(dir_path):\n    xs = np.ndarray((0, img_height, img_width, 3))\n    ts = np.ndarray((0, num_classes))\n    paths = []\n    \n    for dir_path in glob(dir_path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            t = np.zeros((num_classes))\n            if \'akahara\' in path:\n                t[0] = 1\n            elif \'madara\' in path:\n                t[1] = 1\n            ts = np.r_[ts, t[None, ...]]\n\n            paths += [path]\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, train=True)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(100):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, train=True)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/HRED_pytorch.py,79,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nmb = 16\n\nopt = ""Adam"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 10000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\n# RNN parameters\nhidden_dim = 256 # d_h in original paper\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.1 # Dropout ratio\nnum_layers = 1\n\n# Attention parameters\nAttention = True\nAttention_dkv = 64\nEncoder_attention_time = 0  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 0  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\nFeedForwardNetwork_dff = 128\nuse_PositionalEncoding = True # Transformer technique 5\nuse_Hard_Attention_Encoder = True # Hard Attention for Self Attention in Encoder\nuse_Hard_Attention_SourceTargetAttention_Decoder = True # Hard Attention for Source Target Attention in Decoder\nuse_Hard_Attention_SelfAttention_Decoder = True # Hard Attention for Self Attention in Decoder\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n# HRED parameters\nHRED_Session = 10\nHRED_hidden_dim = 512 # d_s in original paper\n#HRED_out_dim = HRED_hidden_dim * 2 if HRED_use_Bidirectional else HRED_hidden_dim\n\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, attention_dkv=64, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, attention_dkv=64, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention_SelfAttention=False,\n        use_Hard_Attention_SourceTargetAttention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SelfAttention\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SourceTargetAttention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, \n        hidden_dim, \n        memory_dim, \n        attention_dkv, \n        output_dim, \n        dropout_p=0.1, \n        max_length=MAX_LENGTH, \n        head_N=1, \n        self_Attention_Decoder=False,\n        hard_Attention=False\n        ):\n\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attention_dkv = attention_dkv\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n        self.hard_Attention=hard_Attention\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, attention_dkv)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        #self.attention_dropout = torch.nn.Dropout(dropout_p)\n\n        self.dense_output = torch.nn.Linear(attention_dkv, output_dim)\n        self.dropout_output = torch.nn.Dropout(dropout_p)\n\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.attention_dkv // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.attention_dkv)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n\n        # hard attention\n        if self.hard_Attention:\n            _attention_weights = torch.zeros(attention_weights.size(), dtype=torch.float)\n            argmax = torch.argmax(attention_weights, dim=-1)[:, 0]\n            _attention_weights[[_x for _x in range(argmax.size()[0])], :, argmax] = 1\n            attention_weights = _attention_weights\n        \n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.attention_dkv)\n\n        # one head -> Multi head\n        V = V.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n\n        # attention + Input\n        #attention_x = torch.cat([_input, attention_feature], dim=-1)\n        #print(attention_x.size())\n        # apply attention dense\n        #attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_dropout(attention_output)\n\n        attention_output = self.dense_output(attention_feature)\n        attention_output = self.dropout_output(attention_output)\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, d_ff, d_model, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\nclass HRED(torch.nn.Module):\n    def __init__(self, decoder_dim, hidden_dim, num_layers=1, use_Bidirectional=False):\n        super(HRED, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        # output GRU\n        self.gru = torch.nn.GRU(decoder_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n    def forward(self, x, hidden):\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros([tensor_dim, 1, hidden_dim], device=device)\n\n    \ndef data_load():\n    session_sentences = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    # each file\n    for file_path in glob(""./dataset/sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            # read all lines in file\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            # add new vocabrary\n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            # add finish flag\n            lines += [\'<FINISH>\']\n\n            # parse lines to [[s1, s2, ..., sN], [s2, s3, ..., sN+1], ..., ]\n            session_sentences += [[lines[i + j] for j in range(HRED_Session)] for i in range(0, len(lines) - HRED_Session)]\n\n    # vocabrary sort\n    voca.sort()\n\n    print(""sentences num:"", len(session_sentences))\n    \n    session_sentence_index = []\n\n    # each session sentences\n    for sentences in session_sentences:\n        sentence_index = []\n\n        # each sentence\n        for i in range(HRED_Session - 1):\n            # parse to semantic element\n            sentence_parsed = mecab.parse(sentences[i]).strip().split(\' \')\n            # get index of element in vocabrary\n            sentence_voca_index = [voca.index(x) for x in sentence_parsed]\n            sentence_index.append(sentence_voca_index)\n\n        # last sentence\n        last_sentence = sentences[-1]\n        # if session finish flag\n        if last_sentence == \'<FINISH>\':\n            sentence_parsed = [last_sentence, \'<EOS>\']\n        else:\n            sentence_parsed = mecab.parse(last_sentence).strip().split(\' \') + [\'<EOS>\']\n        \n        # get index of element in vocabrary\n        sentence_voca_index = [voca.index(x) for x in sentence_parsed]\n        sentence_index.append(sentence_voca_index)\n        \n        session_sentence_index.append(sentence_index)\n\n    return voca, session_sentence_index\n\n\n# train\ndef train():\n    # data load\n    voca, session_sentences = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    hred = HRED(\n        decoder_dim=hidden_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        use_Bidirectional=use_Bidirectional\n    ).to(device)\n\n    mbi = 0\n\n    data_num = len(session_sentences)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        # define optimizer\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n            hred_optimizer = torch.optim.SGD(hred.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            hred_optimizer = torch.optim.Adam(hred.parameters(), lr=lr, betas=(0.9, 0.98))\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        # for each iteration\n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            # get minibatch\n            session_sentences_minibatch = [session_sentences[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            # for each minibatch data\n            for mb_index in range(mb):\n                # get session sentences for one minibatch\n                Xs = session_sentences_minibatch[mb_index]\n                #Xs = torch.tensor(session_sentences_minibatch[mb_index]).to(device).view(HRED_Session, -1, 1)\n                #Xs_float = torch.tensor(Xs, dtype=torch.float).to(device)\n\n                #xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                #xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                #ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                # get initiate state for Encoder and HRED\n                encoder_hidden = encoder.initHidden()\n                hred_hidden = hred.initHidden()\n\n                # reset gradient\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n                hred_optimizer.zero_grad()\n\n                # for each session sentence\n                for session_index in range(HRED_Session - 1):\n            \n                    # get sentence sequence for Encoder\n                    X_encoder = torch.tensor(Xs[session_index]).to(device).view(-1, 1)\n                    #X_encoder_float = torch.tensor(X_encoderm dtype=torch.float).to(device)\n\n                    # sample recent decoder output as encoder\'s input\n                    if (session_index > 0) and (np.random.random() < 0.5):\n                        X_encoder = self_memory\n                            \n                    X_encoder_length = X_encoder.size()[0]\n\n                    # get sentence sequence for Decoder\n                    #X_decoder = Xs[session_index + 1]\n                    X_decoder = torch.tensor(Xs[session_index + 1]).to(device).view(-1, 1)\n                    X_decoder_length = X_decoder.size()[0]\n                    total_len += X_decoder_length\n\n                    encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                    # update Encoder\n                    for ei in range(X_encoder_length):\n                        encoder_output, encoder_hidden = encoder(X_encoder[ei], encoder_hidden, X_encoder)\n                        encoder_outputs[ei] = encoder_output[0, 0]\n\n\n                    # initialize HRED input\n                    hred_input = encoder_output\n                    \n                    # update HRED\n                    hred_output, hred_hidden = hred(hred_input, hred_hidden)\n\n                    # input \n                    decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n                    decoder_hidden = hred_hidden\n                    \n                    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                    self_memory = decoder_xs\n                    \n                    # update Decoder\n                    if use_teacher_forcing:\n                        # Teacher forcing: Feed the target (ground-truth word) as the next input\n                        for di in range(X_decoder_length):\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n            \n                            # add loss\n                            loss += loss_fn(torch.log(decoder_ys), X_decoder[di])\n\n                            # count accuracy\n                            if decoder_ys.argmax() == X_decoder[di]:\n                                accuracy += 1.\n                                \n                            # set next decoder\'s input (ground-truth label)\n                            decoder_xs = X_decoder[di].view(1, -1)\n                            self_memory = torch.cat([self_memory, decoder_xs])\n\n                    else:\n                        # Without teacher forcing: use its own predictions as the next input\n                        for di in range(X_decoder_length):\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                            \n                            # Select top 1 word with highest probability\n                            #topv, topi = decoder_ys.topk(1)\n                            # choice argmax\n                            if next_word_mode == ""argmax"":\n                                topv, topi = decoder_ys.data.topk(1)\n\n                            elif next_word_mode == ""prob"":\n                                topi = torch.multinomial(decoder_ys, 1)\n                            \n                            # set next input for decoder training\n                            decoder_xs = topi.squeeze().detach().view(1, -1)\n                            self_memory = torch.cat([self_memory, decoder_xs])\n\n                            # add loss\n                            loss += loss_fn(torch.log(decoder_ys), X_decoder[di])\n\n                            # count accuracy\n                            if decoder_ys.argmax() == X_decoder[di]:\n                                accuracy += 1.\n\n                            if decoder_xs.item() == voca.index(""<EOS>""):\n                                break\n\n                            \n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / total_len\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    hred = HRED(\n        decoder_dim=hidden_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        use_Bidirectional=use_Bidirectional\n    ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        # get initiate state for Encoder and HRED\n        hred_hidden = hred.initHidden()\n\n        while count < 100:\n            input_length = xs.size()[0]\n            decoded_words = []\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            # update encoder\n            encoder_hidden = encoder.initHidden()\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n\n            # initialize HRED input\n            hred_input = encoder_output\n            \n            # update HRED\n            hred_output, hred_hidden = hred(hred_input, hred_hidden)\n\n            # Decoder input \n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n\n            # Decoder state\n            decoder_hidden = hred_hidden\n            \n            self_memory = decoder_x\n\n            # update Decoder\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                # if EOS or FINISH, finish conversation\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                # next input\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/pytorch_transformer_sample.py,14,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n        self.model_type = \'Transformer\'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float(\'-inf\')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return output\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer(\'pe\', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nTEXT = torchtext.data.Field(tokenize=get_tokenizer(""basic_english""),\n                            init_token=\'<sos>\',\n                            eos_token=\'<eos>\',\n                            lower=True)\ntrain_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\nTEXT.build_vocab(train_txt)\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\ndef batchify(data, bsz):\n    data = TEXT.numericalize([data.examples[0].text])\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_txt, batch_size)\nval_data = batchify(val_txt, eval_batch_size)\ntest_data = batchify(test_txt, eval_batch_size)\n\n\nbptt = 35\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n\n\nntokens = len(TEXT.vocab.stoi) # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()\nlr = 5.0 # learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nimport time\ndef train():\n    model.train() # Turn on the train mode\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(TEXT.vocab.stoi)\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output.view(-1, ntokens), targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = 200\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | \'\n                  \'lr {:02.2f} | ms/batch {:5.2f} | \'\n                  \'loss {:5.2f} | ppl {:8.2f}\'.format(\n                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    ntokens = len(TEXT.vocab.stoi)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)\n\n\nbest_val_loss = float(""inf"")\nepochs = 3 # The number of epochs\nbest_model = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate(model, val_data)\n    print(\'-\' * 89)\n    print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n          \'valid ppl {:8.2f}\'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print(\'-\' * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = model\n\n    scheduler.step()\n\n\n\ntest_loss = evaluate(best_model, test_data)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f}\'.format(\n    test_loss, math.exp(test_loss)))\nprint(\'=\' * 89)'"
Scripts_NLP/seq2seq_attention_hardAttention_pytorch.py,69,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nmb = 1\n\nopt = ""Adam"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.01, 10000]] \n\nnext_word_mode = ""argmax"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nhidden_dim = 256\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 4\n\nAttention = True\nAttention_dkv = 64\nEncoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = False # self attention of Encoder\nuse_Decoder_Self_Attention = False # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = False # Transformer technique 4\nFeedForwardNetwork_dff = 512\nuse_PositionalEncoding = True # Transformer technique 5\nuse_Hard_Attention_Encoder = False # Hard Attention for Self Attention in Encoder\nuse_Hard_Attention_SourceTargetAttention_Decoder = False # Hard Attention for Source Target Attention in Decoder\nuse_Hard_Attention_SelfAttention_Decoder = False # Hard Attention for Self Attention in Decoder\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, attention_dkv=64, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, attention_dkv=64, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention_SelfAttention=False,\n        use_Hard_Attention_SourceTargetAttention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SelfAttention\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SourceTargetAttention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, \n        hidden_dim, \n        memory_dim, \n        attention_dkv, \n        output_dim, \n        dropout_p=0.1, \n        max_length=MAX_LENGTH, \n        head_N=1, \n        self_Attention_Decoder=False,\n        hard_Attention=False\n        ):\n\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attention_dkv = attention_dkv\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n        self.hard_Attention=hard_Attention\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, attention_dkv)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        #self.attention_dropout = torch.nn.Dropout(dropout_p)\n\n        self.dense_output = torch.nn.Linear(attention_dkv, output_dim)\n        self.dropout_output = torch.nn.Dropout(dropout_p)\n\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.attention_dkv // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.attention_dkv)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n\n        # hard attention\n        if self.hard_Attention:\n            _attention_weights = torch.zeros(attention_weights.size(), dtype=torch.float)\n            argmax = torch.argmax(attention_weights, dim=-1)[:, 0]\n            _attention_weights[[_x for _x in range(argmax.size()[0])], :, argmax] = 1\n            attention_weights = _attention_weights\n        \n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.attention_dkv)\n\n        # one head -> Multi head\n        V = V.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n\n        # attention + Input\n        #attention_x = torch.cat([_input, attention_feature], dim=-1)\n        #print(attention_x.size())\n        # apply attention dense\n        #attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_dropout(attention_output)\n\n        attention_output = self.dense_output(attention_feature)\n        attention_output = self.dropout_output(attention_output)\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, d_ff, d_model, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./dataset/sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(0.9, 0.98))\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
pytorch/_main_base_generative.py,0,"b'from glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nimport cv2\n\n# get train data\ndef data_load(cfg):\n    path = cfg.TRAIN.DATA_PATH\n    hf = cfg.TRAIN.DATA_HORIZONTAL_FLIP\n    vf = cfg.TRAIN.DATA_VERTICAL_FLIP\n    rot = cfg.TRAIN.DATA_ROTATION\n\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n    \n    print(\'Dataset >>\', path)\n    print(\' - Found data num >>\', data_num)\n    print(\' - Horizontal >>\', hf)\n    print(\' - Vertical >>\', vf)\n    print(\' - Rotation >>\', rot)        \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(cfg.CLASS_LABEL):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n\n    print(\'all data num >>\', len(paths))\n    print(\'dataset was completely loaded\')\n    print(\'--\')\n\n    \n    return {\'paths\' : np.array(paths), \'paths_gt\' : np.array(paths_gt)}\n\n\ndef get_image(infos, cfg, mode):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (cfg.OUTPUT_WIDTH, cfg.OUTPUT_HEIGHT)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if mode == \'GRAY\':\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n        elif mode == \'EDGE\':\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.Canny(x, 100, 150)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if mode == \'CLASS_LABEL\':\n            _x = x\n            x = np.zeros((cfg.OUTPUT_HEIGHT, cfg.OUTPUT_WIDTH, cfg.CLASS_NUM), dtype=np.int)\n\n            for i, (_, vs) in enumerate(cfg.CLASS_LABEL.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[..., i][ind] = 1\n\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if mode in [\'RGB\']:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    return xs'"
Dataset/test/labelme2voc.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport argparse\nimport glob\nimport json\nimport os\nimport os.path as osp\n\nimport numpy as np\nimport PIL.Image\n\nimport labelme\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('labels_file')\n    parser.add_argument('in_dir', help='input dir with annotated files')\n    parser.add_argument('out_dir', help='output dataset directory')\n    args = parser.parse_args()\n\n    if osp.exists(args.out_dir):\n        print('Output directory already exists:', args.out_dir)\n        quit(1)\n    os.makedirs(args.out_dir)\n    os.makedirs(osp.join(args.out_dir, 'JPEGImages'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClass'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClassPNG'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClassVisualization'))\n    print('Creating dataset:', args.out_dir)\n\n    class_names = []\n    class_name_to_id = {}\n    for i, line in enumerate(open(args.labels_file).readlines()):\n        class_id = i - 1  # starts with -1\n        class_name = line.strip()\n        class_name_to_id[class_name] = class_id\n        if class_id == -1:\n            assert class_name == '__ignore__'\n            continue\n        elif class_id == 0:\n            assert class_name == '_background_'\n        class_names.append(class_name)\n    class_names = tuple(class_names)\n    print('class_names:', class_names)\n    out_class_names_file = osp.join(args.out_dir, 'class_names.txt')\n    with open(out_class_names_file, 'w') as f:\n        f.writelines('\\n'.join(class_names))\n    print('Saved class_names:', out_class_names_file)\n\n    colormap = labelme.utils.label_colormap(255)\n\n    for label_file in glob.glob(osp.join(args.in_dir, '*.json')):\n        print('Generating dataset from:', label_file)\n        with open(label_file) as f:\n            base = osp.splitext(osp.basename(label_file))[0]\n            out_img_file = osp.join(\n                args.out_dir, 'JPEGImages', base + '.jpg')\n            out_lbl_file = osp.join(\n                args.out_dir, 'SegmentationClass', base + '.npy')\n            out_png_file = osp.join(\n                args.out_dir, 'SegmentationClassPNG', base + '.png')\n            out_viz_file = osp.join(\n                args.out_dir, 'SegmentationClassVisualization', base + '.jpg')\n\n            data = json.load(f)\n\n            img_file = osp.join(osp.dirname(label_file), data['imagePath'])\n            img = np.asarray(PIL.Image.open(img_file))\n            PIL.Image.fromarray(img).save(out_img_file)\n\n            lbl = labelme.utils.shapes_to_label(\n                img_shape=img.shape,\n                shapes=data['shapes'],\n                label_name_to_value=class_name_to_id,\n            )\n            labelme.utils.lblsave(out_png_file, lbl)\n\n            np.save(out_lbl_file, lbl)\n\n            viz = labelme.utils.draw_label(\n                lbl, img, class_names, colormap=colormap)\n            PIL.Image.fromarray(viz).save(out_viz_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
Dataset/train/labelme2voc.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport argparse\nimport glob\nimport json\nimport os\nimport os.path as osp\n\nimport numpy as np\nimport PIL.Image\n\nimport labelme\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('labels_file')\n    parser.add_argument('in_dir', help='input dir with annotated files')\n    parser.add_argument('out_dir', help='output dataset directory')\n    args = parser.parse_args()\n\n    if osp.exists(args.out_dir):\n        print('Output directory already exists:', args.out_dir)\n        quit(1)\n    os.makedirs(args.out_dir)\n    os.makedirs(osp.join(args.out_dir, 'JPEGImages'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClass'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClassPNG'))\n    os.makedirs(osp.join(args.out_dir, 'SegmentationClassVisualization'))\n    print('Creating dataset:', args.out_dir)\n\n    class_names = []\n    class_name_to_id = {}\n    for i, line in enumerate(open(args.labels_file).readlines()):\n        class_id = i - 1  # starts with -1\n        class_name = line.strip()\n        class_name_to_id[class_name] = class_id\n        if class_id == -1:\n            assert class_name == '__ignore__'\n            continue\n        elif class_id == 0:\n            assert class_name == '_background_'\n        class_names.append(class_name)\n    class_names = tuple(class_names)\n    print('class_names:', class_names)\n    out_class_names_file = osp.join(args.out_dir, 'class_names.txt')\n    with open(out_class_names_file, 'w') as f:\n        f.writelines('\\n'.join(class_names))\n    print('Saved class_names:', out_class_names_file)\n\n    colormap = labelme.utils.label_colormap(255)\n\n    for label_file in glob.glob(osp.join(args.in_dir, '*.json')):\n        print('Generating dataset from:', label_file)\n        with open(label_file) as f:\n            base = osp.splitext(osp.basename(label_file))[0]\n            out_img_file = osp.join(\n                args.out_dir, 'JPEGImages', base + '.jpg')\n            out_lbl_file = osp.join(\n                args.out_dir, 'SegmentationClass', base + '.npy')\n            out_png_file = osp.join(\n                args.out_dir, 'SegmentationClassPNG', base + '.png')\n            out_viz_file = osp.join(\n                args.out_dir, 'SegmentationClassVisualization', base + '.jpg')\n\n            data = json.load(f)\n\n            img_file = osp.join(osp.dirname(label_file), data['imagePath'])\n            img = np.asarray(PIL.Image.open(img_file))\n            PIL.Image.fromarray(img).save(out_img_file)\n\n            lbl = labelme.utils.shapes_to_label(\n                img_shape=img.shape,\n                shapes=data['shapes'],\n                label_name_to_value=class_name_to_id,\n            )\n            labelme.utils.lblsave(out_png_file, lbl)\n\n            np.save(out_lbl_file, lbl)\n\n            viz = labelme.utils.draw_label(\n                lbl, img, class_names, colormap=colormap)\n            PIL.Image.fromarray(viz).save(out_viz_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
Scripts_Dataset/scripts/answer_data_load.py,0,"b'import cv2\nimport numpy as np\nfrom glob import glob\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\nxs, ts, paths = data_load(""../Dataset/train/images/"")\n'"
Scripts_Dataset/scripts/answer_epoch.py,0,"b""import cv2\nimport numpy as np\nfrom glob import glob\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = ['akahara', 'madara']\n\n# get train data\ndef data_load(path):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + '/*'):\n        for path in glob(dir_path + '/*'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n\n            paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.float32)\n            \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\nxs, ts, paths = data_load('../Dataset/train/images/')\n\nmb = 3\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.seed(0)\nnp.random.shuffle(train_ind)\n\nepoch_max = 3\nepoch = 0\n\nwhile epoch < epoch_max:\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        epoch += 1\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    print(mb_ind)\n"""
Scripts_Dataset/scripts/answer_hf.py,0,"b'import cv2\nimport numpy as np\nfrom glob import glob\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True)\n\nmb = 3\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.seed(0)\nnp.random.shuffle(train_ind)\n\nfor i in range(10):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    print(mb_ind)\n'"
Scripts_Dataset/scripts/answer_minibatch.py,0,"b'import cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n            \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"")\n\nmb = 3\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.seed(0)\nnp.random.shuffle(train_ind)\n\nfor i in range(10):\n    if mbi + mb > len(xs):\n        mb_ind = copy.copy(train_ind)[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    print(mb_ind)\n'"
Scripts_Dataset/scripts/answer_rotation.py,0,"b'import cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                plt.subplot(h_num, w_num, count)\n                plt.axis(\'off\')\n                plt.imshow(x)\n                plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    count += 1\n                    plt.subplot(h_num, w_num, count)\n                    plt.imshow(_x)\n                    plt.axis(\'off\')\n                    plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True, rot=1)\n\nmb = 3\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.seed(0)\nnp.random.shuffle(train_ind)\n\nfor i in range(10):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    print(mb_ind)\n'"
Scripts_Dataset/scripts/answer_vf.py,0,"b'import cv2\nimport numpy as np\nfrom glob import glob\n\nnp.random.seed(0)\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True)\n\nmb = 3\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.seed(0)\nnp.random.shuffle(train_ind)\n\nfor i in range(10):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    print(mb_ind)\n'"
Scripts_Dataset/scripts/load_cifar10.py,0,"b'import numpy as np\n\nimport os\nimport pickle\n\nlabels = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"",\n              ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]\n\ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(\'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'))\n        os.system(""tar xvf {}"".format(\'cifar-10-python.tar.gz\'))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    print(train_x.shape)\n    print(train_y.shape)\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    print(test_x.shape)\n    print(test_y.shape)\n\n    return train_x, train_y, test_x, test_y\n\nload_cifar10()\n'"
Scripts_Dataset/scripts/load_fashion_mnist.py,0,"b'import os\nimport gzip\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_mnist():\n    dir_path = ""fashion_mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if True:#not os.path.exists(dir_path):\n\n        os.makedirs(dir_path, exist_ok=True)\n\n        data_url = ""http://fashion-mnist.s3-website.eu-central-1.amazonaws.com""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n\n        """"""\n        with open(dir_path + \'/\' + f_name, \'rb\') as f:\n            #print(struct.unpack(""b"", f.read(1)))\n            a = f.readlines()\n\n        #print(struct.unpack(""b"", a[0]))\n        print(len(a))\n        for _a in a[:1]:\n            print(int.from_bytes(_a, \'little\'))\n            print(_a)\n        """"""\n\n    return train_x, train_y ,test_x, test_y\n\n\nload_mnist()\n'"
Scripts_Dataset/scripts/load_mnist.py,0,"b'import os\nimport gzip\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path, exist_ok=True)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n\n        """"""\n        with open(dir_path + \'/\' + f_name, \'rb\') as f:\n            #print(struct.unpack(""b"", f.read(1)))\n            a = f.readlines()\n\n        #print(struct.unpack(""b"", a[0]))\n        print(len(a))\n        for _a in a[:1]:\n            print(int.from_bytes(_a, \'little\'))\n            print(_a)\n        """"""\n        \n\n    return train_x, train_y ,test_x, test_y\n\n\nload_mnist()\n'"
Scripts_Generative/scripts_chainer/ae_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 1\n\nGPU = -1\n\ndef crop_layer(layer, size):\n    _, _, h, w = layer.shape\n    _, _, _h, _w = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, :, ph:ph+_h, pw:pw+_w]\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        base = 64\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.dec1 = L.Linear(None, base)\n            self.enc1 = L.Linear(None, out_height * out_width * channel)\n        \n    def forward(self, x):\n        x = self.dec1(x)\n        x = self.enc1(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1.\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side])\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side, _c])\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n\n    xs = np.array(xs)\n\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.001)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy().reshape([mb, -1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.mean_squared_error(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        #accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n        pred = (pred + 1) / 2\n        pred = pred.reshape([channel, out_height, out_width])\n        pred = pred.transpose([1,2,0])\n        pred -= pred.min()\n        pred /= pred.max()\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        x = (x + 1) / 2\n        \n        if channel == 1:\n            pred = pred[..., 0]\n            _x = x[0, 0]\n            cmap = \'gray\'\n        else:\n            _x = x[0].transpose(1,2,0)\n            cmap = None\n        \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/ae_cifar10_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\nGPU = -1\n\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        base = 128\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.dec1 = L.Linear(None, base)\n            self.enc1 = L.Linear(None, out_height * out_width * channel)\n        \n    def forward(self, x):\n        x = self.dec1(x)\n        x = self.enc1(x)\n        return x\n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.001)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy().reshape([mb, -1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.mean_squared_error(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        #accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    for i in range(10):\n        x = xs[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x).data\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0]\n        #pred = (pred + 1) / 2\n        pred = pred.reshape([channel, out_height, out_width])\n        pred = pred.transpose([1,2,0])\n        pred -= pred.min()\n        pred /= pred.max()\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        #x = (x + 1) / 2\n        \n        if channel == 1:\n            pred = pred[..., 0]\n            _x = x[0, 0]\n            cmap = \'gray\'\n        else:\n            _x = x[0].transpose(1,2,0)\n            cmap = None\n        \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/cgan_cifar10_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 10\nimg_height, img_width = 32, 32\nchannel = 3\n\nGPU = -1\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Deconvolution2D(None, base * 8, ksize=int(img_height/16), stride=1, nobias=True)\n            self.bn1 = L.BatchNormalization(base * 8)\n            self.l2 = L.Deconvolution2D(None, base * 4, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn2 = L.BatchNormalization(base * 4)\n            self.l3 = L.Deconvolution2D(None, base * 2, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn3 = L.BatchNormalization(base * 2)\n            self.l4 = L.Deconvolution2D(None, base, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn4 = L.BatchNormalization(base)\n            self.l5 = L.Deconvolution2D(None, channel, ksize=4,  stride=2, pad=1)\n        \n    def forward(self, x, y, test=False):\n        con_x = np.zeros((len(y), num_classes, 1, 1), dtype=np.float32)\n        con_x[np.arange(len(y)), y] = 1\n        if GPU >= 0:\n            con_x = chainer.cuda.to_gpu(con_x)\n\n        x = F.concat([x, con_x], axis=1)\n            \n        x = self.l1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.l4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = self.l5(x)\n        x = F.tanh(x)\n\n        if test:\n            return x\n        else:\n            con_x = np.zeros((len(y), num_classes, img_height, img_width), dtype=np.float32)\n            con_x[np.arange(len(y)), y] = 1\n            if GPU >= 0:\n                con_x = chainer.cuda.to_gpu(con_x)\n            out_x = F.concat([x, con_x], axis=1)\n            return out_x\n\n    \nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Convolution2D(None, base, ksize=5, pad=2, stride=2)\n            self.l2 = L.Convolution2D(None, base * 2, ksize=5, pad=2, stride=2)\n            self.l3 = L.Convolution2D(None, base * 4, ksize=5, pad=2, stride=2)\n            self.l4 = L.Convolution2D(None, base * 8, ksize=5, pad=2, stride=2)\n            self.l5 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l5(x)\n        #x = F.sigmoid(x)\n        return x  \n    \n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_g.setup(gen)\n    \n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(30000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind]\n        con_x = train_y[mb_ind]\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n\n        _con_x = np.zeros((mb, num_classes, img_height, img_width), dtype=np.float32)\n        _con_x[np.arange(mb), con_x] = 1\n        x = np.hstack([x, _con_x])\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise, con_x)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise, con_x)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cgan_cifar10_chainer.npz\', gen)\n\n    \n# test\ndef test():\n    g = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        g.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cgan_cifar10_chainer.npz\', g)\n\n    np.random.seed(100)\n    \n    labels = [""air\xc2\xa5nplane"", ""auto\xc2\xa5bmobile"", ""bird"", ""cat"", ""deer"",\n              ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]\n    \n\n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        con_x = np.arange((num_classes), dtype=np.int)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = g(input_noise, con_x, test=True).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            gen = g_output[i]\n\n            if channel == 1:\n                gen = gen[..., 0]\n                cmap = ""gray""\n            elif channel == 3:\n                cmap = None\n                \n            plt.subplot(1,mb,i+1)\n            plt.title(labels[i])\n            plt.imshow(gen, cmap=cmap)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/cgan_mnist_chainer.py,0,"b'from google.colab import drive\ndrive.mount(""/content/drive"", force_remount=True)\n\nimport chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\nGPU = 0\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Deconvolution2D(None, base, ksize=int(img_height/4), stride=1, nobias=True)\n            self.bn1 = L.BatchNormalization(base)\n            #self.l2 = L.Deconvolution2D(None, base * 4, ksize=4, stride=2, pad=1,  nobias=True)\n            #self.bn2 = L.BatchNormalization(base * 4)\n            #self.l3 = L.Deconvolution2D(None, base * 2, ksize=4, stride=2, pad=1,  nobias=True)\n            #self.bn3 = L.BatchNormalization(base * 2)\n            self.l4 = L.Deconvolution2D(None, base, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn4 = L.BatchNormalization(base)\n            self.l5 = L.Deconvolution2D(None, channel, ksize=4,  stride=2, pad=1)\n        \n    def forward(self, x, y, test=False):\n        con_x = np.zeros((len(y), num_classes, 1, 1), dtype=np.float32)\n        con_x[np.arange(len(y)), y] = 1\n        if GPU >= 0:\n            con_x = chainer.cuda.to_gpu(con_x)\n\n        x = F.concat([x, con_x], axis=1)\n            \n        x = self.l1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        #x = self.l2(x)\n        #x = self.bn2(x)\n        #x = F.relu(x)\n        #x = self.l3(x)\n        #x = self.bn3(x)\n        #x = F.relu(x)\n        x = self.l4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = self.l5(x)\n        x = F.tanh(x)\n\n        if test:\n            return x\n        else:\n            con_x = np.zeros((len(y), num_classes, img_height, img_width), dtype=np.float32)\n            con_x[np.arange(len(y)), y] = 1\n            if GPU >= 0:\n                con_x = chainer.cuda.to_gpu(con_x)\n            out_x = F.concat([x, con_x], axis=1)\n            return out_x\n\n    \nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Convolution2D(None, base, ksize=5, pad=2, stride=2)\n            self.l2 = L.Convolution2D(None, base * 2, ksize=5, pad=2, stride=2)\n            #self.l3 = L.Convolution2D(None, base * 4, ksize=5, pad=2, stride=2)\n            #self.l4 = L.Convolution2D(None, base * 8, ksize=5, pad=2, stride=2)\n            self.l5 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        #x = self.l3(x)\n        #x = F.leaky_relu(x, 0.2)\n        #x = self.l4(x)\n        #x = F.leaky_relu(x, 0.2)\n        x = self.l5(x)\n        #x = F.sigmoid(x)\n        return x  \n    \n    \n\nimport pickle\nimport os\nimport gzip\n    \ndef load_mnist():\n    dir_path = \'drive/My Drive/Colab Notebooks/\'  + ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_g.setup(gen)\n    \n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind]\n        con_x = train_y[mb_ind]\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n\n        _con_x = np.zeros((mb, num_classes, img_height, img_width), dtype=np.float32)\n        _con_x[np.arange(mb), con_x] = 1\n        x = np.hstack([x, _con_x])\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise, con_x)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise, con_x)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', gen)\n\n# test\ndef test():\n    g = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        g.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', g)\n\n    np.random.seed(100)\n    \n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        con_x = np.arange((num_classes), dtype=np.int)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = g(input_noise, con_x, test=True).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            gen = g_output[i]\n            \n            if channel == 1:\n                gen = gen[..., 0]\n                cmap = ""gray""\n            elif channel == 3:\n                cmap = None\n                \n            plt.subplot(1,mb,i+1)\n            plt.title(str(i))\n            plt.imshow(gen, cmap=cmap)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/convae_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nout_height, out_width = 64, 64\nchannel = 3\n\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.dec1 = L.Convolution2D(None, 32, ksize=3, pad=1, stride=1)\n            self.dec2 = L.Convolution2D(None, 16, ksize=3, pad=1, stride=1)\n            self.enc2 = L.Deconvolution2D(None, 32, ksize=2, stride=2)\n            self.enc1 = L.Deconvolution2D(None, channel, ksize=2, stride=2)\n        \n    def forward(self, x):\n        x = self.dec1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = self.dec2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = self.enc2(x)\n        x = self.enc1(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1.\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side])\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side, _c])\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n\n    xs = np.array(xs)\n\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.001)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(300):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy()\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.mean_squared_error(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        #accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n        pred = (pred + 1) / 2\n        pred = pred.transpose([1,2,0])\n        pred -= pred.min()\n        pred /= pred.max()\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        x = (x + 1) / 2\n        \n        if channel == 1:\n            pred = pred[..., 0]\n            _x = x[0, 0]\n            cmap = \'gray\'\n        else:\n            _x = x[0].transpose(1,2,0)\n            cmap = None\n        \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/convae_cifar10_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.dec1 = L.Convolution2D(None, 32, ksize=3, pad=1, stride=1)\n            self.dec2 = L.Convolution2D(None, 16, ksize=3, pad=1, stride=1)\n            self.enc2 = L.Deconvolution2D(None, 32, ksize=2, stride=2)\n            self.enc1 = L.Deconvolution2D(None, channel, ksize=2, stride=2)\n        \n    def forward(self, x):\n        x = self.dec1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = self.dec2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = self.enc2(x)\n        x = self.enc1(x)\n        return x\n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.001)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy()\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.mean_squared_error(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        #accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    for i in range(10):\n        x = xs[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x).data\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0]\n        #pred = (pred + 1) / 2\n        pred = pred.transpose([1,2,0])\n        pred -= pred.min()\n        pred /= pred.max()\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        #x = (x + 1) / 2\n        \n        if channel == 1:\n            pred = pred[..., 0]\n            _x = x[0, 0]\n            cmap = \'gray\'\n        else:\n            _x = x[0].transpose(1,2,0)\n            cmap = None\n        \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/dcgan_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nchannel = 3\n\nGPU = -1\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        base = 64\n        with self.init_scope():\n            #self.linear = L.Linear(None, int(img_height / 16 * img_width / 16 * base))\n            #self.bn = L.BatchNormalization(base)\n            self.l1 = L.Deconvolution2D(None, base * 8, ksize=4, stride=4, nobias=True)\n            self.bn1 = L.BatchNormalization(base * 8)\n            self.l2 = L.Deconvolution2D(None, base * 4, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn2 = L.BatchNormalization(base * 4)\n            self.l3 = L.Deconvolution2D(None, base * 2, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn3 = L.BatchNormalization(base * 2)\n            self.l4 = L.Deconvolution2D(None, base, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn4 = L.BatchNormalization(base)\n            self.l5 = L.Deconvolution2D(None, channel, ksize=4,  stride=2, pad=1)\n        \n    def forward(self, x):\n        #x = F.relu(self.linear(x))\n        #mb, _ = x.data.shape\n        #x = F.reshape(x, [mb, -1, int(img_height / 16), int(img_width / 16)])\n        #x = self.bn(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.l4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = self.l5(x)\n        x = F.tanh(x)\n        return x\n\n    \nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        with self.init_scope():\n            base  = 64\n            self.l1 = L.Convolution2D(None, base, ksize=5, pad=2, stride=2)\n            self.l2 = L.Convolution2D(None, base * 2, ksize=5, pad=2, stride=2)\n            self.l3 = L.Convolution2D(None, base * 4, ksize=5, pad=2, stride=2)\n            self.l4 = L.Convolution2D(None, base * 8, ksize=5, pad=2, stride=2)\n            self.l5 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l5(x)\n        #x = F.sigmoid(x)\n        return x\n    \n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1.\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side])\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side, _c])\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n\n    xs = np.array(xs)\n\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_g.setup(gen)\n    \n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for ite in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind]\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if ite % 500 == 0:\n            print(""iter >>"", ite + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', gen)\n\n# test\ndef test():\n    gen = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        gen.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', gen)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = gen(input_noise).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/dcgan_cifar10_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\nGPU = 0\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Deconvolution2D(None, base * 8, ksize=int(img_height/16), stride=1, nobias=True)\n            self.bn1 = L.BatchNormalization(base * 8)\n            self.l2 = L.Deconvolution2D(None, base * 4, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn2 = L.BatchNormalization(base * 4)\n            self.l3 = L.Deconvolution2D(None, base * 2, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn3 = L.BatchNormalization(base * 2)\n            self.l4 = L.Deconvolution2D(None, base, ksize=4, stride=2, pad=1,  nobias=True)\n            self.bn4 = L.BatchNormalization(base)\n            self.l5 = L.Deconvolution2D(None, channel, ksize=4,  stride=2, pad=1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.l4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = self.l5(x)\n        x = F.tanh(x)\n        return x\n\n    \nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        base = 64\n        with self.init_scope():\n            self.l1 = L.Convolution2D(None, base, ksize=5, pad=2, stride=2)\n            self.l2 = L.Convolution2D(None, base * 2, ksize=5, pad=2, stride=2)\n            self.l3 = L.Convolution2D(None, base * 4, ksize=5, pad=2, stride=2)\n            self.l4 = L.Convolution2D(None, base * 8, ksize=5, pad=2, stride=2)\n            self.l5 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l5(x)\n        #x = F.sigmoid(x)\n        return x  \n    \n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002, beta1=0.5)\n    opt_g.setup(gen)\n    \n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(20000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind]\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', gen)\n\n# test\ndef test():\n    gen = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        gen.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', gen)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1)).astype(np.float32)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = gen(input_noise).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/gan_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nchannel = 3\n\nGPU = -1\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        with self.init_scope():\n            self.l1 = L.Linear(None, 128)\n            self.l2 = L.Linear(None, 256)\n            self.l3 = L.Linear(None, 512)\n            self.l4 = L.Linear(None, img_height * img_width * channel)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = F.tanh(x)\n        return x\n\nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        with self.init_scope():\n            self.l1 = L.Linear(None, 512)\n            self.l2 = L.Linear(None, 256)\n            self.l3 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        #x = F.sigmoid(x)\n        return x  \n    \n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1.\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side])\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros([max_side, max_side, _c])\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n\n    xs = np.array(xs)\n\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002)\n    opt_g.setup(gen)\n    \n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    print(\'A\')\n    \n    for ite in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind].reshape([mb, -1])\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if ite % 500 == 0:\n            print(""iter >>"", ite + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', gen)\n\n# test\ndef test():\n    gen = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        gen.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', gen)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100)).astype(np.float32)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = gen(input_noise).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.reshape([mb, channel, img_height, img_width])\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_chainer/gan_cifar10_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\nGPU = 0\n    \nclass Generator(chainer.Chain):\n    def __init__(self):\n        super(Generator, self).__init__()\n        with self.init_scope():\n            self.l1 = L.Linear(None, 128)\n            self.l2 = L.Linear(None, 256)\n            self.l3 = L.Linear(None, 512)\n            self.l4 = L.Linear(None, img_height * img_width * channel)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = F.tanh(x)\n        return x\n\nclass Discriminator(chainer.Chain):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        with self.init_scope():\n            self.l1 = L.Linear(None, 512)\n            self.l2 = L.Linear(None, 256)\n            self.l3 = L.Linear(None, 1)\n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        #x = F.sigmoid(x)\n        return x  \n    \n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    gen = Generator()\n    dis = Discriminator()\n    gan = chainer.Sequential(gen, dis)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        gen.to_gpu()\n        dis.to_gpu()\n        gan.to_gpu()\n\n    opt_d = chainer.optimizers.Adam(0.0002)\n    opt_d.setup(dis)\n    opt_g = chainer.optimizers.Adam(0.0002)\n    opt_g.setup(gen)\n    \n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(20000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n\n        gen.cleargrads()\n        dis.cleargrads()\n        gan.cleargrads()\n        \n        x = xs[mb_ind].reshape([mb, -1])\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100)).astype(np.float32)\n        dt = np.array([1] * mb + [0] * mb, dtype=np.int32).reshape([mb*2, 1])\n        gt = np.array([1] * mb, dtype=np.int32).reshape([mb, 1])\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            input_noise = chainer.cuda.to_gpu(input_noise)\n            dt = chainer.cuda.to_gpu(dt)\n            gt = chainer.cuda.to_gpu(gt)\n\n        g_output = gen(input_noise)\n\n        #if GPU >= 0:\n        #    g_output = chainer.cuda.to_cpu(g_output)\n\n        X = F.concat((x, g_output), axis=0)\n        y = dis(X)\n        \n        loss_d = F.sigmoid_cross_entropy(y, dt)\n        loss_d.backward()\n        opt_d.update()\n\n        y = gan(input_noise)\n        \n        loss_g = F.sigmoid_cross_entropy(y, gt)\n        loss_g.backward()\n        opt_g.update()\n        \n        loss_d = loss_d.data\n        loss_g = loss_g.data\n        \n        if GPU >= 0:\n            loss_d = chainer.cuda.to_cpu(loss_d)\n            loss_g = chainer.cuda.to_cpu(loss_g)\n\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i + 1, \',G:loss >>\', loss_g.item(), \', D:loss >>\', loss_d.item())\n\n    chainer.serializers.save_npz(\'cnn.npz\', gen)\n\n# test\ndef test():\n    gen = Generator()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        gen.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', gen)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100)).astype(np.float32)\n\n        if GPU >= 0:\n            input_noise = chainer.cuda.to_gpu(input_noise)\n\n        g_output = gen(input_noise).data\n\n        if GPU >= 0:\n            g_output = chainer.cuda.to_cpu(g_output)\n            \n        g_output = (g_output + 1) / 2\n        g_output = g_output.reshape([mb, channel, img_height, img_width])\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/ae_cifar10_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\n\ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, channel), name=\'in\')\n    h = Reshape([-1])(inputs)\n    h = Dense(128)(h)\n    out = Dense(out_height * out_width * channel, name=\'out\')(h)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'mse\'},\n        optimizer=keras.optimizers.Adam(lr=0.001),#""adam"", #keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n    \n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy().reshape([mb, -1])\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n\n    for i in range(10):\n        x = xs[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred -= pred.min()\n        pred /= pred.max()\n        pred = np.reshape(pred, (out_height, out_width, channel))\n\n        if channel == 1:\n            pred = pred[..., 0]\n            _x = (x[0, ..., 0] + 1) / 2\n            cmap = \'gray\'\n        else:\n            _x = (x[0] + 1) / 2\n            cmap = None\n\n        print(""in {}"".format(path))\n            \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/ae_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nout_height, out_width = 64, 64\nchannel = 3\n\n\ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, channel), name=\'in\')\n    h = Reshape([-1])(inputs)\n    h = Dense(64)(h)\n    out = Dense(out_height * out_width * channel, name=\'out\')(h)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'mse\'},\n        optimizer=keras.optimizers.Adam(lr=0.001),#""adam"", #keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = x.copy().reshape([mb, -1])\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred -= pred.min()\n        pred /= pred.max()\n        pred = np.reshape(pred, (out_height, out_width, channel))\n\n        if channel == 1:\n            pred = pred[..., 0]\n            _x = (x[0, ..., 0] + 1) / 2\n            cmap = \'gray\'\n        else:\n            _x = (x[0] + 1) / 2\n            cmap = None\n\n        print(""in {}"".format(path))\n            \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/cgan_cifar10_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU, Conv2DTranspose, concatenate, Lambda\n\nnum_classes = 10\nimg_height, img_width = 32, 32\nchannel = 3\n\nfrom keras.regularizers import l1_l2\nfrom keras.initializers import RandomNormal as RN, Constant\n\n\n\ndef G_model():\n    inputs = Input([100, ], name=""x"")\n    con_x = Input([num_classes, ], name=""con_x"")\n    con_x2 = Input([img_height, img_width, num_classes], name=""con_x2"")\n    \n    #con_x = K.zeros([None, num_classes, 1, 1])\n    #print(con_x.shape)\n    #con_x = np.zeros([len(_con_x), num_classes, 1, 1], dtype=np.float32)\n    #con_x[np.arange(len(_con_x)), _con_x] = 1\n\n    x = concatenate([inputs, con_x], axis=-1)\n    \n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    d_dim = 256\n    base = 128\n    x = Dense(in_h * in_w * d_dim, name=\'g_dense1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Reshape((in_h, in_w, d_dim), input_shape=(d_dim * in_h * in_w,))(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_dense1_bn\')(x)\n    # 1/8\n    x = Conv2DTranspose(base*4, (5, 5), name=\'g_conv1\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv1_bn\')(x)\n    # 1/4\n    x = Conv2DTranspose(base*2, (5, 5), name=\'g_conv2\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv2_bn\')(x)\n    # 1/2\n    x = Conv2DTranspose(base, (5, 5), name=\'g_conv3\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv3_bn\')(x)\n    # 1/1\n    x = Conv2DTranspose(channel, (5, 5), name=\'g_out\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02),  bias_initializer=Constant())(x)\n    x = Activation(\'tanh\')(x)\n\n    #con_x = np.zerns([len(_con_x), num_classes, img_height, img_width], dtype=np.float32)\n    #con_x[np.arange(len(_con_x)), _con_x] = 1\n    x2 = concatenate([x, con_x2], axis=-1)\n\n    model = Model(inputs=[inputs, con_x, con_x2], outputs=[x], name=\'G\')\n    gan_g_model = Model(inputs=[inputs, con_x, con_x2], outputs=[x2], name=\'GAN_G\')\n    \n    return model, gan_g_model\n\n\ndef D_model():\n    base = 32\n    inputs = Input([img_height, img_width, channel + num_classes])\n    x = Conv2D(base, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*2, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv2\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*4, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv3\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*8, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv4\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), bias_initializer=Constant())(x)\n    model = Model(inputs=inputs, outputs=x, name=\'D\')\n    return model\n\n\ndef Combined_model(g, d):\n    inputs = Input([100, ], name=""x"")\n    con_x = Input([num_classes, ], name=""con_x"")\n    con_x2 = Input([img_height, img_width, num_classes], name=""con_x2"")\n    x = g(inputs=[inputs, con_x, con_x2])\n    x = d(x)\n    model = Model(inputs=[inputs, con_x, con_x2], outputs=[x])\n\n    #model = Sequential()\n    #model.add(g)\n    #model.add(d)\n    return model\n\n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n/Users/yoshito/work_space/mypage/pages/deliverables.html\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    _, g = G_model()\n    d = D_model()\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    \n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(30000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        con_x = train_y[mb_ind]\n\n        # Disciminator training\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        _con_x = np.zeros([mb, num_classes], dtype=np.float32)\n        _con_x[np.arange(mb), con_x] = 1\n\n        _con_x2 = np.zeros([mb, img_height, img_width, num_classes], dtype=np.float32)\n        _con_x2[np.arange(mb), ..., con_x] = 1\n        \n        g_output = g.predict(\n            x={""x"":input_noise, ""con_x"": _con_x, ""con_x2"": _con_x2}, verbose=0)\n\n        x = np.concatenate([x, _con_x2], axis=-1)\n        X = np.concatenate((x, g_output))\n        \n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        \n        # Generator training\n        #input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(\n            x={""x"":input_noise, ""con_x"":_con_x, ""con_x2"": _con_x2}, y=np.array([1] * mb))\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'cgan_cifar10_keras.h5\')\n\n\n# test\ndef test():\n    # load trained model\n    g, _ = G_model()\n    g.load_weights(\'cgan_cifar10_keras.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    labels = [""air\xc2\xa5nplane"", ""auto\xc2\xa5bmobile"", ""bird"", ""cat"", ""deer"",\n              ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]\n    \n    con_x = np.zeros([10, num_classes])\n    con_x[np.arange(10), np.arange(num_classes)] = 1\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(10, 100))\n        g_output = g.predict(x={""x"":input_noise, ""con_x"": con_x}, verbose=0)\n        g_output = (g_output + 1 ) / 2\n\n        for i in range(10):\n            gen = g_output[i]\n\n            if channel == 1:\n                gen = gen[..., 0]\n                cmap = ""gray""\n            elif channel == 3:\n                cmap = None\n\n            plt.subplot(1,10,i+1)\n            plt.title(labels[i])\n            plt.imshow(gen, cmap=cmap)\n            plt.axis(\'off\')\n\n        plt.show()\n\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/cgan_mnist_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU, Conv2DTranspose, concatenate, Lambda\n\nnum_classes = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\nfrom keras.regularizers import l1_l2\nfrom keras.initializers import RandomNormal as RN, Constant\n\n\n\ndef G_model():\n    inputs = Input([100, ], name=""x"")\n    con_x = Input([num_classes, ], name=""con_x"")\n    con_x2 = Input([img_height, img_width, num_classes], name=""con_x2"")\n    \n    #con_x = K.zeros([None, num_classes, 1, 1])\n    #print(con_x.shape)\n    #con_x = np.zeros([len(_con_x), num_classes, 1, 1], dtype=np.float32)\n    #con_x[np.arange(len(_con_x)), _con_x] = 1\n\n    x = concatenate([inputs, con_x], axis=-1)\n    \n    in_h = int(img_height / 4)\n    in_w = int(img_width / 4)\n    d_dim = 256\n    base = 128\n    x = Dense(in_h * in_w * d_dim, name=\'g_dense1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Reshape((in_h, in_w, d_dim), input_shape=(d_dim * in_h * in_w,))(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_dense1_bn\')(x)\n    # 1/8\n    #x = Conv2DTranspose(base*4, (5, 5), name=\'g_conv1\', padding=\'same\', strides=(2,2),\n    #    kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    #x = Activation(\'relu\')(x)\n    #x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv1_bn\')(x)\n    # 1/4\n    #x = Conv2DTranspose(base*2, (5, 5), name=\'g_conv2\', padding=\'same\', strides=(2,2),\n    #    kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    #x = Activation(\'relu\')(x)\n    #x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv2_bn\')(x)\n    # 1/2\n    x = Conv2DTranspose(base, (5, 5), name=\'g_conv3\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv3_bn\')(x)\n    # 1/1\n    x = Conv2DTranspose(channel, (5, 5), name=\'g_out\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02),  bias_initializer=Constant())(x)\n    x = Activation(\'tanh\')(x)\n\n    #con_x = np.zerns([len(_con_x), num_classes, img_height, img_width], dtype=np.float32)\n    #con_x[np.arange(len(_con_x)), _con_x] = 1\n    x2 = concatenate([x, con_x2], axis=-1)\n\n    model = Model(inputs=[inputs, con_x], outputs=[x], name=\'G\')\n    gan_g_model = Model(inputs=[inputs, con_x, con_x2], outputs=[x2], name=\'GAN_G\')\n    \n    return model, gan_g_model\n\n\ndef D_model():\n    base = 32\n    inputs = Input([img_height, img_width, channel + num_classes])\n    x = Conv2D(base, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*2, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv2\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    #x = Conv2D(base*4, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv3\',\n    #    kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    #x = LeakyReLU(alpha=0.2)(x)\n    #x = Conv2D(base*8, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv4\',\n    #    kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    #x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), bias_initializer=Constant())(x)\n    model = Model(inputs=inputs, outputs=x, name=\'D\')\n    return model\n\n\ndef Combined_model(g, d):\n    inputs = Input([100, ], name=""x"")\n    con_x = Input([num_classes, ], name=""con_x"")\n    con_x2 = Input([img_height, img_width, num_classes], name=""con_x2"")\n    x = g(inputs=[inputs, con_x, con_x2])\n    x = d(x)\n    model = Model(inputs=[inputs, con_x, con_x2], outputs=[x])\n\n    #model = Sequential()\n    #model.add(g)\n    #model.add(d)\n    return model\n\n\nimport pickle\nimport os\nimport gzip\n    \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n\n\n# train\ndef train():\n    _, g = G_model()\n    d = D_model()\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    \n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        con_x = train_y[mb_ind]\n\n        # Disciminator training\n        \n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        _con_x = np.zeros([mb, num_classes], dtype=np.float32)\n        _con_x[np.arange(mb), con_x] = 1\n\n        _con_x2 = np.zeros([mb, img_height, img_width, num_classes], dtype=np.float32)\n        _con_x2[np.arange(mb), ..., con_x] = 1\n        \n        g_output = g.predict(\n            x={""x"":input_noise, ""con_x"": _con_x, ""con_x2"": _con_x2}, verbose=0)\n\n        x = np.concatenate([x, _con_x2], axis=-1)\n        X = np.concatenate((x, g_output))\n        \n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        \n        # Generator training\n        #input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(\n            x={""x"":input_noise, ""con_x"":_con_x, ""con_x2"": _con_x2}, y=np.array([1] * mb))\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'cgan_cifar10_keras.h5\')\n\n# test\ndef test():\n    # load trained model\n    g, _ = G_model()\n    g.load_weights(\'cgan_cifar10_keras.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    con_x = np.zeros([10, num_classes])\n    con_x[np.arange(10), np.arange(num_classes)] = 1\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(10, 100))\n        g_output = g.predict(x={""x"":input_noise, ""con_x"": con_x}, verbose=0)\n        g_output = (g_output + 1 ) / 2\n\n        for i in range(10):\n            gen = g_output[i]\n            \n            if channel == 1:\n                gen = gen[..., 0]\n                cmap = ""gray""\n            elif channel == 3:\n                cmap = None\n                \n            plt.subplot(1,10,i+1)\n            plt.title(str(i))\n            plt.imshow(gen, cmap=cmap)\n            plt.axis(\'off\')\n\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n\n        \n'"
Scripts_Generative/scripts_keras/convae_cifar10_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\n\ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, channel), name=\'in\')\n    x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'enc1\')(inputs)\n    x = MaxPooling2D((2,2), 2)(x)\n    x = Conv2D(16, (3, 3), padding=\'same\', strides=1, name=\'enc2\')(x)\n    x = MaxPooling2D((2,2), 2)(x)\n    x = keras.layers.Conv2DTranspose(32, (2,2), strides=2, padding=\'same\', name=\'dec2\')(x)\n    out = keras.layers.Conv2DTranspose(channel, (2,2), strides=2, padding=\'same\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'mse\'},\n        optimizer=keras.optimizers.Adam(lr=0.001),#""adam"", #keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        #t = x.copy().reshape([mb, -1])\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':x})\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n\n    for i in range(10):\n        x = xs[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred -= pred.min()\n        pred /= pred.max()\n\n        if channel == 1:\n            pred = pred[..., 0]\n            _x = x[0, ..., 0]\n            #_x = (x[0, ..., 0] + 1) / 2\n            cmap = \'gray\'\n        else:\n            _x = x[0]\n            #_x = (x[0] + 1) / 2\n            cmap = None\n            \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/convae_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nout_height, out_width = 64, 64\nchannel = 3\n\n\ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, channel), name=\'in\')\n    x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'enc1\')(inputs)\n    x = MaxPooling2D((2,2), 2)(x)\n    x = Conv2D(16, (3, 3), padding=\'same\', strides=1, name=\'enc2\')(x)\n    x = MaxPooling2D((2,2), 2)(x)\n    x = keras.layers.Conv2DTranspose(32, (2,2), strides=2, padding=\'same\', name=\'dec2\')(x)\n    out = keras.layers.Conv2DTranspose(channel, (2,2), strides=2, padding=\'same\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'mse\'},\n        optimizer=keras.optimizers.Adam(lr=0.001),#""adam"", #keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        #t = x.copy().reshape([mb, -1])\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':x})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred -= pred.min()\n        pred /= pred.max()\n\n        if channel == 1:\n            pred = pred[..., 0]\n            _x = (x[0, ..., 0] + 1) / 2\n            cmap = \'gray\'\n        else:\n            _x = (x[0] + 1) / 2\n            cmap = None\n\n        print(""in {}"".format(path))\n            \n        plt.subplot(1,2,1)\n        plt.title(""input"")\n        plt.imshow(_x, cmap=cmap)\n        plt.subplot(1,2,2)\n        plt.title(""predicted"")\n        plt.imshow(pred, cmap=cmap)\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/dcgan_cifar10_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU, Conv2DTranspose\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\nfrom keras.regularizers import l1_l2\nfrom keras.initializers import RandomNormal as RN, Constant\n\ndef G_model():\n    inputs = Input((100,))\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    d_dim = 256\n    base = 128\n    x = Dense(in_h * in_w * d_dim, name=\'g_dense1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = Reshape((in_h, in_w, d_dim), input_shape=(d_dim * in_h * in_w,))(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_dense1_bn\')(x)\n    # 1/8\n    x = Conv2DTranspose(base*4, (5, 5), name=\'g_conv1\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv1_bn\')(x)\n    # 1/4\n    x = Conv2DTranspose(base*2, (5, 5), name=\'g_conv2\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv2_bn\')(x)\n    # 1/2\n    x = Conv2DTranspose(base, (5, 5), name=\'g_conv3\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv3_bn\')(x)\n    # 1/1\n    x = Conv2DTranspose(channel, (5, 5), name=\'g_out\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02),  bias_initializer=Constant())(x)\n    x = Activation(\'tanh\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'G\')\n    return model\n\ndef D_model():\n    base = 32\n    inputs = Input((img_height, img_width, channel))\n    x = Conv2D(base, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*2, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv2\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*4, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv3\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*8, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv4\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), bias_initializer=Constant())(x)\n    model = Model(inputs=inputs, outputs=x, name=\'D\')\n    return model\n\ndef Combined_model(g, d):\n    model = Sequential()\n    model.add(g)\n    model.add(d)\n    return model\n\n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    g = G_model()\n    d = D_model()\n    gan = Combined_model(g=g, d=d)\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    \n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        X = np.concatenate((x, g_output))\n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        # Generator training\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(input_noise, [1] * mb)\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    g = G_model()\n    g.load_weights(\'model.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(10, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        g_output = (g_output + 1 ) / 2\n\n        for i in range(10):\n            gen = g_output[i]\n            plt.subplot(1,10,i+1)\n            plt.imshow(gen)\n            plt.axis(\'off\')\n\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/dcgan_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU, Conv2DTranspose\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nchannel = 3\n\nfrom keras.regularizers import l1_l2\nfrom keras.initializers import RandomNormal as RN, Constant\n\ndef G_model():\n    inputs = Input((100,))\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    d_dim = 256\n    base = 128\n    x = Dense(in_h * in_w * d_dim, name=\'g_dense1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = Reshape((in_h, in_w, d_dim), input_shape=(d_dim * in_h * in_w,))(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_dense1_bn\')(x)\n    # 1/8\n    x = Conv2DTranspose(base*4, (5, 5), name=\'g_conv1\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv1_bn\')(x)\n    # 1/4\n    x = Conv2DTranspose(base*2, (5, 5), name=\'g_conv2\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv2_bn\')(x)\n    # 1/2\n    x = Conv2DTranspose(base, (5, 5), name=\'g_conv3\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'g_conv3_bn\')(x)\n    # 1/1\n    x = Conv2DTranspose(channel, (5, 5), name=\'g_out\', padding=\'same\', strides=(2,2),\n        kernel_initializer=RN(mean=0.0, stddev=0.02),  bias_initializer=Constant())(x)\n    x = Activation(\'tanh\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'G\')\n    return model\n\ndef D_model():\n    base = 32\n    inputs = Input((img_height, img_width, channel))\n    x = Conv2D(base, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv1\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*2, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv2\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*4, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv3\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*8, (5, 5), padding=\'same\', strides=(2,2), name=\'d_conv4\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), use_bias=False)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\',\n        kernel_initializer=RN(mean=0.0, stddev=0.02), bias_initializer=Constant())(x)\n    model = Model(inputs=inputs, outputs=x, name=\'D\')\n    return model\n\ndef Combined_model(g, d):\n    model = Sequential()\n    model.add(g)\n    model.add(d)\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n# train\ndef train():\n    g = G_model()\n    d = D_model()\n    gan = Combined_model(g=g, d=d)\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    \n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        X = np.concatenate((x, g_output))\n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        # Generator training\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(input_noise, [1] * mb)\n\n        print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    g = G_model()\n    g.load_weights(\'model.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(10, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        g_output = (g_output + 1 ) / 2\n\n        for i in range(10):\n            gen = g_output[i]\n            plt.subplot(1,10,i+1)\n            plt.imshow(gen)\n            plt.axis(\'off\')\n\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/gan_cifar10_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\ndef G_model():\n    inputs = Input((100,))\n    base = 128\n    x = Dense(base, name=\'g_dense1\')(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(base * 2, name=\'g_dense2\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(base * 4, name=\'g_dense3\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(img_height * img_width * channel, activation=\'tanh\', name=\'g_out\')(x)\n    x = Reshape((img_height, img_width, channel))(x)\n    model = Model(inputs, x, name=\'G\')\n    return model\n\ndef D_model():\n    inputs = Input((img_height, img_width, channel))\n    base = 512\n    x = Flatten()(inputs)\n    x = Dense(base * 2, name=\'d_dense1\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(base, name=\'d_dense2\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\')(x)\n    model = Model(inputs, x, name=\'D\')\n    return model\n\ndef Combined_model(g, d):\n    model = Sequential()\n    model.add(g)\n    model.add(d)\n    return model\n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    g = G_model()\n    d = D_model()\n    gan = Combined_model(g=g, d=d)\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    #g_opt = keras.optimizers.SGD(lr=0.0002, momentum=0.3, decay=1e-5)\n    #d_opt = keras.optimizers.SGD(lr=0.0002, momentum=0.1, decay=1e-5)\n\n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        X = np.concatenate((x, g_output))\n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        # Generator training\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(input_noise, [1] * mb)\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    g = G_model()\n    g.load_weights(\'model.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(9, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        g_output = (g_output + 1) / 2\n\n        for i in range(9):\n            gen = g_output[i]\n            plt.subplot(1,9,i+1)\n            plt.imshow(gen)\n            plt.axis(\'off\')\n            #plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0)\n            \n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_keras/gan_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape, UpSampling2D, LeakyReLU\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nchannel = 3\n\ndef G_model():\n    inputs = Input((100,))\n    x = Dense(128, name=\'g_dense1\')(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(256, name=\'g_dense2\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(512, name=\'g_dense3\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(img_height * img_width * channel, activation=\'tanh\', name=\'g_out\')(x)\n    x = Reshape((img_height, img_width, channel))(x)\n    model = Model(inputs, x, name=\'G\')\n    return model\n\ndef D_model():\n    inputs = Input((img_height, img_width, channel))\n    x = Flatten()(inputs)\n    x = Dense(512, name=\'d_dense1\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(256, name=\'d_dense2\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(1, activation=\'sigmoid\', name=\'d_out\')(x)\n    model = Model(inputs, x, name=\'D\')\n    return model\n\ndef Combined_model(g, d):\n    model = Sequential()\n    model.add(g)\n    model.add(d)\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n# train\ndef train():\n    g = G_model()\n    d = D_model()\n    gan = Combined_model(g=g, d=d)\n\n    g_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    d_opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    #g_opt = keras.optimizers.SGD(lr=0.0002, momentum=0.3, decay=1e-5)\n    #d_opt = keras.optimizers.SGD(lr=0.0002, momentum=0.1, decay=1e-5)\n\n    d.trainable = True\n    for layer in d.layers:\n        layer.trainable = True\n    d.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    g.compile(loss=\'binary_crossentropy\', optimizer=d_opt)\n    d.trainable = False\n    for layer in d.layers:\n        layer.trainable = False\n    gan = Combined_model(g=g, d=d)\n    gan.compile(loss=\'binary_crossentropy\', optimizer=g_opt)\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        X = np.concatenate((x, g_output))\n        Y = [1] * mb + [0] * mb\n        d_loss = d.train_on_batch(X, Y)\n        # Generator training\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        g_loss = gan.train_on_batch(input_noise, [1] * mb)\n\n        print(""iter >>"", i+1, "",g_loss >>"", g_loss, \',d_loss >>\', d_loss)\n    \n    g.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    g = G_model()\n    g.load_weights(\'model.h5\', by_name=True)\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        input_noise = np.random.uniform(-1, 1, size=(9, 100))\n        g_output = g.predict(input_noise, verbose=0)\n        g_output = (g_output + 1) / 2\n\n        for i in range(9):\n            gen = g_output[i]\n            plt.subplot(1,9,i+1)\n            plt.imshow(gen)\n            plt.axis(\'off\')\n            #plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0)\n            \n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/ae_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\n    \ndef Mynet(x, keep_prob, train=False):\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 128, scope=\'enc1\')\n    x = slim.batch_norm(x)\n    x = slim.fully_connected(x, out_height * out_width * channel, scope=\'dec1\')\n\n    return x\n\n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    \n    preds = logits\n    #loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=logits, labels=Y))\n    loss = tf.reduce_mean(tf.square(logits - Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(preds, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(5000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = x.copy().reshape([mb, -1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            if (i+1) % 100 == 0:\n                print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = pred.reshape([out_height, out_width, channel])\n            #pred = (pred + 1) / 2\n            pred -= pred.min()\n            pred /= pred.max()\n\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = x[0, ..., 0]\n                #_x = (x[0, ..., 0] + 1) / 2\n                cmap = \'gray\'\n            else:\n                _x = x[0]\n                #_x = (x[0] + 1) / 2\n                cmap = None\n            \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/ae_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nout_height, out_width = 64, 64\nchannel = 3\n\n    \ndef Mynet(x, keep_prob, train=False):\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 256, scope=\'enc1\')\n    x = slim.batch_norm(x)\n    x = slim.fully_connected(x, out_height * out_width * channel, scope=\'dec1\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    \n    preds = logits\n    #loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=logits, labels=Y))\n    loss = tf.reduce_mean(tf.square(logits - Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(preds, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = x.copy().reshape([mb, -1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n\n    xs, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = pred.reshape([out_height, out_width, channel])\n            pred = (pred + 1) / 2\n            pred -= pred.min()\n            pred /= pred.max()\n\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = (x[0, ..., 0] + 1) / 2\n                cmap = \'gray\'\n            else:\n                _x = (x[0] + 1) / 2\n                cmap = None\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/cgan_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 10\nimg_height, img_width = 32, 32\nchannel = 3\n\n\ndef Generator(x, y, y2=None):\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    base = 128\n\n    x = tf.concat([x, y], axis=-1)\n    \n    x = slim.fully_connected(x, base * 4 * in_h * in_w, activation_fn=tf.nn.relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n    x = tf.reshape(x, [-1, in_h, in_w, base * 4])\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn"")\n\n    # 1/8\n    x = slim.conv2d_transpose(x, base * 4, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv1"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn1"")\n    # 1/4\n    x = slim.conv2d_transpose(x, base * 2, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv2"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn2"")\n    # 1/2\n    x = slim.conv2d_transpose(x, base, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE,  scope=""g_deconv3"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn3"")\n    # 1/1\n    x = slim.conv2d_transpose(x, channel, [5, 5], stride=[2,2], activation_fn=None, reuse=tf.AUTO_REUSE, scope=""g_deconv4"")\n    #x = slim.batch_norm(x)\n    x = tf.nn.tanh(x)\n\n    if y2 is not None:\n        x = tf.concat([x, y2], axis=-1)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n    x = slim.conv2d(x, base, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE,  scope=""d_conv1"")\n    x = slim.conv2d(x, base * 2, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv2"")\n    x = slim.conv2d(x, base * 4, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv3"")\n    x = slim.conv2d(x, base * 8, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv4"")\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'drive/My Drive/Colab Notebooks/\' +  \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel + num_classes])\n    X_CON = tf.placeholder(tf.float32, [None, num_classes])\n    X_CON2 = tf.placeholder(tf.float32, [None, img_height, img_width, num_classes])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X, X_CON, X_CON2)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            x_con = train_y[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            _x_con = np.zeros([mb, num_classes], dtype=np.float32)\n            _x_con[np.arange(mb), x_con] = 1\n\n            _x_con2 = np.zeros([mb, img_height, img_width, num_classes], dtype=np.float32)\n            _x_con2[np.arange(mb), ..., x_con] = 1\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise, X_CON: _x_con, X_CON2: _x_con2})\n\n            x = np.concatenate([x, _x_con2], axis=-1)\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, X_CON: _x_con, X_CON2: _x_con2, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n    X_CON = tf.placeholder(tf.float32, [None, num_classes])\n\n    logits = Generator(X, X_CON)\n    \n    np.random.seed(100)\n    \n    labels = [""air\xc2\xa5nplane"", ""auto\xc2\xa5bmobile"", ""bird"", ""cat"", ""deer"",\n              ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            x_con = np.zeros([10, num_classes], dtype=np.float32)\n            x_con[np.arange(10), np.arange(num_classes)] = 1\n            \n            g_output = sess.run(logits, feed_dict={X: input_noise, X_CON: x_con})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n                \n                if channel == 1:\n                    gen = gen[..., 0]\n                    cmap = ""gray""\n                elif channel == 3:\n                    cmap = None\n                    \n                plt.subplot(1,10,i+1)\n                plt.imshow(gen)\n                plt.title(labels[i])\n                plt.axis(\'off\')\n\n            plt.show()\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/cgan_mnist_tensorflow_slim.py,0,"b'from google.colab import drive\ndrive.mount(""/content/drive"", force_remount=True)\n\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\n\ndef Generator(x, y, y2=None):\n    in_h = int(img_height / 4)\n    in_w = int(img_width / 4)\n    base = 128\n\n    x = tf.concat([x, y], axis=-1)\n    \n    x = slim.fully_connected(x, base * 2 * in_h * in_w, activation_fn=tf.nn.relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n    x = tf.reshape(x, [-1, in_h, in_w, base * 2])\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn"")\n\n    # 1/8\n    #x = slim.conv2d_transpose(x, base * 4, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv1"")\n    #x = tf.nn.relu(x)\n    #x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn1"")\n    # 1/4\n    #x = slim.conv2d_transpose(x, base * 2, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv2"")\n    #x = tf.nn.relu(x)\n    #x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn2"")\n    # 1/2\n    x = slim.conv2d_transpose(x, base, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE,  scope=""g_deconv3"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn3"")\n    # 1/1\n    x = slim.conv2d_transpose(x, channel, [5, 5], stride=[2,2], activation_fn=None, reuse=tf.AUTO_REUSE, scope=""g_deconv4"")\n    #x = slim.batch_norm(x)\n    x = tf.nn.tanh(x)\n\n    if y2 is not None:\n        x = tf.concat([x, y2], axis=-1)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n    x = slim.conv2d(x, base, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE,  scope=""d_conv1"")\n    x = slim.conv2d(x, base * 2, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv2"")\n    #x = slim.conv2d(x, base * 4, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv3"")\n    #x = slim.conv2d(x, base * 8, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv4"")\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\n    \nimport pickle\nimport os\nimport gzip\n    \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel + num_classes])\n    X_CON = tf.placeholder(tf.float32, [None, num_classes])\n    X_CON2 = tf.placeholder(tf.float32, [None, img_height, img_width, num_classes])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X, X_CON, X_CON2)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            x_con = train_y[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            _x_con = np.zeros([mb, num_classes], dtype=np.float32)\n            _x_con[np.arange(mb), x_con] = 1\n\n            _x_con2 = np.zeros([mb, img_height, img_width, num_classes], dtype=np.float32)\n            _x_con2[np.arange(mb), ..., x_con] = 1\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise, X_CON: _x_con, X_CON2: _x_con2})\n\n            x = np.concatenate([x, _x_con2], axis=-1)\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, X_CON: _x_con, X_CON2: _x_con2, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n    X_CON = tf.placeholder(tf.float32, [None, num_classes])\n\n    logits = Generator(X, X_CON)\n    \n    np.random.seed(100)\n    \n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            x_con = np.zeros([10, num_classes], dtype=np.float32)\n            x_con[np.arange(10), np.arange(num_classes)] = 1\n            \n            g_output = sess.run(logits, feed_dict={X: input_noise, X_CON: x_con})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n\n                if channel == 1:\n                    gen = gen[..., 0]\n                    cmap = ""gray""\n                elif channel == 3:\n                    cmap = None\n\n                plt.subplot(1,10,i+1)\n                plt.imshow(gen, cmap=cmap)\n                plt.title(str(i))\n                plt.axis(\'off\')\n\n            plt.show()\n      \n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/convae_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nout_height, out_width = 32, 32\nchannel = 3\n\n    \ndef Mynet(x, keep_prob, train=False):\n    x = slim.conv2d(x, 32, [3,3], padding=\'same\', scope=\'enc1\')\n    x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n    x = slim.conv2d(x, 16, [3,3], padding=\'same\', scope=\'enc2\')\n    x = slim.max_pool2d(x, [2,2], scope=\'pool2\')\n    x = slim.conv2d_transpose(x, 32, [2,2], stride=2, scope=\'dec2\')\n    x = slim.conv2d_transpose(x, channel, [2,2], stride=2, scope=\'dec1\')\n    return x\n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height, out_width, channel])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    \n    preds = logits\n    loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(preds, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(5000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = x.copy()\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            if (i+1) % 100 == 0:\n                print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0][0]\n            #pred = (pred[0] + 1) / 2\n            pred -= pred.min()\n            pred /= pred.max()\n\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = x[0, ..., 0]\n                #_x = (x[0, ..., 0] + 1) / 2\n                cmap = \'gray\'\n            else:\n                _x = x[0]\n                #_x = (x[0] + 1) / 2\n                cmap = None\n            \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/convae_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nout_height, out_width = 64, 64\nchannel = 3\n\n    \ndef Mynet(x, keep_prob, train=False):\n    x = slim.conv2d(x, 32, [3,3], padding=\'same\', scope=\'enc1\')\n    x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n    x = slim.conv2d(x, 16, [3,3], padding=\'same\', scope=\'enc2\')\n    x = slim.max_pool2d(x, [2,2], scope=\'pool2\')\n    x = slim.conv2d_transpose(x, 32, [2,2], stride=2, scope=\'dec2\')\n    x = slim.conv2d_transpose(x, channel, [2,2], stride=2, scope=\'dec1\')\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height, out_width, channel])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    \n    preds = logits\n    loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(preds, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = x.copy()\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, out_height * out_width * channel])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n\n    xs, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = (pred[0] + 1) / 2\n            pred -= pred.min()\n            pred /= pred.max()\n\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = (x[0, ..., 0] + 1) / 2\n                cmap = \'gray\'\n            else:\n                _x = (x[0] + 1) / 2\n                cmap = None\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/dcgan_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\n\ndef Generator(x):\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    base = 128\n    \n    x = slim.fully_connected(x, base * 4 * in_h * in_w, activation_fn=tf.nn.relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n    x = tf.reshape(x, [-1, in_h, in_w, base * 4])\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn"")\n\n    # 1/8\n    x = slim.conv2d_transpose(x, base * 4, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv1"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn1"")\n    # 1/4\n    x = slim.conv2d_transpose(x, base * 2, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv2"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn2"")\n    # 1/2\n    x = slim.conv2d_transpose(x, base, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE,  scope=""g_deconv3"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn3"")\n    # 1/1\n    x = slim.conv2d_transpose(x, channel, [5, 5], stride=[2,2], activation_fn=None, reuse=tf.AUTO_REUSE, scope=""g_deconv4"")\n    #x = slim.batch_norm(x)\n    x = tf.nn.tanh(x)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n    x = slim.conv2d(x, base, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE,  scope=""d_conv1"")\n    x = slim.conv2d(x, base * 2, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv2"")\n    x = slim.conv2d(x, base * 4, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv3"")\n    x = slim.conv2d(x, base * 8, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv4"")\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise})\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n\n    logits = Generator(X)\n    \n    np.random.seed(100)\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            g_output = sess.run(logits, feed_dict={X: input_noise})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n                plt.subplot(1,10,i+1)\n                plt.imshow(gen)\n                plt.axis(\'off\')\n\n        plt.show()\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/dcgan_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\n\ndef Generator(x):\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    base = 128\n    \n    x = slim.fully_connected(x, base * 4 * in_h * in_w, activation_fn=tf.nn.relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n    x = tf.reshape(x, [-1, in_h, in_w, base * 4])\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn"")\n\n    # 1/8\n    x = slim.conv2d_transpose(x, base * 4, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv1"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn1"")\n    # 1/4\n    x = slim.conv2d_transpose(x, base * 2, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=""g_deconv2"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn2"")\n    # 1/2\n    x = slim.conv2d_transpose(x, base, [5, 5], stride=[2,2], activation_fn=None, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE,  scope=""g_deconv3"")\n    x = tf.nn.relu(x)\n    x = slim.batch_norm(x, reuse=tf.AUTO_REUSE, decay=0.9, epsilon=1e-5, scope=""g_bn3"")\n    # 1/1\n    x = slim.conv2d_transpose(x, channel, [5, 5], stride=[2,2], activation_fn=None, reuse=tf.AUTO_REUSE, scope=""g_deconv4"")\n    #x = slim.batch_norm(x)\n    x = tf.nn.tanh(x)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n    x = slim.conv2d(x, base, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE,  scope=""d_conv1"")\n    x = slim.conv2d(x, base * 2, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv2"")\n    x = slim.conv2d(x, base * 4, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv3"")\n    x = slim.conv2d(x, base * 8, [5,5], stride=[2,2], activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_conv4"")\n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise})\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n\n    logits = Generator(X)\n    \n    np.random.seed(100)\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            g_output = sess.run(logits, feed_dict={X: input_noise})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n                plt.subplot(1,10,i+1)\n                plt.imshow(gen)\n                plt.axis(\'off\')\n\n        plt.show()\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/gan_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\n\ndef Generator(x):\n    base = 128\n    \n    x = slim.fully_connected(x, base, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n\n    x = slim.fully_connected(x, base * 2, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense2\')\n\n    x = slim.fully_connected(x, base * 4, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense3\')\n\n    x = slim.fully_connected(x, img_height * img_width * channel, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense4\')\n    \n    x = tf.reshape(x, [-1, img_height, img_width, channel])\n    x = tf.nn.tanh(x)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n\n    x = slim.fully_connected(x, base * 2, activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_dense1"")\n    x = slim.fully_connected(x, base, activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_dense2"")\n    \n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\n\nimport pickle\nimport os\n\ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise})\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n\n    logits = Generator(X)\n    \n    np.random.seed(100)\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            g_output = sess.run(logits, feed_dict={X: input_noise})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n                plt.subplot(1,10,i+1)\n                plt.imshow(gen)\n                plt.axis(\'off\')\n\n        plt.show()\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/scripts_tf_slim/gan_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\n\ndef Generator(x):\n    base = 128\n    \n    x = slim.fully_connected(x, base, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x: x, reuse=tf.AUTO_REUSE, scope=\'g_dense1\')\n\n    x = slim.fully_connected(x, base * 2, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense2\')\n\n    x = slim.fully_connected(x, base * 4, activation_fn=tf.nn.leaky_relu, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense3\')\n\n    x = slim.fully_connected(x, img_height * img_width * channel, normalizer_fn=lambda x:x, reuse=tf.AUTO_REUSE, scope=\'g_dense4\')\n    \n    x = tf.reshape(x, [-1, img_height, img_width, channel])\n    x = tf.nn.tanh(x)\n\n    return x\n\n\ndef Discriminator(x):\n    base = 64\n\n    x = slim.fully_connected(x, base * 2, activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_dense1"")\n    x = slim.fully_connected(x, base, activation_fn=tf.nn.leaky_relu, reuse=tf.AUTO_REUSE, scope=""d_dense2"")\n    \n    x = slim.flatten(x)\n    x = slim.fully_connected(x, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=""d_dense"")\n\n    return x\n    \n\nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n\n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n            pbar.update(1)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    #xs = np.transpose(xs, (0,3,1,2))\n    pbar.close()\n    \n    return xs, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, 100])\n    X2 = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    g_logits = Generator(X)\n    d_logits = Discriminator(X2)\n    gan_logits = Discriminator(g_logits)\n\n    tvars = tf.trainable_variables()\n    \n    d_preds = d_logits\n    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n    #loss = tf.reduce_mean(tf.square(logits - Y))\n    D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    D_vars = [var for var in tvars if \'d_\' in var.name]\n    D_train = D_optimizer.minimize(D_loss, var_list=D_vars)\n\n    gan_preds = gan_logits\n    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=Y))\n    G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n    G_vars = [var for var in tvars if \'g_\' in var.name]\n    G_train = G_optimizer.minimize(G_loss, var_list=G_vars)\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    #d_losses = [0]\n    #g_losses = [0]\n    #ites = [0]\n    #fig, ax = plt.subplots(1, 1)\n    #lines, = ax.plot(d_losses, g_losses)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for ite in range(10000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n\n            g_output = sess.run(g_logits, feed_dict={X: input_noise})\n\n            _X = np.concatenate([x, g_output])\n            _Y = np.array([1] * mb + [0] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n    \n            _, d_loss = sess.run([D_train, D_loss], feed_dict={X2:_X, Y:_Y})\n\n            _Y = np.array([1] * mb, dtype=np.float32)\n            _Y = _Y[..., None]\n            _, g_loss = sess.run([G_train, G_loss], feed_dict={X:input_noise, Y: _Y})\n            \n            #d_losses.append(d_loss)\n            #g_losses.append(g_loss)\n            #ites.append(ite + 1)\n            #lines.set_data(ites, d_losses)\n            #ax.set_xlim((0, ite+2))\n            #plt.pause(0.001)\n            \n            \n            if (ite+1) % 100 == 0:\n                print(""iter >>"", ite+1, \',G:loss >>\', g_loss, \',D:loss >>\', d_loss)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, 100])\n\n    logits = Generator(X)\n    \n    np.random.seed(100)\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    \n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(3):\n            input_noise = np.random.uniform(-1, 1, size=(10, 100))\n            g_output = sess.run(logits, feed_dict={X: input_noise})\n            g_output = (g_output + 1 ) / 2\n\n            for i in range(10):\n                gen = g_output[i]\n                plt.subplot(1,10,i+1)\n                plt.imshow(gen)\n                plt.axis(\'off\')\n\n        plt.show()\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/tf_keras/DCGAN_cifar10_tf2.1_keras.py,0,"b'#%tensorflow_version 2.x\nimport tensorflow as tf\n\nprint(tf.__version__)\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.initializers import RandomNormal as RN, Constant\nimport pickle\nimport os\n\n# config\nclass_N = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\n# GAN config\nZ_dim = 100\n\ntest_N = 10\n\n# model path\nmodel_path = \'DCGAN.h5\'\n\n\n\ndef Generator():\n    inputs = Input((Z_dim,))\n    in_h = int(img_height / 16)\n    in_w = int(img_width / 16)\n    base = 256\n    # 1/16\n    x = Dense(in_h * in_w * base, name=\'Dense1\', use_bias=False)(inputs)\n    x = Reshape((in_h, in_w, base), input_shape=(base * in_h * in_w,))(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'dense1_bn\')(x)\n    # 1/8\n    x = Conv2DTranspose(base*4, (5, 5), name=\'tconv1\', padding=\'same\', strides=(2,2), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'tconv1_bn\')(x)\n    # 1/4\n    x = Conv2DTranspose(base*2, (5, 5), name=\'tconv2\', padding=\'same\', strides=(2,2), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'tconv2_bn\')(x)\n    # 1/2\n    x = Conv2DTranspose(base, (5, 5), name=\'tconv3\', padding=\'same\', strides=(2,2), use_bias=False)(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=\'tconv3_bn\')(x)\n    # 1/1\n    x = Conv2DTranspose(channel, (5, 5), name=\'tconv_out\', padding=\'same\', strides=(2,2))(x)\n    x = Activation(\'tanh\')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=x, name=\'G\')\n    return model\n\n\ndef Discriminator():\n    base = 32\n    inputs = Input((img_height, img_width, channel))\n    x = Conv2D(base, (5, 5), padding=\'same\', strides=(2,2), name=\'conv1\')(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*2, (5, 5), padding=\'same\', strides=(2,2), name=\'conv2\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*4, (5, 5), padding=\'same\', strides=(2,2), name=\'conv3\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Conv2D(base*8, (5, 5), padding=\'same\', strides=(2,2), name=\'conv4\')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=None, name=\'dense_out\')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=x, name=\'D\')\n    return model\n    \ndef load_cifar10():\n    path = \'drive/My Drive/Colab Notebooks/\' + \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    G = Generator()\n    D = Discriminator()\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n\n    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    #loss_fn = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=Y))\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n\n    @tf.function\n    def train_iter(x, z):\n        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n            # feed forward\n            # z -> G -> Gz\n            Gz = G(z, training=True)\n\n            # x -> D -> Dx\n            # z -> G -> Gz -> D -> DGz\n            Dx = D(x, training=True)\n            DGz = D(Gz, training=True)\n\n            # get loss\n            loss_G = loss_fn(tf.ones_like(DGz), DGz)\n            loss_D_real = loss_fn(tf.ones_like(Dx), Dx)\n            loss_D_fake = loss_fn(tf.zeros_like(DGz), DGz)\n            loss_D = loss_D_real + loss_D_fake\n\n        # feed back\n        gradients_of_G = G_tape.gradient(loss_G, G.trainable_variables)\n        gradients_of_D = D_tape.gradient(loss_D, D.trainable_variables)\n\n        # update parameter\n        G_optimizer.apply_gradients(zip(gradients_of_G, G.trainable_variables))\n        D_optimizer.apply_gradients(zip(gradients_of_D, D.trainable_variables))\n\n        return loss_G, loss_D\n\n    #with strategy.scope():\n    # optimizer\n    G_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    for ite in range(10000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n\n        z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n        #z = tf.random.normal([mb, Z_dim])\n\n        loss_G, loss_D = train_iter(x, z)\n        \n        if (ite + 1) % 100 == 0:\n            print(""iter :"", ite+1, \', Loss_G :\', loss_G.numpy(), \',Loss_D :\', loss_D.numpy())\n\n        # display generated image\n        if (ite + 1) % 100 == 0:\n            Gz = G(z)\n            _Gz = (Gz * 127.5 + 127.5).numpy().astype(int)\n            for i in range(9):\n                plt.subplot(3, 3, i + 1)\n                plt.imshow(_Gz[i])\n                plt.axis(\'off\')\n            plt.show()\n\n    # save model\n    G.save_weights(model_path)\n\n# test\ndef test():\n    # model\n    G = Generator()\n    G.load_weights(model_path)\n\n    for i in range(3):\n        z = np.random.uniform(-1, 1, size=(test_N, Z_dim))\n        Gz = G(z, training=False)\n        Gz = Gz * 127.5 + 127.5\n        Gz = Gz.numpy().astype(int)\n\n        for i in range(test_N):\n            _Gz = Gz[i]\n            plt.subplot(1, test_N, i + 1)\n            plt.imshow(_Gz)\n            plt.axis(\'off\')\n\n        plt.show()\n        \n        \ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/tf_keras/StyleTransfer_tf2.1_keras.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\n\n# config \nmax_dim = 128\nchannel = 3\n\nstyle_weight=5e-1\ncontent_weight=1e4\ntotal_variation_weight=30\n\nepochs = 10\nsteps_per_epoch = 100\n\n\ndef load_img(path):\n    # image read\n    \n    img = cv2.imread(path)\n    long_dim = max(img.shape[:2])\n    scale = max_dim / long_dim\n    img = cv2.resize(img, None, fx=scale, fy=scale)\n    \n    img = np.expand_dims(img, axis=0)\n    img = img[..., ::-1]\n    img = img.astype(np.float32)\n    img /= 255.\n    """"""\n    img = tf.io.read_file(path)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim / long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    """"""\n    return img\n\n\ndef vgg_layers(layer_names):\n    # get imagenet pretrained model\n    vgg = tf.keras.applications.VGG16(include_top=False, weights=\'imagenet\')\n    vgg.trainable = False\n    \n    outputs = [vgg.get_layer(name).output for name in layer_names]\n\n    model = tf.keras.Model([vgg.input], outputs)\n    return model\n\nclass StyleContentModel(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(StyleContentModel, self).__init__()\n        self.vgg = vgg_layers(style_layers + content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.vgg.trainable = False\n\n    def call(self, inputs):\n        ""Expects float input in [0,1]""\n        inputs = inputs * 255.0\n        preprocessed_input = tf.keras.applications.vgg16.preprocess_input(inputs)\n        outputs = self.vgg(preprocessed_input)\n        style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n\n        style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n\n        content_dict = {content_name : value for content_name, value in zip(self.content_layers, content_outputs)}\n\n        style_dict = {style_name : value for style_name, value in zip(self.style_layers, style_outputs)}\n        \n        return {\'content\' : content_dict, \'style\' : style_dict}\n\n\ndef gram_matrix(input_tensor):\n    result = tf.linalg.einsum(\'bijc,bijd->bcd\', input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n    return result/(num_locations)\n\n\n# train\ndef train():\n    # image\n    content_image = load_img(\'../Dataset/train/images/madara/madara_0007.jpg\')\n    style_image = load_img(\'../Dataset/train/images/akahara/akahara_0008.jpg\')\n    \n    # Content layer where will pull our feature maps\n    content_layers = [\'block5_conv2\'] \n\n    # Style layer of interest\n    style_layers = [\'block1_conv1\', \'block2_conv1\', \'block3_conv1\', \'block4_conv1\', \'block5_conv1\']\n\n    content_layers_N = len(content_layers)\n    style_layers_N = len(style_layers)\n\n    # extract model\n    extractor = StyleContentModel(style_layers, content_layers)\n\n    # original style and content matrix\n    style_targets = extractor(style_image)[\'style\']\n    content_targets = extractor(content_image)[\'content\']\n\n    # output image to be optimized\n    image = tf.Variable(content_image)\n\n    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n    \n    @tf.function()\n    def train_step(image):\n        with tf.GradientTape() as tape:\n            # feed forward\n            outputs = extractor(image)\n            \n            # style loss\n            style_outputs = outputs[\'style\']\n            style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2) for name in style_outputs.keys()])\n            style_loss *= style_weight / style_layers_N\n\n            # content loss\n            content_outputs = outputs[\'content\']\n            content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2) for name in content_outputs.keys()])\n            content_loss *= content_weight / content_layers_N\n            loss = style_loss + content_loss\n            \n            # total valiation loss\n            x_deltas = image[:, :, 1:] - image[:, :, :-1]\n            y_deltas = image[:, 1:] - image[:, :-1]\n            loss += total_variation_weight * (tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas)))\n\n        # get and apply gradient\n        grad = tape.gradient(loss, image)\n        opt.apply_gradients([(grad, image)])\n        # clip image to [0, 1]\n        image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n\n\n    step = 0\n    for epoch in range(epochs):\n        for step in range(steps_per_epoch):\n            train_step(image)\n        \n        _image = np.array(image * 255).astype(np.uint8)[0]\n        plt.imshow(_image)\n        plt.show()\n        #display.display(tensor_to_image(image))\n        print(""Epoch : {}"".format(epoch + 1))\n\n\n    \n\n# test\ndef test():\n    print(\'not implemented\')\n        \n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/tf_keras/pix2pix_tf2.1_keras.py,0,"b'import tensorflow as tf\n\nprint(tf.__version__)\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\n# class config\nclass_label = OrderedDict({\'background\' : [0, 0, 0], \'akahara\' : [0, 0, 128], \'madara\' : [0, 128, 0]})\nclass_N = len(class_label)\n\n# model config\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 3\nout_channel = class_N\n\nmodel_path_G = \'pix2pix_G.h5\'\nmodel_path_D = \'pix2pix_D.h5\'\n\n# training config\nBatchsize = 16\nIteration = 5000\nLoss_Lambda = 10.\n\ndef UNet():\n    def UNet_block_downSampling(x, filters, size, name, apply_batchnorm=False):\n        x = tf.keras.layers.Conv2D(filters, size, strides=2, padding=\'same\', use_bias=False, name=name + \'_conv\')(x)\n        x = tf.keras.layers.ReLU(name=name + \'_ReLU\')(x)\n        x = tf.keras.layers.BatchNormalization(name=name + \'_bn\')(x)\n        \n        return x\n\n    def UNet_block_upSampling(x, filters, size, name, apply_dropout=False):\n        x = tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\'same\', use_bias=False, name=name + \'_transposedConv\')(x)\n        x = tf.keras.layers.BatchNormalization(name=name + \'_bn\')(x)\n        x = tf.keras.layers.Dropout(0.5, name=name + \'dropout\')(x) if apply_dropout else x\n        \n        return x\n\n    stride_N = int(np.log(img_height) / np.log(2))\n\n    x_encoders = []\n\n    _input = tf.keras.layers.Input(shape=[img_height, img_width, channel])\n    # down sample\n    x = _input\n    for i in range(1, stride_N + 1):\n        x = UNet_block_downSampling(x, min(64 ** i, 512), 3, name=\'Encoder{}\'.format(i))\n        x_encoders.append(x)\n\n    # up sample\n    for i in range(stride_N - 1, 0, -1):\n        x = UNet_block_upSampling(x, min(64 ** i, 512), 3, name=\'Decoder{}\'.format(i - 1))\n        x = tf.keras.layers.concatenate([x, x_encoders[i - 1]])\n\n    x_output = tf.keras.layers.Conv2DTranspose(out_channel, 3, strides=2, padding=\'same\', activation=\'tanh\')(x)\n    return tf.keras.Model(inputs=_input, outputs=x_output)\n\n\ndef Discriminator():\n    def Discriminator_block_downSampling(x, filters, size, name, apply_batchnorm=False):\n        x = tf.keras.layers.Conv2D(filters, size, strides=2, padding=\'same\', use_bias=False, name=name + \'_conv\')(x)\n        x = tf.keras.layers.LeakyReLU(name=name + \'_leakyReLU\')(x) if apply_batchnorm else x\n        return x\n\n    stride_N = int(np.log(out_height) / np.log(2))\n\n    _input1 = tf.keras.layers.Input(shape=[out_height, out_width, channel], name=\'input1\')\n    _input2 = tf.keras.layers.Input(shape=[out_height, out_width, out_channel], name=\'input2\')\n    \n    x = tf.keras.layers.concatenate([_input1, _input2]) # (bs, 256, 256, channels*2)\n\n    for i in range(1, stride_N):\n        x = Discriminator_block_downSampling(x, min(64 ** i, 512), 5, name=\'D{}\'.format(i))\n    \n    x = tf.keras.layers.Conv2D(1, 2, strides=1, padding=\'same\')(x)\n    \n    return tf.keras.Model(inputs=[_input1, _input2], outputs=x)\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(paths_gt)\n\ndef get_image(infos, gt=False):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        if gt:\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        else:\n            x = cv2.resize(x, (out_width, out_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if gt:\n            _x = x\n            x = np.zeros((out_height, out_width, class_N), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[..., i][ind] = 1\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if channel == 3:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    return xs\n\n\n# train\ndef train():\n    # model\n    G = UNet()\n    D = Discriminator()\n\n    # optimizer\n    G_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    \n    paths, paths_gt = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=False)\n\n    @tf.function\n    def train_step(x, target):\n        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n            Gx = G(x, training=True)\n\n            D_real = D([x, target], training=True)\n            D_fake = D([x, Gx], training=True)\n\n            # Generator loss\n            G_loss_fake = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(D_fake), D_fake)\n            G_loss_L1 = tf.reduce_mean(tf.abs(target - Gx))\n\n            G_loss = G_loss_fake + Loss_Lambda * G_loss_L1\n\n            # Discriminator loss\n            D_loss_real = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(D_real), D_real)\n            D_loss_fake = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(D_fake), D_fake)\n\n            D_loss = D_loss_real + D_loss_fake\n\n\n        G_gradients = G_tape.gradient(G_loss, G.trainable_variables)\n        D_gradients = D_tape.gradient(D_loss, D.trainable_variables)\n\n        G_optimizer.apply_gradients(zip(G_gradients, G.trainable_variables))\n        D_optimizer.apply_gradients(zip(D_gradients, D.trainable_variables))\n\n        return {\'G_loss\' : G_loss, \'G_loss_fake\' : G_loss_fake, \'G_loss_L1\' : G_loss_L1, \'D_loss\' : D_loss, \'D_loss_real\' : D_loss_real, \'D_loss_fake\' : D_loss_fake}\n    \n    # training\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    for i in range(Iteration):\n        if mbi + Batchsize > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(Batchsize - (train_N - mbi))]))\n            mbi = Batchsize - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + Batchsize]\n            mbi += Batchsize\n        \n        Xs = get_image(paths[mb_ind])\n        Xs_target = get_image(paths_gt[mb_ind], gt=True)\n        \n        loss_dict = train_step(Xs, Xs_target)\n        \n        print(\'|\', end=\'\')\n\n        if (i + 1) % 10 == 0:\n            print(i + 1, end=\'\')\n        \n        if (i + 1) % 50 == 0:\n            print(\'\\r\' + \' \' * 100, end=\'\')\n            print(\'\\riter : {} , G_Loss : {:.4f} (fake : {:.4f} , L1 : {:.4f}) , D_Loss : {:.4f} (fake : {:.4f} , real : {:.4f})\'.format(\n                i + 1, loss_dict[\'G_loss\'], loss_dict[\'G_loss_fake\'], loss_dict[\'G_loss_L1\'], loss_dict[\'D_loss\'], loss_dict[\'D_loss_fake\'], loss_dict[\'D_loss_real\']))\n\n    G.save_weights(model_path_G)\n    D.save_weights(model_path_D)\n\n    \n# test\ndef test():\n    G = UNet()\n    G.load_weights(model_path_G)\n\n    paths, paths_gt = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    for i in range(len(paths)):\n        path = paths[[i]]\n        path_gt = paths_gt[[i]]\n        x = get_image(path)\n        \n        pred = G(x)[0].numpy()\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (label_name, v) in enumerate(class_label.items()):\n            out[pred == i] = v\n\n        print(""in {}"".format(path[\'path\']))\n        \n        plt.subplot(1, 2, 1)\n        plt.imshow(((x[0] + 1) / 2).astype(np.float32))\n        plt.subplot(1, 2, 2)\n        plt.imshow(out[..., ::-1])\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Interpret/scripts_pytorch/GradCam_pytorch.py,33,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom collections import OrderedDict\n\nClass_label = [\'akahara\', \'madara\']\nClass_N = len(Class_label)\nimg_height, img_width = 128, 128\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ResBlock(torch.nn.Module):\n    def __init__(self, in_f, f_1, out_f, stride=1):\n        super(ResBlock, self).__init__()\n\n        self.stride = stride\n        self.fit_dim = False\n\n        self.block = torch.nn.Sequential(\n            torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n            torch.nn.BatchNorm2d(f_1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n            torch.nn.BatchNorm2d(f_1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n            torch.nn.BatchNorm2d(out_f),\n            torch.nn.ReLU()\n        )\n\n        if in_f != out_f:\n            self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n            self.fit_bn = torch.nn.BatchNorm2d(out_f)\n            self.fit_dim = True\n            \n            \n        \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n            x = self.fit_bn(x)\n            x = F.relu(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        \nclass Res101(torch.nn.Module):\n    def __init__(self):\n        super(Res101, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(22):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, Class_N)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, _Class_label in enumerate(Class_label):\n                if _Class_label in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # model\n    model = Res101().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'Res101.pt\')\n\n# test\ndef test(target_layer_name):\n    model = Res101().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'res101.pt\', map_location=torch.device(device)))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    target_layer = None\n\n    for name, module in model.named_modules():\n      if target_layer_name == name:\n        print(\'target:\', name)\n        target_layer = module\n\n    if target_layer is None:\n      for name, module in model.named_modules():\n        print(name)\n      raise Exception(\'invalid target layer name >>\', target_layer_name)\n\n    if type(target_layer) is torch.nn.Sequential:\n      target_layer = target_layer[-1]\n\n    print(target_layer)\n\n    fmap_pool = OrderedDict()\n    grad_pool = OrderedDict()\n\n    def forward_hook(key):\n        def forward_hook_(module, input, output):\n            # Save featuremaps\n            fmap_pool[key] = output.detach()\n\n        return forward_hook_\n\n    def backward_hook(key):\n        def backward_hook_(module, grad_in, grad_out):\n            # Save the gradients correspond to the featuremaps\n            grad_pool[key] = grad_out[0].detach()\n\n        return backward_hook_\n\n    # If any candidates are not specified, the hook is registered to all the layers.\n    for name, module in model.named_modules():\n            module.register_forward_hook(forward_hook(name))\n            module.register_backward_hook(backward_hook(name))\n\n\n    for i in range(len(paths)):\n        _x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(_x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        # forward network\n        logit = model(x)\n        pred = F.softmax(logit, dim=1).detach().cpu().numpy()\n\n        raw_image = (_x ).transpose(1, 2, 0)\n\n        plt.subplot(1, Class_N + 1, 1)\n        plt.imshow(raw_image)\n        plt.title(\'input\')\n\n        for i, class_label in enumerate(Class_label):\n            # set one-hot class activity\n            class_index = torch.zeros(pred.shape).to(device)\n\n            _index = Class_label.index(class_label)\n            class_index[:, _index] = 1\n\n            logit.backward(gradient=class_index, retain_graph=True)\n            \n            #target_layer_output = target_layer.forward(x)\n            fmaps = fmap_pool[target_layer_name]\n            grads = grad_pool[target_layer_name]\n            weights = F.adaptive_avg_pool2d(grads, 1)\n\n            gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n            gcam = F.relu(gcam)\n\n            gcam = F.interpolate(gcam, [img_height, img_width], mode=""bilinear"", align_corners=False)\n\n            B, C, H, W = gcam.shape\n            gcam = gcam.view(B, -1)\n            gcam -= gcam.min(dim=1, keepdim=True)[0]\n            gcam /= gcam.max(dim=1, keepdim=True)[0]\n            gcam = gcam.view(B, C, H, W)\n\n            gcam = gcam.cpu().numpy()[0, 0]\n            cmap = cm.jet_r(gcam)[..., :3]\n            gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n        \n            plt.subplot(1, Class_N + 1, i + 2)\n            plt.imshow(gcam)\n            plt.title(\'{} :{:.2f}\'.format(class_label, pred[0, i]))\n\n        plt.show()\n\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--target\', dest=\'target_layer\', default=\'conv3\', type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.target_layer)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/answers/alexnet_tensorflow_raw.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 227, 227\ntf.set_random_seed(0)\n\ndef conv2d(x, k=3, in_num=1, out_num=32, strides=1, padding=\'SAME\', activ=None, bias=True, name=\'conv\'):\n    w = tf.Variable(tf.random_normal([k, k, in_num, out_num]), name=name+\'_w\')\n    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=padding)\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.nn.bias_add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\ndef maxpool2d(x, k=2, s=2, padding=\'SAME\'):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding=padding)\n\ndef fc(x, in_num=100, out_num=100, bias=True, activ=None, name=\'fc\'):\n    w = tf.Variable(tf.random_normal([in_num, out_num]), name=name+\'_w\')\n    x = tf.matmul(x, w)\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\n\ndef AlexNet(x, keep_prob):\n    x = conv2d(x, k=11, in_num=3, out_num=96, strides=4, padding=\'VALID\', activ=tf.nn.relu, name=\'conv1\')\n    x = tf.nn.local_response_normalization(x)\n    x = maxpool2d(x, k=3, s=2)\n    x = tf.keras.layers.ZeroPadding2D((1,1))(x)\n    x = conv2d(x, k=5, in_num=96, out_num=256, padding=\'VALID\', activ=tf.nn.relu, name=\'conv2\')\n    x = tf.nn.local_response_normalization(x)\n    x = maxpool2d(x, k=3, s=2)\n    x = conv2d(x, k=3, in_num=256, out_num=384, padding=\'SAME\', activ=tf.nn.relu, name=\'conv3\')\n    x = conv2d(x, k=3, in_num=384, out_num=384, padding=\'SAME\', activ=tf.nn.relu, name=\'conv4\')\n    x = conv2d(x, k=3, in_num=384, out_num=256, padding=\'SAME\', activ=tf.nn.relu, name=\'conv5\')\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = fc(x, in_num=w*h*c, out_num=4096, activ=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = fc(x, in_num=4096, out_num=4096, activ=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = fc(x, in_num=4096, out_num=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = AlexNet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=preds, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = AlexNet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/answers/googlenetv1_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef GoogLeNetv1(x, keep_prob):\n\n    def inception_module(x, in_f, f_1, f_2_1, f_2_2, f_3_1, f_3_2, f_4_2):\n        x1 = slim.conv2d(x, f_1, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n\n        x2_1 = slim.conv2d(x, f_2_1, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n        x2_2 = slim.conv2d(x2_1, f_2_1, [3, 3], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n\n        x3_1 = slim.conv2d(x, f_3_1, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n        x3_2 = slim.conv2d(x3_1, f_3_2, [5, 5], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n\n        x4_1 = slim.max_pool2d(x, [3, 3], stride=1, padding=\'SAME\')\n        x4_2 = slim.conv2d(x4_1, f_4_2, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n        x = tf.concat([x1, x2_2, x3_2, x4_2], axis=-1)\n\n        return x\n\n    \n    x = slim.conv2d(x, 64, [7, 7], stride=2, padding=""VALID"", activation_fn=tf.nn.relu)\n    x = slim.max_pool2d(x, [3, 3], stride=2, padding=\'SAME\')\n    x = tf.nn.local_response_normalization(x)\n\n    x = slim.conv2d(x, 64, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n    x = slim.conv2d(x, 192, [3, 3], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n    x = tf.nn.local_response_normalization(x)\n    x = slim.max_pool2d(x, [3, 3], stride=2, padding=\'SAME\')\n\n    # inception 3a, 3b\n    x = inception_module(x, 194, 64, 96, 128, 16, 32, 32)\n    x = inception_module(x, 256, 128, 128, 192, 32, 96, 64)\n    x = slim.max_pool2d(x, [3, 3], stride=2, padding=\'SAME\')\n\n    # inception 4a\n    x = inception_module(x, 480, 192, 96, 208, 16, 48, 64)\n\n    # auxiliary loss1\n    x_aux1 = slim.avg_pool2d(x, 5, padding=\'SAME\', stride=1)\n    x_aux1 = slim.conv2d(x_aux1, 128, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n    mb, h, w, c = x_aux1.get_shape().as_list()\n    x_aux1 = tf.reshape(x_aux1, [-1, h * w * c])\n    x_aux1 = slim.fully_connected(x_aux1, 1024, activation_fn=tf.nn.relu)\n    x_aux1 = slim.dropout(x_aux1, keep_prob=keep_prob)\n    x_aux1 = slim.fully_connected(x_aux1, num_classes)\n\n    # inception 4b, 4c, 4d\n    x = inception_module(x, 512, 160, 112, 224, 24, 64, 64)\n    x = inception_module(x, 512, 128, 128, 256, 24, 64, 64)\n    x = inception_module(x, 512, 112, 144, 288, 32, 64, 64)\n    \n    # auxiliary loss2\n    x_aux2 = slim.avg_pool2d(x, 5, padding=\'SAME\', stride=1)\n    x_aux2 = slim.conv2d(x_aux2, 128, [1, 1], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n    mb, h, w, c = x_aux2.get_shape().as_list()\n    x_aux2 = tf.reshape(x_aux2, [-1, h * w * c])\n    x_aux2 = slim.fully_connected(x_aux2, 1024, activation_fn=tf.nn.relu)\n    x_aux2 = slim.dropout(x_aux2, keep_prob=keep_prob)\n    x_aux2 = slim.fully_connected(x_aux2, num_classes)\n\n    # inception 4e, 5a, 5b\n    x = inception_module(x, 528, 256, 160, 320, 32, 128, 128)\n    x = slim.max_pool2d(x, 3, padding=\'SAME\', stride=2)\n    x = inception_module(x, 832, 256, 160, 320, 32, 128, 128)\n    x = inception_module(x, 832, 384, 192, 384, 48, 128, 128)\n\n    #x = slim.avg_pool2d(x, 7, stride=1, padding=\'SAME\')\n    #mb, h, w, c = x.get_shape().as_list()\n    #x = tf.reshape(x, [-1, h * w * c])\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = slim.fully_connected(x, num_classes)\n    \n    return x, x_aux1, x_aux2\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits, logits_aux1, logits_aux2 = GoogLeNetv1(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    preds_aux1 = tf.nn.softmax(logits_aux1)\n    preds_aux2 = tf.nn.softmax(logits_aux2)\n    \n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    loss_aux1 = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits_aux1, onehot_labels=Y))\n    loss_aux2 = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits_aux2, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss + loss_aux1 + loss_aux2)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits, _, _ = GoogLeNetv1(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/answers/lenet_tensorflow_raw.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 32, 32\ntf.set_random_seed(0)\n\ndef conv2d(x, k=3, in_num=1, out_num=32, strides=1, padding=\'SAME\', activ=None, bias=True, name=\'conv\'):\n    w = tf.Variable(tf.random_normal([k, k, in_num, out_num]), name=name+\'_w\')\n    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=padding)\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.nn.bias_add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\ndef maxpool2d(x, k=2, padding=\'SAME\'):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=padding)\n\ndef fc(x, in_num=100, out_num=100, bias=True, activ=None, name=\'fc\'):\n    w = tf.Variable(tf.random_normal([in_num, out_num]), name=name+\'_w\')\n    x = tf.matmul(x, w)\n    tf.add_to_collections(\'vars\', w)\n    if bias:\n        b = tf.Variable(tf.random_normal([out_num]), name=name+\'_b\')\n        tf.add_to_collections(\'vars\', b)\n        x = tf.add(x, b)\n    if activ is not None:\n        x = activ(x)\n    return x\n\ndef LeNet(x, keep_prob):\n    x = conv2d(x, k=5, in_num=3, out_num=6, padding=\'VALID\', activ=None, name=\'conv1\')\n    x = tf.nn.sigmoid(maxpool2d(x, k=2))\n    x = conv2d(x, k=5, in_num=6, out_num=16, padding=\'VALID\', activ=None, name=\'conv2\')\n    x = tf.nn.sigmoid(maxpool2d(x, k=2))\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = fc(x, in_num=w*h*c, out_num=120, activ=None, name=\'fc1\')\n    x = fc(x, in_num=120, out_num=64, activ=None, name=\'fc2\')\n    x = fc(x, in_num=64, out_num=num_classes, name=\'fc_out\')\n    return x\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = LeNet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = LeNet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/answers/res50_cifar10_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 10\nimg_height, img_width = 32, 32\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res50(x, keep_prob):\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = slim.conv2d(x, f_1, [1, 1], stride=stride, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = slim.conv2d(res_x, f_1, [3, 3], stride=1, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = slim.conv2d(res_x, out_f, [1, 1], stride=1, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = slim.conv2d(x, out_f, [1, 1], stride=1, padding=""SAME"", activation_fn=None)\n            x = slim.batch_norm(x)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = slim.max_pool2d(x, [2, 2], stride=stride, padding=""SAME"")\n        \n        x = tf.add(res_x, x)\n\n        return x\n\n    \n    x = slim.conv2d(x, 64, [7, 7], stride=2, padding=""SAME"", activation_fn=None)\n    x = slim.batch_norm(x)\n    x = tf.nn.relu(x)\n    \n    x = slim.max_pool2d(x, [3, 3], stride=2, padding=\'SAME\')\n\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    x = slim.avg_pool2d(x, [img_height // 32, img_width // 32], stride=1, padding=\'VALID\')\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h * w * c])\n    x = slim.fully_connected(x, num_classes)\n    \n    return x\n\n\nimport os\nimport pickle\n\ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    print(""train X:"", train_x.shape)\n    print(""train y:"", train_y.shape)\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    print(""test X:"", test_x.shape)\n    print(""test y:"", test_y.shape)\n\n    return train_x, train_y, test_x, test_y\n\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res50(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n    train_X, train_y, test_X, test_y = load_cifar10()\n    xs = train_X / 255\n    ts = np.zeros([len(train_y), num_classes], dtype=np.float32)\n    ts[np.arange(len(train_y)), train_y] = 1\n    #xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 512\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res50(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    #xs, ts, paths = data_load(""../Dataset/test/images/"")\n    train_X, train_y, test_X, test_y = load_cifar10()\n    xs = test_X / 255\n    ts = np.zeros([len(test_y), num_classes], dtype=np.float32)\n    ts[np.arange(len(test_y)), test_y] = 1\n    \n    labels = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"",\n              ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]\n    accuracy = 0.\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(xs)):\n            x = xs[i]\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n            \n            if t.argmax() == pred.argmax():\n                accuracy += 1\n\n            print(""gt: {}, >> pred labels: {}, (prob: {:.4f})"".format(labels[t.argmax()], labels[pred.argmax()], pred.max()))\n    \n    accuracy /= len(xs)\n    print(""accuracy: "", accuracy)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/answers/res50_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res50(x, keep_prob):\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = slim.conv2d(x, f_1, [1, 1], stride=stride, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = slim.conv2d(res_x, f_1, [3, 3], stride=1, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = slim.conv2d(res_x, out_f, [1, 1], stride=1, padding=""SAME"", activation_fn=None)\n        res_x = slim.batch_norm(res_x)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = slim.conv2d(x, out_f, [1, 1], stride=1, padding=""SAME"", activation_fn=None)\n            x = slim.batch_norm(x)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = slim.max_pool2d(x, [2, 2], stride=stride, padding=""SAME"")\n        \n        x = tf.add(res_x, x)\n\n        return x\n\n    \n    x = slim.conv2d(x, 64, [7, 7], stride=2, padding=""SAME"", activation_fn=None)\n    x = slim.batch_norm(x)\n    x = tf.nn.relu(x)\n    \n    x = slim.max_pool2d(x, [3, 3], stride=2, padding=\'SAME\')\n\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    x = slim.avg_pool2d(x, [img_height // 32, img_width // 32], stride=1, padding=\'VALID\')\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h * w * c])\n    x = slim.fully_connected(x, num_classes)\n    \n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res50(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res50(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/alexnet_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 227, 227\nGPU = -1\n\nclass AlexNet(chainer.Chain):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 96, ksize=11, pad=0, stride=4, nobias=False)\n            self.conv2 = L.Convolution2D(None, 256, ksize=5, pad=1, nobias=False)\n            self.conv3 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv4 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv5 = L.Convolution2D(None, 256, ksize=3, pad=1, nobias=False)\n            self.fc1 = L.Linear(None, 4096, nobias=False)\n            self.fc2 = L.Linear(None, 4096, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        \n        x = F.relu(self.fc1(x))\n        x = F.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = AlexNet()\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = AlexNet()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/bn_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = -1\n\nclass VGG16(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(VGG16, self).__init__()\n        with self.init_scope():\n            # block conv1\n            self.conv1 = chainer.Sequential()\n            for i in range(2):\n                self.conv1.append(L.Convolution2D(None, 64, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv1.append(F.relu)\n                self.conv1.append(L.BatchNormalization(64))\n                \n            # block conv2\n            self.conv2 = chainer.Sequential()\n            for i in range(2):\n                self.conv2.append(L.Convolution2D(None, 128, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv2.append(F.relu)\n                self.conv2.append(L.BatchNormalization(128))\n                \n            # block conv3\n            self.conv3 = chainer.Sequential()\n            for i in range(3):\n                self.conv3.append(L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv3.append(F.relu)\n                self.conv3.append(L.BatchNormalization(256))\n                \n            # block conv4\n            self.conv4 = chainer.Sequential()\n            for i in range(3):\n                self.conv4.append(L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv4.append(F.relu)\n                self.conv4.append(L.BatchNormalization(512))\n                \n            # block conv1\n            self.conv5 = chainer.Sequential()\n            for i in range(3):\n                self.conv5.append(L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv5.append(F.relu)\n                self.conv5.append(L.BatchNormalization(512))\n            \n            self.fc1 = L.Linear(None, 4096, nobias=False)\n            self.fc2 = L.Linear(None, 4096, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        \n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    # model\n    model = VGG16(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = VGG16(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/gap_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = -1\n\nclass GAP(chainer.Chain):\n    def __init__(self):\n        super(GAP, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 96, ksize=7, pad=0, stride=2, nobias=False)\n            self.conv2 = L.Convolution2D(None, 256, ksize=5, pad=1, stride=2, nobias=False)\n            self.conv3 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv4 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv5 = L.Convolution2D(None, 256, ksize=3, pad=1, nobias=False)\n            self.conv_out = L.Convolution2D(None, num_classes, ksize=1, pad=1, nobias=False)\n            \n\n    def __call__(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        # GAP\n        x = self.conv_out(x)\n        x = F.average(x, axis=(2,3))\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = GAP()\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = GAP()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/googlenetv1_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = -1\n\n\nclass InceptionModule(chainer.Chain):\n    def __init__(self, f_1, f_2_1, f_2_2, f_3_1, f_3_2, f_4_2):\n        super(InceptionModule, self).__init__()\n\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, f_1, ksize=1, pad=0, stride=1)\n\n            self.conv2_1 = L.Convolution2D(None, f_2_1, ksize=1, pad=0, stride=1)\n            self.conv2_2 = L.Convolution2D(None, f_2_2, ksize=3, pad=1, stride=1)\n            \n            self.conv3_1 = L.Convolution2D(None, f_3_1, ksize=1, pad=0, stride=1)\n            self.conv3_2 = L.Convolution2D(None, f_3_2, ksize=5, pad=2, stride=1)\n            \n            self.conv4_2 = L.Convolution2D(None, f_4_2, ksize=1, pad=0, stride=1)\n\n    def __call__(self, x):\n        x1 = F.relu(self.conv1(x))\n\n        x2 = F.relu(self.conv2_1(x))\n        x2 = F.relu(self.conv2_2(x2))\n\n        x3 = F.relu(self.conv3_1(x))\n        x3 = F.relu(self.conv3_2(x3))\n\n        x4 = F.max_pooling_2d(x, ksize=3, pad=1, stride=1)\n        x4 = F.relu(self.conv4_2(x4))\n\n        x = F.concat([x1, x2, x3, x4], axis=1)\n\n        return x\n\n\nclass GoogLeNetv1(chainer.Chain):\n    def __init__(self):\n        self.train = train\n        super(GoogLeNetv1, self).__init__()\n\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 64, ksize=7, pad=0, stride=1)\n            self.conv2_1 = L.Convolution2D(None, 64, ksize=1, pad=0, stride=1)\n            self.conv2_2 = L.Convolution2D(None, 192, ksize=3, pad=1, stride=1)\n\n            self.inception3a = InceptionModule(64, 96, 128, 16, 32, 32)\n            self.inception3b = InceptionModule(128, 128, 192, 32, 96, 64)\n\n            self.inception4a = InceptionModule(192, 96, 208, 16, 48, 64)\n            self.inception4b = InceptionModule(160, 112, 224, 24, 64, 64)\n            self.inception4c = InceptionModule(128, 128, 256, 24, 64, 64)\n            self.inception4d = InceptionModule(112, 144, 288, 32, 64, 64)\n            self.inception4e = InceptionModule(256, 160, 320, 32, 128 ,128)\n\n            self.inception5a = InceptionModule(256, 160, 320, 32, 128, 128)\n            self.inception5b = InceptionModule(384, 192, 384, 48, 128, 128)\n\n            self.linear = L.Linear(None, num_classes)\n\n            self.aux1_conv1 = L.Convolution2D(None, 128, ksize=1, pad=0, stride=1)\n            self.aux1_linear1 = L.Linear(None, 1024)\n            self.aux1_linear2 = L.Linear(None, num_classes)\n\n            self.aux2_conv1 = L.Convolution2D(None, 128, ksize=1, pad=0, stride=1)\n            self.aux2_linear1 = L.Linear(None, 1024)\n            self.aux2_linear2 = L.Linear(None, num_classes)\n            \n\n    def __call__(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pooling_2d(x, ksize=3, pad=1, stride=2)\n        \n        x = F.relu(self.conv2_1(x))\n        x = F.relu(self.conv2_2(x))\n        x = F.max_pooling_2d(x, ksize=3, pad=1, stride=2)\n\n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = F.max_pooling_2d(x, ksize=3, pad=1, stride=2)\n\n        x = self.inception4a(x)\n\n        x_aux1 = F.average_pooling_2d(x, ksize=5, pad=2, stride=1)\n        x_aux1 = F.relu(self.aux1_conv1(x_aux1))\n        x_aux1 = F.relu(self.aux1_linear1(x_aux1))\n        x_aux1 = F.dropout(x_aux1, ratio=0.7)\n        x_aux1 = self.aux1_linear2(x_aux1)\n\n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n\n        x_aux2 = F.average_pooling_2d(x, ksize=5, pad=2, stride=1)\n        x_aux2 = F.relu(self.aux2_conv1(x_aux2))\n        x_aux2 = F.relu(self.aux2_linear1(x_aux2))\n        x_aux2 = F.dropout(x_aux2, ratio=0.7)\n        x_aux2 = self.aux2_linear2(x_aux2)\n\n        x = self.inception4e(x)\n        x = F.max_pooling_2d(x, ksize=3, pad=1, stride=2)\n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        x = F.average_pooling_2d(x, ksize=7, pad=0, stride=1)\n        x = self.linear(x)\n        \n        return x, x_aux1, x_aux2\n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # model\n    model = GoogLeNetv1()\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.001, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y, y_aux1, y_aux2 = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        loss_aux1 = F.softmax_cross_entropy(y_aux1, t)\n        loss_aux2 = F.softmax_cross_entropy(y_aux2, t)\n\n        loss = loss_aux1 + loss_aux2 + loss\n        \n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n    \n# test\ndef test():\n    model = GoogLeNetv1()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred, _, _ = model(x)\n        pred = F.softmax(pred)\n                \n        pred = pred[0].data\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/lenet_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nGPU = -1\n\nclass LeNet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(LeNet, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 6, ksize=5, pad=0, nobias=False)\n            self.conv2 = L.Convolution2D(None, 16, ksize=5, pad=0, nobias=False)\n            self.fc1 = L.Linear(None, 120, nobias=False)\n            self.fc2 = L.Linear(None, 64, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        x = self.conv1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.sigmoid(x)\n        x = self.conv2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.sigmoid(x)\n\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = LeNet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = LeNet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/nin_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nGPU = -1\n\nclass NIN(chainer.Chain):\n    def __init__(self):\n        super(NIN, self).__init__()\n        with self.init_scope():\n            self.conv1= L.Convolution2D(None, 192, ksize=5, pad=2, stride=1, nobias=False)\n            self.cccp1 = L.Convolution2D(None, 160, ksize=1, pad=0, stride=1, nobias=False)\n            self.cccp2 = L.Convolution2D(None, 96, ksize=1, pad=0, stride=1, nobias=False)\n            self.conv2 = L.Convolution2D(None, 192, ksize=5, pad=2, stride=1, nobias=False)\n            self.cccp3 = L.Convolution2D(None, 192, ksize=1, pad=0, stride=1, nobias=False)\n            self.cccp4 = L.Convolution2D(None, 192, ksize=1, pad=0, stride=1, nobias=False)\n            self.conv3 = L.Convolution2D(None, 192, ksize=3, pad=1, stride=1, nobias=False)\n            self.cccp5 = L.Convolution2D(None, 192, ksize=1, pad=0, stride=1, nobias=False)\n            self.out = L.Convolution2D(None, num_classes, ksize=1, pad=0, stride=1, nobias=False)\n\n    def __call__(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.cccp1(x))\n        x = F.relu(self.cccp2(x))\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.dropout(x)\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.cccp3(x))\n        x = F.relu(self.cccp4(x))\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.dropout(x)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.cccp5(x))\n        x = self.out(x)\n        x = F.average(x, axis=(2,3))\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = NIN()\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = NIN()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/res50_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = -1\n\n\nclass ResBlock(chainer.Chain):\n    def __init__(self, in_f, f_1, out_f, stride=1):\n        super(ResBlock, self).__init__()\n\n        self.stride = stride\n        self.fit_dim = False\n        \n        with self.init_scope():\n            self.block = chainer.Sequential(\n                L.Convolution2D(None, f_1, ksize=1, pad=0, stride=stride),\n                L.BatchNormalization(f_1),\n                F.relu,\n                L.Convolution2D(None, f_1, ksize=3, pad=1, stride=1),\n                L.BatchNormalization(f_1),\n                F.relu,\n                L.Convolution2D(None, out_f, ksize=3, pad=1, stride=1),\n                L.BatchNormalization(out_f),\n                F.relu\n                )\n\n            if in_f != out_f:\n                self.fit_conv = L.Convolution2D(None, out_f, ksize=1, pad=0, stride=1)\n                self.fit_bn = L.BatchNormalization(out_f)\n                self.fit_dim = True\n                \n\n    def __call__(self, x):\n        res_x = self.block(x)\n\n        if self.fit_dim:\n            x = self.fit_conv(x)\n            x = self.fit_bn(x)\n            x = F.relu(x)\n\n        if self.stride == 2:\n            x = F.max_pooling_2d(x, ksize=2, pad=0, stride=self.stride)\n\n        x = F.add(res_x, x)\n        x = F.relu(x)\n        \n        return x\n\n\nclass Res50(chainer.Chain):\n    def __init__(self):\n        self.train = train\n        super(Res50, self).__init__()\n\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 64, ksize=7, pad=3, stride=2)\n            self.bn1 = L.BatchNormalization(64)\n\n            self.resblock2_1 = ResBlock(64, 64, 256)\n            self.resblock2_2 = ResBlock(256, 64, 256)\n            self.resblock2_3 = ResBlock(256, 64, 256)\n\n            self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n            self.resblock3_2 = ResBlock(512, 128, 512)\n            self.resblock3_3 = ResBlock(512, 128, 512)\n            self.resblock3_4 = ResBlock(512, 128, 512)\n\n            self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n            self.resblock4_2 = ResBlock(1024, 256, 1024)\n            self.resblock4_3 = ResBlock(1024, 256, 1024)\n            self.resblock4_4 = ResBlock(1024, 256, 1024)\n            self.resblock4_5 = ResBlock(1024, 256, 1024)\n            self.resblock4_6 = ResBlock(1024, 256, 1024)\n            \n            self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n            self.resblock5_2 = ResBlock(2048, 512, 2048)\n            self.resblock5_3 = ResBlock(2048, 512, 2048)  \n\n            self.linear = L.Linear(None, num_classes)\n            \n\n    def __call__(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pooling_2d(x, ksize=3, pad=1, stride=2)\n        \n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n        x = self.resblock4_3(x)\n        x = self.resblock4_4(x)\n        x = self.resblock4_5(x)\n        x = self.resblock4_6(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.average_pooling_2d(x, ksize=[img_height//32, img_width//32], pad=0, stride=1)\n        x = self.linear(x)\n        \n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # model\n    model = Res50()\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.001, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        \n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n    \n# test\ndef test():\n    model = Res50()\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x)\n        pred = F.softmax(pred)\n                \n        pred = pred[0].data\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/vgg16_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = -1\n\nclass Mynet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            # block conv1\n            self.conv1 = chainer.Sequential()\n            for i in range(2):\n                self.conv1.append(L.Convolution2D(None, 64, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv1.append(F.relu)\n                \n            # block conv2\n            self.conv2 = chainer.Sequential()\n            for i in range(2):\n                self.conv2.append(L.Convolution2D(None, 128, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv2.append(F.relu)\n                \n            # block conv3\n            self.conv3 = chainer.Sequential()\n            for i in range(3):\n                self.conv3.append(L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv3.append(F.relu)\n                \n            # block conv4\n            self.conv4 = chainer.Sequential()\n            for i in range(3):\n                self.conv4.append(L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv4.append(F.relu)\n                \n            # block conv1\n            self.conv5 = chainer.Sequential()\n            for i in range(3):\n                self.conv5.append(L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False))\n                self.conv5.append(F.relu)\n            \n            self.fc1 = L.Linear(None, 4096, nobias=False)\n            self.fc2 = L.Linear(None, 4096, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        \n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/vgg19_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = -1\n\nclass VGG19(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(VGG19, self).__init__()\n        with self.init_scope():\n            self.conv1_1 = L.Convolution2D(None, 64, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv1_2 = L.Convolution2D(None, 64, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv2_1 = L.Convolution2D(None, 128, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv2_2 = L.Convolution2D(None, 128, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv3_1 = L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv3_2 = L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv3_3 = L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv3_4 = L.Convolution2D(None, 256, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv4_1 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv4_2 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv4_3 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv4_4 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv5_1 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv5_2 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv5_3 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.conv5_4 = L.Convolution2D(None, 512, ksize=3, pad=1, stride=1, nobias=False)\n            self.fc1 = L.Linear(None, 4096, nobias=False)\n            self.fc2 = L.Linear(None, 4096, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = F.relu(self.conv1_2(x))\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv2_1(x))\n        x = F.relu(self.conv2_2(x))\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv3_1(x))\n        x = F.relu(self.conv3_2(x))\n        x = F.relu(self.conv3_3(x))\n        x = F.relu(self.conv3_4(x))\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv4_1(x))\n        x = F.relu(self.conv4_2(x))\n        x = F.relu(self.conv4_3(x))\n        x = F.relu(self.conv4_4(x))\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n        x = F.relu(self.conv5_1(x))\n        x = F.relu(self.conv5_2(x))\n        x = F.relu(self.conv5_3(x))\n        x = F.relu(self.conv5_4(x))\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = VGG19(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = VGG19(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_chainer/zfnet_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = -1\n\nclass ZFNet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(ZFNet, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 96, ksize=7, pad=0, stride=2, nobias=False)\n            self.conv2 = L.Convolution2D(None, 256, ksize=5, pad=1, stride=2, nobias=False)\n            self.conv3 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv4 = L.Convolution2D(None, 384, ksize=3, pad=1, nobias=False)\n            self.conv5 = L.Convolution2D(None, 256, ksize=3, pad=1, nobias=False)\n            self.fc1 = L.Linear(None, 4096, nobias=False)\n            self.fc2 = L.Linear(None, 4096, nobias=False)\n            self.fc_out = L.Linear(None, num_classes, nobias=False)\n\n    def __call__(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.local_response_normalization(x)\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.max_pooling_2d(x, ksize=3, stride=2)\n        \n        x = F.relu(self.fc1(x))\n        x = F.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x)\n        x = self.fc_out(x)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = ZFNet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, _ = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = ZFNet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        x = np.expand_dims(x, axis=0)\n        \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n                \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/alexnet_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 227, 227\n\ndef AlexNet():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(96, (11, 11), padding=\'valid\', strides=4, activation=\'relu\', name=\'conv1\')(inputs)\n    x = MaxPooling2D((3, 3), strides=2,  padding=\'same\')(x)\n    x = Conv2D(256, (5, 5), padding=\'valid\', activation=\'relu\', name=\'conv2\')(x)\n    x = keras.layers.ZeroPadding2D(1)(x)\n    x = MaxPooling2D((3, 3), strides=2, padding=\'same\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv3\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv4\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv5\')(x)\n    \n    x = Flatten()(x)\n    x = Dense(4096, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4096, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = AlexNet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = AlexNet()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/bn_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\ndef VGG16():\n    inputs = Input((img_height, img_width, 3))\n    x = inputs\n    # block conv1\n    for i in range(2):\n        x = Conv2D(64, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv1_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv2\n    for i in range(2):\n        x = Conv2D(128, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv2_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv3\n    for i in range(3):\n        x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv4\n    for i in range(3):\n        x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv5\n    for i in range(3):\n        x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_{}\'.format(i))(x)\n        x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    x = Flatten()(x)\n    x = Dense(4096, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4096, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = VGG16()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    print(model.summary())\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = VGG16()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/gap_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 224, 224\n\ndef GAP():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(96, (7, 7), padding=\'valid\', strides=2, activation=\'relu\', name=\'conv1\')(inputs)\n    x = MaxPooling2D((3, 3), strides=2,  padding=\'same\')(x)\n    x = Conv2D(256, (5, 5), padding=\'valid\', strides=2, activation=\'relu\', name=\'conv2\')(x)\n    x = keras.layers.ZeroPadding2D(1)(x)\n    x = MaxPooling2D((3, 3), strides=2, padding=\'same\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv3\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv4\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv5\')(x)\n    x = MaxPooling2D((3, 3), strides=2, padding=\'same\')(x)\n    # GAP\n    x = Conv2D(num_classes, (1, 1), padding=\'same\', activation=None, name=\'out\')(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = Activation(\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = GAP()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = GAP()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/googlenetv1_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\n\ndef GoogLeNetv1():\n\n    def inception_module(x, f_1, f_2_1, f_2_2, f_3_1, f_3_2, f_4_2):\n        x1 = Conv2D(f_1, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x)\n\n        x2_1 = Conv2D(f_2_1, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x)\n        x2_2 = Conv2D(f_2_2, [3, 3], strides=1, padding=\'same\', activation=\'relu\')(x2_1)\n\n        x3_1 = Conv2D(f_3_1, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x)\n        x3_2 = Conv2D(f_3_2, [5, 5], strides=1, padding=\'same\', activation=\'relu\')(x3_1)\n\n        x4_1 = MaxPooling2D([3, 3], strides=1, padding=\'same\')(x)\n        x4_2 = Conv2D(f_4_2, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x4_1)\n\n        x = concatenate([x1, x2_2, x3_2, x4_2])\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, 3))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'valid\', activation=\'relu\')(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = Conv2D(64, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x)\n    x = Conv2D(192, [3, 3], strides=1, padding=\'same\', activation=\'relu\')(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    # inception 3a, 3b\n    x = inception_module(x, 64, 96, 128, 16, 32, 32)\n    x = inception_module(x, 128, 128, 192, 32, 96, 64)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    # inception 4a\n    x = inception_module(x, 192, 96, 208, 16, 48, 64)\n\n    # auxiliary loss1\n    x_aux1 = AveragePooling2D([5, 5], strides=1, padding=\'same\')(x)\n    x_aux1 = Conv2D(128, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x_aux1)\n    x_aux1 = Flatten()(x_aux1)\n    x_aux1 = Dense(1024, activation=\'relu\')(x_aux1)\n    x_aux1 = Dropout(0.7)(x_aux1)\n    x_aux1 = Dense(num_classes, activation=\'softmax\', name=\'out_aux1\')(x_aux1)\n\n    # inception 4b, 4c, 4d\n    x = inception_module(x, 160, 112, 224, 24, 64, 64)\n    x = inception_module(x, 128, 128, 256, 24, 64, 64)\n    x = inception_module(x, 112, 144, 288, 32, 64, 64)\n\n    # auxiliary loss2\n    x_aux2 = AveragePooling2D([5, 5], strides=1, padding=\'same\')(x)\n    x_aux2 = Conv2D(128, [1, 1], strides=1, padding=\'same\', activation=\'relu\')(x_aux2)\n    x_aux2 = Flatten()(x_aux2)\n    x_aux2 = Dense(1024, activation=\'relu\')(x_aux2)\n    x_aux2 = Dropout(0.7)(x_aux2)\n    x_aux2 = Dense(num_classes, activation=\'softmax\', name=\'out_aux2\')(x_aux2)\n    \n\n    # inception 4e, 5a, 5b\n    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n    x = inception_module(x, 384, 192, 384, 48, 128, 128)\n\n    x = AveragePooling2D([7, 7], strides=1, padding=\'same\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=\'out\')(x)\n\n    model = Model(inputs=inputs, outputs=[x, x_aux1, x_aux2])\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = GoogLeNetv1()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss_total, loss, loss_aux1, loss_aux2, acc, acc_aux1, acc_aux2 = \\\n                model.train_on_batch(x=x, y={\'out\':t, \'out_aux1\':t, \'out_aux2\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "",loss >>"", loss_total, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = GoogLeNetv1()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/lenet_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 32, 32\n\ndef LeNet():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(6, (5, 5), padding=\'valid\', activation=None, name=\'conv1\')(inputs)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Activation(\'sigmoid\')(x)\n    x = Conv2D(16, (5, 5), padding=\'valid\', activation=None, name=\'conv2\')(x)\n    x = MaxPooling2D((2,2), padding=\'same\')(x)\n    x = Activation(\'sigmoid\')(x)\n    \n    x = Flatten()(x)\n    x = Dense(120, name=\'dense1\', activation=None)(x)\n    x = Dense(64, name=\'dense2\', activation=None)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = LeNet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = LeNet()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/nin_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, AveragePooling2D\n\nnum_classes = 2\nimg_height, img_width = 128, 128\n\ndef NIN():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(192, (5, 5), padding=\'same\', strides=1, activation=\'relu\', name=\'conv1\')(inputs)\n    x = Conv2D(160, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp1\')(x)\n    x = Conv2D(96, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp2\')(x)\n    x = MaxPooling2D((3, 3), strides=2,  padding=\'same\')(x)\n    x = Dropout(0.5)(x)\n    x = Conv2D(192, (5, 5), padding=\'same\', strides=1, activation=\'relu\', name=\'conv2\')(x)\n    x = Conv2D(192, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp3\')(x)\n    x = Conv2D(192, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp4\')(x)\n    x = AveragePooling2D((3, 3), strides=2,  padding=\'same\')(x)\n    x = Dropout(0.5)(x)\n    x = Conv2D(192, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3\')(x)\n    x = Conv2D(192, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp5\')(x)\n    x = Conv2D(num_classes, (1, 1), padding=\'same\', strides=1, activation=\'relu\', name=\'cccp6\')(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = Activation(\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = NIN()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = NIN()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/res101_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add\n\nnum_classes = 2\nimg_height, img_width = 96, 96\nchannel = 3\n\n\ndef Res101():\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1, name=""res""):\n        res_x = Conv2D(f_1, [1, 1], strides=stride, padding=\'same\', activation=None, name=name+""_conv1"")(x)\n        res_x = BatchNormalization(name=name+""_bn1"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(f_1, [3, 3], strides=1, padding=\'same\', activation=None, name=name+""_conv2"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn2"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [1, 1], strides=1, padding=\'same\', activation=None, name=name+""_conv3"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn3"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None, name=name+""_conv_sc"")(x)\n            x = BatchNormalization(name=name+""_bn_sc"")(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n        x = Activation(""relu"")(x)\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None, name=""conv1"")(x)\n    x = BatchNormalization(name=""bn1"")(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = ResBlock(x, 64, 64, 256, name=""res2_1"")\n    x = ResBlock(x, 256, 64, 256, name=""res2_2"")\n    x = ResBlock(x, 256, 64, 256, name=""res2_3"")\n\n    x = ResBlock(x, 256, 128, 512, stride=2, name=""res3_1"")\n    x = ResBlock(x, 512, 128, 512, name=""res3_2"")\n    x = ResBlock(x, 512, 128, 512, name=""res3_3"")\n    x = ResBlock(x, 512, 128, 512, name=""res3_4"")\n\n    x = ResBlock(x, 512, 256, 1024, stride=2, name=""res4_1"")\n    for i in range(22):\n        x = ResBlock(x, 1024, 256, 1024, name=""res4_{}"".format(i+2))\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2, name=""res5_1"")\n    x = ResBlock(x, 2048, 256, 2048, name=""res5_2"")\n    x = ResBlock(x, 2048, 256, 2048, name=""res5_3"")\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=""fc"")(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Res101()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Res101()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/res152_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add\n\nnum_classes = 2\nimg_height, img_width = 96, 96\nchannel = 3\n\n\ndef Res152():\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1, name=""res""):\n        res_x = Conv2D(f_1, [1, 1], strides=stride, padding=\'same\', activation=None, name=name+""_conv1"")(x)\n        res_x = BatchNormalization(name=name+""_bn1"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(f_1, [3, 3], strides=1, padding=\'same\', activation=None, name=name+""_conv2"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn2"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [1, 1], strides=1, padding=\'same\', activation=None, name=name+""_conv3"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn3"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None, name=name+""_conv_sc"")(x)\n            x = BatchNormalization(name=name+""_bn_sc"")(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n        x = Activation(""relu"")(x)\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None, name=""conv1"")(x)\n    x = BatchNormalization(name=""bn1"")(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = ResBlock(x, 64, 64, 256, name=""res2_1"")\n    x = ResBlock(x, 256, 64, 256, name=""res2_2"")\n    x = ResBlock(x, 256, 64, 256, name=""res2_3"")\n\n    x = ResBlock(x, 256, 128, 512, stride=2, name=""res3_1"")\n    for i in range(7):\n        x = ResBlock(x, 512, 128, 512, name=""res3_{}"".format(i+2))\n\n    x = ResBlock(x, 512, 256, 1024, stride=2, name=""res4_1"")\n    for i in range(35):\n        x = ResBlock(x, 1024, 256, 1024, name=""res4_{}"".format(i+2))\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2, name=""res5_1"")\n    x = ResBlock(x, 2048, 256, 2048, name=""res5_2"")\n    x = ResBlock(x, 2048, 256, 2048, name=""res5_3"")\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=""fc"")(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Res152()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Res152()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/res18_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\n\n\ndef Res18():\n\n    def ResBlock(x, in_f, out_f, stride=1, name=""res""):\n        res_x = Conv2D(out_f, [3, 3], strides=stride, padding=\'same\', activation=None, name=name+""_conv1"")(x)\n        res_x = BatchNormalization(name=name+""_bn1"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [3, 3], strides=1, padding=\'same\', activation=None, name=name+""_conv2"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn2"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None, name=name+""_conv_sc"")(x)\n            x = BatchNormalization(name=name+""_bn_sc"")(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n        x = Activation(""relu"")(x)\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None, name=""conv1"")(x)\n    x = BatchNormalization(name=""bn1"")(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = ResBlock(x, 64, 64, name=""res2_1"")\n    x = ResBlock(x, 64, 64, name=""res2_2"")\n\n    x = ResBlock(x, 64, 128, stride=2, name=""res3_1"")\n    x = ResBlock(x, 128, 128, name=""res3_2"")\n\n    x = ResBlock(x, 128, 256, stride=2, name=""res4_1"")\n    x = ResBlock(x, 256, 256, name=""res4_2"")\n\n    x = ResBlock(x, 256, 512, stride=2, name=""res5_1"")\n    x = ResBlock(x, 512, 512, name=""res5_2"")\n\n    x = AveragePooling2D([7, 7], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=""fc"")(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Res18()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Res18()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/res34_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\n\n\ndef Res34():\n\n    def ResBlock(x, in_f, out_f, stride=1, name=""res""):\n        res_x = Conv2D(out_f, [3, 3], strides=stride, padding=\'same\', activation=None, name=name+""_conv1"")(x)\n        res_x = BatchNormalization(name=name+""_bn1"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [3, 3], strides=1, padding=\'same\', activation=None, name=name+""_conv2"")(res_x)\n        res_x = BatchNormalization(name=name+""_bn2"")(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None, name=name+""_conv_sc"")(x)\n            x = BatchNormalization(name=name+""_bn_sc"")(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n        x = Activation(""relu"")(x)\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None, name=""conv1"")(x)\n    x = BatchNormalization(name=""bn1"")(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = ResBlock(x, 64, 64, name=""res2_1"")\n    x = ResBlock(x, 64, 64, name=""res2_2"")\n    x = ResBlock(x, 64, 64, name=""res2_3"")\n\n    x = ResBlock(x, 64, 128, stride=2, name=""res3_1"")\n    x = ResBlock(x, 128, 128, name=""res3_2"")\n    x = ResBlock(x, 128, 128, name=""res3_3"")\n    x = ResBlock(x, 128, 128, name=""res3_4"")\n\n    x = ResBlock(x, 128, 256, stride=2, name=""res4_1"")\n    x = ResBlock(x, 256, 256, name=""res4_2"")\n    x = ResBlock(x, 256, 256, name=""res4_3"")\n    x = ResBlock(x, 256, 256, name=""res4_4"")\n    x = ResBlock(x, 256, 256, name=""res4_5"")\n    x = ResBlock(x, 256, 256, name=""res4_6"")\n\n    x = ResBlock(x, 256, 512, stride=2, name=""res5_1"")\n    x = ResBlock(x, 512, 512, name=""res5_2"")\n    x = ResBlock(x, 512, 512, name=""res5_3"")\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=""fc"")(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Res34()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Res34()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/res50_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\n\ndef Res50():\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = Conv2D(f_1, [1, 1], strides=stride, padding=\'same\', activation=None)(x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(f_1, [3, 3], strides=1, padding=\'same\', activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [1, 1], strides=1, padding=\'same\', activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None)(x)\n            x = BatchNormalization()(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=\'out\')(x)\n\n    model = Model(inputs=inputs, outputs=[x])\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Res50()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Res50()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/resNeXt101_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add, SeparableConv2D\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nchannel = 3\n\ndef ResNeXt101():\n\n    def Block(x, in_f, f_1, out_f, stride=1, cardinality):\n        res_x = Conv2D(f_1, [1, 1], strides=stride, padding=\'same\', activation=None)(x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        multiplier = f_1 // cardinality\n        res_x = SeparableConv2D(f_1, [3, 3], strides=1, padding=\'same\', depth_multiplier=multiplier, activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [1, 1], strides=1, padding=\'same\', activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None)(x)\n            x = BatchNormalization()(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = Block(x, 64, 64, 256)\n    x = Block(x, 256, 64, 256)\n    x = Block(x, 256, 64, 256)\n\n    x = Block(x, 256, 128, 512, stride=2)\n    x = Block(x, 512, 128, 512)\n    x = Block(x, 512, 128, 512)\n    x = Block(x, 512, 128, 512)\n\n    x = Block(x, 512, 256, 1024, stride=2)\n    for i in range(22):\n        x = Block(x, 1024, 256, 1024)\n\n    x = Block(x, 1024, 512, 2048, stride=2)\n    x = Block(x, 2048, 256, 2048)\n    x = Block(x, 2048, 256, 2048)\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=\'out\')(x)\n\n    model = Model(inputs=inputs, outputs=[x])\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = ResNeXt101()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = ResNeXt101()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/resNeXt50_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add, SeparableConv2D\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nchannel = 3\n\ndef ResNeXt50():\n\n    def Block(x, in_f, f_1, out_f, stride=1, cardinality):\n        res_x = Conv2D(f_1, [1, 1], strides=stride, padding=\'same\', activation=None)(x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        multiplier = f_1 // cardinality\n        res_x = SeparableConv2D(f_1, [3, 3], strides=1, padding=\'same\', depth_multiplier=multiplier, activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        res_x = Conv2D(out_f, [1, 1], strides=1, padding=\'same\', activation=None)(res_x)\n        res_x = BatchNormalization()(res_x)\n        res_x = Activation(""relu"")(res_x)\n\n        if in_f != out_f:\n            x = Conv2D(out_f, [1, 1], strides=1, padding=""same"", activation=None)(x)\n            x = BatchNormalization()(x)\n            x = Activation(""relu"")(x)\n\n        if stride == 2:\n            x = MaxPooling2D([2, 2], strides=2, padding=""same"")(x)\n        \n        x = Add()([res_x, x])\n\n        return x\n        \n    \n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    x = Conv2D(64, [7, 7], strides=2, padding=\'same\', activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n\n    x = Block(x, 64, 64, 256)\n    x = Block(x, 256, 64, 256)\n    x = Block(x, 256, 64, 256)\n\n    x = Block(x, 256, 128, 512, stride=2)\n    x = Block(x, 512, 128, 512)\n    x = Block(x, 512, 128, 512)\n    x = Block(x, 512, 128, 512)\n\n    x = Block(x, 512, 256, 1024, stride=2)\n    x = Block(x, 1024, 256, 1024)\n    x = Block(x, 1024, 256, 1024)\n    x = Block(x, 1024, 256, 1024)\n    x = Block(x, 1024, 256, 1024)\n    x = Block(x, 1024, 256, 1024)\n\n    x = Block(x, 1024, 512, 2048, stride=2)\n    x = Block(x, 2048, 256, 2048)\n    x = Block(x, 2048, 256, 2048)\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\', name=\'out\')(x)\n\n    model = Model(inputs=inputs, outputs=[x])\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = ResNeXt50()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = ResNeXt50()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/vgg16_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 224, 224\n\ndef Mynet():\n    inputs = Input((img_height, img_width, 3))\n    x = inputs\n    # block conv1\n    for i in range(2):\n        x = Conv2D(64, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv1_{}\'.format(i+1))(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv2\n    for i in range(2):\n        x = Conv2D(128, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv2_{}\'.format(i+1))(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv3\n    for i in range(3):\n        x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_{}\'.format(i+1))(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv4\n    for i in range(3):\n        x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_{}\'.format(i+1))(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    # block conv5\n    for i in range(3):\n        x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_{}\'.format(i))(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    \n    x = Flatten()(x)\n    x = Dense(4096, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4096, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/vgg19_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 224, 224\n\ndef VGG19():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(64, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv1_1\')(inputs)\n    x = Conv2D(64, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv1_2\')(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    x = Conv2D(128, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv2_1\')(x)\n    x = Conv2D(128, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv2_2\')(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_1\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_2\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_3\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv3_4\')(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_1\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_2\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_3\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv4_4\')(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_1\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_2\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_3\')(x)\n    x = Conv2D(512, (3, 3), padding=\'same\', strides=1, activation=\'relu\', name=\'conv5_4\')(x)\n    x = MaxPooling2D((2, 2), strides=2,  padding=\'same\')(x)\n    x = Flatten()(x)\n    x = Dense(4096, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4096, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = VGG19()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = VGG19()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/xception_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport copy\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, concatenate, AveragePooling2D, Add, SeparableConv2D\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nchannel = 3\n\n\ndef Xception():\n    \n    def Entry_flow(x, dim):\n        x = SeparableConv2D(dim, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n        x = BatchNormalization()(x)\n        x = Activation(""relu"")(x)\n        x = SeparableConv2D(dim, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n        return x\n        \n\n    def Middle_flow(x, dim=728):\n        x_sc = x\n        \n        for _ in range(3):\n            x = Activation(""relu"")(x)\n            x = SeparableConv2D(dim, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n            x = BatchNormalization()(x)\n            \n        x = Add()([x, x_sc])\n        \n        return x\n\n    inputs = Input((img_height, img_width, channel))\n    x = inputs\n    \n    # Entry flow\n    x = Conv2D(32, [3, 3], strides=2, padding=\'same\', activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    x = Conv2D(64, [3, 3], padding=\'same\', activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    \n    x_sc = Conv2D(128, [1,1], strides=2, padding=""same"", activation=None)(x)\n    x_sc = BatchNormalization()(x_sc)\n    x = Entry_flow(x, 128)\n    x = Add()([x, x_sc])\n    \n    x_sc = Conv2D(256, [1,1], strides=2, padding=""same"", activation=None)(x_sc)\n    x_sc = BatchNormalization()(x_sc)\n    x = Activation(""relu"")(x)\n    x = Entry_flow(x, 256)\n    x = Add()([x, x_sc])\n    x = Activation(""relu"")(x)\n    \n    x_sc = Conv2D(728, [1,1], strides=2, padding=""same"", activation=None)(x)\n    x_sc = BatchNormalization()(x_sc)\n    x = Activation(""relu"")(x)\n    x = Entry_flow(x, 728)\n    x = Add()([x, x_sc])\n    \n    # Middle flow\n    for _ in range(8):\n        x = Middle_flow(x)\n    \n    # Exit flow\n    x_sc = Conv2D(1024, [1, 1], strides=2, padding=""same"", activation=None)(x)\n    x_sc = BatchNormalization()(x_sc)\n    x = Activation(""relu"")(x)\n    x = SeparableConv2D(728, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    x = SeparableConv2D(1024, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D([3, 3], strides=2, padding=\'same\')(x)\n    x = Add()([x, x_sc])\n    \n    x = SeparableConv2D(1536, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n    \n    x = SeparableConv2D(2048, [3, 3], strides=1, padding=\'same\', depth_multiplier=1, activation=None)(x)\n    x = BatchNormalization()(x)\n    x = Activation(""relu"")(x)\n\n    x = AveragePooling2D([img_height // 32, img_width // 32], strides=1, padding=\'valid\')(x)\n    x = Flatten()(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n    \n\n\nCLS = [\'akahara\', \'madara\']\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Xception()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y={\'out\':t})\n\n        if (i+1) % 10 == 0:\n            print(""iter >>"", i+1, "", loss >>"", loss_total, \', accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Xception()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_keras/zfnet_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 224, 224\n\ndef ZFNet():\n    inputs = Input((img_height, img_width, 3))\n    x = Conv2D(96, (7, 7), padding=\'valid\', strides=2, activation=\'relu\', name=\'conv1\')(inputs)\n    x = MaxPooling2D((3, 3), strides=2,  padding=\'same\')(x)\n    x = Conv2D(256, (5, 5), padding=\'valid\', strides=2, activation=\'relu\', name=\'conv2\')(x)\n    x = keras.layers.ZeroPadding2D(1)(x)\n    x = MaxPooling2D((3, 3), strides=2, padding=\'same\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv3\')(x)\n    x = Conv2D(384, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv4\')(x)\n    x = Conv2D(256, (3, 3), padding=\'same\', activation=\'relu\', name=\'conv5\')(x)\n    x = MaxPooling2D((3, 3), strides=2, padding=\'same\')(x)\n    \n    x = Flatten()(x)\n    x = Dense(4096, name=\'dense1\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4096, name=\'dense2\', activation=\'relu\')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    model = ZFNet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n        metrics=[\'accuracy\'])\n\n    xs, ts, paths = data_load(\'../Dataset/train/images\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_pytorch/AlexNet_pytorch.py,18,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 128\ncfg.INPUT_WIDTH = 128\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/AlexNet_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass AlexNet(torch.nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 96, kernel_size=11, padding=0, stride=4)\n        self.conv2 = torch.nn.Conv2d(96, 256, kernel_size=5, padding=1)\n        self.conv3 = torch.nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = torch.nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.fc1 = torch.nn.Linear(256 * (cfg.INPUT_HEIGHT // 64) * (cfg.INPUT_WIDTH // 64), 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, cfg.CLASS_NUM)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = x.view(x.size()[0], -1)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, AlexNet())'"
Scripts_Model/scripts_pytorch/DenseNet121_pytorch.py,42,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/DenseNet121_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = 1\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass DenseNet121(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet121, self).__init__()\n\n        class Block(torch.nn.Module):\n            def __init__(self, first_dim, k=32, L=6):\n                super(Block, self).__init__()\n                self.L = L\n                self.blocks = torch.nn.ModuleList()\n                self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                for i in range(1, L):\n                    self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(k * i + first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                \n            def forward(self, x):\n                xs = [None for _ in range(self.L + 1)]\n                xs[0] = x\n                xs[1] = self.blocks[0](x)\n                \n                for i in range(1, self.L):\n                    x_in = xs[i]\n                    for j in range(i):\n                        x_in = torch.cat([x_in, xs[j]], dim=1)\n                    x = self.blocks[i](x_in)\n                    xs[i + 1] = x\n                        \n                x = xs[0]\n                for i in range(1, (self.L + 1)):\n                    x = torch.cat([x, xs[i]], dim=1)\n                return x\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(cfg.INPUT_CHANNEL)\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 24\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 16\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, DenseNet121())'"
Scripts_Model/scripts_pytorch/DenseNet169_pytorch.py,42,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/DenseNet169_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass DenseNet169(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet169, self).__init__()\n\n        class Block(torch.nn.Module):\n            def __init__(self, first_dim, k=32, L=6):\n                super(Block, self).__init__()\n                self.L = L\n                self.blocks = torch.nn.ModuleList()\n                self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                for i in range(1, L):\n                    self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(k * i + first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n            def forward(self, x):\n                xs = [None for _ in range(self.L + 1)]\n                xs[0] = x\n                xs[1] = self.blocks[0](x)\n                \n                for i in range(1, self.L):\n                    x_in = xs[i]\n                    for j in range(i):\n                        x_in = torch.cat([x_in, xs[j]], dim=1)\n                    x = self.blocks[i](x_in)\n                    xs[i + 1] = x\n                        \n                x = xs[0]\n                for i in range(1, (self.L + 1)):\n                    x = torch.cat([x, xs[i]], dim=1)\n\n                return x\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(cfg.INPUT_CHANNEL)\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 32\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 32\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, DenseNet169())'"
Scripts_Model/scripts_pytorch/DenseNet201_pytorch.py,42,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/DenseNet201_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass DenseNet201(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet201, self).__init__()\n\n        class Block(torch.nn.Module):\n            def __init__(self, first_dim, k=32, L=6):\n                super(Block, self).__init__()\n                self.L = L\n                self.blocks = torch.nn.ModuleList()\n                self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                for i in range(1, L):\n                    self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(k * i + first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n            def forward(self, x):\n                xs = [None for _ in range(self.L + 1)]\n                xs[0] = x\n                xs[1] = self.blocks[0](x)\n                \n                for i in range(1, self.L):\n                    x_in = xs[i]\n                    for j in range(i):\n                        x_in = torch.cat([x_in, xs[j]], dim=1)\n                    x = self.blocks[i](x_in)\n                    xs[i + 1] = x\n                        \n                x = xs[0]\n                for i in range(1, (self.L + 1)):\n                    x = torch.cat([x, xs[i]], dim=1)\n\n                return x\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(cfg.INPUT_CHANNEL)\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 48\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 32\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, DenseNet201())'"
Scripts_Model/scripts_pytorch/DenseNet264_pytorch.py,42,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/DenseNet264_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass DenseNet264(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet264, self).__init__()\n\n        class Block(torch.nn.Module):\n            def __init__(self, first_dim, k=32, L=6):\n                super(Block, self).__init__()\n                self.L = L\n                self.blocks = torch.nn.ModuleList()\n                self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                for i in range(1, L):\n                    self.blocks.append(torch.nn.Sequential(\n                        torch.nn.BatchNorm2d(k * i + first_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(k),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n                    ))\n                \n                \n            def forward(self, x):\n                xs = [None for _ in range(self.L + 1)]\n                xs[0] = x\n                xs[1] = self.blocks[0](x)\n                \n                for i in range(1, self.L):\n                    x_in = xs[i]\n                    for j in range(i):\n                        x_in = torch.cat([x_in, xs[j]], dim=1)\n                    x = self.blocks[i](x_in)\n                    xs[i + 1] = x\n                        \n                x = xs[0]\n                for i in range(1, (self.L + 1)):\n                    x = torch.cat([x, xs[i]], dim=1)\n\n                return x\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(cfg.INPUT_CHANNEL)\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 64\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 48\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, DenseNet264())'"
Scripts_Model/scripts_pytorch/EfficientNetB0_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/EfficientNetB0_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B0\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB1_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB1_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B1\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB2_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB2_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B2\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB3_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB3_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B3\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB4_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB4_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B4\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB5_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB5_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B5\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB6_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB6_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B6\'))'"
Scripts_Model/scripts_pytorch/EfficientNetB7_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'EfficientNetB7_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 2\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNet(torch.nn.Module):\n    def __init__(self, block_type=\'B0\'):\n        super(EfficientNet, self).__init__()\n\n        print(\'--\')\n        print(\'You call EfficientNet\')\n        print(\' - Input dim (h, w, c) >> ({}, {}, {})\'.format(cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, cfg.INPUT_CHANNEL))\n        print(\'--\')\n\n        EFFICIENTNET_CONFIG = {\n            \'B0\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B1\' : {\'WIDTH_COEFFICIENT\' : 1, \'DEPTH_COEFFICIENT\' : 1.1, \'DROPOUT_RATIO\' : 0.2, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B2\' : {\'WIDTH_COEFFICIENT\' : 1.1, \'DEPTH_COEFFICIENT\' : 1.2, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B3\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B4\' : {\'WIDTH_COEFFICIENT\' : 1.2, \'DEPTH_COEFFICIENT\' : 1.4, \'DROPOUT_RATIO\' : 0.3, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B5\' : {\'WIDTH_COEFFICIENT\' : 1.6, \'DEPTH_COEFFICIENT\' : 2.2, \'DROPOUT_RATIO\' : 0.4, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B6\' : {\'WIDTH_COEFFICIENT\' : 1.8, \'DEPTH_COEFFICIENT\' : 2.6, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2},\n            \'B7\' : {\'WIDTH_COEFFICIENT\' : 2.0, \'DEPTH_COEFFICIENT\' : 3.1, \'DROPOUT_RATIO\' : 0.5, \'DEPTH_DIVISOR\' : 8, \'DROP_CONNECT_RATIO\' : 0.2}\n        }\n\n        width_coefficient = EFFICIENTNET_CONFIG[block_type][\'WIDTH_COEFFICIENT\']\n        depth_coefficient = EFFICIENTNET_CONFIG[block_type][\'DEPTH_COEFFICIENT\']\n        dropout_ratio = EFFICIENTNET_CONFIG[block_type][\'DROPOUT_RATIO\']\n        depth_divisor = EFFICIENTNET_CONFIG[block_type][\'DEPTH_DIVISOR\']\n        drop_connect_rate = EFFICIENTNET_CONFIG[block_type][\'DROP_CONNECT_RATIO\']\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(cfg.INPUT_CHANNEL, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), cfg.CLASS_NUM)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n    main(cfg, EfficientNet(\'B7\'))'"
Scripts_Model/scripts_pytorch/LeNet_pytorch.py,11,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 32\ncfg.INPUT_WIDTH = 32\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/LeNet_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\nclass LeNet(torch.nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 6, kernel_size=5, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=5, padding=0)\n        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = torch.nn.Linear(120, 64)\n        self.fc_out = torch.nn.Linear(64, cfg.CLASS_NUM)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.sigmoid(F.max_pool2d(x, 2))\n        x = self.conv2(x)\n        x = F.sigmoid(F.max_pool2d(x, 2))\n        x = x.view(x.size()[0], -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, LeNet())'"
Scripts_Model/scripts_pytorch/MobileNet_v1_pytorch.py,23,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/MobileNet_v1_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass MobileNet_v1(torch.nn.Module): \n    def __init__(self):\n        super(MobileNet_v1, self).__init__()\n\n        class MobileNetBlock(torch.nn.Module):\n            def __init__(self, in_dim, out_dim, repeat=1, stride=1):\n                super(MobileNetBlock, self).__init__()\n                _module = []\n                for _ in range(repeat):\n                    _module += [\n                        torch.nn.Conv2d(in_dim, in_dim, kernel_size=3, padding=1, stride=stride, groups=in_dim),\n                        torch.nn.BatchNorm2d(in_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(in_dim, out_dim, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(out_dim),\n                        torch.nn.ReLU(),\n                    ]\n                    \n                self.module = torch.nn.Sequential(*_module)\n                    \n            def forward(self, x):\n                x = self.module(x)\n                return x\n            \n        class Flatten(torch.nn.Module):\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n        \n        self.module = torch.nn.Sequential(\n            #-----\n            # 1/1 x 1/1 x 3\n            #-----\n            torch.nn.Conv2d(cfg.INPUT_CHANNEL, 32, kernel_size=3, padding=1, stride=2),\n            torch.nn.BatchNorm2d(32),\n            torch.nn.ReLU(),\n\n            #-----\n            # 1/2 x 1/2 x 32\n            #-----\n            MobileNetBlock(32, 64),\n\n            #-----\n            # 1/4 x 1/4 x 64\n            #-----\n            MobileNetBlock(64, 128, stride=2),\n            MobileNetBlock(128, 128),\n\n            #-----\n            # 1/8 x 1/8 x 128\n            #-----\n            MobileNetBlock(128, 256, stride=2),\n            MobileNetBlock(256, 256),\n\n            #-----\n            # 1/16 x 1/16 x 256\n            #-----\n            MobileNetBlock(256, 512, stride=2),\n            MobileNetBlock(512, 512, repeat=5),\n            \n            #-----\n            # 1/32 x 1/32 x 1024\n            #-----\n            MobileNetBlock(512, 1024, stride=2),\n            MobileNetBlock(1024, 1024),\n            #torch.nn.AvgPool2d([img_height // 32, img_width // 32], stride=1, padding=0),\n            torch.nn.AdaptiveAvgPool2d([1, 1]),\n            Flatten(),\n            torch.nn.Linear(1024, cfg.CLASS_NUM),\n            torch.nn.Softmax(dim=1)\n        )\n\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, MobileNet_v1())'"
Scripts_Model/scripts_pytorch/MobileNet_v2_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/MobileNet_v2_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass MobileNet_v2(torch.nn.Module): \n    def __init__(self):\n        super(MobileNet_v2, self).__init__()\n        \n        # define block\n        class MobileNetBlock(torch.nn.Module):\n            def __init__(self, in_dim, out_dim, stride=1, expansion_t=6, split_division_by=8):\n                super(MobileNetBlock, self).__init__()\n                \n                self.module = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_dim, in_dim * expansion_t, kernel_size=1, padding=0, stride=1, groups=in_dim),\n                    torch.nn.BatchNorm2d(in_dim * expansion_t),\n                    torch.nn.ReLU6(),\n                    torch.nn.Conv2d(in_dim * expansion_t, in_dim * expansion_t, kernel_size=3, padding=1, stride=stride, groups=split_division_by),\n                    torch.nn.BatchNorm2d(in_dim * expansion_t),\n                    torch.nn.ReLU6(),\n                    torch.nn.Conv2d(in_dim * expansion_t, out_dim, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_dim),\n                )\n                    \n            def forward(self, _input):\n                x = self.module(_input)\n                \n                # if shape matches, add skip connection\n                if x.size() == _input.size():\n                    x = x + _input\n                \n                return x\n            \n        # define feature dimension flattening layer\n        class Flatten(torch.nn.Module):\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n        \n        self.module = torch.nn.Sequential(\n            # input\n            # 224 x 224 x 3\n            torch.nn.Conv2d(cfg.INPUT_CHANNEL, 32, kernel_size=3, padding=1, stride=2),\n            torch.nn.BatchNorm2d(32),\n            torch.nn.ReLU6(),\n            # 112 x 112 x 32\n            MobileNetBlock(32, 16, expansion_t=1),\n            # 112 x 112 x 16\n            MobileNetBlock(16, 24, stride=2),\n            MobileNetBlock(24, 24),\n            # 56 x 56 x 24\n            MobileNetBlock(24, 32, stride=2),\n            MobileNetBlock(32, 32),\n            MobileNetBlock(32, 32),\n            # 28 x 28 x 32\n            MobileNetBlock(32, 64, stride=2),\n            MobileNetBlock(64, 64),\n            MobileNetBlock(64, 64),\n            MobileNetBlock(64, 64),\n            # 14 x 14 x 64\n            MobileNetBlock(64, 96),\n            MobileNetBlock(96, 96),\n            MobileNetBlock(96, 96),\n            # 14 x 14 x 96\n            MobileNetBlock(96, 160, stride=2),\n            MobileNetBlock(160, 160),\n            MobileNetBlock(160, 160),\n            # 7 x 7 x 160\n            MobileNetBlock(160, 320),\n            # 7 x 7 x 320\n            torch.nn.Conv2d(320, 1280, kernel_size=1, padding=0, stride=1),\n            torch.nn.BatchNorm2d(1280),\n            torch.nn.ReLU6(),\n            # 7 x 7 x 1280\n            torch.nn.AdaptiveAvgPool2d([1, 1]),\n            Flatten(),\n            # 1 x 1 x 1280\n            torch.nn.Linear(1280, cfg.CLASS_NUM),\n            torch.nn.Softmax(dim=1)\n        )\n\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, MobileNet_v2())'"
Scripts_Model/scripts_pytorch/NIN_pytorch.py,18,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/NIN_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass NIN(torch.nn.Module):\n    def __init__(self):\n        super(NIN, self).__init__()\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp1 = torch.nn.Conv2d(192, 160, kernel_size=1, padding=0, stride=1)\n        self.cccp2 = torch.nn.Conv2d(160, 96, kernel_size=1, padding=0, stride=1)\n        self.conv2 = torch.nn.Conv2d(96, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp3 = torch.nn.Conv2d(192, 192, kernel_size=1, padding=0, stride=1)\n        self.cccp4 = torch.nn.Conv2d(192, 192, kernel_size=1, padding=0, stride=1)\n        self.conv3 = torch.nn.Conv2d(192, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp5 = torch.nn.Conv2d(192, 160, kernel_size=1, padding=0, stride=1)\n        self.out = torch.nn.Conv2d(160, cfg.CLASS_NUM, kernel_size=1, padding=0, stride=1)\n        self.gap = torch.nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.cccp1(x))\n        x = F.relu(self.cccp2(x))\n        x = F.max_pool2d(x, 3, stride=2, padding=0)\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.cccp3(x))\n        x = F.relu(self.cccp4(x))\n        x = F.max_pool2d(x, 3, stride=2, padding=0)\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.cccp5(x))\n        x = self.out(x)\n        x = self.gap(x)\n        x = x.view((x.shape[0], -1))\n        x = F.softmax(x, dim=1)\n        return x\n\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, NIN())'"
Scripts_Model/scripts_pytorch/ResNeXt101_pytorch.py,24,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNeXt101_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = 1\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\nclass ResNeXt101(torch.nn.Module):\n    def __init__(self):\n        super(ResNeXt101, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n                    \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                    x = self.fit_bn(x)\n                    x = F.relu(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(22):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNeXt101())'"
Scripts_Model/scripts_pytorch/ResNeXt50_pytorch.py,22,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNeXt50_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = 1\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\nclass ResNeXt50(torch.nn.Module):\n    def __init__(self):\n        super(ResNeXt50, self).__init__()\n\n        class ResNeXtBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1, cardinality=32):\n                super(ResNeXtBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n                \n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1, groups=cardinality),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU(),\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_dim = True\n            \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        \n        self.block2_1 = ResNeXtBlock(64, 64, 256)\n        self.block2_2 = ResNeXtBlock(256, 64, 256)\n        self.block2_3 = ResNeXtBlock(256, 64, 256)\n\n        self.block3_1 = ResNeXtBlock(256, 128, 512, stride=2)\n        self.block3_2 = ResNeXtBlock(512, 128, 512)\n        self.block3_3 = ResNeXtBlock(512, 128, 512)\n        self.block3_4 = ResNeXtBlock(512, 128, 512)\n\n        self.block4_1 = ResNeXtBlock(512, 256, 1024, stride=2)\n        self.block4_2 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_3 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_4 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_5 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_6 = ResNeXtBlock(1024, 256, 1024)\n\n        self.block5_1 = ResNeXtBlock(1024, 512, 2048, stride=2)\n        self.block5_2 = ResNeXtBlock(2048, 512, 2048)\n        self.block5_3 = ResNeXtBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.block2_1(x)\n        x = self.block2_2(x)\n        x = self.block2_3(x)\n\n        x = self.block3_1(x)\n        x = self.block3_2(x)\n        x = self.block3_3(x)\n        x = self.block3_4(x)\n\n        x = self.block4_1(x)\n        x = self.block4_2(x)\n        x = self.block4_3(x)\n        x = self.block4_4(x)\n        x = self.block4_5(x)\n        x = self.block4_6(x)\n\n        x = self.block5_1(x)\n        x = self.block5_2(x)\n        x = self.block5_3(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNeXt50())'"
Scripts_Model/scripts_pytorch/ResNet101_pytorch.py,24,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNet101_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ResNet101(torch.nn.Module):\n    def __init__(self):\n        super(ResNet101, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n            \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                    x = self.fit_bn(x)\n                    x = F.relu(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(22):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNet101())'"
Scripts_Model/scripts_pytorch/ResNet152_pytorch.py,25,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNet152_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\nclass ResNet152(torch.nn.Module):\n    def __init__(self):\n        super(ResNet152, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n        \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                    x = self.fit_bn(x)\n                    x = F.relu(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        block = []\n        for _ in range(7):\n            block.append(ResBlock(512, 128, 512))\n        self.resblock3s = torch.nn.Sequential(*block)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(35):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3s(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNet152())'"
Scripts_Model/scripts_pytorch/ResNet18_pytorch.py,19,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNet18_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ResNet18(torch.nn.Module):\n    def __init__(self):\n        super(ResNet18, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, out_f, kernel_size=3, padding=1, stride=stride),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(out_f, out_f, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_dim = True\n                    \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64)\n        self.resblock2_2 = ResBlock(64, 64)\n\n        self.resblock3_1 = ResBlock(64, 128, stride=2)\n        self.resblock3_2 = ResBlock(128, 128)\n\n        self.resblock4_1 = ResBlock(128, 256, stride=2)\n        self.resblock4_2 = ResBlock(256, 256)\n\n        self.resblock5_1 = ResBlock(256, 512, stride=2)\n        self.resblock5_2 = ResBlock(512, 512)\n        \n        self.linear = torch.nn.Linear(512, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNet18())'"
Scripts_Model/scripts_pytorch/ResNet34_pytorch.py,19,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNet34_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ResNet34(torch.nn.Module):\n    def __init__(self):\n        super(ResNet34, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, out_f, stride=1):\n                super(ResBlock, self).__init__()\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, out_f, kernel_size=3, padding=1, stride=stride),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(out_f, out_f, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_dim = True\n                    \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.resblock2_1 = ResBlock(64, 64)\n        self.resblock2_2 = ResBlock(64, 64)\n        self.resblock2_3 = ResBlock(64, 64)\n\n        self.resblock3_1 = ResBlock(64, 128, stride=2)\n        self.resblock3_2 = ResBlock(128, 128)\n        self.resblock3_3 = ResBlock(128, 128)\n        self.resblock3_4 = ResBlock(128, 128)\n\n        self.resblock4_1 = ResBlock(128, 256, stride=2)\n        self.resblock4_2 = ResBlock(256, 256)\n        self.resblock4_3 = ResBlock(256, 256)\n        self.resblock4_4 = ResBlock(256, 256)\n        self.resblock4_5 = ResBlock(256, 256)\n        self.resblock4_6 = ResBlock(256, 256)\n\n        self.resblock5_1 = ResBlock(256, 512, stride=2)\n        self.resblock5_2 = ResBlock(512, 512)\n        self.resblock5_3 = ResBlock(512, 512)\n        \n        self.linear = torch.nn.Linear(512, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n        x = self.resblock4_3(x)\n        x = self.resblock4_4(x)\n        x = self.resblock4_5(x)\n        x = self.resblock4_6(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNet34())'"
Scripts_Model/scripts_pytorch/ResNet50_pytorch.py,23,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ResNet50_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.001\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ResNet50(torch.nn.Module):\n    def __init__(self):\n        super(ResNet50, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n        \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                    x = self.fit_bn(x)\n                    x = F.relu(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        self.resblock4_2 = ResBlock(1024, 256, 1024)\n        self.resblock4_3 = ResBlock(1024, 256, 1024)\n        self.resblock4_4 = ResBlock(1024, 256, 1024)\n        self.resblock4_5 = ResBlock(1024, 256, 1024)\n        self.resblock4_6 = ResBlock(1024, 256, 1024)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n        x = self.resblock4_3(x)\n        x = self.resblock4_4(x)\n        x = self.resblock4_5(x)\n        x = self.resblock4_6(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ResNet50())'"
Scripts_Model/scripts_pytorch/VGG16_pytorch.py,58,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/VGG16_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.1\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass VGG16(torch.nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n\n        self.conv1 = torch.nn.Sequential(OrderedDict({\n            \'conv1_1\' : torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=3, padding=1, stride=1),\n            \'conv1_1_relu\' : torch.nn.ReLU(),\n            \'conv1_1_bn\' : torch.nn.BatchNorm2d(64),\n            \'conv1_2\' : torch.nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n            \'conv1_2_relu\' : torch.nn.ReLU(),\n            \'conv1_2_bn\' : torch.nn.BatchNorm2d(64),\n        }))\n\n        self.conv2 = torch.nn.Sequential(OrderedDict({\n            \'conv2_1\' : torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n            \'conv2_1_relu\' : torch.nn.ReLU(),\n            \'conv2_1_bn\' : torch.nn.BatchNorm2d(128),\n            \'conv2_2\' : torch.nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n            \'conv2_2_relu\' : torch.nn.ReLU(),\n            \'conv2_2_bn\' : torch.nn.BatchNorm2d(128),\n        }))\n\n        self.conv3 = torch.nn.Sequential(OrderedDict({\n            \'conv3_1\' : torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_1_relu\' : torch.nn.ReLU(),\n            \'conv3_1_bn\' : torch.nn.BatchNorm2d(256),\n            \'conv3_2\' : torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_2_relu\' : torch.nn.ReLU(),\n            \'conv3_2_bn\' : torch.nn.BatchNorm2d(256),\n            \'conv3_3\' : torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_3_relu\' : torch.nn.ReLU(),\n            \'conv3_3_bn\' : torch.nn.BatchNorm2d(256),\n        }))\n\n        self.conv4 = torch.nn.Sequential(OrderedDict({\n            \'conv4_1\' : torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_1_relu\' : torch.nn.ReLU(),\n            \'conv4_1_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv4_2\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_2_relu\' : torch.nn.ReLU(),\n            \'conv4_2_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv4_3\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_3_relu\' : torch.nn.ReLU(),\n            \'conv4_3_bn\' : torch.nn.BatchNorm2d(512),\n        }))\n\n        self.conv5 = torch.nn.Sequential(OrderedDict({\n            \'conv5_1\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_1_relu\' : torch.nn.ReLU(),\n            \'conv5_1_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv5_2\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_2_relu\' : torch.nn.ReLU(),\n            \'conv5_2_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv5_3\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_3_relu\' : torch.nn.ReLU(),\n            \'conv5_3_bn\' : torch.nn.BatchNorm2d(512),\n        }))\n        \n        self.top = torch.nn.Sequential(OrderedDict({\n            \'Dense1\' : torch.nn.Linear(512 * (cfg.INPUT_HEIGHT // 32) * (cfg.INPUT_WIDTH // 32), 256),\n            \'Dense1_relu\' : torch.nn.ReLU(),\n            \'Dense1_dropout\' : torch.nn.Dropout(p=0.5),\n            \'Dense2\' : torch.nn.Linear(256, 256),\n            \'Dense2_relu\' : torch.nn.ReLU(),\n            \'Dense2_dropout\' : torch.nn.Dropout(p=0.5),\n        }))\n\n        self.fc_out = torch.nn.Linear(256, cfg.CLASS_NUM)\n        \n\n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        \n        x = x.view(x.shape[0], -1)\n        x = self.top(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, VGG16())'"
Scripts_Model/scripts_pytorch/VGG19_pytorch.py,67,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/VGG16_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.1\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass VGG19(torch.nn.Module):\n    def __init__(self):\n        super(VGG19, self).__init__()\n\n        self.conv1 = torch.nn.Sequential(OrderedDict({\n            \'conv1_1\' : torch.nn.Conv2d(cfg.INPUT_CHANNEL, 64, kernel_size=3, padding=1, stride=1),\n            \'conv1_1_relu\' : torch.nn.ReLU(),\n            \'conv1_1_bn\' : torch.nn.BatchNorm2d(64),\n            \'conv1_2\' : torch.nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n            \'conv1_2_relu\' : torch.nn.ReLU(),\n            \'conv1_2_bn\' : torch.nn.BatchNorm2d(64),\n        }))\n\n        self.conv2 = torch.nn.Sequential(OrderedDict({\n            \'conv2_1\' : torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n            \'conv2_1_relu\' : torch.nn.ReLU(),\n            \'conv2_1_bn\' : torch.nn.BatchNorm2d(128),\n            \'conv2_2\' : torch.nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n            \'conv2_2_relu\' : torch.nn.ReLU(),\n            \'conv2_2_bn\' : torch.nn.BatchNorm2d(128),\n        }))\n\n        self.conv3 = torch.nn.Sequential(OrderedDict({\n            \'conv3_1\' : torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_1_relu\' : torch.nn.ReLU(),\n            \'conv3_1_bn\' : torch.nn.BatchNorm2d(256),\n            \'conv3_2\' : torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_2_relu\' : torch.nn.ReLU(),\n            \'conv3_2_bn\' : torch.nn.BatchNorm2d(256),\n            \'conv3_3\' : torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_3_relu\' : torch.nn.ReLU(),\n            \'conv3_3_bn\' : torch.nn.BatchNorm2d(256),\n            \'conv3_4\' : torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n            \'conv3_4_relu\' : torch.nn.ReLU(),\n            \'conv3_4_bn\' : torch.nn.BatchNorm2d(256),\n        }))\n\n        self.conv4 = torch.nn.Sequential(OrderedDict({\n            \'conv4_1\' : torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_1_relu\' : torch.nn.ReLU(),\n            \'conv4_1_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv4_2\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_2_relu\' : torch.nn.ReLU(),\n            \'conv4_2_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv4_3\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_3_relu\' : torch.nn.ReLU(),\n            \'conv4_3_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv4_4\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv4_4_relu\' : torch.nn.ReLU(),\n            \'conv4_4_bn\' : torch.nn.BatchNorm2d(512),\n        }))\n\n        self.conv5 = torch.nn.Sequential(OrderedDict({\n            \'conv5_1\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_1_relu\' : torch.nn.ReLU(),\n            \'conv5_1_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv5_2\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_2_relu\' : torch.nn.ReLU(),\n            \'conv5_2_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv5_3\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_3_relu\' : torch.nn.ReLU(),\n            \'conv5_3_bn\' : torch.nn.BatchNorm2d(512),\n            \'conv5_3\' : torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n            \'conv5_3_relu\' : torch.nn.ReLU(),\n            \'conv5_3_bn\' : torch.nn.BatchNorm2d(512),\n        }))\n        \n        self.top = torch.nn.Sequential(OrderedDict({\n            \'Dense1\' : torch.nn.Linear(512 * (cfg.INPUT_HEIGHT // 32) * (cfg.INPUT_WIDTH // 32), 256),\n            \'Dense1_relu\' : torch.nn.ReLU(),\n            \'Dense1_dropout\' : torch.nn.Dropout(p=0.5),\n            \'Dense2\' : torch.nn.Linear(256, 256),\n            \'Dense2_relu\' : torch.nn.ReLU(),\n            \'Dense2_dropout\' : torch.nn.Dropout(p=0.5),\n        }))\n\n        self.fc_out = torch.nn.Linear(256, cfg.CLASS_NUM)\n        \n\n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        \n        x = x.view(x.shape[0], -1)\n        x = self.top(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, VGG19())'"
Scripts_Model/scripts_pytorch/Xception_pytorch.py,73,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/Xception_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass Xception(torch.nn.Module):\n    def __init__(self):\n        super(Xception, self).__init__()\n\n        class Block(torch.nn.Module):\n            def __init__(self, dim=728, cardinality=1):\n                super(Block, self).__init__()\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n                    torch.nn.BatchNorm2d(dim),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n                    torch.nn.BatchNorm2d(dim),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n                    torch.nn.BatchNorm2d(dim),\n                )\n                \n            def forward(self, x):\n                res_x = self.block(x)            \n                x = torch.add(res_x, x)\n                return x\n\n        # Entry flow\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 32, kernel_size=3, padding=1, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        \n        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n        self.bn2 = torch.nn.BatchNorm2d(64)\n        \n        self.conv3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv3_sc = torch.nn.Conv2d(64, 128, kernel_size=1, padding=0, stride=2)\n        self.bn3_sc = torch.nn.BatchNorm2d(128)\n        \n        self.conv4 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv4_sc = torch.nn.Conv2d(128, 256, kernel_size=1, padding=0, stride=2)\n        self.bn4_sc = torch.nn.BatchNorm2d(256)\n        \n        self.conv5 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(256, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv5_sc = torch.nn.Conv2d(256, 728, kernel_size=1, padding=0, stride=2)\n        self.bn5_sc = torch.nn.BatchNorm2d(728)\n        \n        # Middle flow\n        self.middle_flow = torch.nn.Sequential(\n            *[Block() for _ in range(8)]\n        )\n        \n        # Exit flow\n        self.conv_exit1 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 1024, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv_exit1_sc = torch.nn.Conv2d(728, 1024, kernel_size=1, padding=0, stride=2)\n        self.bn_exit1_sc = torch.nn.BatchNorm2d(1024)\n        \n        self.conv_exit2 = torch.nn.Sequential(\n            torch.nn.Conv2d(1024, 1536, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(1536),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(1536, 2048, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(2048),)\n        \n        self.linear = torch.nn.Linear(2048, cfg.CLASS_NUM)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        \n        x_sc = self.conv3_sc(x)\n        x_sc = self.bn3_sc(x_sc)\n        x = self.conv3(x)\n        x = torch.add(x_sc, x)\n        \n        x_sc = self.conv4_sc(x_sc)\n        x_sc = self.bn4_sc(x_sc)\n        x = self.conv4(x)\n        x = torch.add(x_sc, x)\n        \n        x_sc = self.conv5_sc(x_sc)\n        x_sc = self.bn5_sc(x_sc)\n        x = self.conv5(x)\n        x = torch.add(x_sc, x)\n        \n        # Middle flow\n        x = self.middle_flow(x)\n        \n        # Exit flow\n        x_sc = self.conv_exit1_sc(x)\n        x_sc = self.bn_exit1_sc(x_sc)\n        x = self.conv_exit1(x)\n        x = torch.add(x_sc, x)\n        \n        x = self.conv_exit2(x)\n\n        x = F.avg_pool2d(x, [cfg.INPUT_HEIGHT // 32, cfg.INPUT_WIDTH // 32], padding=0, stride=1)\n        x = x.view(x.size()[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, Xception())'"
Scripts_Model/scripts_pytorch/ZFNet_pytorch.py,18,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom easydict import EasyDict\nfrom _main_base import main\nimport os\n\n#---\n# config\n#---\ncfg = EasyDict()\n\n# class\ncfg.CLASS_LABEL = [\'akahara\', \'madara\']\ncfg.CLASS_NUM = len(cfg.CLASS_LABEL)\n\n# model\ncfg.INPUT_HEIGHT = 64\ncfg.INPUT_WIDTH = 64\ncfg.INPUT_CHANNEL = 3\n\ncfg.GPU = False\ncfg.DEVICE = torch.device(""cuda"" if cfg.GPU and torch.cuda.is_available() else ""cpu"")\n\ncfg.MODEL_SAVE_PATH = \'models/ZFNet_{}.pt\'\ncfg.MODEL_SAVE_INTERVAL = 200\ncfg.ITERATION = 1000\ncfg.MINIBATCH = 8\ncfg.OPTIMIZER = torch.optim.SGD\ncfg.LEARNING_RATE = 0.01\ncfg.MOMENTUM = 0.9\ncfg.LOSS_FUNCTION = loss_fn = torch.nn.NLLLoss()\n\ncfg.TRAIN = EasyDict()\ncfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n\ncfg.TRAIN.DATA_PATH = \'../Dataset/train/images/\'\ncfg.TRAIN.DATA_HORIZONTAL_FLIP = True\ncfg.TRAIN.DATA_VERTICAL_FLIP = True\ncfg.TRAIN.DATA_ROTATION = False\n\ncfg.TEST = EasyDict()\ncfg.TEST.MODEL_PATH = cfg.MODEL_SAVE_PATH.format(\'final\')\ncfg.TEST.DATA_PATH = \'../Dataset/test/images/\'\ncfg.TEST.MINIBATCH = 2\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass ZFNet(torch.nn.Module):\n    def __init__(self):\n        super(ZFNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(cfg.INPUT_CHANNEL, 96, kernel_size=7, padding=1, stride=2)\n        self.conv2 = torch.nn.Conv2d(96, 256, kernel_size=5, padding=1, stride=2)\n        self.conv3 = torch.nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = torch.nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.fc1 = torch.nn.Linear(256 * (cfg.INPUT_HEIGHT // 64) * (cfg.INPUT_WIDTH // 64), 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, cfg.CLASS_NUM)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.max_pool2d(x, 3, stride=2)\n        x = x.view(x.size()[0], -1)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# main\nif __name__ == \'__main__\':\n\n    model_save_dir = \'/\'.join(cfg.MODEL_SAVE_PATH.split(\'/\')[:-1])\n    os.makedirs(model_save_dir, exist_ok=True)\n\n    main(cfg, ZFNet())'"
Scripts_Model/scripts_pytorch/_main_base.py,9,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom tqdm import tqdm\n\n\n# get train data\ndef data_load(cfg, path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    print(\'Dataset >>\', path)\n    print(\' - Found data num >>\', data_num)\n    print(\' - Horizontal >>\', hf)\n    print(\' - Vertical >>\', vf)\n    print(\' - Rotation >>\', rot)\n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(cfg.CLASS_LABEL):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    print(\'all data num >>\', len(paths))\n    print(\'dataset was completely loaded\')\n    print(\'--\')\n\n    return np.array(paths), np.array(ts)\n\n\ndef get_image(cfg, infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (cfg.INPUT_WIDTH, cfg.INPUT_HEIGHT)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if cfg.INPUT_CHANNEL == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if cfg.INPUT_CHANNEL == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train(cfg, _model):\n    print(\'-\' * 20)\n    print(\'train function\')\n    print(\'-\' * 20)\n    # model\n    model = _model.to(cfg.DEVICE)\n    opt = cfg.OPTIMIZER(model.parameters(), lr=cfg.LEARNING_RATE, momentum=cfg.MOMENTUM)\n    model.train()\n\n    paths, ts = data_load(cfg, cfg.TRAIN.DATA_PATH, hf=cfg.TRAIN.DATA_HORIZONTAL_FLIP, vf=cfg.TRAIN.DATA_VERTICAL_FLIP, rot=cfg.TRAIN.DATA_ROTATION)\n\n    # training\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = cfg.LOSS_FUNCTION\n\n    print(\'training start\')\n    progres_bar = \'\'\n    \n    for i in range(cfg.ITERATION):\n        if mbi + cfg.MINIBATCH > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[ : (cfg.MINIBATCH - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + cfg.MINIBATCH]\n            mbi += cfg.MINIBATCH\n\n        x = torch.tensor(get_image(cfg, paths[mb_ind]), dtype=torch.float).to(cfg.DEVICE)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(cfg.DEVICE)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        accuracy = pred.eq(t.view_as(pred)).sum().item() / cfg.MINIBATCH\n\n        progres_bar += \'|\'\n        print(\'\\r\' + progres_bar, end=\'\')\n\n        if (i + 1) % 10 == 0:\n            progres_bar += str(i + 1)\n            print(\'\\r\' + progres_bar, end=\'\')\n\n        # display training state\n        if (i + 1) % cfg.TRAIN.DISPAY_ITERATION_INTERVAL == 0:\n            print(\'\\r\' + \' \' * len(progres_bar), end=\'\')\n            print(\'\\rIter : {} , Loss : {:.4f} , Accuracy : {:.4f}\'.format(i+1, loss.item(), accuracy))\n            progres_bar = \'\'\n\n        # save parameters\n        if (cfg.MODEL_SAVE_INTERVAL != False) and ((i + 1) % cfg.MODEL_SAVE_INTERVAL == 0):\n            save_path = cfg.MODEL_SAVE_PATH.format(\'iter{}\'.format(i + 1))\n            torch.save(model.state_dict(), save_path)\n            print(\'model was saved to >>\', save_path)\n\n    save_path = cfg.MODEL_SAVE_PATH.format(\'final\')\n    torch.save(model.state_dict(), save_path)\n    print(\'model was saved to >>\', save_path)\n\n# test\ndef test(cfg, _model):\n    print(\'-\' * 20)\n    print(\'test function\')\n    print(\'-\' * 20)\n    model = _model.to(cfg.DEVICE)\n    model.load_state_dict(torch.load(cfg.TEST.MODEL_PATH, map_location=torch.device(cfg.DEVICE)))\n    model.eval()\n\n    print(\'model loaded >>\', cfg.TEST.MODEL_PATH)\n\n    paths, ts = data_load(cfg, cfg.TEST.DATA_PATH, hf=False, vf=False, rot=False)\n\n    Test_Num = len(paths)\n\n    print(\'test start\')\n\n    with torch.no_grad():\n        for i in range(0, Test_Num, cfg.TEST.MINIBATCH):\n            test_inds = np.arange(i, min(i + cfg.TEST.MINIBATCH, Test_Num))\n            path = paths[test_inds]\n            x = get_image(cfg, path)\n            t = ts[test_inds]\n            \n            x = torch.tensor(x, dtype=torch.float).to(cfg.DEVICE)\n            \n            preds = model(x)\n\n            for j in range(len(preds)):\n                pred = preds.detach().cpu().numpy()[j]\n                print(\'Data : {}, probabilities >> {}\'.format(path[j], pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\ndef main(cfg, model):\n    args = arg_parse()\n\n    if args.train:\n        train(cfg, model)\n    if args.test:\n        test(cfg, model)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/AlexNet_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 227, 227\ntf.set_random_seed(0)\n\n\ndef AlexNet(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=96, kernel_size=[11, 11], strides=4, padding=\'valid\', activation=tf.nn.relu, name=\'conv1\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.keras.layers.ZeroPadding2D((1,1))(x)\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[5, 5], strides=1, padding=\'valid\', activation=tf.nn.relu, name=\'conv2\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3\')\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5\')\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = AlexNet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = AlexNet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/BN_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\ntf.set_random_seed(0)\n\n\ndef VGG16(x, keep_prob, train=False):\n    # block conv1\n    for i in range(2):\n        x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv1_{}\'.format(i+1))\n        x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv2\n    for i in range(2):\n        x = tf.layers.conv2d(inputs=x, filters=128, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv2_{}\'.format(i+1))\n        x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv3\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_{}\'.format(i+1))\n        x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv4\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_{}\'.format(i+1))\n        x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv5\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_{}\'.format(i+1))\n        x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = VGG16(X, keep_prob, train=True)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = VGG16(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/GAP_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\ntf.set_random_seed(0)\n\n\ndef GAP(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=96, kernel_size=[7, 7], strides=2, padding=\'valid\', activation=tf.nn.relu, name=\'conv1\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.keras.layers.ZeroPadding2D((1,1))(x)\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[5, 5], strides=2, padding=\'valid\', activation=tf.nn.relu, name=\'conv2\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3\')\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n\n    # GAP\n    x = tf.layers.conv2d(inputs=x, filters=num_classes, kernel_size=[1, 1], padding=\'same\', name=\'conv_out\')\n    x = tf.reduce_mean(x, axis=1)\n    x = tf.reduce_mean(x, axis=1)\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = GAP(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = GAP(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/LeNet_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 32, 32\ntf.set_random_seed(0)\n\n\n\ndef LeNet(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=6, kernel_size=[5, 5], strides=1, padding=\'valid\', activation=None, name=\'conv1\')\n    x = tf.nn.sigmoid(tf.layers.max_pooling2d(x, pool_size=[2,2], strides=2))\n    x = tf.layers.conv2d(inputs=x, filters=16, kernel_size=[5, 5], strides=1, padding=\'valid\', activation=None, name=\'conv2\')\n    x = tf.nn.sigmoid(tf.layers.max_pooling2d(x, pool_size=[2,2], strides=2))\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=120, activation=None, name=\'fc1\')\n    x = tf.layers.dense(inputs=x, units=64, activation=None, name=\'fc2\')\n    x = tf.layers.dense(inputs=x, units=num_classes, activation=None, name=\'fc_out\')\n    return x\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = LeNet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = LeNet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/MobileNet_v1_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\n#tf.random.set_random_seed(0)\n\n\ndef MobileNet_v1(x, keep_prob, train=True):\n\n    def MobileNetBlock(x, in_dim, out_dim, repeat=1, strides=1):\n        for _ in range(repeat):\n            x = tf.layers.separable_conv2d(x, in_dim, [3, 3], strides=strides, padding=\'same\', activation=None, depth_multiplier=in_dim)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n            x = tf.layers.conv2d(x, out_dim, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n        \n        return x\n\n    #-----\n    # 1/1 x 1/1 x 3\n    #-----\n    x = tf.layers.conv2d(x, 32, [3, 3], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n\n    #-----\n    # 1/2 x 1/2 x 32\n    #-----\n    MobileNetBlock(x, 32, 64)\n\n    #-----\n    # 1/4 x 1/4 x 64\n    #-----\n    MobileNetBlock(x, 64, 128, strides=2)\n    MobileNetBlock(x, 128, 128)\n\n    #-----\n    # 1/8 x 1/8 x 128\n    #-----\n    MobileNetBlock(x, 128, 256, strides=2)\n    MobileNetBlock(x, 256, 256)\n\n    #-----\n    # 1/16 x 1/16 x 256\n    #-----\n    MobileNetBlock(x, 256, 512, strides=2)\n    MobileNetBlock(x, 512, 512, repeat=5)\n\n    #-----\n    # 1/32 x 1/32 x 1024\n    #-----\n    MobileNetBlock(x, 512, 1024, strides=2)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    #tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = MobileNet_v1(X, keep_prob, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = MobileNet_v1(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/NIN_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 128, 128\ntf.set_random_seed(0)\n\n\ndef NIN(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[5, 5], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv1\')\n    x = tf.layers.conv2d(inputs=x, filters=160, kernel_size=[1, 1], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'cccp1\')\n    x = tf.layers.conv2d(inputs=x, filters=96, kernel_size=[1, 1], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'cccp2\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[5, 5], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv2\')\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[1, 1], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'cccp3\')\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[1, 1], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'cccp4\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3\')\n    x = tf.layers.conv2d(inputs=x, filters=192, kernel_size=[1, 1], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'cccp5\')\n    \n    # GAP\n    x = tf.layers.conv2d(inputs=x, filters=num_classes, kernel_size=[1, 1], padding=\'same\', name=\'conv_out\')\n    x = tf.reduce_mean(x, axis=1)\n    x = tf.reduce_mean(x, axis=1)\n    \n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = NIN(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = NIN(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([pred_prob], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Res101_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res101(x, keep_prob, train=True):\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = tf.layers.conv2d(x, f_1, [1, 1], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, f_1, [3, 3], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], strides=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n        \n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'SAME\')\n\n    # Res block 2\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    # Res block 3\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n\n    # Res block 4\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    for _ in range(22):\n        x = ResBlock(x, 1024, 256, 1024)\n\n    # Res block 5\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res101(X, keep_pro, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res101(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Res152_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res152(x, keep_prob, train=True):\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = tf.layers.conv2d(x, f_1, [1, 1], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, f_1, [3, 3], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], strides=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n        \n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'SAME\')\n\n    # Res block 2\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    # Res block 3\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    for _ in range(7):\n        x = ResBlock(x, 512, 128, 512)\n\n    # Res block 4\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    for _ in range(35):\n        x = ResBlock(x, 1024, 256, 1024)\n\n    # Res block 5\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res152(X, keep_pro, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res152(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Res18_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res18(x, keep_prob, train=True):\n\n    def ResBlock(x, in_f, out_f, stride=1):\n        res_x = tf.layers.conv2d(x, out_f, [3, 3], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [3, 3], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], strides=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n        \n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    #x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'SAME\')\n    \n    # Res block 2\n    x = ResBlock(x, 64, 64)\n    x = ResBlock(x, 64, 64)\n\n    # Res block 3\n    x = ResBlock(x, 64, 128, stride=2)\n    x = ResBlock(x, 128, 128)\n\n    # Res block 4\n    x = ResBlock(x, 128, 256, stride=2)\n    x = ResBlock(x, 256, 256)\n\n    # Res block 5\n    x = ResBlock(x, 256, 512, stride=2)\n    x = ResBlock(x, 512, 512)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res18(X, keep_pro, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res18(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Res34_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res34(x, keep_prob, train=True):\n\n    def ResBlock(x, in_f, out_f, stride=1):\n        res_x = tf.layers.conv2d(x, out_f, [3, 3], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [3, 3], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], strides=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n        \n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    #x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'SAME\')\n    \n    # Res block 2\n    x = ResBlock(x, 64, 64)\n    x = ResBlock(x, 64, 64)\n    x = ResBlock(x, 64, 64)\n\n    # Res block 3\n    x = ResBlock(x, 64, 128, stride=2)\n    x = ResBlock(x, 128, 128)\n    x = ResBlock(x, 128, 128)\n    x = ResBlock(x, 128, 128)\n\n    # Res block 4\n    x = ResBlock(x, 128, 256, stride=2)\n    x = ResBlock(x, 256, 256)\n    x = ResBlock(x, 256, 256)\n    x = ResBlock(x, 256, 256)\n    x = ResBlock(x, 256, 256)\n    x = ResBlock(x, 256, 256)\n\n    # Res block 5\n    x = ResBlock(x, 256, 512, stride=2)\n    x = ResBlock(x, 512, 512)\n    x = ResBlock(x, 512, 512)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res34(X, keep_pro, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res34(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Res50_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\ntf.set_random_seed(0)\n\n\ndef Res50(x, keep_prob, train=False):\n\n    def ResBlock(x, in_f, f_1, out_f, stride=1):\n        res_x = tf.layers.conv2d(x, f_1, [1, 1], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, f_1, [3, 3], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], stride=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n\n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation_fn=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'same\')\n\n    x = ResBlock(x, 64, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n    x = ResBlock(x, 256, 64, 256)\n\n    x = ResBlock(x, 256, 128, 512, stride=2)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n    x = ResBlock(x, 512, 128, 512)\n\n    x = ResBlock(x, 512, 256, 1024, stride=2)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n    x = ResBlock(x, 1024, 256, 1024)\n\n    x = ResBlock(x, 1024, 512, 2048, stride=2)\n    x = ResBlock(x, 2048, 256, 2048)\n    x = ResBlock(x, 2048, 256, 2048)\n\n    #x = tf.layers.average_pooling2d(x, [img_height // 32, img_width // 32], strides=1, padding=\'VALID\')\n    #mb, h, w, c = x.get_shape().as_list()\n    #x = tf.reshape(x, [-1, h * w * c])\n    x = tf.reduce_mean(x, axis=[1,2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Res50(X, keep_pro, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Res50(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/ResNeXt50_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\n#tf.random.set_random_seed(0)\n\n\ndef ResNeXt50(x, keep_prob, train=True):\n\n    def ResNeXtBlock(x, in_f, f_1, out_f, stride=1, cardinality=32):\n        \n        depth = f_1 // cardinality\n        \n        res_x = tf.layers.conv2d(x, f_1, [1, 1], strides=stride, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.separable_conv2d(res_x, f_1, [3, 3], strides=1, padding=\'same\', activation=None, depth_multiplier=depth)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.conv2d(res_x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        if in_f != out_f:\n            x = tf.layers.conv2d(x, out_f, [1, 1], strides=1, padding=\'same\', activation=None)\n            x = tf.layers.batch_normalization(x, training=train)\n            x = tf.nn.relu(x)\n\n        if stride != 1:\n            x = tf.layers.max_pooling2d(x, [2, 2], strides=stride, padding=\'same\')\n\n        x = tf.add(res_x, x)\n        x = tf.nn.relu(x)\n        \n        return x\n\n    \n    x = tf.layers.conv2d(x, 64, [7, 7], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    \n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'SAME\')\n\n    x = ResNeXtBlock(x, 64, 64, 256)\n    x = ResNeXtBlock(x, 256, 64, 256)\n    x = ResNeXtBlock(x, 256, 64, 256)\n\n    x = ResNeXtBlock(x, 256, 128, 512, stride=2)\n    x = ResNeXtBlock(x, 512, 128, 512)\n    x = ResNeXtBlock(x, 512, 128, 512)\n    x = ResNeXtBlock(x, 512, 128, 512)\n\n    x = ResNeXtBlock(x, 512, 256, 1024, stride=2)\n    x = ResNeXtBlock(x, 1024, 256, 1024)\n    x = ResNeXtBlock(x, 1024, 256, 1024)\n    x = ResNeXtBlock(x, 1024, 256, 1024)\n    x = ResNeXtBlock(x, 1024, 256, 1024)\n    x = ResNeXtBlock(x, 1024, 256, 1024)\n\n    x = ResNeXtBlock(x, 1024, 512, 2048, stride=2)\n    x = ResNeXtBlock(x, 2048, 256, 2048)\n    x = ResNeXtBlock(x, 2048, 256, 2048)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    #tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = ResNeXt50(X, keep_prob, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = ResNeXt50(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/VGG16_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\ntf.set_random_seed(0)\n\n\ndef Mynet(x, keep_prob):\n    # block conv1\n    for i in range(2):\n        x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv1_{}\'.format(i+1))\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv2\n    for i in range(2):\n        x = tf.layers.conv2d(inputs=x, filters=128, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv2_{}\'.format(i+1))\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv3\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_{}\'.format(i+1))\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv4\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_{}\'.format(i+1))\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n    # block conv5\n    for i in range(3):\n        x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_{}\'.format(i+1))\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/VGG19_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\ntf.set_random_seed(0)\n\n\ndef VGG19(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv1_1\')\n    x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv1_2\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=128, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv2_1\')\n    x = tf.layers.conv2d(inputs=x, filters=128, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv2_2\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_1\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_2\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_3\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3_4\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_1\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_2\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_3\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4_4\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_1\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_2\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_3\')\n    x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5_4\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2)\n    \n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = VGG19(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = VGG19(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([pred_prob], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/Xception_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nCLS = [\'akahara\', \'madara\']\nnum_classes = len(CLS)\nimg_height, img_width = 128, 128\nchannel = 3\n#tf.random.set_random_seed(0)\n\n\ndef Xception(x, keep_prob, train=True):\n\n    def XceptionBlock(x, dim=728, cardinality=32):\n        \n        depth = dim // cardinality\n        \n        res_x = tf.nn.relu(x)\n        res_x = tf.layers.separable_conv2d(res_x, dim, [3, 3], strides=1, padding=\'same\', activation=None, depth_multiplier=depth)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.separable_conv2d(res_x, dim, [3, 3], strides=1, padding=\'same\', activation=None, depth_multiplier=depth)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n        res_x = tf.nn.relu(res_x)\n\n        res_x = tf.layers.separable_conv2d(res_x, dim, [3, 3], strides=1, padding=\'same\', activation=None, depth_multiplier=depth)\n        res_x = tf.layers.batch_normalization(res_x, training=train)\n\n        x = tf.add(res_x, x)\n        \n        return x\n\n    #-----\n    # Entry flow\n    #-----\n    # conv 1\n    x = tf.layers.conv2d(x, 32, [3, 3], strides=2, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n\n    #-----\n    # conv 2\n    x = tf.layers.conv2d(x, 64, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    \n    #-----\n    # conv 3\n    # skip connection\n    x_sc = tf.layers.conv2d(x, 128, [3, 3], strides=2, padding=""same"", activation=None)\n    x_sc = tf.layers.batch_normalization(x_sc, training=train)\n\n    # main stream\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 128, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 128, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'same\')\n\n    # add\n    x = tf.add(x, x_sc)\n\n    #-----\n    # conv 4\n    # skip connection\n    x_sc = tf.layers.conv2d(x, 256, [3, 3], strides=2, padding=""same"", activation=None)\n    x_sc = tf.layers.batch_normalization(x_sc, training=train)\n\n    # main stream\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 256, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 256, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'same\')\n\n    # add\n    x = tf.add(x, x_sc)\n\n    #-----\n    # conv 5\n    # skip connection\n    x_sc = tf.layers.conv2d(x, 728, [3, 3], strides=2, padding=""same"", activation=None)\n    x_sc = tf.layers.batch_normalization(x_sc, training=train)\n\n    # main stream\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 728, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 728, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'same\')\n\n    # add\n    x = tf.add(x, x_sc)\n\n    #-----\n    # Middle flow\n    for _ in range(8):\n        x = XceptionBlock(x)\n\n    #-----\n    # Exit flow\n    # Exit flow 1\n    # skip connection\n    x_sc = tf.layers.conv2d(x, 1024, [3, 3], strides=2, padding=""same"", activation=None)\n    x_sc = tf.layers.batch_normalization(x_sc, training=train)\n\n    # main stream\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 728, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 1024, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.layers.max_pooling2d(x, [3, 3], strides=2, padding=\'same\')\n\n    # add\n    x = tf.add(x, x_sc)\n\n    #-----\n    # Exit flow 2\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 1536, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n    x = tf.nn.relu(x)\n    x = tf.layers.conv2d(x, 2048, [3, 3], strides=1, padding=""same"", activation=None)\n    x = tf.layers.batch_normalization(x, training=train)\n\n    x = tf.reduce_mean(x, axis=[1, 2])\n    x = tf.layers.dense(x, num_classes)\n    \n    return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    #tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, channel])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Xception(X, keep_prob, train=False)\n    preds = tf.nn.softmax(logits)\n    \n    #loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = copy.copy(train_ind)[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.7})\n\n            if (i + 1) % 10 == 0:\n                print(""iter >>"", i+1, \', loss >>\', los / mb, \', accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n        \n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Xception(X, keep_prob, train=False)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/scripts_tf_layers/ZFNet_tf_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\ntf.set_random_seed(0)\n\n\ndef ZFNet(x, keep_prob):\n    x = tf.layers.conv2d(inputs=x, filters=96, kernel_size=[7, 7], strides=2, padding=\'valid\', activation=tf.nn.relu, name=\'conv1\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.keras.layers.ZeroPadding2D((1,1))(x)\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[5, 5], strides=2, padding=\'valid\', activation=tf.nn.relu, name=\'conv2\')\n    x = tf.nn.local_response_normalization(x)\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv3\')\n    x = tf.layers.conv2d(inputs=x, filters=384, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv4\')\n    x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=[3, 3], strides=1, padding=\'same\', activation=tf.nn.relu, name=\'conv5\')\n    x = tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=2)\n\n    mb, h, w, c = x.get_shape().as_list()\n    x = tf.reshape(x, [-1, h*w*c])\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc1\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=4096, activation=tf.nn.relu, name=\'fc2\')\n    x = tf.nn.dropout(x, keep_prob=keep_prob)\n    x = tf.layers.dense(inputs=x, units=num_classes, name=\'fc_out\')\n    return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            t = [0 for _ in range(num_classes)]\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t[i] = 1\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = ZFNet(X, keep_prob)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = ZFNet(X, keep_prob)\n    out = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([pred_prob], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0]\n\n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_chainer/bdlstm_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = -1\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.h = L.NStepBiLSTM(n_layers=1, in_size=num_classes, out_size=128, dropout=0)\n            self.out = L.Linear(None, num_classes)\n        \n    def forward(self, x):\n        hy, cy, x = self.h(None, None, x)\n        x = np.array([_x.data for _x in x])\n        x = F.transpose(x, axes=(1,0,2))\n        x = F.vstack([np.array([_x.data[-1] for _x in x])])\n        #x = chainer.Variable(np.array([_h[-1].data for _h in h]))\n        x = self.out(x)\n        return x\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.01)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        _x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        x = []\n        for __x in _x.transpose(1,0,2):\n            x.append(__x)\n            \n        #if GPU >= 0:\n        #    x = chainer.cuda.to_gpu(x)\n        #    t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        _x = np.expand_dims(np.array(x, dtype=np.float32), axis=0)\n        x = []\n        for __x in _x.transpose(1,0,2):\n            x.append(__x)\n\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_chainer/gru_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = -1\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.h = L.GRU(None, 64)\n            self.out = L.Linear(None, num_classes)\n        \n    def forward(self, x):\n        x = self.h(x)\n        x = self.out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.01)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(200):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x, dtype=np.float32), axis=0)\n\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_chainer/lstm_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = -1\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(chainer.Chain):\n    def __init__(self, train=True):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.h = L.LSTM(None, 64)\n            self.out = L.Linear(None, num_classes)\n        \n    def forward(self, x):\n        x = self.h(x)\n        x = self.out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.Adam(0.01)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(200):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x, dtype=np.float32), axis=0)\n\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x).data\n        pred = F.softmax(pred)\n\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n                \n        pred = pred[0].data\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_keras/bdlstm_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, SimpleRNN, LSTM, Bidirectional\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\ndef Mynet():\n    inputs = Input([n_gram, num_classes])\n    x = Bidirectional(LSTM(128, return_sequences=False))(inputs)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    \n    return model\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.Adam(lr=0.001),\n        metrics=[\'accuracy\'])\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x), axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_keras/gru_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, SimpleRNN, LSTM, GRU\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\ndef Mynet():\n    inputs = Input([n_gram, num_classes])\n    x = GRU(256, return_sequences=False)(inputs)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    \n    return model\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.Adam(lr=0.001),\n        metrics=[\'accuracy\'])\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x), axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_keras/lstm_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, SimpleRNN, LSTM\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\ndef Mynet():\n    inputs = Input([n_gram, num_classes])\n    x = LSTM(128, return_sequences=False)(inputs)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    \n    return model\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.Adam(lr=0.001),\n        metrics=[\'accuracy\'])\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x), axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_keras/rnn_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, SimpleRNN\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\ndef Mynet():\n    inputs = Input([n_gram, num_classes])\n    x = SimpleRNN(128, return_sequences=False)(inputs)\n    x = Dense(num_classes, activation=\'softmax\')(x)\n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    \n    return model\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # model\n    model = Mynet()\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.Adam(lr=0.001),\n        metrics=[\'accuracy\'])\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x=x, y=t)\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet()\n    model.load_weights(\'model.h5\')\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    while pred != \'@\' and count < 1000:\n        in_txt = gens[-n_gram:]\n        x = []\n        for _in in in_txt:\n            _x = [0 for _ in range(num_classes)]\n            _x[chars.index(_in)] = 1\n            x.append(_x)\n        \n        x = np.expand_dims(np.array(x), axis=0)\n        \n        pred = model.predict_on_batch(x)[0]\n\n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_keras/seq2seq_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, SimpleRNN, LSTM\n\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nd_num = 1024\n\ndef Encoder():\n    enc_in = Input([None, num_classes], name=\'enc_in\')\n    enc_out, esh, esc = LSTM(d_num, return_state=True, name=\'enc\')(enc_in)\n    return enc_in, enc_out, esh, esc\n    \n\ndef Decoder(dec_sh, dec_sc):\n    dec_in = Input([None, num_classes], name=\'dec_in\')\n    dec_out, dsh , dsc = LSTM(d_num, return_sequences=True, return_state=True, name=\'dec\')(dec_in, initial_state=[dec_sh, dec_sc])\n    #dec_out = Dense(256, activation=\'sigmoid\')(dec_out)\n    dec_out = Dense(num_classes, activation=\'softmax\', name=\'dec_out\')(dec_out)\n    return dec_in, dec_out, dsh, dsc\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    onehots = []\n    txts = []\n    max_len = 0\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt = \'@\' + l.strip() + \'@\'\n            txts.append(txt)\n            max_len = max(max_len, len(txt))\n\n    for txt in txts:\n        onehot = [[0 for _ in range(num_classes)] for _ in range(max_len)]\n        for i, c in enumerate(txt):\n            onehot[i][chars.index(c)] = 1\n        onehots.append(onehot)\n\n    onehots = np.array(onehots)\n\n    # enc_xs, dec_xs, ts ... [batch, time, num_classes]    \n    enc_xs = np.array([v for v in onehots[:-1]])\n    dec_xs = np.array([v for v in onehots[1:]])\n    ts = np.zeros_like(dec_xs)\n    ts[:, :-1] = onehots[1:, 1:]\n\n    return enc_xs, dec_xs, ts\n\n\n# train\ndef train():\n    # model\n    enc_in, enc_out, esh, esc = Encoder()\n    dec_in, dec_out, dsh, dsc = Decoder(esh, esc)\n    model = Model(inputs=[enc_in, dec_in], outputs=[dec_out])\n    \n\n    model.compile(\n        loss=\'categorical_crossentropy\',\n        optimizer=keras.optimizers.Adam(lr=0.01),\n        metrics=[\'accuracy\'])\n\n    #model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    enc_xs, dec_xs, ts = data_load()\n    \n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(enc_xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for ite in range(500):\n        if mbi + mb > len(enc_xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(enc_xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        enc_x = enc_xs[mb_ind]\n        dec_x = dec_xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x={\'enc_in\': enc_x, \'dec_in\': dec_x}, y=t)\n        print(""iter >>"", ite+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    enc_in, enc_out, esh, esc = Encoder()\n    model_encoder = Model(inputs=[enc_in], outputs=[esh, esc])\n    model_encoder.load_weights(\'model.h5\', by_name=True)\n\n    dec_sh = Input([d_num], name=\'dec_state_h\')\n    dec_sc = Input([d_num], name=\'dec_state_c\')\n    dec_in, dec_out, dsh, dsc = Decoder(dec_sh, dec_sc)\n    model_decoder = Model(inputs=[dec_in, dec_sh, dec_sc], outputs=[dec_out, dsh, dsc])\n    model_decoder.load_weights(\'model.h5\', by_name=True)\n\n    \n    def decode(x):\n        return chars[x.argmax()]\n    \n    encs = \'@\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\x84\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\x8b\xe3\x82\x8f\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84@\'\n\n    enc_x = []\n    for enc in encs:\n        onehot = [0 for _ in range(num_classes)]\n        onehot[chars.index(enc)] = 1\n        enc_x.append(onehot)\n    enc_x = np.expand_dims(np.array(enc_x), axis=0)\n\n    dec_state_h, dec_state_c = model_encoder.predict_on_batch(x={\'enc_in\':enc_x})\n    \n    pred = 0\n    count = 0\n\n    dec_x = np.zeros((1, 1, num_classes))\n    dec_x[..., chars.index(\'@\')] = 1\n    \n    gens = \'\'\n    \n    while pred != \'@\' and count < 1000:\n        \n        pred, dec_state_h, dec_state_c = model_decoder.predict_on_batch(\n            x={\'dec_in\': dec_x, \'dec_state_h\':dec_state_h, \'dec_state_c\':dec_state_c})\n\n        pred = pred[0,0]\n        \n        # sample random from probability distribution\n        ind = np.random.choice(num_classes, 1, p=pred)\n        \n        pred = chars[ind[0]]\n        gens += pred\n        count += 1\n        \n        dec_x = np.zeros((1, 1, num_classes))\n        dec_x[..., ind] = 1\n\n    # pose process\n    gens = gens.replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(\'[In]Speaker.A: >>\', encs.replace(\'@\', \'\'))\n    print(\'[Out]Speaker.B: >>\', gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_tf_slim/bdlstm_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nprint(num_classes)\n\ndef Mynet(x):        \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False))(x)\n    x = slim.fully_connected(x, num_classes)\n    return x\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(2000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X:x, Y:t, keep_prob:0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X)\n    out = tf.nn.softmax(logits)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n        \n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n        \n            x = np.expand_dims(np.array(x), axis=0)\n            \n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n        \n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n\n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_tf_slim/gru_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\n\ndef Mynet(x):        \n    x = tf.keras.layers.GRU(256, return_sequences=False)(x)\n    x = slim.fully_connected(x, num_classes)\n    return x\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X:x, Y:t, keep_prob:0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X)\n    out = tf.nn.softmax(logits)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n        \n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n        \n            x = np.expand_dims(np.array(x), axis=0)\n            \n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n        \n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n\n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_tf_slim/lstm_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nprint(num_classes)\n\ndef Mynet(x):        \n    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n    x = slim.fully_connected(x, num_classes)\n    return x\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(2000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X:x, Y:t, keep_prob:0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X)\n    out = tf.nn.softmax(logits)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n        \n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n        \n            x = np.expand_dims(np.array(x), axis=0)\n            \n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n        \n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n\n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/scripts_tf_slim/rnn_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\ndef Mynet(x):        \n    x = tf.keras.layers.SimpleRNN(128, return_sequences=False)(x)\n    x = slim.fully_connected(x, num_classes)\n    return x\n\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(onehots[i+n_gram])\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X)\n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    xs, ts = data_load()\n    \n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X:x, Y:t, keep_prob:0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, n_gram, num_classes])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X)\n    out = tf.nn.softmax(logits)\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""./cnn.ckpt"")\n        \n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n        \n            x = np.expand_dims(np.array(x), axis=0)\n            \n            pred = sess.run([out], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0]\n        \n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n        \n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/bin_dataset_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/bin_loss_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.conv1 = chainer.Sequential()\n            for i in range(6):\n                self.conv1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))                \n                self.conv1.append(L.BatchNormalization(32))\n                self.conv1.append(F.relu)\n                \n            self.out = L.Convolution2D(None, 1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        loss = F.sigmoid_cross_entropy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/bin_test_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.conv1 = chainer.Sequential()\n            for i in range(6):\n                self.conv1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))                \n                self.conv1.append(L.BatchNormalization(32))\n                self.conv1.append(F.relu)\n                \n            self.out = L.Convolution2D(None, 1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        loss = F.sigmoid_cross_entropy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            \n        pred = model(x)\n    \n        pred = F.sigmoid(pred)\n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0,0]\n\n        ## binalization\n        bin_pred = pred.copy()\n        th = 0.5\n        bin_pred[bin_pred >= th] = 1\n        bin_pred[bin_pred < th] = 0\n\n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,3,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,3,2)\n        plt.imshow(pred, cmap=\'gray\')\n        plt.title(""predicted"")\n        plt.subplot(1,3,3)\n        plt.imshow(bin_pred, cmap=\'gray\')\n        plt.title(""after binalization"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/concat_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.enc1 = chainer.Sequential()\n            for i in range(2):\n                self.enc1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc1.append(F.relu)\n                self.enc1.append(L.BatchNormalization(32))\n\n            self.enc2 = chainer.Sequential()\n            for i in range(2):\n                self.enc2.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc2.append(F.relu)\n                self.enc2.append(L.BatchNormalization(32))\n\n            self.upsample = chainer.Sequential()\n            self.upsample.append(L.Deconvolution2D(None, 32, ksize=2, stride=2))\n            self.upsample.append(F.relu)\n            self.upsample.append(L.BatchNormalization(32))\n\n            self.concat = chainer.Sequential()\n            self.concat.append(L.Convolution2D(None, 32, ksize=1, stride=1))\n            self.concat.append(F.relu)\n            self.concat.append(L.BatchNormalization(32))\n\n            self.dec1 = chainer.Sequential()\n            for i in range(2):\n                self.dec1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec1.append(F.relu)\n                self.dec1.append(L.BatchNormalization(32))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        enc1 = F.copy(x, dst=-1)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = self.enc2(x)\n\n        mb, c, h, w = x.shape\n        x = self.upsample(x)\n\n        x = F.concat((x, enc1), axis=1)\n        x = self.concat(x)\n\n        x = self.dec1(x)\n        \n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/nearest_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.enc1 = chainer.Sequential()\n            for i in range(2):\n                self.enc1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc1.append(F.relu)\n                self.enc1.append(L.BatchNormalization(32))\n\n            self.enc2 = chainer.Sequential()\n            for i in range(2):\n                self.enc2.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc2.append(F.relu)\n                self.enc2.append(L.BatchNormalization(32))\n\n            self.dec1 = chainer.Sequential()\n            for i in range(2):\n                self.dec1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec1.append(F.relu)\n                self.dec1.append(L.BatchNormalization(32))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = self.enc2(x)\n\n        mb, c, h, w = x.shape\n        x = chainer.functions.resize_images(x, output_shape=[h*2, w*2])\n\n        x = self.dec1(x)\n        \n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/semaseg_dataset_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/semaseg_loss_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.conv1 = chainer.Sequential()\n            for i in range(6):\n                self.conv1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.conv1.append(F.relu)\n                self.conv1.append(L.BatchNormalization(32))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/semaseg_test_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.conv1 = chainer.Sequential()\n            for i in range(6):\n                self.conv1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.conv1.append(F.relu)\n                self.conv1.append(L.BatchNormalization(32))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/transposeconv_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = -1\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.enc1 = chainer.Sequential()\n            for i in range(2):\n                self.enc1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc1.append(F.relu)\n                self.enc1.append(L.BatchNormalization(32))\n\n            self.enc2 = chainer.Sequential()\n            for i in range(2):\n                self.enc2.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc2.append(F.relu)\n                self.enc2.append(L.BatchNormalization(32))\n\n            self.upsample = chainer.Sequential()\n            self.upsample.append(L.Deconvolution2D(None, 32, ksize=2, stride=2))\n            self.upsample.append(F.relu)\n            self.upsample.append(L.BatchNormalization(32))\n\n            self.dec1 = chainer.Sequential()\n            for i in range(2):\n                self.dec1.append(L.Convolution2D(None, 32, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec1.append(F.relu)\n                self.dec1.append(L.BatchNormalization(32))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = F.max_pooling_2d(x, ksize=2, stride=2)\n\n        x = self.enc2(x)\n\n        mb, c, h, w = x.shape\n        x = chainer.functions.resize_images(x, output_shape=[h*2, w*2])\n\n        x = self.dec1(x)\n        \n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/unet_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 236, 236 #572, 572\nout_height, out_width = 52, 52 #388, 388\nGPU = -1\n\ndef crop_layer(layer, size):\n    _, _, h, w = layer.shape\n    _, _, _h, _w = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, :, ph:ph+_h, pw:pw+_w]\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        base = 64\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.enc1 = chainer.Sequential()\n            for i in range(2):\n                self.enc1.append(L.Convolution2D(None, base, ksize=3, pad=0, stride=1, nobias=True))\n                self.enc1.append(F.relu)\n                self.enc1.append(L.BatchNormalization(base))\n\n            self.enc2 = chainer.Sequential()\n            for i in range(2):\n                self.enc2.append(L.Convolution2D(None, base*2, ksize=3, pad=0, stride=1, nobias=True))\n                self.enc2.append(F.relu)\n                self.enc2.append(L.BatchNormalization(base*2))\n\n            self.enc3 = chainer.Sequential()\n            for i in range(2):\n                self.enc3.append(L.Convolution2D(None, base*4, ksize=3, pad=0, stride=1, nobias=True))\n                self.enc3.append(F.relu)\n                self.enc3.append(L.BatchNormalization(base*4))\n\n            self.enc4 = chainer.Sequential()\n            for i in range(2):\n                self.enc4.append(L.Convolution2D(None, base*8, ksize=3, pad=0, stride=1, nobias=True))\n                self.enc4.append(F.relu)\n                self.enc4.append(L.BatchNormalization(base*8))\n\n            self.enc5 = chainer.Sequential()\n            for i in range(2):\n                self.enc5.append(L.Convolution2D(None, base*16, ksize=3, pad=0, stride=1, nobias=True))\n                self.enc5.append(F.relu)\n                self.enc5.append(L.BatchNormalization(base*16))\n\n            self.upsample4 = chainer.Sequential()\n            self.upsample4.append(L.Deconvolution2D(None, base*8, ksize=2, stride=2))\n            self.upsample4.append(F.relu)\n            self.upsample4.append(L.BatchNormalization(base*8))\n\n            self.dec4 = chainer.Sequential()\n            for i in range(2):\n                self.dec4.append(L.Convolution2D(None, base*8, ksize=3, pad=0, stride=1, nobias=True))\n                self.dec4.append(F.relu)\n                self.dec4.append(L.BatchNormalization(base*8))\n\n            self.upsample3 = chainer.Sequential()\n            self.upsample3.append(L.Deconvolution2D(None, base*4, ksize=2, stride=2))\n            self.upsample3.append(F.relu)\n            self.upsample3.append(L.BatchNormalization(base*4))\n\n            self.dec3 = chainer.Sequential()\n            for i in range(2):\n                self.dec3.append(L.Convolution2D(None, base*4, ksize=3, pad=0, stride=1, nobias=True))\n                self.dec3.append(F.relu)\n                self.dec3.append(L.BatchNormalization(base*4))\n\n            self.upsample2 = chainer.Sequential()\n            self.upsample2.append(L.Deconvolution2D(None, base*2, ksize=2, stride=2))\n            self.upsample2.append(F.relu)\n            self.upsample2.append(L.BatchNormalization(base*2))\n\n            self.dec2 = chainer.Sequential()\n            for i in range(2):\n                self.dec2.append(L.Convolution2D(None, base*2, ksize=3, pad=0, stride=1, nobias=True))\n                self.dec2.append(F.relu)\n                self.dec2.append(L.BatchNormalization(base*2))\n\n            self.upsample1 = chainer.Sequential()\n            self.upsample1.append(L.Deconvolution2D(None, base, ksize=2, stride=2))\n            self.upsample1.append(F.relu)\n            self.upsample1.append(L.BatchNormalization(base))\n\n            self.dec1 = chainer.Sequential()\n            for i in range(2):\n                self.dec1.append(L.Convolution2D(None, base, ksize=3, pad=0, stride=1, nobias=True))\n                self.dec1.append(F.relu)\n                self.dec1.append(L.BatchNormalization(base))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        enc1 = self.enc1(x)\n\n        enc2 = F.max_pooling_2d(enc1, ksize=2, stride=2)\n        enc2 = self.enc2(enc2)\n\n        enc3 = F.max_pooling_2d(enc2, ksize=2, stride=2)\n        enc3 = self.enc3(enc3)\n\n        enc4 = F.max_pooling_2d(enc3, ksize=2, stride=2)\n        enc4 = self.enc4(enc4)\n\n        enc5 = F.max_pooling_2d(enc4, ksize=2, stride=2)\n        enc5 = self.enc5(enc5)\n\n        dec4 = self.upsample4(enc5)\n        _enc4 = crop_layer(enc4, dec4.shape)\n        dec4 = F.concat([dec4, _enc4], axis=1)\n        dec4 = self.dec4(dec4)\n\n        dec3 = self.upsample3(dec4)\n        _enc3 = crop_layer(enc3, dec3.shape)\n        dec3 = F.concat([dec3, _enc3], axis=1)\n        dec3 = self.dec3(dec3)\n\n        dec2 = self.upsample2(dec3)\n        _enc2 = crop_layer(enc2, dec2.shape)\n        dec2 = F.concat([dec2, _enc2], axis=1)\n        dec2 = self.dec2(dec2)\n\n        dec1 = self.upsample1(dec2)\n        _enc1 = crop_layer(enc1, dec1.shape)\n        dec1 = F.concat([dec1, _enc1], axis=1)\n        dec1 = self.dec1(dec1)\n        \n        out = self.out(dec1)\n        return out\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_chainer/unetlike_chainer.py,0,"b'import chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nGPU = -1\n\ndef crop_layer(layer, size):\n    _, _, h, w = layer.shape\n    _, _, _h, _w = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, :, ph:ph+_h, pw:pw+_w]\n    \nclass Mynet(chainer.Chain):\n    def __init__(self, train=False):\n        self.train = train\n        base = 64\n        \n        super(Mynet, self).__init__()\n        with self.init_scope():\n            self.enc1 = chainer.Sequential()\n            for i in range(2):\n                self.enc1.append(L.Convolution2D(None, base, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc1.append(F.relu)\n                self.enc1.append(L.BatchNormalization(base))\n\n            self.enc2 = chainer.Sequential()\n            for i in range(2):\n                self.enc2.append(L.Convolution2D(None, base*2, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc2.append(F.relu)\n                self.enc2.append(L.BatchNormalization(base*2))\n\n            self.enc3 = chainer.Sequential()\n            for i in range(2):\n                self.enc3.append(L.Convolution2D(None, base*4, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc3.append(F.relu)\n                self.enc3.append(L.BatchNormalization(base*4))\n\n            self.enc4 = chainer.Sequential()\n            for i in range(2):\n                self.enc4.append(L.Convolution2D(None, base*8, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc4.append(F.relu)\n                self.enc4.append(L.BatchNormalization(base*8))\n\n            self.enc5 = chainer.Sequential()\n            for i in range(2):\n                self.enc5.append(L.Convolution2D(None, base*16, ksize=3, pad=1, stride=1, nobias=True))\n                self.enc5.append(F.relu)\n                self.enc5.append(L.BatchNormalization(base*16))\n\n            self.upsample4 = chainer.Sequential()\n            self.upsample4.append(L.Deconvolution2D(None, base*8, ksize=2, stride=2))\n            self.upsample4.append(F.relu)\n            self.upsample4.append(L.BatchNormalization(base*8))\n\n            self.dec4 = chainer.Sequential()\n            for i in range(2):\n                self.dec4.append(L.Convolution2D(None, base*8, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec4.append(F.relu)\n                self.dec4.append(L.BatchNormalization(base*8))\n\n            self.upsample3 = chainer.Sequential()\n            self.upsample3.append(L.Deconvolution2D(None, base*4, ksize=2, stride=2))\n            self.upsample3.append(F.relu)\n            self.upsample3.append(L.BatchNormalization(base*4))\n\n            self.dec3 = chainer.Sequential()\n            for i in range(2):\n                self.dec3.append(L.Convolution2D(None, base*4, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec3.append(F.relu)\n                self.dec3.append(L.BatchNormalization(base*4))\n\n            self.upsample2 = chainer.Sequential()\n            self.upsample2.append(L.Deconvolution2D(None, base*2, ksize=2, stride=2))\n            self.upsample2.append(F.relu)\n            self.upsample2.append(L.BatchNormalization(base*2))\n\n            self.dec2 = chainer.Sequential()\n            for i in range(2):\n                self.dec2.append(L.Convolution2D(None, base*2, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec2.append(F.relu)\n                self.dec2.append(L.BatchNormalization(base*2))\n\n            self.upsample1 = chainer.Sequential()\n            self.upsample1.append(L.Deconvolution2D(None, base, ksize=2, stride=2))\n            self.upsample1.append(F.relu)\n            self.upsample1.append(L.BatchNormalization(base))\n\n            self.dec1 = chainer.Sequential()\n            for i in range(2):\n                self.dec1.append(L.Convolution2D(None, base, ksize=3, pad=1, stride=1, nobias=True))\n                self.dec1.append(F.relu)\n                self.dec1.append(L.BatchNormalization(base))\n                \n            self.out = L.Convolution2D(None, num_classes+1, ksize=1, pad=0, stride=1, nobias=False)\n        \n    def forward(self, x):\n        # block conv1\n        enc1 = self.enc1(x)\n\n        enc2 = F.max_pooling_2d(enc1, ksize=2, stride=2)\n        enc2 = self.enc2(enc2)\n\n        enc3 = F.max_pooling_2d(enc2, ksize=2, stride=2)\n        enc3 = self.enc3(enc3)\n\n        enc4 = F.max_pooling_2d(enc3, ksize=2, stride=2)\n        enc4 = self.enc4(enc4)\n\n        enc5 = F.max_pooling_2d(enc4, ksize=2, stride=2)\n        enc5 = self.enc5(enc5)\n\n        dec4 = self.upsample4(enc5)\n        _enc4 = crop_layer(enc4, dec4.shape)\n        dec4 = F.concat([dec4, _enc4], axis=1)\n        dec4 = self.dec4(dec4)\n\n        dec3 = self.upsample3(dec4)\n        _enc3 = crop_layer(enc3, dec3.shape)\n        dec3 = F.concat([dec3, _enc3], axis=1)\n        dec3 = self.dec3(dec3)\n\n        dec2 = self.upsample2(dec3)\n        _enc2 = crop_layer(enc2, dec2.shape)\n        dec2 = F.concat([dec2, _enc2], axis=1)\n        dec2 = self.dec2(dec2)\n\n        dec1 = self.upsample1(dec2)\n        _enc1 = crop_layer(enc1, dec1.shape)\n        dec1 = F.concat([dec1, _enc1], axis=1)\n        dec1 = self.dec1(dec1)\n        \n        out = self.out(dec1)\n        return out\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = Mynet(train=True)\n\n    if GPU >= 0:\n        chainer.cuda.get_device(GPU).use()\n        model.to_gpu()\n    \n    opt = chainer.optimizers.MomentumSGD(0.01, momentum=0.9)\n    opt.setup(model)\n    #opt.add_hook(chainer.optimizer.WeightDecay(0.0005))\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n            \n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n            t = chainer.cuda.to_gpu(t)\n        #else:\n        #    x = chainer.Variable(x)\n        #    t = chainer.Variable(t)\n\n        y = model(x)\n\n        #accu = F.accuracy(y, t[..., 0])\n        y = F.transpose(y, axes=(0,2,3,1))\n        y = F.reshape(y, [-1, num_classes+1])\n        t = F.reshape(t, [-1])\n        loss = F.softmax_cross_entropy(y, t)\n        accu = F.accuracy(y, t)\n\n        model.cleargrads()\n        loss.backward()\n        opt.update()\n\n        loss = loss.data\n        accu = accu.data\n        if GPU >= 0:\n            loss = chainer.cuda.to_cpu(loss)\n            accu = chainer.cuda.to_cpu(accu)\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', accu)\n\n    chainer.serializers.save_npz(\'cnn.npz\', model)\n\n# test\ndef test():\n    model = Mynet(train=False)\n\n    if GPU >= 0:\n        chainer.cuda.get_device_from_id(cf.GPU).use()\n        model.to_gpu()\n\n    ## Load pretrained parameters\n    chainer.serializers.load_npz(\'cnn.npz\', model)\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        if GPU >= 0:\n            x = chainer.cuda.to_gpu(x)\n\n        pred = model(x)\n\n        pred = F.transpose(pred, axes=(0,2,3,1))\n        pred = F.reshape(pred, [-1, num_classes+1])\n        pred = F.softmax(pred)\n        pred = F.reshape(pred, [-1, out_height, out_width, num_classes+1])\n        \n        if GPU >= 0:\n            pred = chainer.cuda.to_cpu(pred)\n        pred = pred.data[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n        \n        x = chainer.cuda.to_cpu(x) if GPU >= 0 else x\n        plt.subplot(1,2,1)\n        plt.imshow(x[0].transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/bin_dataset_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/bin_loss_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(6):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n\n    x = Conv2D(1, (1, 1), padding=\'same\', strides=1, name=\'out\', activation=\'sigmoid\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'binary_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/bin_test_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(6):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n\n    x = Conv2D(1, (1, 1), padding=\'same\', strides=1, name=\'out\', activation=\'sigmoid\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'binary_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0, ..., 0]\n        \n        #pred = pred.detach().cpu().numpy()[0, ..., 0]\n\n        ## binalization\n        bin_pred = pred.copy()\n        th = 0.5\n        bin_pred[bin_pred >= th] = 1\n        bin_pred[bin_pred < th] = 0\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,3,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,3,2)\n        plt.imshow(pred, cmap=\'gray\')\n        plt.title(""predicted"")\n        plt.subplot(1,3,3)\n        plt.imshow(bin_pred, cmap=\'gray\')\n        plt.title(""after binalization"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/concat_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    enc1 = x\n\n    x = MaxPooling2D((2,2), 2)(x)\n\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv2_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = keras.layers.Conv2DTranspose(32, (2,2), strides=2, padding=\'same\')(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization()(x)\n\n    x = keras.layers.concatenate([x, enc1])\n    x = Conv2D(32, (1, 1), padding=\'same\', strides=1, name=\'concat_conv\')(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization()(x)\n    \n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'dec1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n    \n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/nearest_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = MaxPooling2D((2,2), 2)(x)\n    \n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv2_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = keras.layers.UpSampling2D(size=(2,2), interpolation=\'nearest\')(x)\n\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'dec1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n    \n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/semaseg_dataset_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(6):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n\n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/semaseg_loss_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(6):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n\n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/semaseg_test_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(6):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/transposeconv_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(train=False):\n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    x = inputs\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = MaxPooling2D((2,2), 2)(x)\n    \n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'conv2_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n\n    x = keras.layers.Conv2DTranspose(32, (2,2), strides=2, padding=\'same\')(x)\n    x = Activation(\'relu\')(x)\n    x = BatchNormalization()(x)\n\n    for i in range(2):\n        x = Conv2D(32, (3, 3), padding=\'same\', strides=1, name=\'dec1_{}\'.format(i+1))(x)\n        x = Activation(\'relu\')(x)\n        x = BatchNormalization()(x)\n    \n    x = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(x)\n    x = Reshape([-1, num_classes+1])(x)\n    x = Activation(\'softmax\', name=\'out\')(x)\n    \n    model = Model(inputs=inputs, outputs=x, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/unet_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 236, 236 #572, 572\nout_height, out_width = 52, 52 #388, 388\n\ndef crop_layer(layer, size):\n    _, h, w, _ = keras.backend.int_shape(layer)\n    _, _h, _w, _ = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return keras.layers.Cropping2D(cropping=((ph, ph), (pw, pw)))(layer)\n\n\ndef Mynet(train=False):\n    base = 64\n    \n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    enc1= inputs\n\n    for i in range(2):\n        enc1 = Conv2D(base, (3, 3), padding=\'valid\', strides=1, name=\'conv1_{}\'.format(i+1))(enc1)\n        enc1 = Activation(\'relu\')(enc1)\n        enc1 = BatchNormalization()(enc1)\n\n    enc2 = MaxPooling2D((2,2), 2)(enc1)\n    \n    for i in range(2):\n        enc2 = Conv2D(base*2, (3, 3), padding=\'valid\', strides=1, name=\'conv2_{}\'.format(i+1))(enc2)\n        enc2 = Activation(\'relu\')(enc2)\n        enc2 = BatchNormalization()(enc2)\n\n    enc3 = MaxPooling2D((2,2), 2)(enc2)\n\n    for i in range(2):\n        enc3 = Conv2D(base*4, (3, 3), padding=\'valid\', strides=1, name=\'conv3_{}\'.format(i+1))(enc3)\n        enc3 = Activation(\'relu\')(enc3)\n        enc3 = BatchNormalization()(enc3)\n\n    enc4 = MaxPooling2D((2,2), 2)(enc3)\n\n    for i in range(2):\n        enc4 = Conv2D(base*8, (3, 3), padding=\'valid\', strides=1, name=\'conv4_{}\'.format(i+1))(enc4)\n        enc4 = Activation(\'relu\')(enc4)\n        enc4 = BatchNormalization()(enc4)\n\n    enc5 = MaxPooling2D((2,2), 2)(enc4)\n\n    for i in range(2):\n        enc5 = Conv2D(base*16, (3, 3), padding=\'valid\', strides=1, name=\'conv5_{}\'.format(i+1))(enc5)\n        enc5 = Activation(\'relu\')(enc5)\n        enc5 = BatchNormalization()(enc5)\n\n    dec4 = keras.layers.Conv2DTranspose(base*8, (2,2), strides=2, padding=\'same\')(enc5)\n    dec4 = Activation(\'relu\')(dec4)\n    dec4 = BatchNormalization()(dec4)\n    _enc4 = crop_layer(enc4, keras.backend.int_shape(dec4))\n    dec4 = keras.layers.concatenate([dec4, _enc4])\n    for i in range(2):\n        dec4 = Conv2D(base*8, (3, 3), padding=\'valid\', strides=1, name=\'dec4_{}\'.format(i+1))(dec4)\n        dec4 = Activation(\'relu\')(dec4)\n        dec4 = BatchNormalization()(dec4)\n\n    dec3 = keras.layers.Conv2DTranspose(base*4, (2,2), strides=2, padding=\'same\')(dec4)\n    dec3 = Activation(\'relu\')(dec3)\n    dec3 = BatchNormalization()(dec3)\n    _enc3 = crop_layer(enc3, keras.backend.int_shape(dec3))\n    dec3 = keras.layers.concatenate([dec3, _enc3])\n    for i in range(2):\n        dec3 = Conv2D(base*4, (3, 3), padding=\'valid\', strides=1, name=\'dec3_{}\'.format(i+1))(dec3)\n        dec3 = Activation(\'relu\')(dec3)\n        dec3 = BatchNormalization()(dec3)\n\n    dec2 = keras.layers.Conv2DTranspose(base*2, (2,2), strides=2, padding=\'same\')(dec3)\n    dec2 = Activation(\'relu\')(dec2)\n    dec2 = BatchNormalization()(dec2)\n    _enc2 = crop_layer(enc2, keras.backend.int_shape(dec2))\n    dec2 = keras.layers.concatenate([dec2, _enc2])\n    for i in range(2):\n        dec2 = Conv2D(base*2, (3, 3), padding=\'valid\', strides=1, name=\'dec2_{}\'.format(i+1))(dec2)\n        dec2 = Activation(\'relu\')(dec2)\n        dec2 = BatchNormalization()(dec2)\n\n    dec1 = keras.layers.Conv2DTranspose(base, (2,2), strides=2, padding=\'same\')(dec2)\n    dec1 = Activation(\'relu\')(dec1)\n    dec1 = BatchNormalization()(dec1)\n    _enc1 = crop_layer(enc1, keras.backend.int_shape(dec1))\n    dec1 = keras.layers.concatenate([dec1, _enc1])\n    for i in range(2):\n        dec1 = Conv2D(base, (3, 3), padding=\'valid\', strides=1, name=\'dec1_{}\'.format(i+1))(dec1)\n        dec1 = Activation(\'relu\')(dec1)\n        dec1 = BatchNormalization()(dec1)\n\n    out = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(dec1)\n    out = Reshape([-1, num_classes+1])(out)\n    out = Activation(\'softmax\', name=\'out\')(out)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_keras/unetlike_keras.py,0,"b'import keras\nimport cv2\nimport numpy as np\nimport argparse\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# GPU config\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nimport tensorflow as tf\nfrom keras import backend as K\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list=""0""\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n# network\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Reshape\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\n\ndef crop_layer(layer, size):\n    _, h, w, _ = keras.backend.int_shape(layer)\n    _, _h, _w, _ = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return keras.layers.Cropping2D(cropping=((ph, ph), (pw, pw)))(layer)\n\n\ndef Mynet(train=False):\n    base = 16\n    \n    inputs = Input((img_height, img_width, 3), name=\'in\')\n    enc1= inputs\n\n    for i in range(2):\n        enc1 = Conv2D(base, (3, 3), padding=\'same\', strides=1, name=\'conv1_{}\'.format(i+1))(enc1)\n        enc1 = Activation(\'relu\')(enc1)\n        enc1 = BatchNormalization()(enc1)\n\n    enc2 = MaxPooling2D((2,2), 2)(enc1)\n    \n    for i in range(2):\n        enc2 = Conv2D(base*2, (3, 3), padding=\'same\', strides=1, name=\'conv2_{}\'.format(i+1))(enc2)\n        enc2 = Activation(\'relu\')(enc2)\n        enc2 = BatchNormalization()(enc2)\n\n    enc3 = MaxPooling2D((2,2), 2)(enc2)\n\n    for i in range(2):\n        enc3 = Conv2D(base*4, (3, 3), padding=\'same\', strides=1, name=\'conv3_{}\'.format(i+1))(enc3)\n        enc3 = Activation(\'relu\')(enc3)\n        enc3 = BatchNormalization()(enc3)\n\n    enc4 = MaxPooling2D((2,2), 2)(enc3)\n\n    for i in range(2):\n        enc4 = Conv2D(base*8, (3, 3), padding=\'same\', strides=1, name=\'conv4_{}\'.format(i+1))(enc4)\n        enc4 = Activation(\'relu\')(enc4)\n        enc4 = BatchNormalization()(enc4)\n\n    enc5 = MaxPooling2D((2,2), 2)(enc4)\n\n    for i in range(2):\n        enc5 = Conv2D(base*16, (3, 3), padding=\'same\', strides=1, name=\'conv5_{}\'.format(i+1))(enc5)\n        enc5 = Activation(\'relu\')(enc5)\n        enc5 = BatchNormalization()(enc5)\n\n    dec4 = keras.layers.Conv2DTranspose(base*8, (2,2), strides=2, padding=\'same\')(enc5)\n    dec4 = Activation(\'relu\')(dec4)\n    dec4 = BatchNormalization()(dec4)\n    _enc4 = crop_layer(enc4, keras.backend.int_shape(dec4))\n    dec4 = keras.layers.concatenate([dec4, _enc4])\n    for i in range(2):\n        dec4 = Conv2D(base*8, (3, 3), padding=\'same\', strides=1, name=\'dec4_{}\'.format(i+1))(dec4)\n        dec4 = Activation(\'relu\')(dec4)\n        dec4 = BatchNormalization()(dec4)\n\n    dec3 = keras.layers.Conv2DTranspose(base*4, (2,2), strides=2, padding=\'same\')(dec4)\n    dec3 = Activation(\'relu\')(dec3)\n    dec3 = BatchNormalization()(dec3)\n    _enc3 = crop_layer(enc3, keras.backend.int_shape(dec3))\n    dec3 = keras.layers.concatenate([dec3, _enc3])\n    for i in range(2):\n        dec3 = Conv2D(base*4, (3, 3), padding=\'same\', strides=1, name=\'dec3_{}\'.format(i+1))(dec3)\n        dec3 = Activation(\'relu\')(dec3)\n        dec3 = BatchNormalization()(dec3)\n\n    dec2 = keras.layers.Conv2DTranspose(base*2, (2,2), strides=2, padding=\'same\')(dec3)\n    dec2 = Activation(\'relu\')(dec2)\n    dec2 = BatchNormalization()(dec2)\n    _enc2 = crop_layer(enc2, keras.backend.int_shape(dec2))\n    dec2 = keras.layers.concatenate([dec2, _enc2])\n    for i in range(2):\n        dec2 = Conv2D(base*2, (3, 3), padding=\'same\', strides=1, name=\'dec2_{}\'.format(i+1))(dec2)\n        dec2 = Activation(\'relu\')(dec2)\n        dec2 = BatchNormalization()(dec2)\n\n    dec1 = keras.layers.Conv2DTranspose(base, (2,2), strides=2, padding=\'same\')(dec2)\n    dec1 = Activation(\'relu\')(dec1)\n    dec1 = BatchNormalization()(dec1)\n    _enc1 = crop_layer(enc1, keras.backend.int_shape(dec1))\n    dec1 = keras.layers.concatenate([dec1, _enc1])\n    for i in range(2):\n        dec1 = Conv2D(base, (3, 3), padding=\'same\', strides=1, name=\'dec1_{}\'.format(i+1))(dec1)\n        dec1 = Activation(\'relu\')(dec1)\n        dec1 = BatchNormalization()(dec1)\n\n    out = Conv2D(num_classes+1, (1, 1), padding=\'same\', strides=1)(dec1)\n    out = Reshape([-1, num_classes+1])(out)\n    out = Activation(\'softmax\', name=\'out\')(out)\n    \n    model = Model(inputs=inputs, outputs=out, name=\'model\')\n    return model\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    model = Mynet(train=True)\n\n    for layer in model.layers:\n        layer.trainable = True\n\n    model.compile(\n        loss={\'out\': \'categorical_crossentropy\'},\n        optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n        loss_weights={\'out\': 1},\n        metrics=[\'accuracy\'])\n\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = xs[mb_ind]\n        t = ts[mb_ind]\n\n        t = np.reshape(t, (mb, -1, num_classes+1))\n\n        loss, acc = model.train_on_batch(x={\'in\':x}, y={\'out\':t})\n        print(""iter >>"", i+1, "",loss >>"", loss, \',accuracy >>\', acc)\n\n    model.save(\'model.h5\')\n\n# test\ndef test():\n    # load trained model\n    model = Mynet(train=False)\n    model.load_weights(\'model.h5\')\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        \n        pred = model.predict_on_batch(x={\'in\': x})[0]\n        pred = np.reshape(pred, (out_height, out_width, num_classes+1))\n\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == i] = vs\n\n\n        print(""in {}"".format(path))\n   \n        plt.subplot(1,2,1)\n        plt.imshow(x[0])\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.title(""predicted"")\n        plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/SegNet_Binarization_pytorch.py,24,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom tqdm import tqdm\n\n# class config\nclass_label = {\'akahara\' : [0, 0, 128], \'madara\' : [0, 128, 0]}\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 3\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'UNet.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass SegNet(torch.nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        \n        # VGG block\n        class VGG_block(torch.nn.Module):\n            def __init__(self, dim1, dim2, layer_N):\n                super(VGG_block, self).__init__()\n\n                _module = []\n\n                for i in range(layer_N):\n                    dim = dim1 if i == 0 else dim2\n                    _module.append(torch.nn.Conv2d(dim, dim2, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n            \n        # VGG Decoder block\n        class VGG_block_decoder(torch.nn.Module):\n            def __init__(self, dim1, dim2, layer_N):\n                super(VGG_block_decoder, self).__init__()\n\n                _module = []\n\n                for i in range(layer_N):\n                    dim = dim1 if i < (layer_N-1) else dim2\n                    _module.append(torch.nn.Conv2d(dim1, dim, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n\n        \n        self.enc1 = VGG_block(3, 64, 2)\n        self.enc2 = VGG_block(64, 128, 2)\n        self.enc3 = VGG_block(128, 256, 3)\n        self.enc4 = VGG_block(256, 512, 3)\n        self.enc5 = VGG_block(512, 512, 3)\n\n        self.dec5 = VGG_block(512, 512, 3)\n        self.dec4 = VGG_block(512, 256, 3)\n        self.dec3 = VGG_block(256, 128, 3)\n        self.dec2 = VGG_block(128, 64, 2)\n        self.dec1 = VGG_block(64, 64, 2)\n\n        self.out = torch.nn.Conv2d(64, 1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # Encoder block 1\n        x_enc1 = self.enc1(x)\n        x, pool1_ind = F.max_pool2d(x_enc1, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 2\n        x_enc2 = self.enc2(x)\n        x, pool2_ind = F.max_pool2d(x_enc2, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 3\n        x_enc3 = self.enc3(x)\n        x, pool3_ind = F.max_pool2d(x_enc3, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 4\n        x_enc4 = self.enc4(x)\n        x, pool4_ind = F.max_pool2d(x_enc4, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 5\n        x_enc5 = self.enc5(x)\n        x, pool5_ind = F.max_pool2d(x_enc5, 2, stride=2, padding=0, return_indices=True)\n\n        # Decoder block 5\n        x = F.max_unpool2d(x, pool5_ind, 2, stride=2, padding=0)\n        x = self.dec5(x)\n        \n        # Decoder block 4\n        x = F.max_unpool2d(x, pool4_ind, 2, stride=2, padding=0)\n        x = self.dec4(x)\n        \n        # Decoder block 3\n        x = F.max_unpool2d(x, pool3_ind, 2, stride=2, padding=0)\n        x = self.dec3(x)\n        \n        # Decoder block 2\n        x = F.max_unpool2d(x, pool2_ind, 2, stride=2, padding=0)\n        x = self.dec2(x)\n        \n        # Decoder block 1\n        x = F.max_unpool2d(x, pool1_ind, 2, stride=2, padding=0)\n        x = self.dec1(x)\n\n        # output\n        x = self.out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(paths_gt)\n\ndef get_image(infos, gt=False):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        if gt:\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        else:\n            x = cv2.resize(x, (out_width, out_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if gt:\n            _x = x\n            x = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[ind] = 1\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if channel == 3:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    if not gt:\n        xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = SegNet().to(device)\n    model.train()\n\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    \n    paths, paths_gt = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=False)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.BCELoss()\n    \n    for i in range(1000):\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[ : (mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        # data load\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(np.expand_dims(get_image(paths_gt[mb_ind], gt=True), axis=1), dtype=torch.float).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        \n        loss = loss_fn(y, t)\n        loss.backward()\n        opt.step()\n    \n        MAE = np.abs((y - t).detach().cpu().numpy()).mean()\n        \n        if (i + 1) % 50 == 0:\n            print(\'Iter : {} , Loss : {} , MAE : {}\'.format(i + 1, loss.item(), MAE))\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = SegNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, path_gt = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[[i]]\n            x = get_image(path)\n\n            x = torch.tensor(get_image(paths[[i]]), dtype=torch.float).to(device)\n\n            pred = model(x)\n\n            #pred = pred.permute(0,2,3,1).reshape(-1, class_num+1)\n            pred = pred.detach().cpu().numpy()[0, 0]\n\n\n            print("">> {}"".format(path[0][\'path\']))\n\n            plt.subplot(1, 2, 1)\n            plt.imshow((x.detach().cpu().numpy()[0].transpose(1,2,0) * 127.5 + 127.5).astype(np.uint8))\n            plt.subplot(1, 2, 2)\n            plt.imshow(pred, cmap=\'gray\')\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")'"
Scripts_Segmentation/scripts_pytorch/SegNet_pytorch.py,24,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom tqdm import tqdm\n\n# class config\nclass_label = {\'akahara\' : [0, 0, 128], \'madara\' : [0, 128, 0]}\nclass_N = len(class_label) + 1 # class + background\n\n# config\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 3\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'SegNet.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass SegNet(torch.nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        \n        # VGG block\n        class VGG_block(torch.nn.Module):\n            def __init__(self, dim1, dim2, layer_N):\n                super(VGG_block, self).__init__()\n\n                _module = []\n\n                for i in range(layer_N):\n                    dim = dim1 if i == 0 else dim2\n                    _module.append(torch.nn.Conv2d(dim, dim2, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n            \n        # VGG Decoder block\n        class VGG_block_decoder(torch.nn.Module):\n            def __init__(self, dim1, dim2, layer_N):\n                super(VGG_block_decoder, self).__init__()\n\n                _module = []\n\n                for i in range(layer_N):\n                    dim = dim1 if i < (layer_N-1) else dim2\n                    _module.append(torch.nn.Conv2d(dim1, dim, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n\n        \n        self.enc1 = VGG_block(3, 64, 2)\n        self.enc2 = VGG_block(64, 128, 2)\n        self.enc3 = VGG_block(128, 256, 3)\n        self.enc4 = VGG_block(256, 512, 3)\n        self.enc5 = VGG_block(512, 512, 3)\n\n        self.dec5 = VGG_block(512, 512, 3)\n        self.dec4 = VGG_block(512, 256, 3)\n        self.dec3 = VGG_block(256, 128, 3)\n        self.dec2 = VGG_block(128, 64, 2)\n        self.dec1 = VGG_block(64, 64, 2)\n\n        self.out = torch.nn.Conv2d(64, class_N, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # Encoder block 1\n        x_enc1 = self.enc1(x)\n        x, pool1_ind = F.max_pool2d(x_enc1, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 2\n        x_enc2 = self.enc2(x)\n        x, pool2_ind = F.max_pool2d(x_enc2, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 3\n        x_enc3 = self.enc3(x)\n        x, pool3_ind = F.max_pool2d(x_enc3, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 4\n        x_enc4 = self.enc4(x)\n        x, pool4_ind = F.max_pool2d(x_enc4, 2, stride=2, padding=0, return_indices=True)\n        \n        # Encoder block 5\n        x_enc5 = self.enc5(x)\n        x, pool5_ind = F.max_pool2d(x_enc5, 2, stride=2, padding=0, return_indices=True)\n\n        # Decoder block 5\n        x = F.max_unpool2d(x, pool5_ind, 2, stride=2, padding=0)\n        x = self.dec5(x)\n        \n        # Decoder block 4\n        x = F.max_unpool2d(x, pool4_ind, 2, stride=2, padding=0)\n        x = self.dec4(x)\n        \n        # Decoder block 3\n        x = F.max_unpool2d(x, pool3_ind, 2, stride=2, padding=0)\n        x = self.dec3(x)\n        \n        # Decoder block 2\n        x = F.max_unpool2d(x, pool2_ind, 2, stride=2, padding=0)\n        x = self.dec2(x)\n        \n        # Decoder block 1\n        x = F.max_unpool2d(x, pool1_ind, 2, stride=2, padding=0)\n        x = self.dec1(x)\n\n        # output\n        x = self.out(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(paths_gt)\n\ndef get_image(infos, gt=False):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        if gt:\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        else:\n            x = cv2.resize(x, (out_width, out_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if gt:\n            _x = x\n            x = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[ind] = i + 1\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if channel == 3:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    if not gt:\n        xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = SegNet().to(device)\n    model.train()\n\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    paths, paths_gt = data_load(\'drive/My Drive/Colab Notebooks/\' + \'/Dataset/train/images/\', hf=True, vf=True, rot=5)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(1000):\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[ : (mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        # data load\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(get_image(paths_gt[mb_ind], gt=True), dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        # reshape gt\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, class_N)\n        t = t.view(-1)\n        \n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb / out_height / out_width\n        \n        if (i + 1) % 50 == 0:\n            print(\'Iter : {} , Loss : {} , Accuracy : {}\'.format(i + 1, loss.item(), acc))\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = SegNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, path_gt = data_load(\'drive/My Drive/Colab Notebooks/\' + \'/Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[[i]]\n            x = get_image(path)\n\n            x = torch.tensor(get_image(paths[[i]]), dtype=torch.float).to(device)\n\n            pred = model(x)\n\n            #pred = pred.permute(0,2,3,1).reshape(-1, class_num+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=0)\n\n            # prediction -> RGB\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(class_label.items()):\n                out[pred == (i + 1)] = vs\n\n            print("">> {}"".format(path[0][\'path\']))\n\n            plt.subplot(1, 2, 1)\n            plt.imshow((x.detach().cpu().numpy()[0].transpose(1,2,0) * 127.5 + 127.5).astype(np.uint8))\n            plt.subplot(1, 2, 2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/UNetLike_Binarization_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom tqdm import tqdm\n\n# class config\nclass_label = {\'akahara\' : [0, 0, 128], \'madara\' : [0, 128, 0]}\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 3\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'UNet.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\nclass UNet(torch.nn.Module):\n    def __init__(self):\n        \n        class UNet_block(torch.nn.Module):\n            def __init__(self, dim1, dim2):\n                super(UNet_block, self).__init__()\n\n                _module = []\n\n                for i in range(2):\n                    f = dim1 if i == 0 else dim2\n                    _module.append(torch.nn.Conv2d(f, dim2, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n\n        class UNet_deconv_block(torch.nn.Module):\n            def __init__(self, dim1, dim2):\n                super(UNet_deconv_block, self).__init__()\n\n                self.module = torch.nn.Sequential(\n                    torch.nn.ConvTranspose2d(dim1, dim2, kernel_size=2, stride=2),\n                    torch.nn.BatchNorm2d(dim2)\n                )\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n        \n        super(UNet, self).__init__()\n\n        base = 16\n        \n        self.enc1 = UNet_block(3, base)\n        self.enc2 = UNet_block(base, base * 2)\n        self.enc3 = UNet_block(base * 2, base * 4)\n        self.enc4 = UNet_block(base * 4, base * 8)\n        self.enc5 = UNet_block(base * 8, base * 16)\n\n        self.tconv4 = UNet_deconv_block(base * 16, base * 8)\n        self.tconv3 = UNet_deconv_block(base * 8, base * 4)\n        self.tconv2 = UNet_deconv_block(base * 4, base * 2)\n        self.tconv1 = UNet_deconv_block(base * 2, base)\n\n        self.dec4 = UNet_block(base * 16, base * 8)\n        self.dec3 = UNet_block(base * 8, base * 4)\n        self.dec2 = UNet_block(base * 4, base * 2)\n        self.dec1 = UNet_block(base * 2, base)\n\n        self.out = torch.nn.Conv2d(base, 1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        x = self.tconv4(x)\n\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        x = self.tconv3(x)\n\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        x = self.tconv2(x)\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        x = self.tconv1(x)\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(paths_gt)\n\ndef get_image(infos, gt=False):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        if gt:\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        else:\n            x = cv2.resize(x, (out_width, out_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if gt:\n            _x = x\n            x = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[ind] = 1\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if channel == 3:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    if not gt:\n        xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = UNet().to(device)\n    model.train()\n\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    \n    paths, paths_gt = data_load(\'drive/My Drive/Colab Notebooks/\' + \'/Dataset/train/images/\', hf=True, vf=True, rot=False)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.BCELoss()\n    \n    for i in range(1000):\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[ : (mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        # data load\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(np.expand_dims(get_image(paths_gt[mb_ind], gt=True), axis=1), dtype=torch.float).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        # reshape gt\n        #y = y.permute(0, 2, 3, 1).contiguous()\n        #y = y.view(-1, class_N + 1)\n        #y = y.view(mb, -1)\n        #t = t.view(mb, -1)\n        \n        loss = loss_fn(y, t)\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        #acc = pred.eq(t.view_as(pred)).sum().item() / mb / out_height / out_width\n        MAE = np.abs((y - t).detach().cpu().numpy()).mean()\n        \n        if (i + 1) % 50 == 0:\n            print(\'Iter : {} , Loss : {} , MAE : {}\'.format(i + 1, loss.item(), MAE))\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = UNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, path_gt = data_load(\'drive/My Drive/Colab Notebooks/\' + \'/Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[[i]]\n            x = get_image(path)\n\n            x = torch.tensor(get_image(paths[[i]]), dtype=torch.float).to(device)\n\n            pred = model(x)\n\n            #pred = pred.permute(0,2,3,1).reshape(-1, class_num+1)\n            pred = pred.detach().cpu().numpy()[0, 0]\n\n\n            print("">> {}"".format(path[0][\'path\']))\n\n            plt.subplot(1, 2, 1)\n            plt.imshow((x.detach().cpu().numpy()[0].transpose(1,2,0) * 127.5 + 127.5).astype(np.uint8))\n            plt.subplot(1, 2, 2)\n            plt.imshow(pred, cmap=\'gray\')\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")'"
Scripts_Segmentation/scripts_pytorch/UNetLike_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom tqdm import tqdm\n\n# class config\nclass_label = {\'akahara\' : [0, 0, 128], \'madara\' : [0, 128, 0]}\nclass_N = len(class_label) + 1 # class + background\n\n# config\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\nchannel = 3\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'UNet.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\nclass UNet(torch.nn.Module):\n    def __init__(self):\n        \n        class UNet_block(torch.nn.Module):\n            def __init__(self, dim1, dim2):\n                super(UNet_block, self).__init__()\n\n                _module = []\n\n                for i in range(2):\n                    f = dim1 if i == 0 else dim2\n                    _module.append(torch.nn.Conv2d(f, dim2, kernel_size=3, padding=1, stride=1))\n                    _module.append(torch.nn.BatchNorm2d(dim2))\n                    _module.append(torch.nn.ReLU())\n\n                self.module = torch.nn.Sequential(*_module)\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n\n        class UNet_deconv_block(torch.nn.Module):\n            def __init__(self, dim1, dim2):\n                super(UNet_deconv_block, self).__init__()\n\n                self.module = torch.nn.Sequential(\n                    torch.nn.ConvTranspose2d(dim1, dim2, kernel_size=2, stride=2),\n                    torch.nn.BatchNorm2d(dim2)\n                )\n\n            def forward(self, x):\n                x = self.module(x)\n                return x\n        \n        super(UNet, self).__init__()\n\n        base = 16\n        \n        self.enc1 = UNet_block(3, base)\n        self.enc2 = UNet_block(base, base * 2)\n        self.enc3 = UNet_block(base * 2, base * 4)\n        self.enc4 = UNet_block(base * 4, base * 8)\n        self.enc5 = UNet_block(base * 8, base * 16)\n\n        self.tconv4 = UNet_deconv_block(base * 16, base * 8)\n        self.tconv3 = UNet_deconv_block(base * 8, base * 4)\n        self.tconv2 = UNet_deconv_block(base * 4, base * 2)\n        self.tconv1 = UNet_deconv_block(base * 2, base)\n\n        self.dec4 = UNet_block(base * 16, base * 8)\n        self.dec3 = UNet_block(base * 8, base * 4)\n        self.dec2 = UNet_block(base * 4, base * 2)\n        self.dec1 = UNet_block(base * 2, base)\n\n        self.out = torch.nn.Conv2d(base, class_N, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        x = self.tconv4(x)\n\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        x = self.tconv3(x)\n\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        x = self.tconv2(x)\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        x = self.tconv1(x)\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    paths_gt = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            \n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': 0})\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                paths_gt.append({\'path\': gt_path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    paths_gt.append({\'path\': gt_path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(paths_gt)\n\ndef get_image(infos, gt=False):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        if gt:\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        else:\n            x = cv2.resize(x, (out_width, out_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        x = _x[tx:tx+_w, ty:ty+_h]\n\n        if gt:\n            _x = x\n            x = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (_x[..., 0] == vs[0]) * (_x[..., 1] == vs[1]) * (_x[..., 2] == vs[2])\n                x[ind] = i + 1\n        else:\n            # normalization [0, 255] -> [-1, 1]\n            x = x / 127.5 - 1\n\n            # channel BGR -> RGB\n            if channel == 3:\n                x = x[..., ::-1]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n\n    if not gt:\n        xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = UNet().to(device)\n    model.train()\n\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    \n    paths, paths_gt = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=False)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(1000):\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[ : (mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        # data load\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(get_image(paths_gt[mb_ind], gt=True), dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        # reshape gt\n        y = y.permute(0, 2, 3, 1).contiguous()\n        y = y.view(-1, class_N)\n        t = t.view(-1)\n        \n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb / out_height / out_width\n        \n        if (i + 1) % 50 == 0:\n            print(\'Iter : {} , Loss : {} , Accuracy : {}\'.format(i + 1, loss.item(), acc))\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = UNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, path_gt = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[[i]]\n            x = get_image(path)\n\n            x = torch.tensor(get_image(paths[[i]]), dtype=torch.float).to(device)\n\n            pred = model(x)\n\n            #pred = pred.permute(0,2,3,1).reshape(-1, class_num+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=0)\n\n            # prediction -> RGB\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(class_label.items()):\n                out[pred == (i + 1)] = vs\n\n            print("">> {}"".format(path[0][\'path\']))\n\n            plt.subplot(1, 2, 1)\n            plt.imshow((x.detach().cpu().numpy()[0].transpose(1,2,0) * 127.5 + 127.5).astype(np.uint8))\n            plt.subplot(1, 2, 2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")'"
Scripts_Segmentation/scripts_pytorch/bin_dataset_pytorch.py,2,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/bin_loss_pytorch.py,15,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        enc1 = []\n        for i in range(6):\n            f = 3 if i == 0 else 32\n            enc1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc1.append(torch.nn.BatchNorm2d(32))\n            enc1.append(torch.nn.ReLU())\n        self.enc1 = torch.nn.Sequential(*enc1)\n\n        self.out = torch.nn.Conv2d(32, 1, kernel_size=1, padding=0, stride=1)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[...,::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        \n        y = torch.sigmoid(y)\n        loss = torch.nn.BCELoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/bin_test_pytorch.py,20,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        enc1 = []\n        for i in range(6):\n            f = 3 if i == 0 else 32\n            enc1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc1.append(torch.nn.BatchNorm2d(32))\n            enc1.append(torch.nn.ReLU())\n        self.enc1 = torch.nn.Sequential(*enc1)\n\n        self.out = torch.nn.Conv2d(32, 1, kernel_size=1, padding=0, stride=1)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = self.out(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.int)\n\n            ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        \n        y = torch.sigmoid(y)\n        loss = torch.nn.BCELoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = torch.sigmoid(pred)\n            pred = pred.detach().cpu().numpy()[0, 0]\n\n            ## binalization\n            bin_pred = pred.copy()\n            th = 0.5\n            bin_pred[bin_pred >= th] = 1\n            bin_pred[bin_pred < th] = 0\n    \n            plt.subplot(1,3,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.title(""input"")\n            plt.subplot(1,3,2)\n            plt.imshow(pred, cmap=\'gray\')\n            plt.title(""predicted"")\n            plt.subplot(1,3,3)\n            plt.imshow(bin_pred, cmap=\'gray\')\n            plt.title(""after binalization"")\n            plt.show()\n\n            print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/concat_pytorch.py,35,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else 32\n            self.enc1.add_module(""conv1_{}"".format(i+1), torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            self.enc1.add_module(""conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            self.enc2.add_module(""conv2_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.enc2.add_module(""conv2_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""bn2_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.upsample = torch.nn.Sequential()\n        self.upsample.add_module(""tconv"", torch.nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0))\n        self.upsample.add_module(""tconv_relu"", torch.nn.ReLU())\n        self.upsample.add_module(""tconv_bn"", torch.nn.BatchNorm2d(32))\n\n        self.concat = torch.nn.Sequential()\n        self.concat.add_module(""concat_conv"", torch.nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0))\n        self.concat.add_module(""concat_relu"", torch.nn.ReLU())\n        self.concat.add_module(""concat_bn"", torch.nn.BatchNorm2d(32))\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            self.dec1.add_module(""dec1_conv1_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.dec1.add_module(""dec1_conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        enc1 = self.enc1(x)\n        x = F.max_pool2d(enc1, 2)\n\n        x = self.enc2(x)\n\n        x = self.upsample(x)\n\n        x = torch.cat((x, enc1), dim=1)\n\n        x = self.concat(x)\n        \n        x = self.dec1(x)\n        \n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i+1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = pred.permute(0,2,3,1).reshape(-1, num_classes+1)\n            pred = F.softmax(pred, dim=1)\n            pred = pred.reshape(-1, out_height, out_width, num_classes+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == (i+1)] = vs\n\n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/easy_pytorch.py,65,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 128, 128#572, 572\nout_height, out_width = 128, 128#388, 388\nGPU = True\ntorch.manual_seed(0)\n\n\ndef crop_layer(layer, size):\n    _, _, h, w = layer.size()\n    _, _, _h, _w = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, :, ph:ph+_h, pw:pw+_w]\n\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        enc1 = []\n        for i in range(2):\n            f = 3 if i == 0 else 32\n            enc1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc1.append(torch.nn.BatchNorm2d(32))\n            enc1.append(torch.nn.ReLU())\n        self.enc1 = torch.nn.Sequential(*enc1)\n\n        enc2 = []\n        for i in range(2):\n            f = 32 if i == 0 else 32\n            enc2.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc2.append(torch.nn.BatchNorm2d(32))\n            enc2.append(torch.nn.ReLU())\n        self.enc2 = torch.nn.Sequential(*enc2)\n\n        enc3 = []\n        for i in range(2):\n            f = 32 if i == 0 else 64\n            enc3.append(torch.nn.Conv2d(f, 64, kernel_size=3, padding=1, stride=1))\n            enc3.append(torch.nn.BatchNorm2d(64))\n            enc3.append(torch.nn.ReLU())\n        self.enc3 = torch.nn.Sequential(*enc3)\n\n        enc4 = []\n        for i in range(2):\n            f = 64 if i == 0 else 128\n            enc4.append(torch.nn.Conv2d(f, 128, kernel_size=3, padding=1, stride=1))\n            enc4.append(torch.nn.BatchNorm2d(128))\n            enc4.append(torch.nn.ReLU())\n        self.enc4 = torch.nn.Sequential(*enc4)\n\n        enc5 = []\n        for i in range(2):\n            f = 128 if i == 0 else 128\n            enc5.append(torch.nn.Conv2d(f, 128, kernel_size=3, padding=1, stride=1))\n            enc5.append(torch.nn.BatchNorm2d(128))\n            enc5.append(torch.nn.ReLU())\n        self.enc5 = torch.nn.Sequential(*enc5)\n\n        self.tconv4 = torch.nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n        dec4 = []\n        for i in range(2):\n            f = 256 if i == 0 else 64\n            dec4.append(torch.nn.Conv2d(f, 64, kernel_size=3, padding=1, stride=1))\n            dec4.append(torch.nn.BatchNorm2d(64))\n            dec4.append(torch.nn.ReLU())\n        self.dec4 = torch.nn.Sequential(*dec4)\n\n        self.tconv3 = torch.nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n        dec3 = []\n        for i in range(2):\n            f = 64 if i == 0 else 64\n            dec3.append(torch.nn.Conv2d(f, 64, kernel_size=3, padding=1, stride=1))\n            dec3.append(torch.nn.BatchNorm2d(64))\n            dec3.append(torch.nn.ReLU())\n        self.dec3 = torch.nn.Sequential(*dec3)\n\n        self.tconv2 = torch.nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n        dec2 = []\n        for i in range(2):\n            f = 96 if i == 0 else 32\n            dec2.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            dec2.append(torch.nn.BatchNorm2d(32))\n            dec2.append(torch.nn.ReLU())\n        self.dec2 = torch.nn.Sequential(*dec2)\n\n        self.tconv1 = torch.nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        dec1 = []\n        for i in range(2):\n            f = 64 if i == 0 else 32\n            dec1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            dec1.append(torch.nn.BatchNorm2d(32))\n            dec1.append(torch.nn.ReLU())\n        self.dec1 = torch.nn.Sequential(*dec1)\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv3\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        #x = self.tconv4(x)\n        #x = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')(x)\n        x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\'nearest\')\n        _x = crop_layer(x_enc4, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec4(x)\n\n        #x = self.tconv3(x)\n        #x = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')(x)\n        x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\'nearest\')\n        _x = crop_layer(x_enc3, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec3(x_enc3)\n\n        #x = self.tconv2(x)\n        #x = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')(x)\n        x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\'nearest\')\n        _x = crop_layer(x_enc2, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec2(x)\n\n        #x = self.tconv1(x)\n        #x = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')(x)\n        x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\'nearest\')\n        _x = crop_layer(x_enc1, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = np.ndarray((0, img_height, img_width, 3), dtype=np.float32)\n    ts = np.ndarray((0, out_height, out_width), dtype=np.int)\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs = np.r_[xs, x[None, ...]]\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t)\n            #plt.show()\n\n            ts = np.r_[ts, t[None, ...]]\n            \n            paths.append(path)\n\n            if hf:\n                xs = np.r_[xs, x[:, ::-1][None, ...]]\n                ts = np.r_[ts, t[:, ::-1][None, ...]]\n                paths.append(path)\n\n            if vf:\n                xs = np.r_[xs, x[::-1][None, ...]]\n                ts = np.r_[ts, t[::-1][None, ...]]\n                paths.append(path)\n\n            if hf and vf:\n                xs = np.r_[xs, x[::-1, ::-1][None, ...]]\n                ts = np.r_[ts, t[::-1, ::-1][None, ...]]\n                paths.append(path)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(2000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        pred = model(x)\n    \n        pred = pred.permute(0,2,3,1).reshape(-1, num_classes+1)\n        pred = F.softmax(pred, dim=1)\n        pred = pred.reshape(-1, out_height, out_width, num_classes+1)\n        pred = pred.detach().cpu().numpy()[0]\n        pred = pred.argmax(axis=-1)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (_, vs) in enumerate(CLS.items()):\n            out[pred == (i+1)] = vs\n   \n        import matplotlib.pyplot as plt\n        plt.subplot(1,2,1)\n        plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0)[..., ::-1])\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/nearest_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else 32\n            self.enc1.add_module(""conv1_{}"".format(i+1), torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            self.enc1.add_module(""conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            self.enc2.add_module(""conv2_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.enc2.add_module(""conv2_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""bn2_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            self.dec1.add_module(""dec1_conv1_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.dec1.add_module(""dec1_conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.enc2(x)\n        x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\'nearest\')\n\n        x = self.dec1(x)\n        \n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i+1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = pred.permute(0,2,3,1).reshape(-1, num_classes+1)\n            pred = F.softmax(pred, dim=1)\n            pred = pred.reshape(-1, out_height, out_width, num_classes+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == (i+1)] = vs\n\n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/semaseg_dataset_pytorch.py,2,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/semaseg_loss_pytorch.py,14,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        enc1 = []\n        for i in range(6):\n            f = 3 if i == 0 else 32\n            enc1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc1.append(torch.nn.ReLU())\n            enc1.append(torch.nn.BatchNorm2d(32))\n        self.enc1 = torch.nn.Sequential(*enc1)\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/semaseg_test_pytorch.py,18,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        enc1 = []\n        for i in range(6):\n            f = 3 if i == 0 else 32\n            enc1.append(torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            enc1.append(torch.nn.ReLU())\n            enc1.append(torch.nn.BatchNorm2d(32))\n        self.enc1 = torch.nn.Sequential(*enc1)\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i+1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = pred.permute(0,2,3,1).reshape(-1, num_classes+1)\n            pred = F.softmax(pred, dim=1)\n            pred = pred.reshape(-1, out_height, out_width, num_classes+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == (i+1)] = vs\n\n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/transposeconv_pytorch.py,30,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else 32\n            self.enc1.add_module(""conv1_{}"".format(i+1), torch.nn.Conv2d(f, 32, kernel_size=3, padding=1, stride=1))\n            self.enc1.add_module(""conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            self.enc2.add_module(""conv2_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.enc2.add_module(""conv2_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""bn2_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.upsample = torch.nn.Sequential()\n        self.upsample.add_module(""tconv"", torch.nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0))\n        self.upsample.add_module(""tconv_relu"", torch.nn.ReLU())\n        self.upsample.add_module(""tconv_bn"", torch.nn.BatchNorm2d(32))\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            self.dec1.add_module(""dec1_conv1_{}"".format(i+1), torch.nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1))\n            self.dec1.add_module(""dec1_conv1_{}_relu"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn1_{}"".format(i+1), torch.nn.BatchNorm2d(32))\n\n        self.out = torch.nn.Conv2d(32, num_classes+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x = self.enc1(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.enc2(x)\n\n        x = self.upsample(x)\n\n        x = self.dec1(x)\n        \n        x = self.out(x)\n        \n        return x\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i+1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.subplot(1,2,1)\n            #plt.imshow(x)\n            #plt.subplot(1,2,2)\n            #plt.imshow(t, vmin=0, vmax=2)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, num_classes+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = pred.permute(0,2,3,1).reshape(-1, num_classes+1)\n            pred = F.softmax(pred, dim=1)\n            pred = pred.reshape(-1, out_height, out_width, num_classes+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == (i+1)] = vs\n\n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_pytorch/unet_pytorch.py,61,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\n# class config\nclass_label = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 236, 236 #572, 572\nout_height, out_width = 52, 52 #388, 388\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'UNet.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\n    \nclass UNet(torch.nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        def crop_layer(layer, size):\n            _, _, h, w = layer.size()\n            _, _, _h, _w = size\n            ph = int((h - _h) / 2)\n            pw = int((w - _w) / 2)\n            return layer[:, :, ph:ph+_h, pw:pw+_w]\n\n        base = 64\n\n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else base\n            self.enc1.add_module(""enc1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=0, stride=1))\n            self.enc1.add_module(""enc1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""enc1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base if i == 0 else base*2\n            self.enc2.add_module(""enc2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=0, stride=1))\n            self.enc2.add_module(""enc2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""enc2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.enc3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base*4\n            self.enc3.add_module(""enc3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=0, stride=1))\n            self.enc3.add_module(""enc3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc3.add_module(""enc3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.enc4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*8\n            self.enc4.add_module(""enc4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=0, stride=1))\n            self.enc4.add_module(""enc4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc4.add_module(""enc4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n\n        self.enc5 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*16\n            self.enc5.add_module(""enc5_{}"".format(i+1), torch.nn.Conv2d(f, base*16, kernel_size=3, padding=0, stride=1))\n            self.enc5.add_module(""enc5_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc5.add_module(""enc5_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*16))\n\n        self.tconv4 = torch.nn.ConvTranspose2d(base*16, base*8, kernel_size=2, stride=2)\n        self.tconv4_bn = torch.nn.BatchNorm2d(base*8)\n\n        self.dec4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*16 if i == 0 else base*8\n            self.dec4.add_module(""dec4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=0, stride=1))\n            self.dec4.add_module(""dec4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec4.add_module(""dec4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n        \n\n        self.tconv3 = torch.nn.ConvTranspose2d(base*8, base*4, kernel_size=2, stride=2)\n        self.tconv3_bn = torch.nn.BatchNorm2d(base*4)\n\n        self.dec3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*4\n            self.dec3.add_module(""dec3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=0, stride=1))\n            self.dec3.add_module(""dec3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec3.add_module(""dec3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.tconv2 = torch.nn.ConvTranspose2d(base*4, base*2, kernel_size=2, stride=2)\n        self.tconv2_bn = torch.nn.BatchNorm2d(base*2)\n\n        self.dec2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*2\n            self.dec2.add_module(""dec2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=0, stride=1))\n            self.dec2.add_module(""dec2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec2.add_module(""dec2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.tconv1 = torch.nn.ConvTranspose2d(base*2, base, kernel_size=2, stride=2)\n        self.tconv1_bn = torch.nn.BatchNorm2d(base)\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base\n            self.dec1.add_module(""dec1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=0, stride=1))\n            self.dec1.add_module(""dec1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.out = torch.nn.Conv2d(base, class_N+1, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        x = self.tconv4_bn(self.tconv4(x))\n\n        _x = crop_layer(x_enc4, x.size())\n\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec4(x)\n\n        x = self.tconv3_bn(self.tconv3(x))\n        _x = crop_layer(x_enc3, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec3(x)\n\n        x = self.tconv2_bn(self.tconv2(x))\n        _x = crop_layer(x_enc2, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec2(x)\n\n        x = self.tconv1_bn(self.tconv1(x))\n        _x = crop_layer(x_enc1, x.size())\n        x = torch.cat((_x, x), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        \n        return x\n\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(class_label.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = i + 1\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t)\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    xs = xs.transpose(0,3,1,2)\n    \n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    model = UNet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n\n        y = y.permute(0,2,3,1).contiguous()\n        y = y.view(-1, class_N+1)\n        t = t.view(-1)\n        \n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb / img_height / img_width\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = UNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n        \n            pred = pred.permute(0,2,3,1).reshape(-1, class_N+1)\n            pred = F.softmax(pred, dim=1)\n            pred = pred.reshape(-1, out_height, out_width, class_N+1)\n            pred = pred.detach().cpu().numpy()[0]\n            pred = pred.argmax(axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(class_label.items()):\n                out[pred == (i+1)] = vs\n\n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x.detach().cpu().numpy()[0].transpose(1,2,0))\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/bin_dataset_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/bin_loss_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(6):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = slim.batch_norm(x, is_training=train)\n            #x = tf.nn.relu(x)\n            \n    x = slim.conv2d(x, 1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, out_height, out_width, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    preds = tf.nn.sigmoid(logits)\n    loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(logits=logits, multi_class_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/bin_test_tensorflow_layers.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n    for i in range(6):\n        x = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[3, 3], strides=1, padding=\'same\', name=\'conv1_{}\'.format(i+1))\n        x = tf.nn.relu(x)\n        x = tf.layers.batch_normalization(x, training=train)\n        #x = tf.nn.relu(x)\n\n    x = tf.layers.conv2d(inputs=x, filters=1, kernel_size=[1, 1], strides=1, padding=\'same\', name=\'out\')\n    \n    return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, out_height, out_width, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    preds = tf.nn.sigmoid(logits)\n    loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(logits=logits, multi_class_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    pred_prob = tf.nn.sigmoid(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([pred_prob], feed_dict={X:x, keep_prob:1.})[0]\n            pred = pred[0, ..., 0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0, ..., 0]\n            \n            ## binalization\n            bin_pred = pred.copy()\n            th = 0.5\n            bin_pred[bin_pred >= th] = 1\n            bin_pred[bin_pred < th] = 0\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,3,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,3,2)\n            plt.imshow(pred, cmap=\'gray\')\n            plt.title(""predicted"")\n            plt.subplot(1,3,3)\n            plt.imshow(bin_pred, cmap=\'gray\')\n            plt.title(""after binalization"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/bin_test_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(6):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = slim.batch_norm(x, is_training=train)\n            #x = tf.nn.relu(x)\n            \n    x = slim.conv2d(x, 1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, 1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[ind] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, out_height, out_width, 1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    preds = tf.nn.sigmoid(logits)\n    loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(logits=logits, multi_class_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = pred[0, ..., 0]\n            #pred = out.eval(feed_dict={X: x, keep_prob: 1.0})[0, ..., 0]\n            \n            ## binalization\n            bin_pred = pred.copy()\n            th = 0.5\n            bin_pred[bin_pred >= th] = 1\n            bin_pred[bin_pred < th] = 0\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,3,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,3,2)\n            plt.imshow(pred, cmap=\'gray\')\n            plt.title(""predicted"")\n            plt.subplot(1,3,3)\n            plt.imshow(bin_pred, cmap=\'gray\')\n            plt.title(""after binalization"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/concat_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n\n        enc1 = tf.identity(x)\n        x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv2_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n    \n        x = slim.conv2d_transpose(x, 32, [2,2], stride=2, scope=\'tconv\')\n        x = tf.nn.relu(x)\n        x = slim.batch_norm(x, is_training=train)\n\n        x = tf.concat((x, enc1), axis=-1)\n        x = slim.conv2d(x, 32, [1,1], stride=1, scope=\'concat_conv\')\n        x = tf.nn.relu(x)\n        x = slim.batch_norm(x, is_training=train)\n        \n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'dec1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n    x = slim.conv2d(x, num_classes+1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/nearest_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n        x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv2_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n    \n        _, _h, _w, _c = x.shape\n        x = tf.image.resize_images(x, [_h*2, _w*2], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'dec1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n    x = slim.conv2d(x, num_classes+1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/semaseg_dataset_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    #if args.train:\n    #    train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/semaseg_loss_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(6):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n    x = slim.conv2d(x, num_classes+1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    #if args.test:\n    #    test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/semaseg_test_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(6):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n    x = slim.conv2d(x, num_classes+1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/transposeconv_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64#572, 572\nout_height, out_width = 64, 64#388, 388\n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n        x = slim.max_pool2d(x, [2,2], scope=\'pool1\')\n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'conv2_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n    \n        #_, _h, _w, _c = x.shape\n        #x = tf.image.resize_images(x, [_h*2, _w*2], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        x = slim.conv2d_transpose(x, 32, [2,2], stride=2, scope=\'tconv\')\n        x = tf.nn.relu(x)\n        x = slim.batch_norm(x, is_training=train)\n\n        for i in range(2):\n            x = slim.conv2d(x, 32, [3,3], scope=\'dec1_{}\'.format(i+1))\n            x = tf.nn.relu(x)\n            x = slim.batch_norm(x, is_training=train)\n            \n    x = slim.conv2d(x, num_classes+1, [1, 1], scope=\'out\')\n\n    return x\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(500):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/unet_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 236, 236 #572, 572\nout_height, out_width = 52, 52 #388, 388\n\ndef crop_layer(layer, size):\n    _, h, w, _ = layer.get_shape().as_list()\n    _, _h, _w, _ = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, ph:ph+_h, pw:pw+_w]\n    \n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    base = 64\n    \n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        enc1 = x\n        for i in range(2):\n            enc1 = slim.conv2d(enc1, base, [3,3], padding=""valid"", scope=\'conv1_{}\'.format(i+1))\n            enc1 = tf.nn.relu(enc1)\n            enc1 = slim.batch_norm(enc1, is_training=train)\n            \n        enc2 = slim.max_pool2d(enc1, [2,2], scope=\'pool1\')\n        for i in range(2):\n            enc2 = slim.conv2d(enc2, base*2, [3,3], padding=""valid"", scope=\'conv2_{}\'.format(i+1))\n            enc2 = tf.nn.relu(enc2)\n            enc2 = slim.batch_norm(enc2, is_training=train)\n\n        enc3 = slim.max_pool2d(enc2, [2,2], scope=\'pool2\')\n        for i in range(2):\n            enc3 = slim.conv2d(enc3, base*4, [3,3], padding=""valid"", scope=\'conv3_{}\'.format(i+1))\n            enc3 = tf.nn.relu(enc3)\n            enc3 = slim.batch_norm(enc3, is_training=train)\n\n        enc4 = slim.max_pool2d(enc3, [2,2], scope=\'pool3\')\n        for i in range(2):\n            enc4 = slim.conv2d(enc4, base*8, [3,3], padding=""valid"", scope=\'conv4_{}\'.format(i+1))\n            enc4 = tf.nn.relu(enc4)\n            enc4 = slim.batch_norm(enc4, is_training=train)\n\n        enc5 = slim.max_pool2d(enc4, [2,2], scope=\'pool4\')\n        for i in range(2):\n            enc5 = slim.conv2d(enc5, base*16, [3,3], padding=""valid"", scope=\'conv5_{}\'.format(i+1))\n            enc5 = tf.nn.relu(enc5)\n            enc5 = slim.batch_norm(enc5, is_training=train)\n\n        # decoder4\n        dec4 = slim.conv2d_transpose(enc5, base*8, [2,2], stride=2, scope=\'tconv4\')\n        dec4 = tf.nn.relu(dec4)\n        dec4 = slim.batch_norm(dec4, is_training=train)\n\n        _enc4 = crop_layer(enc4, dec4.get_shape().as_list())\n\n        dec4 = tf.concat((dec4, _enc4), axis=-1)\n        \n        for i in range(2):\n            dec4 = slim.conv2d(dec4, base*8, [3,3], padding=\'valid\', scope=\'dec4_{}\'.format(i+1))\n            dec4 = tf.nn.relu(dec4)\n            dec4 = slim.batch_norm(dec4, is_training=train)\n\n        # decoder 3\n        dec3 = slim.conv2d_transpose(dec4, base*4, [2,2], stride=2, scope=\'tconv3\')\n        dec3 = tf.nn.relu(dec3)\n        dec3 = slim.batch_norm(dec3, is_training=train)\n\n        _enc3 = crop_layer(enc3, dec3.get_shape().as_list())\n        dec3 = tf.concat((dec3, _enc3), axis=-1)\n        \n        for i in range(2):\n            dec3 = slim.conv2d(dec3, base*4, [3,3], padding=\'valid\', scope=\'dec3_{}\'.format(i+1))\n            dec3 = tf.nn.relu(dec3)\n            dec3 = slim.batch_norm(dec3, is_training=train)\n\n        # decoder 2\n        dec2 = slim.conv2d_transpose(dec3, base*2, [2,2], stride=2, scope=\'tconv2\')\n        dec2 = tf.nn.relu(dec2)\n        dec2 = slim.batch_norm(dec2, is_training=train)\n\n        _enc2 = crop_layer(enc2, dec2.get_shape().as_list())\n        dec2 = tf.concat((dec2, _enc2), axis=-1)\n        \n        for i in range(2):\n            dec2 = slim.conv2d(dec2, base*2, [3,3], padding=\'valid\', scope=\'dec2_{}\'.format(i+1))\n            dec2 = tf.nn.relu(dec2)\n            dec2 = slim.batch_norm(dec2, is_training=train)\n\n        # decoder 1\n        dec1 = slim.conv2d_transpose(dec2, base, [2,2], stride=2, scope=\'tconv1\')\n        dec1 = tf.nn.relu(dec1)\n        dec1 = slim.batch_norm(dec1, is_training=train)\n\n        _enc1 = crop_layer(enc1, dec1.get_shape().as_list())\n\n        dec1 = tf.concat((dec1, _enc1), axis=-1)\n        \n        for i in range(2):\n            dec1 = slim.conv2d(dec1, base, [3,3], padding=\'valid\', scope=\'dec1_{}\'.format(i+1))\n            dec1 = tf.nn.relu(dec1)\n            dec1 = slim.batch_norm(dec1, is_training=train)\n            \n    out = slim.conv2d(dec1, num_classes+1, [1, 1], scope=\'out\')\n\n    return out\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(100):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Segmentation/scripts_tf_slim/unetlike_tensorflow_slim.py,0,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\n\ndef crop_layer(layer, size):\n    _, h, w, _ = layer.get_shape().as_list()\n    _, _h, _w, _ = size\n    ph = int((h - _h) / 2)\n    pw = int((w - _w) / 2)\n    return layer[:, ph:ph+_h, pw:pw+_w]\n    \n    \ndef Mynet(x, keep_prob, train=False):\n    # block conv1\n\n    base = 16\n    \n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      #activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n                      #weights_regularizer=slim.l2_regularizer(0.0005)):\n        enc1 = x\n        for i in range(2):\n            enc1 = slim.conv2d(enc1, base, [3,3], padding=""same"", scope=\'conv1_{}\'.format(i+1))\n            enc1 = tf.nn.relu(enc1)\n            enc1 = slim.batch_norm(enc1, is_training=train)\n            \n        enc2 = slim.max_pool2d(enc1, [2,2], scope=\'pool1\')\n        for i in range(2):\n            enc2 = slim.conv2d(enc2, base*2, [3,3], padding=""same"", scope=\'conv2_{}\'.format(i+1))\n            enc2 = tf.nn.relu(enc2)\n            enc2 = slim.batch_norm(enc2, is_training=train)\n\n        enc3 = slim.max_pool2d(enc2, [2,2], scope=\'pool2\')\n        for i in range(2):\n            enc3 = slim.conv2d(enc3, base*4, [3,3], padding=""same"", scope=\'conv3_{}\'.format(i+1))\n            enc3 = tf.nn.relu(enc3)\n            enc3 = slim.batch_norm(enc3, is_training=train)\n\n        enc4 = slim.max_pool2d(enc3, [2,2], scope=\'pool3\')\n        for i in range(2):\n            enc4 = slim.conv2d(enc4, base*8, [3,3], padding=""same"", scope=\'conv4_{}\'.format(i+1))\n            enc4 = tf.nn.relu(enc4)\n            enc4 = slim.batch_norm(enc4, is_training=train)\n\n        enc5 = slim.max_pool2d(enc4, [2,2], scope=\'pool4\')\n        for i in range(2):\n            enc5 = slim.conv2d(enc5, base*16, [3,3], padding=""same"", scope=\'conv5_{}\'.format(i+1))\n            enc5 = tf.nn.relu(enc5)\n            enc5 = slim.batch_norm(enc5, is_training=train)\n\n        # decoder4\n        dec4 = slim.conv2d_transpose(enc5, base*8, [2,2], stride=2, scope=\'tconv4\')\n        dec4 = tf.nn.relu(dec4)\n        dec4 = slim.batch_norm(dec4, is_training=train)\n\n        _enc4 = crop_layer(enc4, dec4.get_shape().as_list())\n\n        dec4 = tf.concat((dec4, _enc4), axis=-1)\n        \n        for i in range(2):\n            dec4 = slim.conv2d(dec4, base*8, [3,3], padding=\'same\', scope=\'dec4_{}\'.format(i+1))\n            dec4 = tf.nn.relu(dec4)\n            dec4 = slim.batch_norm(dec4, is_training=train)\n\n        # decoder 3\n        dec3 = slim.conv2d_transpose(dec4, base*4, [2,2], stride=2, scope=\'tconv3\')\n        dec3 = tf.nn.relu(dec3)\n        dec3 = slim.batch_norm(dec3, is_training=train)\n\n        _enc3 = crop_layer(enc3, dec3.get_shape().as_list())\n        dec3 = tf.concat((dec3, _enc3), axis=-1)\n        \n        for i in range(2):\n            dec3 = slim.conv2d(dec3, base*4, [3,3], padding=\'same\', scope=\'dec3_{}\'.format(i+1))\n            dec3 = tf.nn.relu(dec3)\n            dec3 = slim.batch_norm(dec3, is_training=train)\n\n        # decoder 2\n        dec2 = slim.conv2d_transpose(dec3, base*2, [2,2], stride=2, scope=\'tconv2\')\n        dec2 = tf.nn.relu(dec2)\n        dec2 = slim.batch_norm(dec2, is_training=train)\n\n        _enc2 = crop_layer(enc2, dec2.get_shape().as_list())\n        dec2 = tf.concat((dec2, _enc2), axis=-1)\n        \n        for i in range(2):\n            dec2 = slim.conv2d(dec2, base*2, [3,3], padding=\'same\', scope=\'dec2_{}\'.format(i+1))\n            dec2 = tf.nn.relu(dec2)\n            dec2 = slim.batch_norm(dec2, is_training=train)\n\n        # decoder 1\n        dec1 = slim.conv2d_transpose(dec2, base, [2,2], stride=2, scope=\'tconv1\')\n        dec1 = tf.nn.relu(dec1)\n        dec1 = slim.batch_norm(dec1, is_training=train)\n\n        _enc1 = crop_layer(enc1, dec1.get_shape().as_list())\n\n        dec1 = tf.concat((dec1, _enc1), axis=-1)\n        \n        for i in range(2):\n            dec1 = slim.conv2d(dec1, base, [3,3], padding=\'same\', scope=\'dec1_{}\'.format(i+1))\n            dec1 = tf.nn.relu(dec1)\n            dec1 = slim.batch_norm(dec1, is_training=train)\n            \n    out = slim.conv2d(dec1, num_classes+1, [1, 1], scope=\'out\')\n\n    return out\n\n    \nCLS = {\'background\': [0,0,0],\n       \'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((out_height, out_width, num_classes+1), dtype=np.float)\n\n            for i , (label, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                ind = np.where(ind == True)\n                t[ind[0], ind[1], i] = 1\n\n            #ind = (gt[..., 0] == 0) * (gt[..., 1] == 0) * (gt[..., 2] == 0)\n            #ind = np.where(ind == True)\n            #t[ind[0], ind[1], 0] = 1\n            #ind = (gt[...,0] > 0) + (gt[..., 1] > 0) + (gt[...,2] > 0)\n            #t[ind] = 1\n\n            #print(gt_path)\n            #import matplotlib.pyplot as plt\n            #plt.imshow(t, cmap=\'gray\')\n            #plt.show()\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    tf.reset_default_graph()\n\n    # place holder\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes+1])\n    keep_prob = tf.placeholder(tf.float32)\n    \n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    \n    preds = tf.nn.softmax(logits)\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    train = optimizer.minimize(loss)\n\n    correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    \n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 4\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n    \n        for i in range(1000):\n            if mbi + mb > len(xs):\n                mb_ind = train_ind[mbi:]\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n                mbi = mb - (len(xs) - mbi)\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x = xs[mb_ind]\n            t = ts[mb_ind]\n\n            t = np.reshape(t, [-1, num_classes+1])\n\n            _, acc, los = sess.run([train, accuracy, loss], feed_dict={X: x, Y: t, keep_prob: 0.5})\n            print(""iter >>"", i+1, \',loss >>\', los / mb, \',accuracy >>\', acc)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \'./cnn.ckpt\')\n\n# test\ndef test():\n    tf.reset_default_graph()\n\n    X = tf.placeholder(tf.float32, [None, img_height, img_width, 3])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    logits = Mynet(X, keep_prob, train=True)\n    logits = tf.reshape(logits, [-1, num_classes+1])\n    logits = tf.nn.softmax(logits)\n\n    xs, ts, paths = data_load(""../Dataset/test/images/"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list=""0""\n    with tf.Session(config=config) as sess:\n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(""./cnn.ckpt.meta"")\n        saver.restore(sess, ""./cnn.ckpt"")\n\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n\n            pred = sess.run([logits], feed_dict={X: x, keep_prob:1.0})[0]\n            pred = np.reshape(pred, [out_height, out_width, num_classes+1])\n            pred = np.argmax(pred, axis=-1)\n\n            # visualize\n            out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n            for i, (_, vs) in enumerate(CLS.items()):\n                out[pred == i] = vs\n            \n            print(""in {}"".format(path))\n            \n            plt.subplot(1,2,1)\n            plt.imshow(x[0])\n            plt.title(""input"")\n            plt.subplot(1,2,2)\n            plt.imshow(out[..., ::-1])\n            plt.title(""predicted"")\n            plt.show()\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Theory1/answers/_multi_perceptron.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[1], [-1], [-1], [1]], dtype=np.float32)\n\nlr = 0.1\n\nz1 = np.hstack([xs, [[1] for _ in range(4)]])\n\n# perceptron\nw1 = np.random.normal(0, 1, [3, 2])\nw2 = np.random.normal(0, 1, [3, 1])\nprint(""weight1 >>\\n"", w1)\nprint(""weight2 >>\\n"", w2)\n\n# add bias\n#_x = np.hstack([x, [[1] for _ in range(4)]])\n\n# train\nite = 0\nfor _ in range(5):\n    ite += 1\n\n    # feed forward\n    z2 = np.dot(z1, w1)\n    _z2 = np.hstack((z2, [[1] for _ in range(4)]))\n    ys = np.dot(_z2, w2)\n\n    print(""iteration:"", ite, ""y >>\\n"", ys)\n\n    # back propagation\n    #if len(np.where((ys * ts) < 0)[0]) < 1:\n    #    break\n\n    _ts = ts.copy()\n    _ts[ys * ts >= 0] = 0\n    En = w2\n    print(""En"", En)\n    grad_w2 = np.dot(_z2.T, En)\n    w2 -= lr * grad_w2\n\n    #grad_w1 = np.dot(z1.T, np.dot(grad_w2, w2.T) * w1)\n    \n    grad_w1 = np.dot(En, w2.T)\n    grad_w1 *= _z2\n    grad_w1 = np.dot(xs.T, grad_w1)\n    #w1 -= lr * grad_w1.T\n\n    \nprint(""training finished!"")\nprint(""weight1 >>\\n"", w1)\nprint(""weight2 >>\\n"", w2)\n\n# test\nfor i in range(4):\n    z2 = np.dot(z1[i], w1)\n    z2 = np.hstack((z2, [1]))\n    y = np.dot(z2, w2)\n    print(""in >>"", xs[i], "", out >>"", y) \n    \n'"
Scripts_Theory1/answers/_neuralnet.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nclass NN:\n    def __init__(self, ind=2, w=64, outd=1, lr=0.1):\n        self.w2 = np.random.randn(ind, w)\n        self.b2 = np.random.randn(w)\n        self.wout = np.random.randn(w, outd)\n        self.bout = np.random.randn(outd)\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = self.sigmoid(np.dot(self.z1, self.w2) + self.b2)\n        self.out = self.sigmoid(np.dot(self.z2, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        out_d = 2*(self.out - t) * self.out * (1 - self.out)\n        out_dW = np.dot(self.z2.T, out_d)\n        out_dB = np.dot(np.ones([1, out_d.shape[0]]), out_d)\n        self.wout -= self.lr * out_dW\n        self.bout -= self.lr * out_dB[0]\n\n        # backpropagation inter layer\n        w2_d = np.dot(out_d, self.wout.T) * self.z2 * (1 - self.z2)\n        w2_dW = np.dot(self.z1.T, w2_d)\n        w2_dB = np.dot(np.ones([1, w2_d.shape[0]]), w2_d)\n        self.w2 -= self.lr * w2_dW\n        self.b2 -= self.lr * w2_dB[0]\n\n    def sigmoid(self, x):\n        return 1. / (1. + np.exp(-x))\n\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\ntrain_t = np.array([[1], [0], [0], [1]], dtype=np.float32)\n\nnn = NN(ind=train_x.shape[1])\n\n# train\nfor i in range(1000):\n    nn.forward(train_x)\n    nn.train(train_x, train_t)\n\n# test\nfor j in range(4):\n    x = train_x[j]\n    t = train_t[j]\n    print(""in:"", x, ""pred:"", nn.forward(x))\n'"
Scripts_Theory1/answers/_neuralnet_sample.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nclass NN:\n    def __init__(self, ind=2, w=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.wout = np.random.normal(0, 1, [w, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.out = sigmoid(np.dot(self.z2, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_En = En #np.array([En for _ in range(t.shape[0])])\n        grad_wout = np.dot(self.z2.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout#np.expand_dims(grad_wout, axis=-1)\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u1 = np.dot(En, self.wout.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\ntrain_t = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nnn = NN()\n\nprint(""weight >>"", nn.w1)\nprint(""bias >>"", nn.b1)\n\n# train\nfor i in range(5000):\n    nn.forward(train_x)\n    #print(""ite>>"", i, \'y >>\', nn.forward(train_x))\n    nn.train(train_x, train_t)\n\n# test\nfor j in range(4):\n    x = train_x[j]\n    t = train_t[j]\n    print(""in:"", x, ""pred:"", nn.forward(x), ""z2:"", nn.z2)\n'"
Scripts_Theory1/answers/_perceptron4.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array(((0,0), (0,1), (1,0), (1,1)), dtype=np.float32)\nts = np.array(((0), (0), (0), (1)), dtype=np.float32)\n\nlrs = [0.1, 0.01]\nlinestyles = [\'solid\', \'dashed\']\nplts = []\n\ndef sigmoid(x):\n    return 1. / (1 + np.exp(-x))\n\nfor _i in range(len(lrs)):\n    lr = lrs[_i]\n    \n    # perceptron\n    np.random.seed(0)\n    w = np.random.normal(0., 1, (3))\n    print(""weight >>"", w)\n\n    # add bias\n    z1 = np.hstack([xs, [[1] for _ in range(4)]])\n\n    # train\n    ite = 1\n    w1 = [w[0]]\n    w2 = [w[1]]\n    w3 = [w[2]]\n\n    for _ in range(1000):\n        # feed forward\n        ys = sigmoid(np.dot(z1, w))\n        #ys = sigmoid(np.array(list(map(lambda x: np.dot(w, x), z1))))\n\n        print(""iteration:"", ite, ""y >>"", ys)\n\n        En = -2 * (ys - ts) * ys * (1 - ys)\n        grad_w = np.dot(z1.T, En)\n        w += lr * grad_w\n\n        w1.append(w[0])\n        w2.append(w[1])\n        w3.append(w[2])\n\n        ite += 1\n\n    print(""training finished!"")\n    print(""weight >>"", w)\n\n    inds = list(range(ite))\n    import matplotlib.pyplot as plt\n    linestyle = linestyles[_i]\n    plts.append(plt.plot(inds, w1, markeredgewidth=0, linestyle=linestyle)[0])\n    plts.append(plt.plot(inds, w2, markeredgewidth=0, linestyle=linestyle)[0])\n    plts.append(plt.plot(inds, w3, markeredgewidth=0, linestyle=linestyle)[0])\n\nplt.legend(plts, [""w1:lr=0.1"",""w2:lr=0.1"",""w3:lr=0.1"",""w1:lr=0.01"",""w2:lr=0.01"",""w3:lr=0.01""], loc=1)\nplt.savefig(""answer_perceptron3.png"")\nplt.show()\n\n# test\n#ys = np.array(list(map(lambda x: np.dot(w, x), _xs)))\n\nfor i in range(4):\n    ys = sigmoid(np.dot(w, np.hstack([xs[i], [1]])))\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/multi_perceptron_1.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw1 = np.random.normal(0, 1, [2, 2])\nb1 = np.random.normal(0, 1, [2])\nwout = np.random.normal(0, 1, [2, 1])\nbout = np.random.normal(0, 1, [1])\n                             \nprint(""weight1 >>\\n"", w1)\nprint(""bias1 >>\\n"", b1)\nprint(""weight_out >>\\m"", wout)\nprint(""bias_out >>\\n"", bout)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nz1 = xs\n\n\n# test\nfor i in range(4):\n    z2 = sigmoid(np.dot(z1[i], w1) + b1)\n    out = sigmoid(np.dot(z2, wout) + bout)\n    print(""in >>"", xs[i], "", out >>"", out)\n    \n'"
Scripts_Theory1/answers/multi_perceptron_2.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw1 = np.random.normal(0, 1, [2, 2])\nb1 = np.random.normal(0, 1, [2])\nwout = np.random.normal(0, 1, [2, 1])\nbout = np.random.normal(0, 1, [1])\n                             \nprint(""weight1 >>\\n"", w1)\nprint(""bias1 >>\\n"", b1)\nprint(""weight_out >>\\m"", wout)\nprint(""bias_out >>\\n"", bout)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nz1 = xs\n\nfor ite in range(5000):\n    ite += 1\n\n    # feed forward\n    z2 = sigmoid(np.dot(z1, w1) + b1)\n    out = sigmoid(np.dot(z2, wout) + bout)\n\n    # back propagate\n    En = (out - ts) * out * (1 - out)\n    grad_wout = np.dot(z2.T, En)\n    grad_bout = np.dot(np.ones([En.shape[0]]), En)\n    wout -= lr * grad_wout\n    bout -= lr * grad_bout\n    \n    # backpropagation inter layer\n    grad_u1 = np.dot(En, wout.T) * z2 * (1 - z2)\n    grad_w1 = np.dot(z1.T, grad_u1)\n    grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n    w1 -= lr * grad_w1\n    b1 -= lr * grad_b1\n\n\nprint(""weight1 >>\\n"", w1)\nprint(""bias1 >>\\n"", b1)\nprint(""weight_out >>\\n"", wout)\nprint(""bias_out >>\\n"", bout)\n\n# test\nfor i in range(4):\n    z2 = sigmoid(np.dot(z1[i], w1) + b1)\n    out = sigmoid(np.dot(z2, wout) + bout)\n    print(""in >>"", xs[i], "", out >>"", out)\n    \n'"
Scripts_Theory1/answers/multi_perceptron_3.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw1 = np.random.normal(0, 1, [2, 2])\nb1 = np.random.normal(0, 1, [2])\nw2 = np.random.normal(0, 1, [2, 2])\nb2 = np.random.normal(0, 1, [2])\nwout = np.random.normal(0, 1, [2, 1])\nbout = np.random.normal(0, 1, [1])\n                             \nprint(""weight1 >>\\n"", w1)\nprint(""bias1 >>\\n"", b1)\nprint(""weight_out >>\\m"", wout)\nprint(""bias_out >>\\n"", bout)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nz1 = xs\n\nfor ite in range(10000):\n    ite += 1\n\n    # feed forward\n    z2 = sigmoid(np.dot(z1, w1) + b1)\n    z3 = sigmoid(np.dot(z2, w2) + b2)\n    out = sigmoid(np.dot(z3, wout) + bout)\n\n    # back propagate\n    En = (out - ts) * out * (1 - out)\n    grad_wout = np.dot(z3.T, En)\n    grad_bout = np.dot(np.ones([En.shape[0]]), En)\n    wout -= lr * grad_wout\n    bout -= lr * grad_bout\n    \n    # backpropagation inter layer\n    grad_u2 = np.dot(En, wout.T) * z3 * (1 - z3)\n    grad_w2 = np.dot(z2.T, grad_u2)\n    grad_b2 = np.dot(np.ones([grad_u2.shape[0]]), grad_u2)\n    w2 -= lr * grad_w2\n    b2 -= lr * grad_b2\n        \n    grad_u1 = np.dot(grad_u2, w2.T) * z2 * (1 - z2)\n    grad_w1 = np.dot(z1.T, grad_u1)\n    grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n    w1 -= lr * grad_w1\n    b1 -= lr * grad_b1\n\n\nprint(""weight1 >>\\n"", w1)\nprint(""bias1 >>\\n"", b1)\nprint(""weight_out >>\\n"", wout)\nprint(""bias_out >>\\n"", bout)\n\n# test\nfor i in range(4):\n    z2 = sigmoid(np.dot(z1[i], w1) + b1)\n    z3 = sigmoid(np.dot(z2, w2) + b2)\n    out = sigmoid(np.dot(z3, wout) + bout)\n    print(""in >>"", xs[i], "", out >>"", out)\n    \n'"
Scripts_Theory1/answers/multi_perceptron_class.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass FullyConnectedLayer():\n    def __init__(self, in_n, out_n, use_bias=True, activation=None):\n        self.w = np.random.normal(0, 1, [in_n, out_n])\n        if use_bias:\n            self.b = np.random.normal(0, 1, [out_n])\n        else:\n            self.b = None\n        if activation is not None:\n            self.activation = activation\n        else:\n            self.activation = None\n\n    def set_lr(self, lr=0.1):\n        self.lr = lr\n\n    def forward(self, feature_in):\n        self.x_in = feature_in\n        x = np.dot(feature_in, self.w)\n        \n        if self.b is not None:\n            x += self.b\n            \n        if self.activation is not None:\n            x = self.activation(x)\n        self.x_out = x\n        \n        return x\n\n    \n    def backward(self, w_pro, grad_pro):\n        grad = np.dot(grad_pro, w_pro.T)\n        if self.activation is sigmoid:\n            grad *= (self.x_out * (1 - self.x_out))\n        grad_w = np.dot(self.x_in.T, grad)\n        self.w -= self.lr * grad_w\n\n        if self.b is not None:\n            grad_b = np.dot(np.ones([grad.shape[0]]), grad)\n            self.b -= self.lr * grad_b\n\n        return grad\n\n    \nclass Model():\n    def __init__(self, *args, lr=0.1):\n        self.layers = args\n        for l in self.layers:\n            l.set_lr(lr=lr)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        self.output = x\n        \n        return x\n\n    def backward(self, t):\n        En = (self.output - t) * self.output * (1 - self.output)\n        grad_pro = En\n        w_pro = np.eye(En.shape[-1])\n        \n        for i, layer in enumerate(self.layers[::-1]):\n            grad_pro = layer.backward(w_pro=w_pro, grad_pro=grad_pro)\n            w_pro = layer.w\n            \n\n\nmodel = Model(FullyConnectedLayer(in_n=2, out_n=64, activation=sigmoid),\n              FullyConnectedLayer(in_n=64, out_n=32, activation=sigmoid),\n              FullyConnectedLayer(in_n=32, out_n=1, activation=sigmoid), lr=0.1)\n\n\nfor ite in range(10000):\n    ite += 1\n\n    model.forward(xs)\n    model.backward(ts)\n\n\n# test\nfor i in range(4):\n    out = model.forward(xs[i])\n    print(""in >>"", xs[i], "", out >>"", out)\n    \n'"
Scripts_Theory1/answers/perceptron_1.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([[-1], [-1], [-1], [1]], dtype=np.float32)\n\n# perceptron\nw = np.random.normal(0., 1, (3))\nprint(""weight >>"", w)\n\n# add bias\n_xs = np.hstack([xs, [[1] for _ in range(4)]])\n\nfor i in range(4):\n    ys = np.dot(w, _xs[i])\n    print(""in >>"", _xs[i], ""y >>"", ys) \n'"
Scripts_Theory1/answers/perceptron_2.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array(((0,0), (0,1), (1,0), (1,1)), dtype=np.float32)\nts = np.array(((-1), (-1), (-1), (1)), dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, (3))\nprint(""weight >>"", w)\n\n# add bias\n_xs = np.hstack([xs, [[1] for _ in range(4)]])\n\n# train\nite = 0\nwhile True:\n    ite += 1\n    # feed forward\n    ys = np.dot(_xs, w)\n    #ys = np.array(list(map(lambda x: np.dot(w, x), _xs)))\n\n    print(""iteration:"", ite, ""y >>"", ys)\n\n    # update parameters\n    if len(np.where(ys * ts < 0)[0]) < 1:\n        break\n\n    _ys = ys.copy()\n    _ts = ts.copy()\n    _ys[ys * ts >= 0] = 0\n    _ts[ys * ts >= 0] = 0\n    En = np.dot(_ts, _xs)\n\n    w += lr * En\n    \n    """"""\n    for i in np.where(ys * ts < 0)[0]:\n        En += ts[i]\n        if ys[i] * ts[i] < 0:\n            En += ts[i] * _xs[i]\n\n    print(""iteration:"", ite, ""y >>"", ys)\n    if np.any(En != 0):\n        w += lr * En\n    else:\n        break\n    """"""\n    \nprint(""training finished!"")\nprint(""weight >>"", w)\n\n# test\nys = np.array(list(map(lambda x: np.dot(w, x), _xs)))\n\nfor i in range(4):\n    ys = np.dot(w, _xs[i])\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_3.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array(((0,0), (0,1), (1,0), (1,1)), dtype=np.float32)\nts = np.array(((-1), (-1), (-1), (1)), dtype=np.float32)\n\nlrs = [0.1, 0.01]\nlinestyles = [\'solid\', \'dashed\']\nplts = []\n\nfor _i in range(len(lrs)):\n    lr = lrs[_i]\n    \n    # perceptron\n    np.random.seed(0)\n    w = np.random.normal(0., 1, (3))\n    print(""weight >>"", w)\n\n    # add bias\n    _xs = np.hstack([xs, [[1] for _ in range(4)]])\n\n    # train\n    ite = 0\n    w1 = [w[0]]\n    w2 = [w[1]]\n    w3 = [w[2]]\n\n    while True:\n        ite += 1\n        # feed forward\n        ys = np.dot(_xs, w)\n\n        print(""iteration:"", ite, ""y >>"", ys)\n\n        # update parameters\n        if len(np.where(ys * ts < 0)[0]) < 1:\n            break\n\n        _ys = ys.copy()\n        _ts = ts.copy()\n        _ys[ys * ts >= 0] = 0\n        _ts[ys * ts >= 0] = 0\n        En = np.dot(_ts, _xs)\n        w += lr * En\n\n        w1.append(w[0])\n        w2.append(w[1])\n        w3.append(w[2])\n\n    print(""training finished!"")\n    print(""weight >>"", w)\n\n    inds = list(range(ite))\n    import matplotlib.pyplot as plt\n    linestyle = linestyles[_i]\n    plts.append(plt.plot(inds, w1, markeredgewidth=0, linestyle=linestyle)[0])\n    plts.append(plt.plot(inds, w2, markeredgewidth=0, linestyle=linestyle)[0])\n    plts.append(plt.plot(inds, w3, markeredgewidth=0, linestyle=linestyle)[0])\n\nplt.legend(plts, [""w1:lr=0.1"",""w2:lr=0.1"",""w3:lr=0.1"",""w1:lr=0.01"",""w2:lr=0.01"",""w3:lr=0.01""], loc=1)\nplt.savefig(""answer_perceptron3.png"")\nplt.show()\n\n# test\nys = np.array(list(map(lambda x: np.dot(w, x), _xs)))\n\nfor i in range(4):\n    ys = np.dot(w, _xs[i])\n    print(""in >>"", _xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_not.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0], [1]], dtype=np.float32)\nts = np.array([1, 0], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, [1])\nb = np.random.normal(0., 1, [1])\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# add bias\nz1 = xs\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# train\nfor ite in range(5000):\n    ite += 1\n    # feed forward\n    ys = sigmoid(np.dot(z1, w) + b)\n\n    #print(""iteration:"", ite, ""y >>"", ys)\n\n    En = -(ts - ys) * ys * (1 - ys)\n    grad_w = np.dot(z1.T, En)\n    grad_b = np.dot(np.ones([En.shape[0]]), En)\n    w -= lr * grad_w\n    b -= lr * grad_b\n    \n    \nprint(""training finished!"")\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# test\nfor i in range(2):\n    ys = sigmoid(np.dot(z1[i], w) + b)[0]\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_or.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([0, 1, 1, 1], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, [2])\nb = np.random.normal(0., 1, [1])\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# add bias\nz1 = xs\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# train\nfor ite in range(5000):\n    ite += 1\n    # feed forward\n    ys = sigmoid(np.dot(z1, w) + b)\n\n    #print(""iteration:"", ite, ""y >>"", ys)\n\n    En = -(ts - ys) * ys * (1 - ys)\n    grad_w = np.dot(z1.T, En)\n    grad_b = np.dot(np.ones([En.shape[0]]), En)\n    w -= lr * grad_w\n    b -= lr * grad_b\n    \n    \nprint(""training finished!"")\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# test\nfor i in range(4):\n    ys = sigmoid(np.dot(z1[i], w) + b)[0]\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_sigmoid.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([0, 0, 0, 1], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, (3))\nprint(""weight >>"", w)\n\n# add bias\nz1 = np.hstack([xs, [[1] for _ in range(4)]])\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# train\nfor ite in range(5000):\n    ite += 1\n    # feed forward\n    ys = sigmoid(np.dot(z1, w))\n\n    #print(""iteration:"", ite, ""y >>"", ys)\n\n    En = -(ts - ys) * ys * (1 - ys)\n    grad_w = np.dot(z1.T, En)\n    w -= lr * grad_w\n    \n    \nprint(""training finished!"")\nprint(""weight >>"", w)\n\n# test\n\nfor i in range(4):\n    ys = sigmoid(np.dot(z1[i], w))\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_sigmoid_bias.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([0, 0, 0, 1], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, [2])\nb = np.random.normal(0., 1, [1])\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# add bias\nz1 = xs\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# train\nfor ite in range(5000):\n    ite += 1\n    # feed forward\n    ys = sigmoid(np.dot(z1, w) + b)\n\n    #print(""iteration:"", ite, ""y >>"", ys)\n\n    En = -(ts - ys) * ys * (1 - ys)\n    grad_w = np.dot(z1.T, En)\n    grad_b = np.dot(np.ones([En.shape[0]]), En)\n    w -= lr * grad_w\n    b -= lr * grad_b\n    \n    \nprint(""training finished!"")\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# test\nfor i in range(4):\n    ys = sigmoid(np.dot(z1[i], w) + b)[0]\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory1/answers/perceptron_xor.py,0,"b'import numpy as np\n\nnp.random.seed(0)\n\nxs = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\nts = np.array([0, 1, 1, 0], dtype=np.float32)\n\nlr = 0.1\n\n# perceptron\nw = np.random.normal(0., 1, [2])\nb = np.random.normal(0., 1, [1])\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\nz1 = xs\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# train\nfor ite in range(5000):\n    ite += 1\n    # feed forward\n    ys = sigmoid(np.dot(z1, w) + b)\n\n    #print(""iteration:"", ite, ""y >>"", ys)\n\n    En = -(ts - ys) * ys * (1 - ys)\n    grad_w = np.dot(z1.T, En)\n    grad_b = np.dot(np.ones([En.shape[0]]), En)\n    w -= lr * grad_w\n    b -= lr * grad_b\n    \n    \nprint(""training finished!"")\nprint(""weight >>"", w)\nprint(""bias >>"", b)\n\n# test\nfor i in range(4):\n    ys = sigmoid(np.dot(z1[i], w) + b)[0]\n    print(""in >>"", xs[i], "", out >>"", ys) \n    \n'"
Scripts_Theory2/answers/avepool.py,0,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nheight, width = 64, 64\n\nimg = cv2.imread(""akahara_0001.jpg"")\nimg = cv2.resize(img, (width, height)).astype(np.float32)\n\nnp.random.seed(0)\n\nin_c = img.shape[-1]\nk_size = 2\nstride = 2\npad = 0\n\npad_img = np.zeros((pad * 2 + height, pad * 2 + width, in_c), np.float32)\npad_img[pad: pad+height, pad: pad+width] = img\n\nout_height = (height + pad) // stride\nout_width = (width + pad) // stride\n\nout = np.zeros((out_height, out_width, in_c), dtype=np.float32)\n\nfor y in range(out_height):\n    for x in range(out_width):\n        for c in range(in_c):\n            out[y, x, c] = np.mean(pad_img[y * stride: y * stride + k_size,\n                                           x * stride: x * stride + k_size, c])\n\nfor i in range(in_c):\n    plt.subplot(1,in_c,i+1)\n    plt.imshow(out[..., i], cmap=\'gray\')\n\nplt.show()\n'"
Scripts_Theory2/answers/conv_kernel.py,0,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nheight, width = 64, 64\n\nimg = cv2.imread(""akahara_0001.jpg"")\nimg = cv2.resize(img, (width, height)).astype(np.float32)\nimg = img.transpose(2,0,1)\n\nnp.random.seed(0)\n\nk_channel = 4\nk_size = 3\nkernels = np.random.normal(0, 0.01, [k_channel, k_size, k_size])\n\nout = np.zeros((k_channel, height-2, width-2), dtype=np.float32)\n\nfor y in range(height-2):\n    for x in range(width-2):\n        for ki in range(k_channel):\n            out[ki, y, x] = np.sum(img[..., y:y+3, x:x+3] * kernels[ki])\n\nfor i in range(k_channel):\n    plt.subplot(1,k_channel,i+1)\n    plt.imshow(out[i], cmap=\'gray\')\n\nplt.show()\n'"
Scripts_Theory2/answers/conv_pad.py,0,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nheight, width = 64, 64\n\nimg = cv2.imread(""akahara_0001.jpg"")\nimg = cv2.resize(img, (width, height)).astype(np.float32)\nimg = img.transpose(2,0,1)\nin_c = img.shape[0]\n\nnp.random.seed(0)\n\nk_channel = 4\nk_size = 3\nkernels = np.random.normal(0, 0.01, [k_channel, k_size, k_size])\n\nout = np.zeros((k_channel, height, width), dtype=np.float32)\n\npad = np.floor(k_size / 2).astype(np.int)\npad_img = np.zeros((in_c, pad * 2 + height, pad * 2 + width), np.float32)\npad_img[..., pad: pad+height, pad: pad+width] = img\n\nfor y in range(height):\n    for x in range(width):\n        for ki in range(k_channel):\n            out[ki, y, x] = np.sum(pad_img[..., y:y+3, x:x+3] * kernels[ki])\n\nfor i in range(k_channel):\n    plt.subplot(1,k_channel,i+1)\n    plt.imshow(out[i], cmap=\'gray\')\n\nplt.show()\n'"
Scripts_Theory2/answers/conv_stride.py,0,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nheight, width = 64, 64\n\nimg = cv2.imread(""akahara_0001.jpg"")\nimg = cv2.resize(img, (width, height)).astype(np.float32)\nimg = img.transpose(2,0,1)\nin_c = img.shape[0]\n\nnp.random.seed(0)\n\nk_channel = 4\nk_size = 3\nstride = 2\nkernels = np.random.normal(0, 0.01, [k_channel, k_size, k_size])\n\npad = np.floor(k_size / 2).astype(np.int)\npad_img = np.zeros((in_c, pad * 2 + height, pad * 2 + width), np.float32)\npad_img[..., pad: pad+height, pad: pad+width] = img\n\nout_height = (height + pad) // stride\nout_width = (width + pad) // stride\n\nout = np.zeros((k_channel, out_height, out_width), dtype=np.float32)\n\nfor y in range(out_height):\n    for x in range(out_width):\n        for ki in range(k_channel):\n            out[ki, y, x] = np.sum(pad_img[..., y * stride: y * stride + 3,\n                                           x * stride: x * stride + 3] * kernels[ki])\n\nfor i in range(k_channel):\n    plt.subplot(1,k_channel,i+1)\n    plt.imshow(out[i], cmap=\'gray\')\n\nplt.show()\n'"
Scripts_Theory2/answers/maxpool.py,0,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nheight, width = 64, 64\n\nimg = cv2.imread(""akahara_0001.jpg"")\nimg = cv2.resize(img, (width, height)).astype(np.float32)\n\nnp.random.seed(0)\n\nin_c = img.shape[-1]\nk_size = 2\nstride = 2\npad = 0\n\npad_img = np.zeros((pad * 2 + height, pad * 2 + width, in_c), np.float32)\npad_img[pad: pad+height, pad: pad+width] = img\n\nout_height = (height + pad) // stride\nout_width = (width + pad) // stride\n\nout = np.zeros((out_height, out_width, in_c), dtype=np.float32)\n\nfor y in range(out_height):\n    for x in range(out_width):\n        for c in range(in_c):\n            out[y, x, c] = np.max(pad_img[y * stride: y * stride + k_size,\n                                           x * stride: x * stride + k_size, c])\n\nfor i in range(in_c):\n    plt.subplot(1,in_c,i+1)\n    plt.imshow(out[..., i], cmap=\'gray\')\n\nplt.show()\n'"
Scripts_Theory2/answers/neuralnet.py,0,"b'import numpy as np\nfrom glob import glob\nimport cv2\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass FullyConnectedLayer():\n    def __init__(self, in_n, out_n, use_bias=True, activation=None):\n        self.w = np.random.normal(0, 1, [in_n, out_n])\n        if use_bias:\n            self.b = np.random.normal(0, 1, [out_n])\n        else:\n            self.b = None\n        if activation is not None:\n            self.activation = activation\n        else:\n            self.activation = None\n\n    def set_lr(self, lr=0.1):\n        self.lr = lr\n\n    def forward(self, feature_in):\n        self.x_in = feature_in\n        x = np.dot(feature_in, self.w)\n        \n        if self.b is not None:\n            x += self.b\n            \n        if self.activation is not None:\n            x = self.activation(x)\n        self.x_out = x\n        \n        return x\n\n    \n    def backward(self, w_pro, grad_pro):\n        grad = np.dot(grad_pro, w_pro.T)\n        if self.activation is sigmoid:\n            grad *= (self.x_out * (1 - self.x_out))\n        grad_w = np.dot(self.x_in.T, grad)\n        self.w -= self.lr * grad_w\n\n        if self.b is not None:\n            grad_b = np.dot(np.ones([grad.shape[0]]), grad)\n            self.b -= self.lr * grad_b\n\n        return grad\n\n    \nclass Model():\n    def __init__(self, *args, lr=0.1):\n        self.layers = args\n        for l in self.layers:\n            l.set_lr(lr=lr)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        self.output = x\n        \n        return x\n\n    def backward(self, t):\n        En = (self.output - t) * self.output * (1 - self.output)\n        grad_pro = En\n        w_pro = np.eye(En.shape[-1])\n        \n        for i, layer in enumerate(self.layers[::-1]):\n            grad_pro = layer.backward(w_pro=w_pro, grad_pro=grad_pro)\n            w_pro = layer.w\n\n\n    def loss(self, t):\n        Loss = np.sum((self.output - t) ** 2) / 2 / t.shape[0]\n        return Loss\n    \n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=None):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot is not None:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n                    angle += rot\n\n    ts = [[t] for t in ts]\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\nmodel = Model(FullyConnectedLayer(in_n=img_height * img_width * 3, out_n=64, activation=sigmoid),\n              FullyConnectedLayer(in_n=64, out_n=32, activation=sigmoid),\n              FullyConnectedLayer(in_n=32, out_n=1, activation=sigmoid), lr=0.1)\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True, rot=1)\n\nmb = 64\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.shuffle(train_ind)\n\nfor ite in range(1000):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    x = xs[mb_ind]\n    t = ts[mb_ind]\n\n    x = x.reshape(mb, -1)\n\n    model.forward(x)\n    model.backward(t)\n    loss = model.loss(t)\n\n    if ite % 50 == 0:\n        print(""ite:"", ite+1, ""Loss >>"", np.sum(loss))\n    \n\n# test\nxs, ts, paths = data_load(""../Dataset/test/images/"")\n\nfor i in range(len(xs)):\n    x = xs[i]\n    x = x.reshape(1, -1)\n    out = model.forward(x)\n    print(""in >>"", paths[i], "", out >>"", out)\n    \n'"
Scripts_Theory2/answers/neuralnet_classification.py,0,"b'import numpy as np\nfrom glob import glob\nimport cv2\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass FullyConnectedLayer():\n    def __init__(self, in_n, out_n, use_bias=True, activation=None):\n        self.w = np.random.normal(0, 1, [in_n, out_n])\n        if use_bias:\n            self.b = np.random.normal(0, 1, [out_n])\n        else:\n            self.b = None\n        if activation is not None:\n            self.activation = activation\n        else:\n            self.activation = None\n\n    def set_lr(self, lr=0.1):\n        self.lr = lr\n\n    def forward(self, feature_in):\n        self.x_in = feature_in\n        x = np.dot(feature_in, self.w)\n        \n        if self.b is not None:\n            x += self.b\n            \n        if self.activation is not None:\n            x = self.activation(x)\n        self.x_out = x\n        \n        return x\n\n    \n    def backward(self, w_pro, grad_pro):\n        grad = np.dot(grad_pro, w_pro.T)\n        if self.activation is sigmoid:\n            grad *= (self.x_out * (1 - self.x_out))\n        grad_w = np.dot(self.x_in.T, grad)\n        self.w -= self.lr * grad_w\n\n        if self.b is not None:\n            grad_b = np.dot(np.ones([grad.shape[0]]), grad)\n            self.b -= self.lr * grad_b\n\n        return grad\n\n    \nclass Model():\n    def __init__(self, *args, lr=0.1):\n        self.layers = args\n        for l in self.layers:\n            l.set_lr(lr=lr)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        self.output = x\n        \n        return x\n\n    def backward(self, t):\n        En = (self.output - t) * self.output * (1 - self.output)\n        grad_pro = En\n        w_pro = np.eye(En.shape[-1])\n        \n        for i, layer in enumerate(self.layers[::-1]):\n            grad_pro = layer.backward(w_pro=w_pro, grad_pro=grad_pro)\n            w_pro = layer.w\n    \n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=None):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot is not None:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n                    angle += rot\n\n    ts = [[t] for t in ts]\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\nmodel = Model(FullyConnectedLayer(in_n=img_height * img_width * 3, out_n=64, activation=sigmoid),\n              FullyConnectedLayer(in_n=64, out_n=32, activation=sigmoid),\n              FullyConnectedLayer(in_n=32, out_n=1, activation=sigmoid), lr=0.1)\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True, rot=1)\n\nmb = 64\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.shuffle(train_ind)\n\nfor ite in range(1000):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    x = xs[mb_ind]\n    t = ts[mb_ind]\n\n    x = x.reshape(mb, -1)\n\n    model.forward(x)\n    model.backward(t)\n\n# test\nxs, ts, paths = data_load(""../Dataset/test/images/"")\n\nfor i in range(len(xs)):\n    x = xs[i]\n    x = x.reshape(1, -1)\n    out = model.forward(x)\n    print(""in >>"", paths[i], "", out >>"", out)\n    \n'"
Scripts_Theory2/answers/neuralnet_loss.py,0,"b'import numpy as np\nfrom glob import glob\nimport cv2\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass FullyConnectedLayer():\n    def __init__(self, in_n, out_n, use_bias=True, activation=None):\n        self.w = np.random.normal(0, 1, [in_n, out_n])\n        if use_bias:\n            self.b = np.random.normal(0, 1, [out_n])\n        else:\n            self.b = None\n        if activation is not None:\n            self.activation = activation\n        else:\n            self.activation = None\n\n    def set_lr(self, lr=0.1):\n        self.lr = lr\n\n    def forward(self, feature_in):\n        self.x_in = feature_in\n        x = np.dot(feature_in, self.w)\n        \n        if self.b is not None:\n            x += self.b\n            \n        if self.activation is not None:\n            x = self.activation(x)\n        self.x_out = x\n        \n        return x\n\n    \n    def backward(self, w_pro, grad_pro):\n        grad = np.dot(grad_pro, w_pro.T)\n        if self.activation is sigmoid:\n            grad *= (self.x_out * (1 - self.x_out))\n        grad_w = np.dot(self.x_in.T, grad)\n        self.w -= self.lr * grad_w\n\n        if self.b is not None:\n            grad_b = np.dot(np.ones([grad.shape[0]]), grad)\n            self.b -= self.lr * grad_b\n\n        return grad\n\n    \nclass Model():\n    def __init__(self, *args, lr=0.1):\n        self.layers = args\n        for l in self.layers:\n            l.set_lr(lr=lr)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        self.output = x\n        \n        return x\n\n    def backward(self, t):\n        En = (self.output - t) * self.output * (1 - self.output)\n        grad_pro = En\n        w_pro = np.eye(En.shape[-1])\n        \n        for i, layer in enumerate(self.layers[::-1]):\n            grad_pro = layer.backward(w_pro=w_pro, grad_pro=grad_pro)\n            w_pro = layer.w\n\n\n    def loss(self, t):\n        Loss = np.sum((self.output - t) ** 2) / 2 / t.shape[0]\n        return Loss\n    \n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=None):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot is not None:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n                    angle += rot\n\n    ts = [[t] for t in ts]\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\nmodel = Model(FullyConnectedLayer(in_n=img_height * img_width * 3, out_n=64, activation=sigmoid),\n              FullyConnectedLayer(in_n=64, out_n=32, activation=sigmoid),\n              FullyConnectedLayer(in_n=32, out_n=1, activation=sigmoid), lr=0.1)\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True, rot=1)\n\nmb = 64\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.shuffle(train_ind)\n\nfor ite in range(1000):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    x = xs[mb_ind]\n    t = ts[mb_ind]\n\n    x = x.reshape(mb, -1)\n\n    model.forward(x)\n    model.backward(t)\n    loss = model.loss(t)\n\n    if ite % 50 == 0:\n        print(""ite:"", ite+1, ""Loss >>"", loss)\n    \n\n# test\nxs, ts, paths = data_load(""../Dataset/test/images/"")\n\nfor i in range(len(xs)):\n    x = xs[i]\n    x = x.reshape(1, -1)\n    out = model.forward(x)\n    print(""in >>"", paths[i], "", out >>"", out)\n    \n'"
Scripts_Theory2/answers/neuralnet_sce.py,0,"b'import numpy as np\nfrom glob import glob\nimport cv2\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass FullyConnectedLayer():\n    def __init__(self, in_n, out_n, use_bias=True, activation=None):\n        self.w = np.random.normal(0, 1, [in_n, out_n])\n        if use_bias:\n            self.b = np.random.normal(0, 1, [out_n])\n        else:\n            self.b = None\n        if activation is not None:\n            self.activation = activation\n        else:\n            self.activation = None\n\n    def set_lr(self, lr=0.1):\n        self.lr = lr\n\n    def forward(self, feature_in):\n        self.x_in = feature_in\n        x = np.dot(feature_in, self.w)\n        \n        if self.b is not None:\n            x += self.b\n            \n        if self.activation is not None:\n            x = self.activation(x)\n        self.x_out = x\n        \n        return x\n\n    \n    def backward(self, w_pro, grad_pro):\n        grad = np.dot(grad_pro, w_pro.T)\n        if self.activation is sigmoid:\n            grad *= (self.x_out * (1 - self.x_out))\n        grad_w = np.dot(self.x_in.T, grad)\n        self.w -= self.lr * grad_w\n\n        if self.b is not None:\n            grad_b = np.dot(np.ones([grad.shape[0]]), grad)\n            self.b -= self.lr * grad_b\n\n        return grad\n\n    \nclass Model():\n    def __init__(self, *args, lr=0.1):\n        self.layers = args\n        for l in self.layers:\n            l.set_lr(lr=lr)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        self.output = x\n        \n        return x\n\n    def backward(self, t):\n        En = (self.output - t)# * self.output * (1 - self.output)\n        grad_pro = En\n        w_pro = np.eye(En.shape[-1])\n        \n        for i, layer in enumerate(self.layers[::-1]):\n            grad_pro = layer.backward(w_pro=w_pro, grad_pro=grad_pro)\n            w_pro = layer.w\n\n\n    def loss(self, t):\n        Loss = np.sum(-t * np.log(self.output) - (1 - t) * np.log(1 - self.output)) / t.shape[0]\n        return Loss\n    \n\nnum_classes = 2\nimg_height, img_width = 64, 64\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=None):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot is not None:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n                    angle += rot\n\n    ts = [[t] for t in ts]\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\nmodel = Model(FullyConnectedLayer(in_n=img_height * img_width * 3, out_n=64, activation=sigmoid),\n              FullyConnectedLayer(in_n=64, out_n=32, activation=sigmoid),\n              FullyConnectedLayer(in_n=32, out_n=1, activation=sigmoid), lr=0.001)\n\n\nxs, ts, paths = data_load(""../Dataset/train/images/"", hf=True, vf=True, rot=1)\n\nmb = 64\nmbi = 0\ntrain_ind = np.arange(len(xs))\nnp.random.shuffle(train_ind)\n\nfor ite in range(2000):\n    if mbi + mb > len(xs):\n        mb_ind = train_ind[mbi:]\n        np.random.shuffle(train_ind)\n        mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        mbi = mb - (len(xs) - mbi)\n    else:\n        mb_ind = train_ind[mbi: mbi+mb]\n        mbi += mb\n\n    x = xs[mb_ind]\n    t = ts[mb_ind]\n\n    x = x.reshape(mb, -1)\n\n    model.forward(x)\n    model.backward(t)\n    loss = model.loss(t)\n\n    if ite % 50 == 0:\n        print(""ite:"", ite+1, ""Loss >>"", loss)\n    \n\n# test\nxs, ts, paths = data_load(""../Dataset/test/images/"")\n\nfor i in range(len(xs)):\n    x = xs[i]\n    x = x.reshape(1, -1)\n    out = model.forward(x)\n    print(""in >>"", paths[i], "", out >>"", out)\n    \n'"
Scripts_Generative/old/scripts_pytorch/WGAN-GP_cifar10_pytorch.py,66,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nimport os\nfrom collections import OrderedDict\nimport pickle\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_num = len(CLS)\nimg_height, img_width = 32, 32 #572, 572\nchannel = 3\nmb = 64\n\n# GAN parameter\nZ_dim = 128\n\n# Gradient penalty parameter\nLambda = 10\n\nmodel_path = \'WGAN_GP.pt\'\n\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\ntorch.manual_seed(0)\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'bn\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n\n\nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n\nclass Reshape(torch.nn.Module):\n    def __init__(self, c, h, w):\n        super(Reshape, self).__init__()\n        self.c = c\n        self.h = h\n        self.w = w\n    \n    def forward(self, x):\n        x = x.view(x.size()[0], self.c, self.h, self.w)\n        return x\n    \n    \n# ResNet block\nclass ResBlock(torch.nn.Module):\n    def __init__(self, dim_first=None, dim=128, activation_fn=torch.nn.ReLU(), batch_norm=False):\n        super(ResBlock, self).__init__()\n\n        if dim_first is None:\n            dim_first = dim\n        else:\n            if batch_norm:\n                self.skip_conv = torch.nn.Sequential(\n                    torch.nn.BatchNorm2d(dim_first),\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n            else:\n                self.skip_conv = torch.nn.Sequential(\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n        \n        if batch_norm:\n            self.block = torch.nn.Sequential(\n                torch.nn.BatchNorm2d(dim_first),\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                torch.nn.BatchNorm2d(dim),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n        else:\n            self.block = torch.nn.Sequential(\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n\n\n    def forward(self, x):\n        res_x = self.block(x)\n\n        if hasattr(self, \'skip_conv\'):\n            x = self.skip_conv(x)\n        \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n        \n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        in_h = img_height // 8\n        in_w = img_width // 8\n        dim = 128\n        \n        super(Generator, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, dim * in_h * in_w),\n            Reshape(dim, in_h, in_w),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),     \n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n        \n            #ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n\n            torch.nn.Conv2d(dim, channel, kernel_size=3, stride=1, padding=1),\n            torch.nn.Tanh(),\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        dim = 128\n        \n        self.module = torch.nn.Sequential(\n            ResBlock(dim_first=channel, dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            #torch.nn.Conv2d(channel, dim, kernel_size=3, padding=1, stride=1),\n            #torch.nn.LeakyReLU(0.2),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.ReLU(),\n\n            torch.nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            #torch.nn.Linear(dim * (img_height // 8) * (img_width // 8), 1),\n            torch.nn.Linear(dim, 1),\n            #torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(\'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'))\n        os.system(""tar xvf {}"".format(\'cifar-10-python.tar.gz\'))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    print(train_x.shape)\n    print(train_y.shape)\n\n    # test data\n    \n    data_path =  path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    print(test_x.shape)\n    print(test_y.shape)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    G = Generator().to(device)\n    D = Discriminator().to(device)\n    \n    G.apply(weights_init)\n    D.apply(weights_init)\n    \n    # wgan hyper-parameter\n    n_critic = 5\n\n    opt_D = torch.optim.Adam(D.parameters(), lr=0.0001, betas=(0, 0.9))\n    opt_G = torch.optim.Adam(G.parameters(), lr=0.0001, betas=(0, 0.9))\n\n\n    #xs, paths = data_load(\'drive/My Drive/Colab Notebooks/datasets/\', hf=True, vf=True, rot=False)\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mbi = 0\n    data_N = len(xs)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    one = torch.FloatTensor([1])\n    minus_one = one * -1\n    if GPU:\n        one = one.cuda()\n        minus_one = mone.cuda()\n    \n    for ite in range(50000):\n        if mbi + mb > data_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        # Discriminator training\n        for _ in range(n_critic):\n            opt_D.zero_grad()\n\n            # sample x from dataset\n            x = xs[mb_ind]\n            x = torch.tensor(x, dtype=torch.float).to(device)\n\n            # sample z from uniform distribution [-1, 1]\n            z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            # sample epsilon from [0, 1]\n            epsilon = np.random.random() #np.random.uniform(0, 1, 1)\n\n            # sample x_hat \n            x_hat = (epsilon * x + (1 - epsilon) * Gz).requires_grad_(True)\n\n            # loss for fake\n            loss_D_fake = D(Gz).mean(0).view(1)\n            # loss for real\n            loss_D_real = D(x).mean(0).view(1)\n\n            # gradient penalty\n            Dx_hat = D(x_hat)\n            musk = torch.ones_like(Dx_hat)\n            gradients = torch.autograd.grad(Dx_hat, x_hat, grad_outputs=musk,\n                             retain_graph=True, create_graph=True,\n                             allow_unused=True)[0]\n            gradients = gradients.view(-1, 1)\n            gradient_penalty = Lambda * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\n            # loss backpropagation\n            loss_D_real.backward(one, retain_graph=True)\n            loss_D_fake.backward(minus_one, retain_graph=True)\n            gradient_penalty.backward(retain_graph=True)\n\n            # total loss\n            loss_D = loss_D_fake - loss_D_real + gradient_penalty\n\n            opt_D.step()\n\n        # Generator training\n        opt_G.zero_grad()\n\n        # sample z from normal distribution [-1, 1]\n        z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        \n        # loss for fake\n        loss_G = D(G(z)).mean(0).view(1)\n\n        # loss backpropagation\n        loss_G.backward(one)\n        opt_G.step()\n\n        if (ite + 1) % 1000 == 0:\n            print(""iter :"", ite + 1, "", G:loss :"", loss_G.item(), "",D:loss :"", loss_D.item())\n            \n            \n            # save training process Generator output\n            img_N = 16\n            z = np.random.uniform(-1, 1, size=(img_N, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for i in range(img_N):\n                generated = Gz[i]\n                plt.subplot(1, img_N, i+1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(\'{}/WGAN-gp_iter_{:05d}.jpg\'.format(save_dir, ite + 1), bbox_inches=\'tight\')\n            plt.show()\n            \n\n    torch.save(G.state_dict(), model_path)\n    \n    \n\n# test\ndef test():\n    # load Generator\n    G = Generator().to(device)\n    G.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    G.eval()\n\n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n                \n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for i in range(mb):\n                generated = Gz[i]\n                plt.subplot(1,mb,i+1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/WGAN-GP_pytorch.py,66,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nimport os\nfrom collections import OrderedDict\nimport pickle\nfrom tqdm import tqdm\n\n# config\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_N = len(CLS)\nimg_height, img_width = 32, 32 #572, 572\nchannel = 3\nmb = 64\n\n# GAN parameter\nZ_dim = 128\n\n# Gradient penalty parameter\nLambda = 10\n\nmodel_path = \'WGAN_GP.pt\'\n\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\ntorch.manual_seed(0)\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'bn\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n\n\nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n\nclass Reshape(torch.nn.Module):\n    def __init__(self, c, h, w):\n        super(Reshape, self).__init__()\n        self.c = c\n        self.h = h\n        self.w = w\n    \n    def forward(self, x):\n        x = x.view(x.size()[0], self.c, self.h, self.w)\n        return x\n    \n    \n# ResNet block\nclass ResBlock(torch.nn.Module):\n    def __init__(self, dim_first=None, dim=128, activation_fn=torch.nn.ReLU(), batch_norm=False):\n        super(ResBlock, self).__init__()\n\n        if dim_first is None:\n            dim_first = dim\n        else:\n            if batch_norm:\n                self.skip_conv = torch.nn.Sequential(\n                    torch.nn.BatchNorm2d(dim_first),\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n            else:\n                self.skip_conv = torch.nn.Sequential(\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n        \n        if batch_norm:\n            self.block = torch.nn.Sequential(\n                torch.nn.BatchNorm2d(dim_first),\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                torch.nn.BatchNorm2d(dim),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n        else:\n            self.block = torch.nn.Sequential(\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n\n\n    def forward(self, x):\n        res_x = self.block(x)\n\n        if hasattr(self, \'skip_conv\'):\n            x = self.skip_conv(x)\n        \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n        \n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        in_h = img_height // 8\n        in_w = img_width // 8\n        dim = 128\n        \n        super(Generator, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, dim * in_h * in_w),\n            Reshape(dim, in_h, in_w),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),     \n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n        \n            #ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n\n            torch.nn.Conv2d(dim, channel, kernel_size=3, stride=1, padding=1),\n            torch.nn.Tanh(),\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        dim = 128\n        \n        self.module = torch.nn.Sequential(\n            ResBlock(dim_first=channel, dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            #torch.nn.Conv2d(channel, dim, kernel_size=3, padding=1, stride=1),\n            #torch.nn.LeakyReLU(0.2),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.ReLU(),\n\n            torch.nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            #torch.nn.Linear(dim * (img_height // 8) * (img_width // 8), 1),\n            torch.nn.Linear(dim, 1),\n            #torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if rot == 0:\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    \n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    \n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    G = Generator().to(device)\n    D = Discriminator().to(device)\n    \n    G.apply(weights_init)\n    D.apply(weights_init)\n    \n    # wgan hyper-parameter\n    n_critic = 5\n\n    opt_D = torch.optim.Adam(D.parameters(), lr=0.0001, betas=(0, 0.9))\n    opt_G = torch.optim.Adam(G.parameters(), lr=0.0001, betas=(0, 0.9))\n\n    paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n    \n    # training\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    one = torch.FloatTensor([1])\n    minus_one = one * -1\n\n    if GPU:\n        one = one.cuda()\n        minus_one = minus_one.cuda()\n    \n    for ite in range(50000):\n        if mbi + mb > data_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        # Discriminator training\n        for _ in range(n_critic):\n            opt_D.zero_grad()\n\n            # sample x from dataset\n            x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n\n            # sample z from uniform distribution [-1, 1]\n            z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            # sample epsilon from [0, 1]\n            epsilon = np.random.random() #np.random.uniform(0, 1, 1)\n\n            # sample x_hat \n            x_hat = (epsilon * x + (1 - epsilon) * Gz).requires_grad_(True)\n\n            # loss for fake\n            loss_D_fake = D(Gz).mean(0).view(1)\n            # loss for real\n            loss_D_real = D(x).mean(0).view(1)\n\n            # gradient penalty\n            Dx_hat = D(x_hat)\n            musk = torch.ones_like(Dx_hat)\n            gradients = torch.autograd.grad(Dx_hat, x_hat, grad_outputs=musk,\n                             retain_graph=True, create_graph=True,\n                             allow_unused=True)[0]\n            gradients = gradients.view(-1, 1)\n            gradient_penalty = Lambda * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\n            # loss backpropagation\n            loss_D_real.backward(one, retain_graph=True)\n            loss_D_fake.backward(minus_one, retain_graph=True)\n            gradient_penalty.backward(retain_graph=True)\n\n            # total loss\n            loss_D = loss_D_fake - loss_D_real + gradient_penalty\n\n            opt_D.step()\n\n        # Generator training\n        opt_G.zero_grad()\n\n        # sample z from normal distribution [-1, 1]\n        z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        \n        # loss for fake\n        loss_G = D(G(z)).mean(0).view(1)\n\n        # loss backpropagation\n        loss_G.backward(one)\n        opt_G.step()\n\n        if (ite + 1) % 1000 == 0:\n            print(""iter :"", ite + 1, "", G:loss :"", loss_G.item(), "",D:loss :"", loss_D.item())\n            \n            \n            # save training process Generator output\n            img_N = 16\n            z = np.random.uniform(-1, 1, size=(img_N, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for i in range(img_N):\n                generated = Gz[i]\n                plt.subplot(1, img_N, i+1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(\'{}/WGAN-gp_iter_{:05d}.jpg\'.format(save_dir, ite + 1), bbox_inches=\'tight\')\n            plt.show()\n            \n\n    torch.save(G.state_dict(), model_path)\n    \n    \n\n# test\ndef test():\n    # load Generator\n    G = Generator().to(device)\n    G.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    G.eval()\n\n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n                \n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for i in range(mb):\n                generated = Gz[i]\n                plt.subplot(1,mb,i+1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/WGAN_cifar10_pytorch.py,43,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nimport os\nfrom collections import OrderedDict\nimport pickle\n\n# config\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_N = len(CLS)\nimg_height, img_width = 32, 32 #572, 572\nchannel = 3\nmb = 64\n\n# GAN parameter\nZ_dim = 128\n\n# Gradient penalty parameter\nLambda = 10\n\nmodel_path = \'WGAN.pt\'\n\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'bn\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n\n\nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n    \n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        dim = 256\n        \n        super(Generator, self).__init__()\n        \n        self.module = torch.nn.Sequential(OrderedDict({\n            ""conv1"": torch.nn.ConvTranspose2d(100, dim * 4, kernel_size=img_height // 8, stride=1, bias=False),\n            ""bn1"": torch.nn.BatchNorm2d(dim * 4),\n            ""relu1"": torch.nn.ReLU(),\n            ""conv2"": torch.nn.ConvTranspose2d(dim * 4, dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            ""bn2"": torch.nn.BatchNorm2d(dim * 2),\n            ""relu2"": torch.nn.ReLU(),\n            ""conv3"": torch.nn.ConvTranspose2d(dim * 2, dim, kernel_size=4, stride=2, padding=1, bias=False),\n            ""bn3"": torch.nn.BatchNorm2d(dim),\n            ""relu3"": torch.nn.ReLU(),\n            ""conv4"": torch.nn.ConvTranspose2d(dim , channel, kernel_size=4, stride=2, padding=1, bias=False),\n            ""tanh"": torch.nn.Tanh(),\n        }))\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        dim = 256\n        \n        super(Discriminator, self).__init__()\n        \n        self.module = torch.nn.Sequential(OrderedDict({\n            ""conv1"": torch.nn.Conv2d(channel, dim, kernel_size=4, stride=2, padding=1),\n            ""bn1"": torch.nn.BatchNorm2d(dim),\n            ""relu1"": torch.nn.LeakyReLU(0.2),\n            ""conv2"": torch.nn.Conv2d(dim,dim * 2, kernel_size=4, stride=2, padding=1),\n            ""bn2"": torch.nn.BatchNorm2d(dim * 2),\n            ""relu2"": torch.nn.LeakyReLU(0.2),\n            ""conv3"": torch.nn.Conv2d(dim * 2, dim * 4, kernel_size=4, stride=2, padding=1),\n            ""bn3"": torch.nn.BatchNorm2d(dim * 4),\n            ""relu3"": torch.nn.LeakyReLU(0.2),\n            ""conv4"": torch.nn.Conv2d(dim * 4, 1, kernel_size=img_height // 8, stride=1, padding=0)\n            #""flatten"": Flatten(),\n            #""linear1"": torch.nn.Linear((img_height // 8) * (img_width // 8) * dim * 4, 1),\n            #""sigmoid"": torch.nn.Sigmoid(),\n        }))\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(\'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'))\n        os.system(""tar xvf {}"".format(\'cifar-10-python.tar.gz\'))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    print(train_x.shape)\n    print(train_y.shape)\n\n    # test data\n    \n    data_path =  path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    print(test_x.shape)\n    print(test_y.shape)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    G = Generator().to(device)\n    D = Discriminator().to(device)\n    \n    G.apply(weights_init)\n    D.apply(weights_init)\n    \n    # wgan hyper-parameter\n    clip_value = 0.01\n    n_critic = 5\n\n    opt_D = torch.optim.RMSprop(D.parameters(), lr=0.00005)\n    opt_G = torch.optim.RMSprop(G.parameters(), lr=0.00005)\n\n\n    #xs, paths = data_load(\'drive/My Drive/Colab Notebooks/datasets/\', hf=True, vf=True, rot=False)\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 64\n    mbi = 0\n    data_N = len(xs)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    one = torch.FloatTensor([1])\n    mone = one * -1\n    if GPU:\n        one = one.cuda()\n        minus_one = mone.cuda()\n    \n    for i in range(100000):\n        if mbi + mb > len(xs):\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - ( data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        # Discriminator training\n        #for _ in range(n_critic):\n        \n        for _ in range(n_critic):\n            opt_D.zero_grad()\n            \n            # parameter clipping > [-clip_value, clip_value]\n            for param in D.parameters():\n                param.data.clamp_(-clip_value, clip_value)\n\n            # sample real image from dataset\n            x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n            # sample noize z from normal distribution [-1, 1]\n            z = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            # feedforward G(z)\n            Gz = G(z)\n\n            # loss for fake\n            loss_D_fake = D(Gz).mean(0).view(1)\n\n            # loss for real\n            loss_D_real = D(x).mean(0).view(1)\n            loss_D_real.backward(one)\n            loss_D_fake.backward(minus_one)\n\n            # total loss\n            loss_D = loss_D_fake - loss_D_real \n\n            Wasserstein_distance = loss_D_real - loss_D_fake\n            opt_D.step()\n\n\n        # Generator training\n        opt_G.zero_grad()\n\n        # sample noize z from normal distribution [-1, 1]\n        z = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        z = torch.tensor(z, dtype=torch.float).to(device)\n    \n        # loss for fake\n        loss_G = D(G(z)).mean(0).view(1)\n\n        loss_G.backward(one)\n        opt_G.step()\n\n        if (i + 1) % 50 == 0:\n            print(""iter :"", i+1, ""WDistance :"", Wasserstein_distance.item(),  "", G:loss :"", loss_G.item(), "",D:loss :"", loss_D.item())\n            \n        if (i + 1) % 100 == 0:\n            # save training process Generator output\n            img_N = 16\n            z = np.random.uniform(-1, 1, size=(img_N, 100, 1, 1))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for j in range(img_N):\n                generated = Gz[j]\n                plt.subplot(1, img_N, j + 1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(\'{}/wgan_iter_{:05d}.jpg\'.format(save_dir, i + 1), bbox_inches=\'tight\')\n            \n\n    torch.save(G.state_dict(), model_path)\n    \n    \n\n# test\ndef test():\n    G = Generator().to(device)\n    G.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    G.eval()\n    \n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            z = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n                \n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for i in range(mb):\n                generated = Gz[i]\n                plt.subplot(1,mb,i+1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/ae_cifar10_pytorch.py,14,"b'import torch\nimport torch.nn.functional as F\nimport torchvision\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# class config\nclass_label = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 32, 32 #572, 572\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'AE.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n    \nclass AE(torch.nn.Module):\n    def __init__(self):\n        super(AE, self).__init__()\n\n        self.enc = torch.nn.Linear(img_height * img_width * channel, 128)\n        self.dec = torch.nn.Linear(128, img_height * img_width * channel)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = self.enc(x)\n        x = self.dec(x)\n        return x\n\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    model = AE().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    # training\n    mb = 512\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        y = model(x)\n        \n        loss = torch.nn.MSELoss()(y, t.view(mb, -1))\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = AE().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/ae_pytorch.py,14,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\n# class config\nclass_label = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 64, 64 #572, 572\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'AE.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n    \nclass AE(torch.nn.Module):\n    def __init__(self):\n        super(AE, self).__init__()\n\n        self.enc = torch.nn.Linear(img_height * img_width * channel, 64)\n        self.dec = torch.nn.Linear(64, img_height * img_width * channel)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = self.enc(x)\n        x = self.dec(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side))\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side, _c))\n                    \n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n                    \n    xs = np.array(xs, dtype=np.float32)\n\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = np.transpose(xs, [0,3,1,2])\n    \n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    model = AE().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 256\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        y = model(x)\n        \n        loss = torch.nn.MSELoss()(y, t.view(mb, -1))\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = Mynet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            _x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n\n            print(""in {}"".format(path))\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/alphaGAN_cifar10_pytorch.py,112,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nimport os\nfrom copy import copy\n\n\n# embedding vector\nZ_dim = 128\n\n# L1 reconstruction loss balance\nreconstruction_loss_lambda = 1.\n# to avoid log(0) of loss\nepsilon = 1e-12\n\nimg_height, img_width = 32, 32\nchannel = 3\n\nGPU = True\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\ntorch.manual_seed(0)\n\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_path_G = \'alphaGAN_G.pt\'\nmodel_path_D = \'alphaGAN_D.pt\'\nmodel_path_E = \'alphaGAN_E.pt\'\nmodel_path_CD = \'alphaGAN_CD.pt\'\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'bn\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n        \n        \nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n    \n    \nclass Reshape(torch.nn.Module):\n    def __init__(self, c, h, w):\n        super(Reshape, self).__init__()\n        self.c = c\n        self.h = h\n        self.w = w\n    \n    def forward(self, x):\n        x = x.view(x.size()[0], self.c, self.h, self.w)\n        return x\n\n    \n# ResNet block\nclass ResBlock(torch.nn.Module):\n    def __init__(self, dim_first=None, dim=128, activation_fn=torch.nn.ReLU(), batch_norm=False):\n        super(ResBlock, self).__init__()\n\n        if dim_first is None:\n            dim_first = dim\n        else:\n            if batch_norm:\n                self.skip_conv = torch.nn.Sequential(\n                    torch.nn.BatchNorm2d(dim_first),\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n            else:\n                self.skip_conv = torch.nn.Sequential(\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n        \n        if batch_norm:\n            self.block = torch.nn.Sequential(\n                torch.nn.BatchNorm2d(dim_first),\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                torch.nn.BatchNorm2d(dim),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n        else:\n            self.block = torch.nn.Sequential(\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n\n\n    def forward(self, x):\n        res_x = self.block(x)\n\n        if hasattr(self, \'skip_conv\'):\n            x = self.skip_conv(x)\n        \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n\n        \n        \nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        in_h = img_height // 8\n        in_w = img_width // 8\n        dim = 128\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Conv2d(channel, dim, kernel_size=3, stride=1, padding=1),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.MaxPool2d(2, stride=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.MaxPool2d(2, stride=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.MaxPool2d(2, stride=2),\n            \n            Flatten(),\n            torch.nn.Linear(dim * in_h * in_w, Z_dim)\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n        \n        \n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        in_h = img_height // 8\n        in_w = img_width // 8\n        dim = 128\n        \n        super(Generator, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, dim * in_h * in_w),\n            Reshape(dim, in_h, in_w),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),     \n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n            \n            ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n            torch.nn.UpsamplingBilinear2d(scale_factor=2),\n        \n            #ResBlock(dim=dim, activation_fn=torch.nn.ReLU(), batch_norm=True),\n\n            torch.nn.Conv2d(dim, channel, kernel_size=3, stride=1, padding=1),\n            torch.nn.Tanh(),\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        dim = 128\n        \n        self.module = torch.nn.Sequential(\n            ResBlock(dim_first=channel, dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            #torch.nn.Conv2d(channel, dim, kernel_size=3, padding=1, stride=1),\n            #torch.nn.LeakyReLU(0.2),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.AvgPool2d(2, stride=2),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n\n            ResBlock(dim=dim, activation_fn=torch.nn.LeakyReLU(0.2), batch_norm=False),\n            torch.nn.ReLU(),\n\n            torch.nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            #torch.nn.Linear(dim * (img_height // 8) * (img_width // 8), 1),\n            torch.nn.Linear(dim, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \n    \nclass Code_Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Code_Discriminator, self).__init__()\n        \n        hidden_dim = 750\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, hidden_dim),\n            torch.nn.LeakyReLU(0.2),\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.LeakyReLU(0.2),\n            #torch.nn.Linear(hidden_dim, hidden_dim),\n            #torch.nn.LeakyReLU(0.2),\n            torch.nn.Linear(hidden_dim, 1),\n            torch.nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n        \n    \n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path =  path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    print(train_x.shape)\n    print(train_y.shape)\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    print(test_x.shape)\n    print(test_y.shape)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n\n    # model\n    G = Generator().to(device)\n    D = Discriminator().to(device)\n    E = Encoder().to(device)\n    CD = Code_Discriminator().to(device)\n    \n\n    opt_G = torch.optim.Adam(G.parameters(), lr=0.0005, betas=(0.5, 0.9))\n    opt_D = torch.optim.Adam(D.parameters(), lr=0.0005,  betas=(0.5, 0.9))\n    opt_E = torch.optim.Adam(E.parameters(), lr=0.0001, betas=(0.5, 0.9))\n    opt_CD = torch.optim.Adam(CD.parameters(), lr=0.0005, betas=(0.5, 0.9))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    BCE_loss = torch.nn.BCELoss()\n    L1_loss = torch.nn.L1Loss()\n\n\n    # get next minibatch index\n    def get_next_minibatch(train_ind, mbi, mb=mb):\n        train_N = len(train_ind)\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        return mb_ind, train_ind\n    \n    \n    for i in range(100000):\n        #if mbi + mb > train_N:\n        #    mb_ind = copy(train_ind[mbi:])\n        #    np.random.shuffle(train_ind)\n        #    mb_ind = np.hstack((mb_ind, train_ind[:(mb-(train_N-mbi))]))\n        #    mbi = mb - (train_N - mbi)\n        #else:\n        #    mb_ind = train_ind[mbi: mbi+mb]\n        #    mbi += mb\n\n        mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        #----\n        # update\n        \n        # Encoder update\n        opt_E.zero_grad()\n    \n        #loss_L1.backward(retain_graph=True)\n        #loss_CD_Gen.backward(retain_graph=True)\n        #loss_E = loss_L1 + loss_CD_Gen\n        #loss_L1.backward()\n        #R_Cw_z_hat.backward()\n        #loss_E = loss_L1 + R_Cw_z_hat\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        z_hat = E(x)\n        x_hat = G(E(x))\n    \n        Cw_z_hat = CD(z_hat)\n    \n        loss_Reconstruction = reconstruction_loss_lambda * L1_loss(x, x_hat)\n        loss_E = loss_Reconstruction \n        #loss_E += (- torch.log(Cw_z_hat + epsilon) + torch.log(1 - Cw_z_hat + epsilon)).mean()\n        #loss_E += (- torch.log(Cw_z_hat + epsilon)).mean()\n        loss_E += BCE_loss(Cw_z_hat, torch.ones(mb).to(device))\n        loss_E.backward(retain_graph=True)\n        \n        opt_E.step()\n\n        \n        # Generator update\n        opt_G.zero_grad()\n\n        # get x from p(x)\n        #mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        #x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        x_hat = G(E(x))\n        z_hat = E(x)\n        Dphi_x_hat = D(x_hat)\n    \n        # get z from p(z)\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Dphi_Gz = D(G(z))\n    \n        loss_G = reconstruction_loss_lambda * L1_loss(x, x_hat) # Reconstruction loss\n        #loss_G += (- torch.log(Dphi_x_hat + epsilon) + torch.log(1 - Dphi_x_hat + epsilon)).mean() # R_Dphi_x_hat loss\n        #loss_G += (- torch.log(Dphi_Gz + epsilon) + torch.log(1 - Dphi_Gz + epsilon)).mean() # R_Dphi_Gz loss\n        #loss_G += (- torch.log(Dphi_Gz + epsilon)).mean()\n        loss_G += BCE_loss(Dphi_Gz, torch.ones(mb).to(device))\n        loss_G.backward(retain_graph=True)\n        \n        for _ in range(2):\n            opt_G.step()\n\n        \n        # Discriminator update\n        opt_D.zero_grad()\n        \n        # get x from p(x)\n        #mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        #x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        Dphi_x = D(x)\n        Dphi_x_hat = D(G(E(x)))\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Dphi_Gz = D(G(z))\n        #loss_D = - torch.log(Dphi_x + epsilon).mean() - torch.log(1 - Dphi_x_hat + epsilon).mean()  - torch.log(1 - Dphi_Gz + epsilon).mean()\n        #loss_D.backward(retain_graph=True)\n        _loss_D = BCE_loss(Dphi_x, torch.ones(mb).to(device)) + BCE_loss(Dphi_x_hat, torch.zeros(mb).to(device))\n        _loss_D.backward(retain_graph=True)\n        loss_D = _loss_D\n        _loss_D_ = BCE_loss(Dphi_Gz, torch.zeros(mb).to(device))\n        _loss_D.backward(retain_graph=True)\n        loss_D += _loss_D\n        \n        opt_D.step()\n        \n        \n        # Code Discriminator update\n        opt_CD.zero_grad()\n        \n        # get x from p(x)\n        #mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        #x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        z_hat = E(x)    \n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Cw_z = CD(z)\n        Cw_z_hat = CD(z_hat)\n        #loss_CD = - torch.log(1 - Cw_z_hat + epsilon).mean() - torch.log(Cw_z + epsilon).mean()\n        loss_CD = BCE_loss(Cw_z, torch.ones(mb).to(device)) + BCE_loss(Cw_z_hat, torch.zeros(mb).to(device))\n        loss_CD.backward(retain_graph=True)\n        \n        opt_CD.step()\n        \n        \n        if (i + 1) % 50 == 0:\n            print(\'iter : {} , Loss E : {:.5f} , G : {:.5f} , D : {:.5f} , CD : {:.5f}\'.format(\n                i + 1, loss_E.item(), loss_G.item(), loss_D.item(), loss_CD.item()))\n\n        if (i + 1) % 1000 == 0:\n            img_N = 16\n            z = np.random.randn(img_N, Z_dim)\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0, 2, 3, 1)\n\n            for j in range(img_N):\n                generated = Gz[j]\n                plt.subplot(1, img_N, j + 1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(\'{}/alphaGAN_iter_{:05d}.jpg\'.format(save_dir, i + 1), bbox_inches=\'tight\')\n            plt.close()\n\n    torch.save(G.state_dict(), model_path_G)\n    torch.save(D.state_dict(), model_path_D)\n    torch.save(E.state_dict(), model_path_E)\n    torch.save(CD.state_dict(), model_path_CD)\n\n# test\ndef test():\n    G = Generator().to(device)\n    G.load_state_dict(torch.load(model_path_G, map_location=torch.device(device)))\n    G.eval()\n    \n    D = Discriminator().to(device)\n    D.load_state_dict(torch.load(model_path_D, map_location=torch.device(device)))\n    D.eval()\n    \n    E = Encoder().to(device)\n    E.load_state_dict(torch.load(model_path_E, map_location=torch.device(device)))\n    E.eval()\n    \n    CD = Code_Discriminator().to(device)\n    CD.load_state_dict(torch.load(model_path_CD, map_location=torch.device(device)))\n    CD.eval()\n\n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            #z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = np.random.randn(mb, Z_dim)\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for j in range(mb):\n                generated = Gz[j]\n                plt.subplot(1, mb, j + 1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(save_dir + \'/alphaGAN_test_{}.jpg\'.format(i))\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\ntrain()\ntest()\n'"
Scripts_Generative/old/scripts_pytorch/alphaGAN_mnist_pytorch.py,112,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nimport os\nfrom copy import copy\nimport gzip\nimport pickle\n\n\n# embedding vector\nZ_dim = 10\n\n# L1 reconstruction loss balance\nreconstruction_loss_lambda = 1.\n# to avoid log(0) of loss\nepsilon = 1e-12\n\nimg_height, img_width = 28, 28\nchannel = 1\n\nGPU = True\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\ntorch.manual_seed(0)\n\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_path_G = \'alphaGAN_G.pt\'\nmodel_path_D = \'alphaGAN_D.pt\'\nmodel_path_E = \'alphaGAN_E.pt\'\nmodel_path_CD = \'alphaGAN_CD.pt\'\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'bn\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n        \n        \nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n    \n    \nclass Reshape(torch.nn.Module):\n    def __init__(self, c, h, w):\n        super(Reshape, self).__init__()\n        self.c = c\n        self.h = h\n        self.w = w\n    \n    def forward(self, x):\n        x = x.view(x.size()[0], self.c, self.h, self.w)\n        return x\n\n    \n# ResNet block\nclass ResBlock(torch.nn.Module):\n    def __init__(self, dim_first=None, dim=128, activation_fn=torch.nn.ReLU(), batch_norm=False):\n        super(ResBlock, self).__init__()\n\n        if dim_first is None:\n            dim_first = dim\n        else:\n            if batch_norm:\n                self.skip_conv = torch.nn.Sequential(\n                    torch.nn.BatchNorm2d(dim_first),\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n            else:\n                self.skip_conv = torch.nn.Sequential(\n                    activation_fn,\n                    torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1)\n                )\n        \n        if batch_norm:\n            self.block = torch.nn.Sequential(\n                torch.nn.BatchNorm2d(dim_first),\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                torch.nn.BatchNorm2d(dim),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n        else:\n            self.block = torch.nn.Sequential(\n                activation_fn,\n                torch.nn.Conv2d(dim_first, dim, kernel_size=3, padding=1, stride=1),\n                activation_fn,\n                torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1)\n            )\n\n\n    def forward(self, x):\n        res_x = self.block(x)\n\n        if hasattr(self, \'skip_conv\'):\n            x = self.skip_conv(x)\n        \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n\n        \n        \nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        in_h = img_height // 4\n        in_w = img_width // 4\n        dim = 32\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Conv2d(channel, dim, kernel_size=5, stride=2, padding=2),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(dim, dim * 2, kernel_size=5, stride=1, padding=2),\n            torch.nn.BatchNorm2d(dim * 2),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(dim * 2, dim * 2, kernel_size=5, stride=2, padding=2),\n            torch.nn.BatchNorm2d(dim * 2),\n            torch.nn.ReLU(),\n            \n            Flatten(),\n            torch.nn.Linear(dim * 2 * in_h * in_w, Z_dim)\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n        \n        \n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        in_h = img_height // 4\n        in_w = img_width // 4\n        dim = 32\n        \n        super(Generator, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, dim * 2 * in_h * in_w),\n            Reshape(dim * 2, in_h, in_w),\n            torch.nn.BatchNorm2d(dim * 2),\n            torch.nn.ReLU(),\n            \n            torch.nn.ConvTranspose2d(dim * 2, dim * 2, kernel_size=4, stride=2, padding=1, bias=False), \n            torch.nn.BatchNorm2d(dim * 2),\n            torch.nn.ReLU(),\n            \n            torch.nn.ConvTranspose2d(dim * 2, dim, kernel_size=5, stride=1, padding=2, bias=False),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),\n\n            torch.nn.ConvTranspose2d(dim, channel, kernel_size=4, stride=2, padding=1),\n            torch.nn.Tanh(),\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        dim = 8\n        \n        self.module = torch.nn.Sequential(\n            torch.nn.Conv2d(channel, dim, kernel_size=5, stride=2, padding=2),\n            torch.nn.LeakyReLU(0.2),\n\n            torch.nn.Conv2d(dim, dim * 2, kernel_size=5, stride=2, padding=2),\n            torch.nn.LeakyReLU(0.2),\n            \n            torch.nn.Conv2d(dim * 2, dim * 4, kernel_size=5, stride=1, padding=2),\n            torch.nn.LeakyReLU(0.2),\n            \n            torch.nn.Conv2d(dim * 4, dim * 8, kernel_size=5, stride=1, padding=2),\n            torch.nn.LeakyReLU(0.2),\n            \n            torch.nn.Conv2d(dim * 8, dim * 8, kernel_size=5, stride=2, padding=2),\n            torch.nn.LeakyReLU(0.2),\n            \n            Flatten(),\n            torch.nn.Linear(dim * 8 * 4 * 4, 2),\n            torch.nn.LeakyReLU(0.2),\n            torch.nn.Linear(2, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \n    \nclass Code_Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Code_Discriminator, self).__init__()\n        \n        hidden_dim = 750\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(Z_dim, hidden_dim),\n            torch.nn.LeakyReLU(0.2),\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.LeakyReLU(0.2),\n            #torch.nn.Linear(hidden_dim, hidden_dim),\n            #torch.nn.LeakyReLU(0.2),\n            torch.nn.Linear(hidden_dim, 1),\n            torch.nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n        \n    \n\ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n\n\n    return train_x, train_y ,test_x, test_y\n\n\n# train\ndef train():\n\n    # model\n    G = Generator().to(device)\n    D = Discriminator().to(device)\n    E = Encoder().to(device)\n    CD = Code_Discriminator().to(device)\n    \n\n    opt_G = torch.optim.Adam(G.parameters(), lr=0.0005, betas=(0.5, 0.9))\n    opt_D = torch.optim.Adam(D.parameters(), lr=0.0005,  betas=(0.5, 0.9))\n    opt_E = torch.optim.Adam(E.parameters(), lr=0.0001, betas=(0.5, 0.9))\n    opt_CD = torch.optim.Adam(CD.parameters(), lr=0.0005, betas=(0.5, 0.9))\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 127.5 - 1\n    xs = np.expand_dims(xs, axis=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    BCE_loss = torch.nn.BCELoss()\n    L1_loss = torch.nn.L1Loss()\n\n\n    # get next minibatch index\n    def get_next_minibatch(train_ind, mbi, mb=mb):\n        train_N = len(train_ind)\n        if mbi + mb > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        return mb_ind, train_ind\n    \n    \n    for i in range(100000):\n        #if mbi + mb > train_N:\n        #    mb_ind = copy(train_ind[mbi:])\n        #    np.random.shuffle(train_ind)\n        #    mb_ind = np.hstack((mb_ind, train_ind[:(mb-(train_N-mbi))]))\n        #    mbi = mb - (train_N - mbi)\n        #else:\n        #    mb_ind = train_ind[mbi: mbi+mb]\n        #    mbi += mb\n\n        mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        #----\n        # update\n        \n        # Encoder update\n        opt_E.zero_grad()\n    \n        #loss_L1.backward(retain_graph=True)\n        #loss_CD_Gen.backward(retain_graph=True)\n        #loss_E = loss_L1 + loss_CD_Gen\n        #loss_L1.backward()\n        #R_Cw_z_hat.backward()\n        #loss_E = loss_L1 + R_Cw_z_hat\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        z_hat = E(x)\n        x_hat = G(E(x))\n    \n        Cw_z_hat = CD(z_hat)\n    \n        loss_Reconstruction = reconstruction_loss_lambda * L1_loss(x, x_hat)\n        loss_E = loss_Reconstruction \n        #loss_E += (- torch.log(Cw_z_hat + epsilon) + torch.log(1 - Cw_z_hat + epsilon)).mean()\n        #loss_E += (- torch.log(Cw_z_hat + epsilon)).mean()\n        loss_E += BCE_loss(Cw_z_hat, torch.ones(mb).to(device))\n        loss_E.backward(retain_graph=True)\n        \n        opt_E.step()\n\n        \n        # Generator update\n        opt_G.zero_grad()\n\n        # get x from p(x)\n        mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        x_hat = G(E(x))\n        z_hat = E(x)\n        Dphi_x_hat = D(x_hat)\n    \n        # get z from p(z)\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Dphi_Gz = D(G(z))\n    \n        loss_G = reconstruction_loss_lambda * L1_loss(x, x_hat) # Reconstruction loss\n        #loss_G += (- torch.log(Dphi_x_hat + epsilon) + torch.log(1 - Dphi_x_hat + epsilon)).mean() # R_Dphi_x_hat loss\n        #loss_G += (- torch.log(Dphi_Gz + epsilon) + torch.log(1 - Dphi_Gz + epsilon)).mean() # R_Dphi_Gz loss\n        #loss_G += (- torch.log(Dphi_Gz + epsilon)).mean()\n        loss_G += BCE_loss(Dphi_Gz, torch.ones(mb).to(device))\n        loss_G.backward(retain_graph=True)\n        \n        for _ in range(2):\n            opt_G.step()\n\n        \n        # Discriminator update\n        opt_D.zero_grad()\n        \n        # get x from p(x)\n        mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        Dphi_x = D(x)\n        Dphi_x_hat = D(G(E(x)))\n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Dphi_Gz = D(G(z))\n        #loss_D = - torch.log(Dphi_x + epsilon).mean() - torch.log(1 - Dphi_x_hat + epsilon).mean()  - torch.log(1 - Dphi_Gz + epsilon).mean()\n        #loss_D.backward(retain_graph=True)\n        _loss_D = BCE_loss(Dphi_x, torch.ones(mb).to(device)) + BCE_loss(Dphi_x_hat, torch.zeros(mb).to(device))\n        _loss_D.backward(retain_graph=True)\n        loss_D = _loss_D\n        _loss_D_ = BCE_loss(Dphi_Gz, torch.zeros(mb).to(device))\n        _loss_D.backward(retain_graph=True)\n        loss_D += _loss_D\n        \n        opt_D.step()\n        \n        \n        # Code Discriminator update\n        opt_CD.zero_grad()\n        \n        # get x from p(x)\n        mb_ind, train_ind = get_next_minibatch(train_ind, mbi, mb=mb)\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        z_hat = E(x)    \n        z = np.random.randn(mb, Z_dim)\n        z = torch.tensor(z, dtype=torch.float).to(device)\n        Cw_z = CD(z)\n        Cw_z_hat = CD(z_hat)\n        #loss_CD = - torch.log(1 - Cw_z_hat + epsilon).mean() - torch.log(Cw_z + epsilon).mean()\n        loss_CD = BCE_loss(Cw_z, torch.ones(mb).to(device)) + BCE_loss(Cw_z_hat, torch.zeros(mb).to(device))\n        loss_CD.backward(retain_graph=True)\n        \n        opt_CD.step()\n        \n        \n        if (i + 1) % 50 == 0:\n            #print(""iter : {} , Loss D {:.5f} (Loss D fake : {:.5f} , Loss D real : {:.5f} , loss D rec : {:.5f}) ,\\n          loss G : {:.5f} , (loss G fake : {:.5f} , loss G_rec : {:.5f}) \\n          loss CD : {:.5f} (loss CD fake : {:.5f} , loss CD real : {:.5f})"".format(\n            #    i+1, loss_D.item(), loss_D_fake.item(), loss_D_real.item(), loss_D_rec.item(),\n            #    loss_G.item(), loss_G_fake.item(), loss_G_rec.item(), loss_CD.item(), loss_CD_fake.item(), loss_CD_real.item()))\n            \n            print(\'iter : {} , Loss E : {:.5f} , G : {:.5f} , D : {:.5f} , CD : {:.5f}\'.format(\n                i + 1, loss_E.item(), loss_G.item(), loss_D.item(), loss_CD.item()))\n\n        if (i + 1) % 1000 == 0:\n            img_N = 16\n            z = np.random.randn(img_N, Z_dim)\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0, 2, 3, 1)\n\n            for j in range(img_N):\n                generated = Gz[j, ..., 0]\n                plt.subplot(1, img_N, j + 1)\n                plt.imshow(generated, cmap=\'gray\')\n                plt.axis(\'off\')\n\n            plt.savefig(\'{}/alphaGAN_mnist_iter_{:05d}.jpg\'.format(save_dir, i + 1), bbox_inches=\'tight\')\n            plt.close()\n\n    torch.save(G.state_dict(), model_path_G)\n    torch.save(D.state_dict(), model_path_D)\n    torch.save(E.state_dict(), model_path_E)\n    torch.save(CD.state_dict(), model_path_CD)\n\n# test\ndef test():\n    G = Generator().to(device)\n    G.load_state_dict(torch.load(model_path_G, map_location=torch.device(device)))\n    G.eval()\n    \n    D = Discriminator().to(device)\n    D.load_state_dict(torch.load(model_path_D, map_location=torch.device(device)))\n    D.eval()\n    \n    E = Encoder().to(device)\n    E.load_state_dict(torch.load(model_path_E, map_location=torch.device(device)))\n    E.eval()\n    \n    CD = Code_Discriminator().to(device)\n    CD.load_state_dict(torch.load(model_path_CD, map_location=torch.device(device)))\n    CD.eval()\n\n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            #z = np.random.uniform(-1, 1, size=(mb, Z_dim))\n            z = np.random.randn(mb, Z_dim)\n            z = torch.tensor(z, dtype=torch.float).to(device)\n\n            Gz = G(z)\n\n            if GPU:\n                Gz = Gz.cpu()\n\n            Gz = Gz.detach().numpy()\n            Gz = (Gz + 1) / 2\n            Gz = Gz.transpose(0,2,3,1)\n\n            for j in range(mb):\n                generated = Gz[j]\n                plt.subplot(1, mb, j + 1)\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.savefig(save_dir + \'/alphaGAN_test_{}.jpg\'.format(i))\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\ntrain()\ntest()\n'"
Scripts_Generative/old/scripts_pytorch/cgan_cifar10_pytorch.py,60,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport pickle\nimport os\n\n# config\nclass_N = 10\n\nimg_height, img_width = 32, 32\nchannel = 3\n\ntorch.manual_seed(0)\n\n# save directory\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n# model path\nmodel_path = \'CGAN.pt\'\n    \n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        self.in_h = img_height // 16\n        self.in_w = img_width // 16\n        self.base = 128\n        \n        super(Generator, self).__init__()\n        #self.lin = torch.nn.Linear(100, self.in_h * self.in_w * self.base * 8)\n        self.lin = torch.nn.ConvTranspose2d(100 +class_N, self.base * 8, kernel_size=self.in_h, stride=1, bias=False)\n        self.bnin = torch.nn.BatchNorm2d(self.base * 8)\n\n        #self.y_in = torch.nn.Linear(num_classes, self.base * 8 * self.in_h * self.in_h)\n        #self.concat = torch.nn.Conv2d(self.base * 16, self.base * 8, kernel_size=1, padding=0, stride=1)\n        \n        self.l1 = torch.nn.ConvTranspose2d(self.base* 8, self.base * 4, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l2 = torch.nn.ConvTranspose2d(self.base * 4, self.base * 2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.ConvTranspose2d(self.base * 2, self.base, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn3 = torch.nn.BatchNorm2d(self.base)\n        self.l4 = torch.nn.ConvTranspose2d(self.base, channel, kernel_size=4, stride=2, padding=1, bias=False)\n        \n        \n    def forward(self, x, y, test=False):\n        #x = torch.cat((x, y), dim=1)\n        con_x = np.zeros((len(y),class_N, 1, 1), dtype=np.float32)\n        con_x[np.arange(len(y)), y] = 1\n        con_x = torch.tensor(con_x, dtype=torch.float).to(device)\n        \n        x = torch.cat((x, con_x), dim=1)\n        \n        x = self.lin(x)\n        x = self.bnin(x)\n        \n        #x = x.view([-1, self.base*8, self.in_h, self.in_w])\n        x = torch.nn.functional.relu(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l4(x)\n        x = torch.tanh(x)\n\n        if test:\n            return x\n        \n        else:\n            con_x = np.zeros((len(y),class_N, img_height, img_width), dtype=np.float32)\n            con_x[np.arange(len(y)), y] = 1\n            con_x = torch.tensor(con_x).to(device)\n        \n            out_x = torch.cat((x, con_x), dim=1)\n        \n            return out_x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 64\n        \n        super(Discriminator, self).__init__()\n        self.l1 = torch.nn.Conv2d(channel +class_N, self.base, kernel_size=5, padding=2, stride=2)\n        self.l2 = torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2)\n        #self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.Conv2d(self.base * 2, self.base * 4, kernel_size=5, padding=2, stride=2)\n        #self.bn3 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l4 = torch.nn.Conv2d(self.base * 4, self.base * 8, kernel_size=5, padding=2, stride=2)\n        #self.bn4 = torch.nn.BatchNorm2d(self.base * 8)\n        self.l5 = torch.nn.Linear((img_height // 16) * (img_width // 16) * self.base * 8, 1)\n\n    def forward(self, x):\n        \n        #con_x = np.zeros((len(y),class_N, img_height, img_width), dtype=np.float32)\n        #con_x[np.arange(len(y)), y] = 1\n        #con_x = torch.tensor(con_x).to(device)\n        #x = torch.cat((x, con_x), dim=1)\n        \n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        #x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        #x = self.bn3(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        #x = self.bn4(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = x.view([-1, (img_height // 16) * (img_width // 16) * self.base * 8])\n        x = self.l5(x)\n        x = torch.sigmoid(x)\n        return x\n\n    \nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x, y):\n        x = self.g(x, y)\n        x = self.d(x)\n        return x\n    \n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002,  betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    ys = np.zeros([train_y.shape[0],class_N, 1, 1], np.float32)\n    ys[np.arange(train_y.shape[0]), train_y] = 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(30000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        con_x = train_y[mb_ind].astype(np.int)\n        \n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(input_noise, con_x)\n\n        con_x2 = np.zeros((mb,class_N, img_height, img_width), dtype=np.float32)\n        con_x2[np.arange(mb), con_x] = 1\n        con_x2 = torch.tensor(con_x2, dtype=torch.float).to(device)\n        x = torch.cat((x, con_x2), dim=1)\n\n        X = torch.cat([x, g_output])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)[..., 0]\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        \n        y = gan(input_noise, con_x)[..., 0]\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), model_path)\n\n# test\ndef test():\n    gen = Generator().to(device)\n    gen.load_state_dict(torch.load(model_path, map_location=device))\n    gen.eval()\n\n    np.random.seed(100)\n\n    labels = [""air\\nplane"", ""auto\\nmobile"", ""bird"", ""cat"", ""deer"", ""dog"",\n              ""frog"", ""horse"", ""ship"", ""truck""]\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n            input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n            y = np.arange(num_classes, dtype=np.int)\n\n            g_output = gen(input_noise, y, test=True)\n\n            if GPU:\n                g_output = g_output.cpu()\n                \n            g_output = g_output.detach().numpy()\n            g_output = (g_output + 1) / 2\n            g_output = g_output.transpose(0,2,3,1)\n\n            for i in range(mb):\n                generated = g_output[i]\n                plt.subplot(1,mb,i+1)\n                plt.title(labels[i])\n                plt.imshow(generated)\n                plt.axis(\'off\')\n\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/cgan_mnist_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport gzip\n\n# config\nclass_N = 10\n\nimg_height, img_width = 28, 28\nchannel = 1\n\ntorch.manual_seed(0)\n\n# save directory\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n# model path\nmodel_path = \'CGAN.pt\'\n    \n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        self.in_h = img_height // 4\n        self.in_w = img_width // 4\n        self.base = 128\n        \n        super(Generator, self).__init__()\n        \n        self.lin = torch.nn.ConvTranspose2d(100 +class_N, self.base * 2, kernel_size=self.in_h, stride=1, bias=False)\n        self.bnin = torch.nn.BatchNorm2d(self.base * 2)\n\n        self.l3 = torch.nn.ConvTranspose2d(self.base * 2, self.base, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn3 = torch.nn.BatchNorm2d(self.base)\n        self.l4 = torch.nn.ConvTranspose2d(self.base, channel, kernel_size=4, stride=2, padding=1, bias=False)\n        \n        \n    def forward(self, x, y, test=False):\n        #x = torch.cat((x, y), dim=1)\n        con_x = np.zeros((len(y),class_N, 1, 1), dtype=np.float32)\n        con_x[np.arange(len(y)), y] = 1\n        con_x = torch.tensor(con_x, dtype=torch.float).to(device)\n        \n        x = torch.cat((x, con_x), dim=1)\n        \n        x = self.lin(x)\n        x = self.bnin(x)\n        \n        x = torch.nn.functional.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l4(x)\n        x = torch.tanh(x)\n\n        if test:\n            return x\n        \n        else:\n            con_x = np.zeros((len(y),class_N, img_height, img_width), dtype=np.float32)\n            con_x[np.arange(len(y)), y] = 1\n            con_x = torch.tensor(con_x).to(device)\n        \n            out_x = torch.cat((x, con_x), dim=1)\n        \n            return out_x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 64\n        \n        super(Discriminator, self).__init__()\n        self.l1 = torch.nn.Conv2d(channel +class_N, self.base, kernel_size=5, padding=2, stride=2)\n        self.l2 = torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2)\n        #self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l5 = torch.nn.Linear((img_height // 4) * (img_width // 4) * self.base * 2, 1)\n\n    def forward(self, x):\n        \n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        #x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = x.view([-1, (img_height // 4) * (img_width // 4) * self.base * 2])\n        x = self.l5(x)\n        x = torch.sigmoid(x)\n        return x\n\nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x, y):\n        x = self.g(x, y)\n        x = self.d(x)\n        return x\n\n\ndef load_mnist():\n    dir_path = \'drive/My Drive/Colab Notebooks/\'  + ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n# train\ndef train():\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    gan = Gan(gen, dis)\n    #gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002,  betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    ys = np.zeros([train_y.shape[0],class_N, 1, 1], np.float32)\n    ys[np.arange(train_y.shape[0]), train_y] = 1\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(20000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        x_con = torch.tensor(ys[mb_ind], dtype=torch.float).to(device)\n        \n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(torch.cat((input_noise, x_con), dim=1))\n\n        X = torch.cat([x, g_output])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)[..., 0]\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        \n        y = gan(torch.cat((input_noise, x_con), dim=1))[..., 0]\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), model_path)\n\n# test\ndef test():\n    gen = Generator().to(device)\n    gen.load_state_dict(torch.load(model_path, map_location=device))\n    gen.eval()\n\n    np.random.seed(100)\n    \n    with torch.no_grad():\n        for i in range(3):\n            mb = 10\n            input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n            input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n            y = np.arange(num_classes, dtype=np.int)\n\n            g_output = gen(input_noise, y, test=True)\n\n            if GPU:\n                g_output = g_output.cpu()\n                \n            g_output = g_output.detach().numpy()\n            g_output = (g_output + 1) / 2\n            g_output = g_output.transpose(0,2,3,1)\n\n            if channel == 1:\n                cmap = \'gray\'\n            else:\n                cmap = None\n\n            for i in range(mb):\n                generated = g_output[i, ..., 0]\n                plt.subplot(1,mb,i+1)\n                plt.title(str(i))\n                plt.imshow(generated, cmap=cmap)\n                plt.axis(\'off\')\n\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/convae_cifar10_pytorch.py,16,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport pickle\nimport os\n\n# config\nclass_N = 10\n\nimg_height, img_width = 32, 32\nchannel = 3\n\ntorch.manual_seed(0)\n\n# save directory\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n# model path\nmodel_path = \'ConvAE.pt\'\n    \n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n    \nclass ConvAE(torch.nn.Module):\n    def __init__(self):\n        super(ConvAE, self).__init__()\n\n        self.enc1 = torch.nn.Conv2d(channel, 32, kernel_size=3, padding=1)\n        self.enc2 = torch.nn.Conv2d(32, 16, kernel_size=3, padding=1)\n        self.dec2 = torch.nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)\n        self.dec1 = torch.nn.ConvTranspose2d(32, channel, kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        x = self.enc1(x)\n        x = F.max_pool2d(x, 2)\n        x = self.enc2(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dec2(x)\n        x = self.dec1(x)\n        return x\n\n\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    model = ConvAE().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 512\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        y = model(x)\n        loss = torch.nn.MSELoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = ConvAE().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n\n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/convae_pytorch.py,16,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n\n# config\nclass_labels = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\nclass_N = len(class_labels)\n\nimg_height, img_width = 64, 64\nchannel = 3\n\ntorch.manual_seed(0)\n\n# save directory\nsave_dir = \'output_gan\'\nos.makedirs(save_dir, exist_ok=True)\n\n# model path\nmodel_path = \'ConvAE.pt\'\n    \n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n    \nclass ConvAE(torch.nn.Module):\n    def __init__(self):\n        super(ConvAE, self).__init__()\n\n        self.enc1 = torch.nn.Conv2d(channel, 32, kernel_size=3, padding=1)\n        self.enc2 = torch.nn.Conv2d(32, 16, kernel_size=3, padding=1)\n        self.dec2 = torch.nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)\n        self.dec1 = torch.nn.ConvTranspose2d(32, channel, kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        x = self.enc1(x)\n        x = F.max_pool2d(x, 2)\n        x = self.enc2(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dec2(x)\n        x = self.dec1(x)\n        return x\n\n    \n\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side))\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side, _c))\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs, paths\n\n\n# train\ndef train():\n    # model\n    model = ConvAE().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 64\n    mbi = 0\n    train_N = len(xs)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > train_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (train_N - mbi))]))\n            mbi = mb - (train_n - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        y = model(x)\n        loss = torch.nn.MSELoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = ConvAE().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\n    xs, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n\n            print(""in {}"".format(path))\n            \n            _x = x.detach().cpu().numpy()[0]\n            _x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n\n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/dcgan_cifar10_pytorch.py,48,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\nmb = 64\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\ntorch.manual_seed(0)\n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        self.in_h = img_height // 16\n        self.in_w = img_width // 16\n        self.base = 128\n        \n        super(Generator, self).__init__()\n        #self.lin = torch.nn.Linear(100, self.in_h * self.in_w * self.base * 8)\n        self.lin = torch.nn.ConvTranspose2d(100, self.base * 8, kernel_size=self.in_h, stride=1, bias=False)\n        self.bnin = torch.nn.BatchNorm2d(self.base * 8)\n        self.l1 = torch.nn.ConvTranspose2d(self.base* 8, self.base * 4, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l2 = torch.nn.ConvTranspose2d(self.base * 4, self.base * 2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.ConvTranspose2d(self.base * 2, self.base, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn3 = torch.nn.BatchNorm2d(self.base)\n        self.l4 = torch.nn.ConvTranspose2d(self.base, channel, kernel_size=4, stride=2, padding=1, bias=False)\n        \n        \n    def forward(self, x):\n        x = self.lin(x)\n        x = self.bnin(x)\n        #x = x.view([-1, self.base*8, self.in_h, self.in_w])\n        x = torch.nn.functional.relu(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l4(x)\n        x = torch.nn.functional.tanh(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 32\n        \n        super(Discriminator, self).__init__()\n        self.l1 = torch.nn.Conv2d(channel, self.base, kernel_size=5, padding=2, stride=2)\n        self.l2 = torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2)\n        #self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.Conv2d(self.base * 2, self.base * 4, kernel_size=5, padding=2, stride=2)\n        #self.bn3 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l4 = torch.nn.Conv2d(self.base * 4, self.base * 8, kernel_size=5, padding=2, stride=2)\n        #self.bn4 = torch.nn.BatchNorm2d(self.base * 8)\n        self.l5 = torch.nn.Linear((img_height // 16) * (img_width // 16) * self.base * 8, 1)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        #x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        #x = self.bn3(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        #x = self.bn4(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = x.view([-1, (img_height // 16) * (img_width // 16) * self.base * 8])\n        x = self.l5(x)\n        x = torch.nn.functional.sigmoid(x)\n        return x\n\n\n    \nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x, y):\n        x = self.g(x, y)\n        x = self.d(x)\n        return x\n    \n\nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    gan = Gan(gen, dis)\n    #gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002,  betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mbi = 0\n    data_N = len(xs)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(20000):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(input_noise)\n\n        X = torch.cat([x, g_output])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)[..., 0]\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        y = gan(input_noise)[..., 0]\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    gen = Generator().to(device)\n    gen.eval()\n    gen.load_state_dict(torch.load(\'cnn.pt\'))\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n        g_output = gen(input_noise)\n\n        if GPU:\n            g_output = g_output.cpu()\n            \n        g_output = g_output.detach().numpy()\n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/dcgan_pytorch.py,48,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\nnum_classes = len(CLS)\nimg_height, img_width = 64, 64\nchannel = 3\nmb = 64\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\ntorch.manual_seed(0)\n    \nclass Generator(torch.nn.Module):\n\n    def __init__(self):\n        self.in_h = img_height // 16\n        self.in_w = img_width // 16\n        self.base = 64\n        \n        super(Generator, self).__init__()\n        #self.lin = torch.nn.Linear(100, self.in_h * self.in_w * self.base * 8)\n        self.lin = torch.nn.ConvTranspose2d(100, self.base * 8, kernel_size=[self.in_h, self.in_w], stride=1, bias=False)\n        self.bnin = torch.nn.BatchNorm2d(self.base * 8)\n        self.l1 = torch.nn.ConvTranspose2d(self.base* 8, self.base * 4, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l2 = torch.nn.ConvTranspose2d(self.base * 4, self.base * 2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.ConvTranspose2d(self.base * 2, self.base, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn3 = torch.nn.BatchNorm2d(self.base)\n        self.l4 = torch.nn.ConvTranspose2d(self.base, channel, kernel_size=4, stride=2, padding=1, bias=False)\n        \n        \n    def forward(self, x):\n        x = self.lin(x)\n        x = self.bnin(x)\n        #x = x.view([-1, self.base*8, self.in_h, self.in_w])\n        x = torch.nn.functional.relu(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.relu(x)\n        x = self.l4(x)\n        x = torch.nn.functional.tanh(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 64\n        \n        super(Discriminator, self).__init__()\n        self.l1 = torch.nn.Conv2d(channel, self.base, kernel_size=5, padding=2, stride=2)\n        self.l2 = torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2)\n        #self.bn2 = torch.nn.BatchNorm2d(self.base * 2)\n        self.l3 = torch.nn.Conv2d(self.base * 2, self.base * 4, kernel_size=5, padding=2, stride=2)\n        #self.bn3 = torch.nn.BatchNorm2d(self.base * 4)\n        self.l4 = torch.nn.Conv2d(self.base * 4, self.base * 8, kernel_size=5, padding=2, stride=2)\n        #self.bn4 = torch.nn.BatchNorm2d(self.base * 8)\n        self.l5 = torch.nn.Linear((img_height // 16) * (img_width // 16) * self.base * 8, 1)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        #x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        #x = self.bn3(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        #x = self.bn4(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = x.view([-1, (img_height // 16) * (img_width // 16) * self.base * 8])\n        x = self.l5(x)\n        x = torch.nn.functional.sigmoid(x)\n        return x\n\n\n    \nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x):\n        x = self.g(x)\n        x = self.d(x)\n        return x\n    \n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if rot == 0:\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    \n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    \n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    #gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002,  betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n\n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(input_noise)\n\n        X = torch.cat([x, g_output])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)[:, 0]\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        y = dis(gen(input_noise))[:, 0]\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    gen = Generator().to(device)\n    gen.eval()\n    gen.load_state_dict(torch.load(\'cnn.pt\'))\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100, 1, 1))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n        g_output = gen(input_noise)\n\n        if GPU:\n            g_output = g_output.cpu()\n            \n        g_output = g_output.detach().numpy()\n        g_output = (g_output + 1) / 2\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/deepae_pytorch.py,18,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n    \nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n\n        self.enc1 = torch.nn.Linear(img_height * img_width * channel, 64)\n        self.enc2 = torch.nn.Linear(64, 32)\n        self.enc3 = torch.nn.Linear(32, 16)\n        self.dec3 = torch.nn.Linear(16, 32)\n        self.dec2 = torch.nn.Linear(32, 64)\n        self.dec1 = torch.nn.Linear(64, img_height * img_width * channel)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = self.enc1(x)\n        x = self.enc2(x)\n        #x = self.enc3(x)\n        #x = self.dec3(x)\n        x = self.dec2(x)\n        x = self.dec1(x)\n        return x\n\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 1:\n                x = x[..., None]\n            else:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(1000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        y = model(x)\n        \n        loss = torch.nn.MSELoss()(y, t.view(mb, -1))\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        pred = model(x)\n\n        pred = pred.view(channel, img_height, img_width)\n        pred = pred.detach().cpu().numpy()\n        pred -= pred.min()\n        pred /= pred.max()\n        pred = pred.transpose(1,2,0)\n        \n        plt.subplot(1,2,1)\n        _x = x.detach().cpu().numpy()[0]\n        _x = (_x + 1) / 2\n        if channel == 1:\n            plt.imshow(_x[0])\n        else:\n            plt.imshow(_x.transpose(1,2,0))\n        plt.title(""input"")\n        plt.subplot(1,2,2)\n        if channel == 1:\n            plt.imshow(pred[0], cmap=\'gray\')\n        else:\n            plt.imshow(pred)\n        plt.title(""predicted"")\n        plt.show()\n\n        print(""in {}"".format(path))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/gan_cifar10_pytorch.py,39,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\n\nGPU = True\ntorch.manual_seed(0)\n    \nclass Generator(torch.nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        base = 256\n        self.l1 = torch.nn.Linear(100, base)\n        self.bn1 = torch.nn.BatchNorm1d(base)\n        self.l2 = torch.nn.Linear(base, base * 2)\n        self.bn2 = torch.nn.BatchNorm1d(base * 2)\n        self.l3 = torch.nn.Linear(base * 2, base * 4)\n        self.bn3 = torch.nn.BatchNorm1d(base * 4)\n        self.l4 = torch.nn.Linear(base * 4, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = torch.nn.functional.tanh(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        base = 256\n        self.l1 = torch.nn.Linear(img_height * img_width * channel, base * 2)\n        self.l2 = torch.nn.Linear(base * 2, base)\n        self.l3 = torch.nn.Linear(bsae, 1)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = torch.nn.functional.sigmoid(x)\n        return x\n\n    \n    \nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x, y):\n        x = self.g(x, y)\n        x = self.d(x)\n        return x\n    \nimport pickle\nimport os\n    \ndef load_cifar10():\n\n    path = \'cifar-10-batches-py\'\n\n    if not os.path.exists(path):\n        os.system(""wget {}"".format(path))\n        os.system(""tar xvf {}"".format(path))\n\n    # train data\n    \n    train_x = np.ndarray([0, 32, 32, 3], dtype=np.float32)\n    train_y = np.ndarray([0, ], dtype=np.int)\n    \n    for i in range(1, 6):\n        data_path = path + \'/data_batch_{}\'.format(i)\n        with open(data_path, \'rb\') as f:\n            datas = pickle.load(f, encoding=\'bytes\')\n            print(data_path)\n            x = datas[b\'data\']\n            x = x.reshape(x.shape[0], 3, 32, 32)\n            x = x.transpose(0, 2, 3, 1)\n            train_x = np.vstack((train_x, x))\n        \n            y = np.array(datas[b\'labels\'], dtype=np.int)\n            train_y = np.hstack((train_y, y))\n\n    # test data\n    \n    data_path = path + \'/test_batch\'\n    \n    with open(data_path, \'rb\') as f:\n        datas = pickle.load(f, encoding=\'bytes\')\n        print(data_path)\n        x = datas[b\'data\']\n        x = x.reshape(x.shape[0], 3, 32, 32)\n        test_x = x.transpose(0, 2, 3, 1)\n    \n        test_y = np.array(datas[b\'labels\'], dtype=np.int)\n\n    return train_x, train_y, test_x, test_y\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    gan = Gan(gen, dis)\n    #gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002)\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002)\n\n\n    train_x, train_y, test_x, test_y = load_cifar10()\n    xs = train_x / 127.5 - 1\n    xs = xs.transpose(0, 3, 1, 2)\n\n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(50000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(input_noise)\n        g_output = torch.reshape(g_output, [mb, channel, img_height, img_width])\n\n        X = torch.cat([x, g_output])\n        X = X.view([mb * 2, -1])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)[..., 0]\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        y = gan(input_noise)[..., 0]\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    gen = Generator().to(device)\n    gen.eval()\n    gen.load_state_dict(torch.load(\'cnn.pt\'))\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n        g_output = gen(input_noise)\n\n        if GPU:\n            g_output = g_output.cpu()\n            \n        g_output = g_output.detach().numpy()\n        g_output = (g_output + 1) / 2\n        g_output = g_output.reshape([mb, channel, img_height, img_width])\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/gan_pytorch.py,39,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nnum_classes = 2\nimg_height, img_width = 64, 64 #572, 572\nchannel = 3\n\nGPU = False\ntorch.manual_seed(0)\n    \nclass Generator(torch.nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.l1 = torch.nn.Linear(100, 128)\n        self.bn1 = torch.nn.BatchNorm1d(128)\n        self.l2 = torch.nn.Linear(128, 256)\n        self.bn2 = torch.nn.BatchNorm1d(256)\n        self.l3 = torch.nn.Linear(256, 512)\n        self.bn3 = torch.nn.BatchNorm1d(512)\n        self.l4 = torch.nn.Linear(512, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = self.bn3(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l4(x)\n        x = torch.nn.functional.tanh(x)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.l1 = torch.nn.Linear(img_height * img_width * channel, 512)\n        self.l2 = torch.nn.Linear(512, 256)\n        self.l3 = torch.nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l2(x)\n        x = torch.nn.functional.leaky_relu(x, 0.2)\n        x = self.l3(x)\n        x = torch.nn.functional.sigmoid(x)\n        return x\n\n\n    \nclass GAN(torch.nn.Module):\n    def __init__(self, g, d):\n        super(GAN, self).__init__()\n        self.g = g\n        self.d = d\n        \n    def forward(self, x, y):\n        x = self.g(x, y)\n        x = self.d(x)\n        return x\n    \nCLS = {\'akahara\': [0,0,128],\n       \'madara\': [0,128,0]}\n    \n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            if channel == 1:\n                x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x = x / 127.5 - 1\n            if channel == 3:\n                x = x[..., ::-1]\n            xs.append(x)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                paths.append(path)\n\n            if rot != False:\n                angle = 0\n                scale = 1\n                while angle < 360:\n                    angle += rot\n                    if channel == 1:\n                        _h, _w = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side))\n                    else:\n                        _h, _w, _c = x.shape\n                        max_side = max(_h, _w)\n                        tmp = np.zeros((max_side, max_side, _c))\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    paths.append(path)\n                    \n    xs = np.array(xs, dtype=np.float32)\n    if channel == 1:\n        xs = np.expand_dims(xs, axis=-1)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    gen = Generator().to(device)\n    dis = Discriminator().to(device)\n    gan = Gan(gen, dis)\n    #gan = torch.nn.Sequential(gen, dis)\n\n    opt_d = torch.optim.Adam(dis.parameters(), lr=0.0002)\n    opt_g = torch.optim.Adam(gen.parameters(), lr=0.0002)\n\n\n    xs, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(5000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        opt_d.zero_grad()\n        opt_g.zero_grad()\n            \n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        #for param in dis.parameters():\n        #    param.requires_grad = True\n        #dis.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        g_output = gen(input_noise)\n        g_output = torch.reshape(g_output, [mb, channel, img_height, img_width])\n\n        X = torch.cat([x, g_output])\n        X = X.view([mb * 2, -1])\n        t = [1] * mb + [0] * mb\n        t = torch.tensor(t, dtype=torch.float).to(device)\n\n        dy = dis(X)\n        loss_d = torch.nn.BCELoss()(dy, t)\n\n        loss_d.backward()\n        opt_d.step()\n\n        #for param in dis.parameters():\n        #    param.requires_grad = False\n        #dis.eval()\n        #gen.train()\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n        y = gan(input_noise)\n        t = torch.tensor([1] * mb, dtype=torch.float).to(device)\n        loss_g = torch.nn.BCELoss()(y, t)\n\n        loss_g.backward()\n        opt_g.step()\n\n        if i % 100 == 0:\n            print(""iter >>"", i+1, \',G:loss >>\', loss_g.item(), \',D:loss >>\', loss_d.item())\n\n    torch.save(gen.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    gen = Generator().to(device)\n    gen.eval()\n    gen.load_state_dict(torch.load(\'cnn.pt\'))\n\n    np.random.seed(100)\n    \n    for i in range(3):\n        mb = 10\n        input_noise = np.random.uniform(-1, 1, size=(mb, 100))\n        input_noise = torch.tensor(input_noise, dtype=torch.float).to(device)\n\n        g_output = gen(input_noise)\n\n        if GPU:\n            g_output = g_output.cpu()\n            \n        g_output = g_output.detach().numpy()\n        g_output = (g_output + 1) / 2\n        g_output = g_output.reshape([mb, channel, img_height, img_width])\n        g_output = g_output.transpose(0,2,3,1)\n\n        for i in range(mb):\n            generated = g_output[i]\n            plt.subplot(1,mb,i+1)\n            plt.imshow(generated)\n            plt.axis(\'off\')\n\n        plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/pix2pixGP_pytorch.py,112,"b'from google.colab import drive\ndrive.mount(""/content/drive"", force_remount=True)\n\n\nimport torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nimport random\n\n\nCLS = OrderedDict({\n    ""background"": [0, 0, 0],\n    ""akahara"": [0,0,128],\n    ""madara"": [0,128,0]\n      })\n\n\nCLS = [""akahara_imori"", ""fire_salamander"", ""ibo_imori"", ""madara_imori"", ""marble_salamander"", ""minamiibo_imori"", ""shiriken_imori"", ""tiger_salamander""]\n\nclass_N = len(CLS)\n\nimg_height, img_width = 128, 128  #572, 572\nout_height, out_width = 128, 128  #388, 388\n\nUNet_dropout_ratio = 0.5 # False, (0 , 1)\nLambda = 1. # Loss balance  Ldis + Lambda * L1 norm\nmb_N = 8 # minibatch\niteration_N = 10000 # iteration\nlr = 0.0001 # learning rate\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\nmodel_path = \'drive/My Drive/Colab Notebooks/pix2pix.pt\'\n\ntorch.manual_seed(0)\n\n# wgan hyper-parameter\nn_critic = 5\n    \nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n    \nclass Interpolate(torch.nn.Module):\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        return x\n    \n    \nclass UNet_block(torch.nn.Module):\n    def __init__(self, dim1, dim2, name, dropout=False):\n        super(UNet_block, self).__init__()\n\n        _module = OrderedDict()\n\n        for i in range(2):\n            f = dim1 if i == 0 else dim2\n            _module[""unet_{}_bn{}"".format(name, i+1)] = torch.nn.BatchNorm2d(f)\n            _module[""unet_{}_relu{}"".format(name, i+1)] = torch.nn.ReLU()\n            _module[""unet_{}_conv{}"".format(name, i+1)] = torch.nn.Conv2d(f, dim2, kernel_size=3, padding=1, stride=1)\n            if dropout != False:\n                _module[""unet_{}_dropout{}"".format(name, i+1)] = torch.nn.Dropout2d(p=dropout)\n            \n            \n        self.module = torch.nn.Sequential(_module)\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\nclass UNet_deconv_block(torch.nn.Module):\n    def __init__(self, dim1, dim2):\n        super(UNet_deconv_block, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.ConvTranspose2d(dim1, dim2, kernel_size=2, stride=2),\n            torch.nn.BatchNorm2d(dim2)\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass UNet(torch.nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        base = 32\n        \n        self.enc1 = UNet_block(1, base, name=""enc1"")\n        self.enc2 = UNet_block(base, base * 2, name=""enc2"")\n        self.enc3 = UNet_block(base * 2, base * 4, name=""enc3"")\n        self.enc4 = UNet_block(base * 4, base * 8, name=""enc4"")\n        self.enc5 = UNet_block(base * 8, base * 16, name=""enc5"")\n        self.enc6 = UNet_block(base * 16, base * 16, name=""enc6"")\n        self.enc7 = UNet_block(base * 16, base * 16, name=""enc7"")\n\n        self.tconv6 = UNet_deconv_block(base * 16, base * 16)\n        self.tconv5 = UNet_deconv_block(base * 16, base * 16)\n        self.tconv4 = UNet_deconv_block(base * 16, base * 8)\n        self.tconv3 = UNet_deconv_block(base * 8, base * 4)\n        self.tconv2 = UNet_deconv_block(base * 4, base * 2)\n        self.tconv1 = UNet_deconv_block(base * 2, base)\n\n        self.dec7 = UNet_block(base * 32, base * 16, name=""dec7"", dropout=UNet_dropout_ratio)\n        self.dec6 = UNet_block(base * 32, base * 16, name=""dec6"", dropout=UNet_dropout_ratio)\n        self.dec5 = UNet_block(base * 32, base * 16, name=""dec5"", dropout=UNet_dropout_ratio)\n        self.dec4 = UNet_block(base * 24, base * 8, name=""dec4"", dropout=UNet_dropout_ratio)\n        self.dec3 = UNet_block(base * 12, base * 4, name=""dec3"", dropout=UNet_dropout_ratio)\n        self.dec2 = UNet_block(base * 6, base * 2, name=""dec2"", dropout=UNet_dropout_ratio)\n        self.dec1 = UNet_block(base * 3, base, name=""dec1"", dropout=UNet_dropout_ratio)\n\n        self.out = torch.nn.Conv2d(base, 3, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # Encoder block 1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # Encoder block 2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # Encoder block 3\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # Encoder block 4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # Encoder block 5\n        x_enc5 = self.enc5(x)\n        x = F.max_pool2d(x_enc5, 2, stride=2, padding=0)\n        \n        # Encoder block 6\n        x_enc6 = self.enc6(x)\n        x = F.max_pool2d(x_enc6, 2, stride=2, padding=0)\n        \n        # Encoder block 7\n        x_enc7 = self.enc7(x)\n        x = F.max_pool2d(x_enc7, 2, stride=2, padding=0)\n        \n        # Decoder block 7\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc7), dim=1)\n        x = self.dec7(x)\n        \n        # Decoder block 6\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc6), dim=1)\n        x = self.dec6(x)\n        \n        # Decoder block 5\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc5), dim=1)\n        x = self.dec5(x)\n\n        # Decoder block 4\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        # Decoder block 3\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        # Decoder block 2\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        # Decoder block 1\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = torch.tanh(x)\n        #x = F.softmax(x, dim=1)\n        #x = x * 2 - 1\n        \n        return x\n\n    \n    \nclass UNet2(torch.nn.Module):\n    def __init__(self):\n        super(UNet2, self).__init__()\n\n        base = 16\n        \n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else base\n            self.enc1.add_module(""enc1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=1, stride=1))\n            self.enc1.add_module(""enc1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""enc1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base if i == 0 else base * 2\n            self.enc2.add_module(""enc2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=1, stride=1))\n            self.enc2.add_module(""enc2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""enc2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.enc3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base*4\n            self.enc3.add_module(""enc3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=1, stride=1))\n            self.enc3.add_module(""enc3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc3.add_module(""enc3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.enc4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*8\n            self.enc4.add_module(""enc4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=1, stride=1))\n            self.enc4.add_module(""enc4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc4.add_module(""enc4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n\n        self.enc5 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*16\n            self.enc5.add_module(""enc5_{}"".format(i+1), torch.nn.Conv2d(f, base*16, kernel_size=3, padding=1, stride=1))\n            self.enc5.add_module(""enc5_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc5.add_module(""enc5_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*16))\n\n        self.tconv4 = torch.nn.ConvTranspose2d(base*16, base*8, kernel_size=2, stride=2)\n        self.tconv4_bn = torch.nn.BatchNorm2d(base*8)\n\n        self.dec4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*16 if i == 0 else base*8\n            self.dec4.add_module(""dec4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=1, stride=1))\n            self.dec4.add_module(""dec4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec4.add_module(""dec4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n        \n\n        self.tconv3 = torch.nn.ConvTranspose2d(base*8, base*4, kernel_size=2, stride=2)\n        self.tconv3_bn = torch.nn.BatchNorm2d(base*4)\n\n        self.dec3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*4\n            self.dec3.add_module(""dec3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=1, stride=1))\n            self.dec3.add_module(""dec3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec3.add_module(""dec3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.tconv2 = torch.nn.ConvTranspose2d(base*4, base*2, kernel_size=2, stride=2)\n        self.tconv2_bn = torch.nn.BatchNorm2d(base*2)\n\n        self.dec2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*2\n            self.dec2.add_module(""dec2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=1, stride=1))\n            self.dec2.add_module(""dec2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec2.add_module(""dec2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.tconv1 = torch.nn.ConvTranspose2d(base*2, base, kernel_size=2, stride=2)\n        self.tconv1_bn = torch.nn.BatchNorm2d(base)\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base\n            self.dec1.add_module(""dec1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=1, stride=1))\n            self.dec1.add_module(""dec1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.out = torch.nn.Conv2d(base, class_num, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n        x = self.tconv4_bn(self.tconv4(x))\n\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        x = self.tconv3_bn(self.tconv3(x))\n\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        x = self.tconv2_bn(self.tconv2(x))\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        x = self.tconv1_bn(self.tconv1(x))\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = torch.tanh(x)\n        #x = F.softmax(x, dim=1)\n        #x = x * 2 - 1\n        \n        return x\n    \n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 32\n        \n        super(Discriminator, self).__init__()\n        \n        self.module = torch.nn.Sequential(OrderedDict({\n            ""conv1"": torch.nn.Conv2d(4, self.base, kernel_size=5, padding=2, stride=2),\n            ""bn1"": torch.nn.BatchNorm2d(self.base),\n            ""relu1"": torch.nn.LeakyReLU(0.2),\n            ""conv2"": torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2),\n            ""bn2"": torch.nn.BatchNorm2d(self.base * 2),\n            ""relu2"": torch.nn.LeakyReLU(0.2),\n            ""conv3"": torch.nn.Conv2d(self.base * 2, self.base * 4, kernel_size=5, padding=2, stride=2),\n            ""bn3"": torch.nn.BatchNorm2d(self.base * 4),\n            ""relu3"": torch.nn.LeakyReLU(0.2),\n            ""conv4"": torch.nn.Conv2d(self.base * 4, self.base * 8, kernel_size=5, padding=2, stride=2),\n            ""bn4"": torch.nn.BatchNorm2d(self.base * 8),\n            ""relu4"": torch.nn.LeakyReLU(0.2),\n            ""flatten"": Flatten(),\n            ""linear1"": torch.nn.Linear((img_height // 16) * (img_width // 16) * self.base * 8, 1),\n            ""sigmoid"": torch.nn.Sigmoid(),\n        }))\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n    \n    \n    \n# get train data\ndef data_path_load(path):\n    paths = []\n    \n    num = 0\n    \n    # each directory\n    for dir_path in glob(path + ""/*""):\n        # get image file by extension jpg, jpeg, png\n        _paths = glob(dir_path + ""/*.jp*g"") + glob(dir_path + ""/*.png"")\n        # get image number\n        _num = len(_paths)\n        paths += _paths\n        num += _num\n        print(""load :"", dir_path, "" , N :"", _num)\n            \n    print(""total :"", num)\n    \n    return paths\n\n\n\ndef data_load(paths, hf=False, vf=False):\n    imgs = []\n    edges = []\n    \n    for path in paths:\n        # read image\n        img = cv2.imread(path)\n        # resize image\n        img = cv2.resize(img, (img_width, img_height))\n        # get gray\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        # get canny\n        edge = cv2.Canny(gray, 100, 150)\n        # transpose BGR to RGB\n        img = img[..., ::-1]\n        \n        # horizontal flip\n        if (random.random() < 0.5) and hf:\n            img = img[:, ::-1]\n            edge = edge[:, ::-1]\n        \n        # vertical flip\n        if (random.random() < 0.5) and vf:\n            img = img[::-1]\n            edge = edge[::-1]\n            \n        # add data\n        imgs += [img]\n        edges += [edge]\n        \n    # list -> np.array\n    imgs = np.array(imgs, dtype=np.float32)\n    edges = np.array(edges, dtype=np.float32)\n    \n    # normalize [0, 255] to [-1, 1]\n    imgs = imgs / 127.5 - 1.\n    edges = edges / 127.5 - 1.\n    \n    # add channel dimension\n    edges = np.expand_dims(edges, axis=1)\n    \n    # transpose dimension [mb, h, w, c] -> [mb, c, h, w]\n    imgs = imgs.transpose(0, 3, 1, 2)\n    \n    return edges, imgs\n            \n    \n    \n\n\n# train\ndef train():\n    # model\n    # generator\n    G = UNet().to(device)\n    opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    D = Discriminator().to(device)\n    opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    G.train()\n    D.train()\n\n    #imgs, gts, paths = data_load(\'drive/My Drive/Colab Notebooks/\' + \'datasets/\', hf=True, vf=True)\n    paths = data_path_load(\'drive/My Drive/Colab Notebooks/datasets/\')\n\n    # training\n    mbi = 0\n    train_N = len(paths)\n    train_ind = np.arange(train_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n                          \n    loss_fn = torch.nn.BCELoss()\n    loss_l1 = torch.nn.L1Loss()\n\n    # prepare label for Discriminator\n    one = torch.FloatTensor([1])\n    mone = one * -1\n\n    if GPU:\n        one = one.cuda()\n        minus_one = mone.cuda()\n    \n    for i in range(iteration_N):\n        if mbi + mb_N > train_N:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[: (mb_N - (train_N - mbi))]))\n            mbi = mb_N - (train_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi+mb_N]\n            mbi += mb_N\n            \n        opt_G.zero_grad()\n\n        for _ in range(n_critic):\n        \n            opt_D.zero_grad()\n            imgs, gts = data_load([paths[mb_index] for mb_index in mb_ind], hf=True)\n                \n            x = torch.tensor(imgs, dtype=torch.float).to(device)\n            y = torch.tensor(gts, dtype=torch.float).to(device)\n            \n            \n            # Discirminator training\n            Gx = G(x)\n                            \n            fake_x = torch.cat([Gx, x], dim=1)\n                            \n            loss_D_fake = loss_fn(D(fake_x), torch.ones(mb_N, dtype=torch.float).to(device))\n            #loss_D_fake.backward(retain_graph=True)\n            \n            real_x = torch.cat([y, x], dim=1)\n            \n            loss_D_real = loss_fn(D(real_x), torch.zeros(mb_N, dtype=torch.float).to(device))\n            #loss_D_real.backward()\n            \n            loss_D = loss_D_real + loss_D_fake\n\n            #----\n            # Gradient Penalty\n            #---\n            # sample epsilon from [0, 1]\n            epsilon = np.random.random() #np.random.uniform(0, 1, 1)\n\n            # sample x_hat \n            x_hat = (epsilon * real_x + (1 - epsilon) * fake_x).requires_grad_(True)\n\n            # gradient penalty\n            Dx_hat = D(x_hat)\n            musk = torch.ones_like(Dx_hat)\n            gradients = torch.autograd.grad(Dx_hat, x_hat, grad_outputs=musk,\n                                retain_graph=True, create_graph=True,\n                                allow_unused=True)[0]\n            gradients = gradients.view(-1, 1)\n            gradient_penalty = Lambda * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\n            # loss backpropagation\n            #loss_D_real.backward(one, retain_graph=True)\n            #loss_D_fake.backward(minus_one, retain_graph=True)\n            loss_D_real.backward(retain_graph=True)\n            loss_D_fake.backward(retain_graph=True)\n            gradient_penalty.backward(retain_graph=True)\n                          \n            opt_D.step()\n            \n        # UNet training\n        loss_G_fake = loss_fn(D(fake_x), torch.zeros(mb_N, dtype=torch.float).to(device))\n        #loss_G_fake.backward(retain_graph=True)\n        \n        loss_G_l1 = Lambda * loss_l1(Gx, x)\n        #loss_G_l1.backward()\n        loss_G = loss_G_fake + loss_G_l1\n        loss_G.backward()\n        \n        opt_G.step()\n\n\n        \n        \n        if (i+1) % 10 == 0:\n            print(""iter : "", i+1, "", loss D : "", loss_D.item(), \'loss GP : \', gradient_penalty.item(), "", loss G :"", loss_G.item())\n            \n        if (i+1) % 10000 == 0:\n            torch.save(G.state_dict(), model_path)\n\n    torch.save(G.state_dict(), model_path)\n\n    \n# test\ndef test():\n    model = UNet().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    #xs, ts, paths = data_load(\'drive/My Drive/Colab Notebooks/\'  + \'datasets/\')\n    paths = data_path_load(\'drive/My Drive/Colab Notebooks/datasets/\')\n\n    with torch.no_grad():\n        for i in range(40):\n            # get data\n            path = paths[i]\n            imgs, ts = data_load([path])\n            \n            x = torch.tensor(imgs, dtype=torch.float).to(device)\n            \n            # predict image\n            pred = model(x)\n        \n            # change type torch.tensor -> numpy\n            pred = pred.detach().cpu().numpy()[0]\n\n            # visualize\n            # [-1, 1] -> [0, 255]\n            out = (pred + 1) * 127.5\n            # clipping to [0, 255]\n            out = np.clip(out, 0, 255)\n            # exchange dimension [c, h, w] -> [h, w, c]\n            out = out.transpose(1,2,0).astype(np.uint8)\n\n            print(""in {}"".format(path))\n            \n            # for display\n            # [mb, c, h, w] and [-1, 1] -> [h, w] and [0, 1]\n            edge_img = (imgs[0, 0] + 1) / 2.\n            # [mb, c, h, w] and [-1, 1] -> [h, w, c] and [0, 1]\n            original_img = (ts[0].transpose(1, 2, 0) + 1) / 2.\n            \n            \n            plt.subplot(1, 3, 1)\n            plt.imshow(edge_img, cmap=""gray"")\n            plt.subplot(1, 3, 2)\n            plt.imshow(out)\n            plt.subplot(1, 3, 3)\n            plt.imshow(original_img)\n            plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\ntrain()\ntest()'"
Scripts_Generative/old/scripts_pytorch/pix2pix_segment_pytorch.py,98,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom copy import copy\nfrom collections import OrderedDict\n\n\nCLS = OrderedDict({\n    ""background"": [0, 0, 0],\n    ""akahara"": [0,0,128],\n    ""madara"": [0,128,0]\n      })\n\nclass_num = len(CLS)\n\nimg_height, img_width = 64, 64 #572, 572\nout_height, out_width = 64, 64 #388, 388\n\n# GPU\nGPU = True\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\ntorch.manual_seed(0)\n\n    \nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        return x\n    \nclass Interpolate(torch.nn.Module):\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        return x\n    \n    \nclass UNet_block(torch.nn.Module):\n    def __init__(self, dim1, dim2, name):\n        super(UNet_block, self).__init__()\n\n        _module = OrderedDict()\n\n        for i in range(2):\n            f = dim1 if i == 0 else dim2\n            _module[""unet_{}_conv{}"".format(name, i+1)] = torch.nn.Conv2d(f, dim2, kernel_size=3, padding=1, stride=1)\n            _module[""unet_{}_relu{}"".format(name, i+1)] = torch.nn.ReLU()\n            _module[""unet_{}_bn{}"".format(name, i+1)] = torch.nn.BatchNorm2d(dim2)\n            \n            \n\n        self.module = torch.nn.Sequential(_module)\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\nclass UNet_deconv_block(torch.nn.Module):\n    def __init__(self, dim1, dim2):\n        super(UNet_deconv_block, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.ConvTranspose2d(dim1, dim2, kernel_size=2, stride=2),\n            torch.nn.BatchNorm2d(dim2)\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass UNet(torch.nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        base = 32\n        \n        self.enc1 = UNet_block(3, base, name=""enc1"")\n        self.enc2 = UNet_block(base, base * 2, name=""enc2"")\n        self.enc3 = UNet_block(base * 2, base * 4, name=""enc3"")\n        self.enc4 = UNet_block(base * 4, base * 8, name=""enc4"")\n        self.enc5 = UNet_block(base * 8, base * 16, name=""enc5"")\n\n        self.tconv4 = UNet_deconv_block(base * 16, base * 8)\n        self.tconv3 = UNet_deconv_block(base * 8, base * 4)\n        self.tconv2 = UNet_deconv_block(base * 4, base * 2)\n        self.tconv1 = UNet_deconv_block(base * 2, base)\n\n        self.dec4 = UNet_block(base * 24, base * 8, name=""dec4"")\n        self.dec3 = UNet_block(base * 12, base * 4, name=""dec3"")\n        self.dec2 = UNet_block(base * 6, base * 2, name=""dec2"")\n        self.dec1 = UNet_block(base * 3, base, name=""dec1"")\n\n        self.out = torch.nn.Conv2d(base, class_num, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        #x = self.tconv4(x)\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        #x = self.tconv3(x)\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        #x = self.tconv2(x)\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        #x = self.tconv1(x)\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = torch.tanh(x)\n        #x = F.softmax(x, dim=1)\n        #x = x * 2 - 1\n        \n        return x\n\n    \n    \nclass UNet2(torch.nn.Module):\n    def __init__(self):\n        super(UNet2, self).__init__()\n\n        base = 16\n        \n        self.enc1 = torch.nn.Sequential()\n        for i in range(2):\n            f = 3 if i == 0 else base\n            self.enc1.add_module(""enc1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=1, stride=1))\n            self.enc1.add_module(""enc1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc1.add_module(""enc1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.enc2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base if i == 0 else base * 2\n            self.enc2.add_module(""enc2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=1, stride=1))\n            self.enc2.add_module(""enc2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc2.add_module(""enc2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.enc3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base*4\n            self.enc3.add_module(""enc3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=1, stride=1))\n            self.enc3.add_module(""enc3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc3.add_module(""enc3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.enc4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*8\n            self.enc4.add_module(""enc4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=1, stride=1))\n            self.enc4.add_module(""enc4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc4.add_module(""enc4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n\n        self.enc5 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*16\n            self.enc5.add_module(""enc5_{}"".format(i+1), torch.nn.Conv2d(f, base*16, kernel_size=3, padding=1, stride=1))\n            self.enc5.add_module(""enc5_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.enc5.add_module(""enc5_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*16))\n\n        self.tconv4 = torch.nn.ConvTranspose2d(base*16, base*8, kernel_size=2, stride=2)\n        self.tconv4_bn = torch.nn.BatchNorm2d(base*8)\n\n        self.dec4 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*16 if i == 0 else base*8\n            self.dec4.add_module(""dec4_{}"".format(i+1), torch.nn.Conv2d(f, base*8, kernel_size=3, padding=1, stride=1))\n            self.dec4.add_module(""dec4_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec4.add_module(""dec4_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*8))\n        \n\n        self.tconv3 = torch.nn.ConvTranspose2d(base*8, base*4, kernel_size=2, stride=2)\n        self.tconv3_bn = torch.nn.BatchNorm2d(base*4)\n\n        self.dec3 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*8 if i == 0 else base*4\n            self.dec3.add_module(""dec3_{}"".format(i+1), torch.nn.Conv2d(f, base*4, kernel_size=3, padding=1, stride=1))\n            self.dec3.add_module(""dec3_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec3.add_module(""dec3_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*4))\n\n        self.tconv2 = torch.nn.ConvTranspose2d(base*4, base*2, kernel_size=2, stride=2)\n        self.tconv2_bn = torch.nn.BatchNorm2d(base*2)\n\n        self.dec2 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*4 if i == 0 else base*2\n            self.dec2.add_module(""dec2_{}"".format(i+1), torch.nn.Conv2d(f, base*2, kernel_size=3, padding=1, stride=1))\n            self.dec2.add_module(""dec2_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec2.add_module(""dec2_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base*2))\n\n        self.tconv1 = torch.nn.ConvTranspose2d(base*2, base, kernel_size=2, stride=2)\n        self.tconv1_bn = torch.nn.BatchNorm2d(base)\n\n        self.dec1 = torch.nn.Sequential()\n        for i in range(2):\n            f = base*2 if i == 0 else base\n            self.dec1.add_module(""dec1_{}"".format(i+1), torch.nn.Conv2d(f, base, kernel_size=3, padding=1, stride=1))\n            self.dec1.add_module(""dec1_relu_{}"".format(i+1), torch.nn.ReLU())\n            self.dec1.add_module(""dec1_bn_{}"".format(i+1), torch.nn.BatchNorm2d(base))\n\n        self.out = torch.nn.Conv2d(base, class_num, kernel_size=1, padding=0, stride=1)\n        \n        \n    def forward(self, x):\n        # block conv1\n        x_enc1 = self.enc1(x)\n        x = F.max_pool2d(x_enc1, 2, stride=2, padding=0)\n        \n        # block conv2\n        x_enc2 = self.enc2(x)\n        x = F.max_pool2d(x_enc2, 2, stride=2, padding=0)\n        \n        # block conv31\n        x_enc3 = self.enc3(x)\n        x = F.max_pool2d(x_enc3, 2, stride=2, padding=0)\n        \n        # block conv4\n        x_enc4 = self.enc4(x)\n        x = F.max_pool2d(x_enc4, 2, stride=2, padding=0)\n        \n        # block conv5\n        x = self.enc5(x)\n\n        x = self.tconv4_bn(self.tconv4(x))\n\n        x = torch.cat((x, x_enc4), dim=1)\n        x = self.dec4(x)\n\n        x = self.tconv3_bn(self.tconv3(x))\n\n        x = torch.cat((x, x_enc3), dim=1)\n        x = self.dec3(x)\n\n        x = self.tconv2_bn(self.tconv2(x))\n        x = torch.cat((x, x_enc2), dim=1)\n        x = self.dec2(x)\n\n        x = self.tconv1_bn(self.tconv1(x))\n        x = torch.cat((x, x_enc1), dim=1)\n        x = self.dec1(x)\n\n        x = self.out(x)\n        x = torch.tanh(x)\n        #x = F.softmax(x, dim=1)\n        #x = x * 2 - 1\n        \n        return x\n    \n\nclass Discriminator(torch.nn.Module):\n    def __init__(self):\n        self.base = 32\n        \n        super(Discriminator, self).__init__()\n        \n        self.module = torch.nn.Sequential(OrderedDict({\n            ""conv1"": torch.nn.Conv2d(class_num * 2, self.base, kernel_size=5, padding=2, stride=2),\n            ""relu1"": torch.nn.LeakyReLU(0.2),\n            ""conv2"": torch.nn.Conv2d(self.base, self.base * 2, kernel_size=5, padding=2, stride=2),\n            ""relu2"": torch.nn.LeakyReLU(0.2),\n            ""conv3"": torch.nn.Conv2d(self.base * 2, self.base * 4, kernel_size=5, padding=2, stride=2),\n            ""relu3"": torch.nn.LeakyReLU(0.2),\n            ""conv4"": torch.nn.Conv2d(self.base * 4, self.base * 8, kernel_size=5, padding=2, stride=2),\n            ""relu4"": torch.nn.LeakyReLU(0.2),\n            ""flatten"": Flatten(),\n            ""linear1"": torch.nn.Linear((img_height // 16) * (img_width // 16) * self.base * 8, 1),\n            ""sigmoid"": torch.nn.Sigmoid(),\n        }))\n\n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n    \n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 127.5 - 1\n            x = x[..., ::-1]\n            xs.append(x)\n\n            gt_path = path.replace(""images"", ""seg_images"").replace("".jpg"", "".png"")\n            gt = cv2.imread(gt_path)\n            gt = cv2.resize(gt, (out_width, out_height), interpolation=cv2.INTER_NEAREST)\n\n            t = np.zeros((class_num, out_height, out_width), dtype=np.int)\n\n            for i, (_, vs) in enumerate(CLS.items()):\n                ind = (gt[...,0] == vs[0]) * (gt[...,1] == vs[1]) * (gt[...,2] == vs[2])\n                t[i][ind] = 1\n\n            ts.append(t)\n            \n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t[:, :, ::-1])\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t[:, ::-1])\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t[:, ::-1, ::-1])\n                paths.append(path)\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    ts = ts * 2 - 1\n\n    xs = xs.transpose(0,3,1,2)\n    \n    return xs, ts, paths\n\n\n# train\ndef train():\n    # model\n    G = UNet().to(device)\n    opt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    D = Discriminator().to(device)\n    opt_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    G.train()\n    D.train()\n\n    imgs, gts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_num = len(imgs)\n    train_ind = np.arange(train_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n                          \n    loss_fn = torch.nn.BCELoss()\n    loss_l1 = torch.nn.L1Loss()\n    \n    Lambda = 1.\n    \n    for i in range(5000):\n        if mbi + mb > train_num:\n            mb_ind = copy(train_ind[mbi:])\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(train_num-mbi))]))\n            mbi = mb - (train_num - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n            \n             \n        opt_D.zero_grad()\n        opt_G.zero_grad()\n            \n        # Discriminator training\n        x = torch.tensor(imgs[mb_ind], dtype=torch.float).to(device)\n        y = torch.tensor(gts[mb_ind], dtype=torch.float).to(device)\n\n        Gx= G(x)\n                          \n        fake_x = torch.cat([Gx, x], dim=1)                    \n        loss_D_fake = loss_fn(D(fake_x), torch.ones(mb, dtype=torch.float).to(device))\n        \n        real_x = torch.cat([y, x], dim=1)      \n        loss_D_real = loss_fn(D(real_x), torch.zeros(mb, dtype=torch.float).to(device))\n\n        oss_D = loss_D_real + loss_D_fake\n        loss_D.backward(retain_graph=True)\n        \n        opt_D.step()\n                          \n            \n        # UNet training\n        loss_G_fake = loss_fn(D(fake_x), torch.zeros(mb, dtype=torch.float).to(device))\n        loss_G_l1 = Lambda * loss_l1(Gx, x)\n        loss_G = loss_G_fake + loss_G_l1\n        loss_G.backward()\n        \n        opt_G.step()\n        \n        if (i+1) % 10 == 0:\n            print(""iter : "", i+1, "", loss D : "", loss_D.item(), "", loss G :"", loss_G.item())\n\n    torch.save(G.state_dict(), \'cnn.pt\')\n\n\n# test\ndef test():\n    model = UNet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        img = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(img, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        pred = model(x)\n  \n        pred = F.softmax(pred, dim=1)\n\n        pred = pred.detach().cpu().numpy()[0]\n        pred = pred.argmax(axis=0)\n\n        # visualize\n        out = np.zeros((out_height, out_width, 3), dtype=np.uint8)\n        for i, (label_name, v) in enumerate(CLS.items()):\n            out[pred == i] = v\n\n        print(""in {}"".format(path))\n        \n        plt.subplot(1,2,1)\n        plt.imshow(((img.transpose(1, 2, 0) + 1) / 2).astype(np.float32))\n        plt.subplot(1,2,2)\n        plt.imshow(out[..., ::-1])\n        plt.show()\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Generative/old/scripts_pytorch/vae_latent_change2_mnist_pytorch.py,37,"b'import torch\nimport torch.nn.functional as F\n#import torchvision\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport os\nimport gzip\n    \n# config\nclass_N = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\ntorch.manual_seed(0)\n\n# other config\nmodel_path = \'VAE.pt\'\n    \n# VAE paramater\nZ_dim = 2\ndim = 256\n\nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        self.enc1 = torch.nn.Linear(img_height * img_width * channel, dim)\n        \n        self.enc_mu = torch.nn.Linear(dim, Z_dim)\n        self.enc_sigma = torch.nn.Linear(dim, Z_dim)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = F.relu(self.enc1(x))\n        mu = self.enc_mu(x)\n        sigma = self.enc_sigma(x)\n        self.mu = mu\n        self.sigma = sigma\n        \n        return mu, sigma\n    \n    \nclass Sampler(torch.nn.Module):\n    def __init__(self):\n        super(Sampler, self).__init__()\n        \n    def forward(self, x):\n        mu, sigma = x\n        mb, _ = mu.size()\n        epsilon = torch.tensor(np.random.normal(0, 1, [mb, Z_dim]), dtype=torch.float32).to(device)\n        std = torch.exp(0.5 * sigma)\n        sample_z = mu + epsilon * std\n        self.sample_z = sample_z\n        return sample_z\n    \n    \nclass Decoder(torch.nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n        self.dec1 = torch.nn.Linear(Z_dim, dim)\n\n        self.dec_out = torch.nn.Linear(dim, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = F.relu(self.dec1(x))\n        x = self.dec_out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n\ndef loss_KLDivergence(mu, sigma):\n    return -0.5 * torch.sum(1 + sigma - torch.pow(mu, 2) - torch.exp(sigma))\n    \n    \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n    \n\n# train\ndef train():\n    # model\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        #y_mu, y_sigma = model_encoder(x)\n        #y = model_decoder(y_mu, y_sigma)\n        y = model(x)\n        y_mu = model_encoder.mu\n        y_sigma = model_encoder.sigma\n        \n        #loss = torch.nn.BCELoss()(y, t.view(mb, -1))\n        loss = F.binary_cross_entropy(y, t.view(mb, img_height * img_width * channel), reduction=\'sum\')\n        loss_kld = loss_KLDivergence(y_mu, y_sigma)\n        loss = loss + loss_kld\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n                \n            #print(mu, sigma)\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n        \n    \n    \ndef test_latent_show():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    plt.figure(figsize=[10, 10])\n    \n    colors = [""red"", ""blue"", ""orange"", ""green"", ""purple"", \n              ""magenta"", ""yellow"", ""aqua"", ""black"", ""khaki""]\n    \n    with torch.no_grad():\n        for i in range(len(xs)):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            y = model(x)\n            mu = model_encoder.mu\n            sigma = model_encoder.sigma\n            sample_z = model_sampler.sample_z\n            \n            mu = mu.detach().cpu().numpy()[0]\n            sigma = sigma.detach().cpu().numpy()[0]\n            sample_z = sample_z.detach().cpu().numpy()[0]\n            \n            t = test_y[i]\n            \n            plt.scatter(sample_z[0], sample_z[1], c=colors[t])\n    \n    plt.savefig(\'vae_latent_show.png\')\n    plt.show()\n\n\ndef test_latent_change():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    plt.figure(figsize=[12, 2])\n    \n    # split number (show image number)\n    show_num = 15\n\n    # latent lower and upper bound\n    z1_lower, z1_upper = -2, 2\n    z2_lower, z2_upper = -2, 2\n\n    z1_diff = float(z1_upper - z1_lower) / show_num\n    z2_diff = float(z2_upper - z2_lower) / show_num\n    \n    with torch.no_grad():\n        for ind in range(show_num):\n            # get latent vector\n            z1 = z1_lower + z1_diff * ind\n            z2 = z2_lower + z2_diff * ind\n            z = [z1, z2]\n            z = torch.tensor(z).to(device)\n\n            # decode latent vector\n            pred = model_decoder(z)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n\n            # normalize to [0, 1]\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1, 2, 0)\n            \n            if channel == 1:\n                cmap = ""gray""\n                pred = pred[..., 0]\n            elif channel == 3:\n                cmap = None\n\n            plt.subplot(1, show_num, ind + 1)\n            #plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.axis(""off"")\n\n    plt.savefig(\'vae_latent_change.png\')\n    plt.show()\n   \n\ndef test_latent_change2():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    plt.figure(figsize=[12, 12])\n    \n    # split number (show image number)\n    z1_num = 30\n    z2_num = 30\n\n    # latent lower and upper bound\n    z1_lower, z1_upper = -4, 4\n    z2_lower, z2_upper = -4, 4\n    z1_diff = float(z1_upper - z1_lower) / z1_num\n    z2_diff = float(z2_upper - z2_lower) / z2_num\n    \n    with torch.no_grad():\n        for z2_i in range(z2_num):\n            for z1_i in range(z1_num):\n                \n                # get latent vector\n                z1 = z1_lower + z1_diff * z1_i\n                z2 = z2_upper - z2_diff * z2_i\n                z = [z1, z2]\n                z = torch.tensor(z).to(device)\n\n                # decode latent vector\n                pred = model_decoder(z)\n\n                pred = pred.view(channel, img_height, img_width)\n                pred = pred.detach().cpu().numpy()\n\n                # normalize to [0, 1]\n                pred -= pred.min()\n                pred /= pred.max()\n                pred = pred.transpose(1, 2, 0)\n                \n                if channel == 1:\n                    cmap = ""gray""\n                    pred = pred[..., 0]\n                elif channel == 3:\n                    cmap = None\n\n                plt.subplot(z1_num, z2_num, z2_i * z1_num + z1_i +1)\n                #plt.title(""predicted"")\n                plt.imshow(pred, cmap=cmap)\n                plt.axis(""off"")\n\n    plt.savefig(\'vae_latent_change2.png\')\n    plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test_latent_change2()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n    '"
Scripts_Generative/old/scripts_pytorch/vae_latent_change_mnist_pytorch.py,33,"b'import torch\nimport torch.nn.functional as F\n#import torchvision\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport os\nimport gzip\n    \n# config\nclass_N = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\ntorch.manual_seed(0)\n\n# other config\nmodel_path = \'VAE.pt\'\n    \n# VAE paramater\nZ_dim = 2\ndim = 256\n\nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        self.enc1 = torch.nn.Linear(img_height * img_width * channel, dim)\n        \n        self.enc_mu = torch.nn.Linear(dim, Z_dim)\n        self.enc_sigma = torch.nn.Linear(dim, Z_dim)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = F.relu(self.enc1(x))\n        mu = self.enc_mu(x)\n        sigma = self.enc_sigma(x)\n        self.mu = mu\n        self.sigma = sigma\n        \n        return mu, sigma\n    \n    \nclass Sampler(torch.nn.Module):\n    def __init__(self):\n        super(Sampler, self).__init__()\n        \n    def forward(self, x):\n        mu, sigma = x\n        mb, _ = mu.size()\n        epsilon = torch.tensor(np.random.normal(0, 1, [mb, Z_dim]), dtype=torch.float32).to(device)\n        std = torch.exp(0.5 * sigma)\n        sample_z = mu + epsilon * std\n        self.sample_z = sample_z\n        return sample_z\n    \n    \nclass Decoder(torch.nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n        self.dec1 = torch.nn.Linear(Z_dim, dim)\n\n        self.dec_out = torch.nn.Linear(dim, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = F.relu(self.dec1(x))\n        x = self.dec_out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n        \n       \n\n\ndef loss_KLDivergence(mu, sigma):\n    return -0.5 * torch.sum(1 + sigma - torch.pow(mu, 2) - torch.exp(sigma))\n        \n        \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n    \n\n# train\ndef train():\n    # model\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        #y_mu, y_sigma = model_encoder(x)\n        #y = model_decoder(y_mu, y_sigma)\n        y = model(x)\n        y_mu = model_encoder.mu\n        y_sigma = model_encoder.sigma\n        \n        #loss = torch.nn.BCELoss()(y, t.view(mb, -1))\n        loss = F.binary_cross_entropy(y, t.view(mb, img_height * img_width * channel), reduction=\'sum\')\n        loss_kld = loss_KLDivergence(y_mu, y_sigma)\n        loss = loss + loss_kld\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n                \n            #print(mu, sigma)\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n        \n    \n    \ndef test_latent_show():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    plt.figure(figsize=[10, 10])\n    \n    colors = [""red"", ""blue"", ""orange"", ""green"", ""purple"", \n              ""magenta"", ""yellow"", ""aqua"", ""black"", ""khaki""]\n    \n    with torch.no_grad():\n        for i in range(len(xs)):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            y = model(x)\n            mu = model_encoder.mu\n            sigma = model_encoder.sigma\n            sample_z = model_sampler.sample_z\n            \n            mu = mu.detach().cpu().numpy()[0]\n            sigma = sigma.detach().cpu().numpy()[0]\n            sample_z = sample_z.detach().cpu().numpy()[0]\n            \n            t = test_y[i]\n            \n            plt.scatter(sample_z[0], sample_z[1], c=colors[t])\n    \n    plt.savefig(\'vae_latent_show.png\')\n    plt.show()\n   \n\ndef test_latent_change():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    plt.figure(figsize=[12, 2])\n    \n    # split number (show image number)\n    show_num = 15\n\n    # latent lower and upper bound\n    z1_lower, z1_upper = -2, 2\n    z2_lower, z2_upper = -2, 2\n\n    z1_diff = float(z1_upper - z1_lower) / show_num\n    z2_diff = float(z2_upper - z2_lower) / show_num\n    \n    with torch.no_grad():\n        for ind in range(show_num):\n            # get latent vector\n            z1 = z1_lower + z1_diff * ind\n            z2 = z2_lower + z2_diff * ind\n            z = [z1, z2]\n            z = torch.tensor(z).to(device)\n\n            # decode latent vector\n            pred = model_decoder(z)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n\n            # normalize to [0, 1]\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1, 2, 0)\n            \n            if channel == 1:\n                cmap = ""gray""\n                pred = pred[..., 0]\n            elif channel == 3:\n                cmap = None\n\n            plt.subplot(1, show_num, ind + 1)\n            #plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.axis(""off"")\n\n    plt.savefig(\'vae_latent_change.png\')\n    plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test_latent_change()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n    '"
Scripts_Generative/old/scripts_pytorch/vae_latent_show_mnist_pytorch.py,28,"b'import torch\nimport torch.nn.functional as F\n#import torchvision\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport os\nimport gzip\n    \n# config\nclass_N = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\ntorch.manual_seed(0)\n\n# other config\nmodel_path = \'VAE.pt\'\n    \n# VAE paramater\nZ_dim = 2\ndim = 256\n\nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        self.enc1 = torch.nn.Linear(img_height * img_width * channel, dim)\n        \n        self.enc_mu = torch.nn.Linear(dim, Z_dim)\n        self.enc_sigma = torch.nn.Linear(dim, Z_dim)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = F.relu(self.enc1(x))\n        mu = self.enc_mu(x)\n        sigma = self.enc_sigma(x)\n        self.mu = mu\n        self.sigma = sigma\n        \n        return mu, sigma\n    \n    \nclass Sampler(torch.nn.Module):\n    def __init__(self):\n        super(Sampler, self).__init__()\n        \n    def forward(self, x):\n        mu, sigma = x\n        mb, _ = mu.size()\n        epsilon = torch.tensor(np.random.normal(0, 1, [mb, Z_dim]), dtype=torch.float32).to(device)\n        std = torch.exp(0.5 * sigma)\n        sample_z = mu + epsilon * std\n        self.sample_z = sample_z\n        return sample_z\n    \n    \nclass Decoder(torch.nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n        self.dec1 = torch.nn.Linear(Z_dim, dim)\n\n        self.dec_out = torch.nn.Linear(dim, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = F.relu(self.dec1(x))\n        x = self.dec_out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n\ndef loss_KLDivergence(mu, sigma):\n    return -0.5 * torch.sum(1 + sigma - torch.pow(mu, 2) - torch.exp(sigma))\n        \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n    \n\n# train\ndef train():\n    # model\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    # training\n    mb = 256\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n            mbi = mb - (len(xs) - mbi)\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        #y_mu, y_sigma = model_encoder(x)\n        #y = model_decoder(y_mu, y_sigma)\n        y = model(x)\n        y_mu = model_encoder.mu\n        y_sigma = model_encoder.sigma\n        \n        #loss = torch.nn.BCELoss()(y, t.view(mb, -1))\n        loss = F.binary_cross_entropy(y, t.view(mb, img_height * img_width * channel), reduction=\'sum\')\n        loss_kld = loss_KLDivergence(y_mu, y_sigma)\n        loss = loss + loss_kld\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n                \n            #print(mu, sigma)\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n        \n    \n    \ndef test_latent_show():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n    \n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    plt.figure(figsize=[10, 10])\n    \n    colors = [""red"", ""blue"", ""orange"", ""green"", ""purple"", \n              ""magenta"", ""yellow"", ""aqua"", ""black"", ""khaki""]\n    \n    class_counts = [0 for _ in range(class_N)]\n    \n    for i in range(len(xs)):\n        x = xs[i]\n        \n        x = np.expand_dims(x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        y = model(x)\n        mu = model_encoder.mu\n        sigma = model_encoder.sigma\n        sample_z = model_sampler.sample_z\n        \n        mu = mu.detach().cpu().numpy()[0]\n        sigma = sigma.detach().cpu().numpy()[0]\n        sample_z = sample_z.detach().cpu().numpy()[0]\n        \n        t = test_y[i]\n\n        class_counts[t] += 1\n\n        if class_counts[t] == 1:\n            plt.scatter(sample_z[0], sample_z[1], c=colors[t], label=str(t))\n        else:\n            plt.scatter(sample_z[0], sample_z[1], c=colors[t])\n    \n    plt.legend()\n    plt.savefig(\'vae_latent_show.png\')\n    plt.show()\n   \n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test_latent_show()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n    '"
Scripts_Generative/old/scripts_pytorch/vae_mnist_pytorch.py,25,"b'import torch\nimport torch.nn.functional as F\n#import torchvision\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# config\nclass_N = 10\nimg_height, img_width = 28, 28\nchannel = 1\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\ntorch.manual_seed(0)\n\n# other config\nmodel_path = \'VAE.pt\'\n    \n# VAE paramater\nZ_dim = 2\ndim = 256\n\nclass Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        self.enc1 = torch.nn.Linear(img_height * img_width * channel, dim)\n        \n        self.enc_mu = torch.nn.Linear(dim, Z_dim)\n        self.enc_sigma = torch.nn.Linear(dim, Z_dim)\n        \n    def forward(self, x):\n        mb, c, h, w = x.size()\n        x = x.view(mb, -1)\n        x = F.relu(self.enc1(x))\n        mu = self.enc_mu(x)\n        sigma = self.enc_sigma(x)\n        self.mu = mu\n        self.sigma = sigma\n        \n        return mu, sigma\n    \n    \nclass Sampler(torch.nn.Module):\n    def __init__(self):\n        super(Sampler, self).__init__()\n        \n    def forward(self, x):\n        mu, sigma = x\n        mb, _ = mu.size()\n        epsilon = torch.tensor(np.random.normal(0, 1, [mb, Z_dim]), dtype=torch.float32).to(device)\n        std = torch.exp(0.5 * sigma)\n        sample_z = mu + epsilon * std\n        self.sample_z = sample_z\n        return sample_z\n    \n    \nclass Decoder(torch.nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n        self.dec1 = torch.nn.Linear(Z_dim, dim)\n\n        self.dec_out = torch.nn.Linear(dim, img_height * img_width * channel)\n        \n        \n    def forward(self, x):\n        x = F.relu(self.dec1(x))\n        x = self.dec_out(x)\n        x = torch.sigmoid(x)\n        \n        return x\n        \ndef loss_KLDivergence(mu, sigma):\n    return -0.5 * torch.sum(1 + sigma - torch.pow(mu, 2) - torch.exp(sigma))\n        \ndef load_mnist():\n    dir_path = ""mnist_datas""\n\n    files = [""train-images-idx3-ubyte.gz"",\n             ""train-labels-idx1-ubyte.gz"",\n             ""t10k-images-idx3-ubyte.gz"",\n             ""t10k-labels-idx1-ubyte.gz""]\n\n    # download mnist datas\n    if not os.path.exists(dir_path):\n\n        os.makedirs(dir_path)\n\n        data_url = ""http://yann.lecun.com/exdb/mnist/""\n\n        for file_url in files:\n\n            after_file = file_url.split(\'.\')[0]\n            \n            if os.path.exists(dir_path + \'/\' + after_file):\n                continue\n            \n            os.system(""wget {}/{}"".format(data_url, file_url))\n            os.system(""mv {} {}"".format(file_url, dir_path))\n\n        \n    # load mnist data\n\n    # load train data\n    with gzip.open(dir_path + \'/\' + files[0], \'rb\') as f:\n        train_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        train_x = train_x.astype(np.float32)\n        train_x = train_x.reshape((-1, 28, 28, 1))\n        print(""train images >>"", train_x.shape)\n\n    with gzip.open(dir_path + \'/\' + files[1], \'rb\') as f:\n        train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""train labels >>"", train_y.shape)\n\n    # load test data\n    with gzip.open(dir_path + \'/\' + files[2], \'rb\') as f:\n        test_x = np.frombuffer(f.read(), np.uint8, offset=16)\n        test_x = test_x.astype(np.float32)\n        test_x = test_x.reshape((-1, 28, 28, 1))\n        print(""test images >>"", test_x.shape)\n    \n    with gzip.open(dir_path + \'/\' + files[3], \'rb\') as f:\n        test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n        print(""test labels >>"", test_y.shape)\n        \n\n    return train_x, train_y ,test_x, test_y\n\n    \n\n# train\ndef train():\n    # model\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = train_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n    \n    # training\n    mb = 256\n    mbi = 0\n    data_N = len(xs)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(10000):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n            mbi = mb - (data_N - mbi)\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n\n        opt.zero_grad()\n\n        #y_mu, y_sigma = model_encoder(x)\n        #y = model_decoder(y_mu, y_sigma)\n        y = model(x)\n        y_mu = model_encoder.mu\n        y_sigma = model_encoder.sigma\n        \n        #loss = torch.nn.BCELoss()(y, t.view(mb, -1))\n        loss = F.binary_cross_entropy(y, t.view(mb, img_height * img_width * channel), reduction=\'sum\')\n        loss_kld = loss_KLDivergence(y_mu, y_sigma)\n        loss = loss + loss_kld\n        loss.backward()\n        opt.step()\n    \n        #pred = y.argmax(dim=1, keepdim=True)\n        acc = y.eq(t.view_as(y)).sum().item() / mb\n\n        if (i+1) % 100 == 0:\n            print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model_encoder = Encoder().to(device)\n    model_sampler = Sampler().to(device)\n    model_decoder = Decoder().to(device)\n    model = torch.nn.Sequential(model_encoder, model_sampler, model_decoder)\n    \n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    train_x, train_y, test_x, test_y = load_mnist()\n    xs = test_x / 255\n    xs = xs.transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        for i in range(10):\n            x = xs[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n\n            pred = pred.view(channel, img_height, img_width)\n            pred = pred.detach().cpu().numpy()\n            pred -= pred.min()\n            pred /= pred.max()\n            pred = pred.transpose(1,2,0)\n            \n            _x = x.detach().cpu().numpy()[0]\n            #_x = (_x + 1) / 2\n            if channel == 1:\n                pred = pred[..., 0]\n                _x = _x[0]\n                cmap = \'gray\'\n            else:\n                _x = _x.transpose(1,2,0)\n                cmap = None\n                \n            #print(mu, sigma)\n                \n            plt.subplot(1,2,1)\n            plt.title(""input"")\n            plt.imshow(_x, cmap=cmap)\n            plt.subplot(1,2,2)\n            plt.title(""predicted"")\n            plt.imshow(pred, cmap=cmap)\n            plt.show()\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n    '"
Scripts_Model/old/scripts_pytorch/DenseNet121_pytorch.py,49,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'DenseNet121.pt\'\n\n# random seed\ntorch.manual_seed(0)\n            \nclass Block(torch.nn.Module):\n    def __init__(self, first_dim, k=32, L=6):\n        self.L = L\n        \n        super(Block, self).__init__()\n\n        self.blocks = torch.nn.ModuleList()\n        \n        self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        for i in range(1, L):\n            self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(k * i + first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        \n    def forward(self, x):\n        xs = [None for _ in range(self.L + 1)]\n        xs[0] = x\n        xs[1] = self.blocks[0](x)\n        \n        for i in range(1, self.L):\n            x_in = xs[i]\n            for j in range(i):\n                x_in = torch.cat([x_in, xs[j]], dim=1)\n            x = self.blocks[i](x_in)\n            xs[i + 1] = x\n                \n        x = xs[0]\n        for i in range(1, (self.L + 1)):\n            x = torch.cat([x, xs[i]], dim=1)\n\n        return x\n\n        \n\nclass DenseNet121(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet121, self).__init__()\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(channel)\n        self.conv1 = torch.nn.Conv2d(channel, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 24\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 16\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, num_classes)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [img_height // 32, img_width // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = DensetNet121().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = DensetNet121().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/DenseNet169_pytorch.py,49,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'DenseNet169.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n            \nclass Block(torch.nn.Module):\n    def __init__(self, first_dim, k=32, L=6):\n        self.L = L\n        \n        super(Block, self).__init__()\n\n        self.blocks = torch.nn.ModuleList()\n        \n        self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        for i in range(1, L):\n            self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(k * i + first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        \n    def forward(self, x):\n        xs = [None for _ in range(self.L + 1)]\n        xs[0] = x\n        xs[1] = self.blocks[0](x)\n        \n        for i in range(1, self.L):\n            x_in = xs[i]\n            for j in range(i):\n                x_in = torch.cat([x_in, xs[j]], dim=1)\n            x = self.blocks[i](x_in)\n            xs[i + 1] = x\n                \n        x = xs[0]\n        for i in range(1, (self.L + 1)):\n            x = torch.cat([x, xs[i]], dim=1)\n\n        return x\n\n        \n\nclass DenseNet169(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet169, self).__init__()\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(channel)\n        self.conv1 = torch.nn.Conv2d(channel, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 32\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 32\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, num_classes)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [img_height // 32, img_width // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n# train\ndef train():\n    # model\n    model = DensetNet169().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = DensetNet169().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/DenseNet201_pytorch.py,49,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'DenseNet201.pt\'\n\n# random seed\ntorch.manual_seed(0)\n            \nclass Block(torch.nn.Module):\n    def __init__(self, first_dim, k=32, L=6):\n        self.L = L\n        \n        super(Block, self).__init__()\n\n        self.blocks = torch.nn.ModuleList()\n        \n        self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        for i in range(1, L):\n            self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(k * i + first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        \n    def forward(self, x):\n        xs = [None for _ in range(self.L + 1)]\n        xs[0] = x\n        xs[1] = self.blocks[0](x)\n        \n        for i in range(1, self.L):\n            x_in = xs[i]\n            for j in range(i):\n                x_in = torch.cat([x_in, xs[j]], dim=1)\n            x = self.blocks[i](x_in)\n            xs[i + 1] = x\n                \n        x = xs[0]\n        for i in range(1, (self.L + 1)):\n            x = torch.cat([x, xs[i]], dim=1)\n\n        return x\n\n        \n\nclass DenseNet201(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet201, self).__init__()\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(channel)\n        self.conv1 = torch.nn.Conv2d(channel, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 48\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 32\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, num_classes)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [img_height // 32, img_width // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = DensetNet201().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = DensetNet201().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/DenseNet264_pytorch.py,49,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'DenseNet264.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n            \nclass Block(torch.nn.Module):\n    def __init__(self, first_dim, k=32, L=6):\n        self.L = L\n        \n        super(Block, self).__init__()\n\n        self.blocks = torch.nn.ModuleList()\n        \n        self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        for i in range(1, L):\n            self.blocks.append(torch.nn.Sequential(\n                torch.nn.BatchNorm2d(k * i + first_dim),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k * i + first_dim, k, kernel_size=1, padding=0, stride=1),\n                torch.nn.BatchNorm2d(k),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(k, k, kernel_size=3, padding=1, stride=1),\n            ))\n        \n        \n    def forward(self, x):\n        xs = [None for _ in range(self.L + 1)]\n        xs[0] = x\n        xs[1] = self.blocks[0](x)\n        \n        for i in range(1, self.L):\n            x_in = xs[i]\n            for j in range(i):\n                x_in = torch.cat([x_in, xs[j]], dim=1)\n            x = self.blocks[i](x_in)\n            xs[i + 1] = x\n                \n        x = xs[0]\n        for i in range(1, (self.L + 1)):\n            x = torch.cat([x, xs[i]], dim=1)\n\n        return x\n\n        \n\nclass DenseNet264(torch.nn.Module):\n    def __init__(self):\n        super(DenseNet264, self).__init__()\n\n        k = 32\n        theta = 0.5\n        self.bn1 = torch.nn.BatchNorm2d(channel)\n        self.conv1 = torch.nn.Conv2d(channel, k * 2, kernel_size=7, padding=3, stride=2)\n        \n        # Dense block1\n        block1_L = 6\n        block1_dim = int(k * block1_L * theta)\n        \n        self.block1 = Block(first_dim = k * 2, L = block1_L)\n        \n        # Transition layer1\n        self.transition1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block1_L + k * 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block1_L + k * 2, block1_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n    \n        # Dense block2\n        block2_L = 12\n        block2_dim = int(k * block2_L * theta)\n        \n        self.block2 = Block(first_dim = block1_dim, L = block2_L)\n\n        # Transition layer2        \n        self.transition2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block2_L + block1_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block2_L + block1_dim, block2_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block3\n        block3_L = 64\n        block3_dim = int(k * block3_L * theta)\n        \n        self.block3 = Block(first_dim = block2_dim, L = block3_L)\n        \n        # Transition layer3\n        self.transition3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(k * block3_L + block2_dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(k * block3_L + block2_dim, block3_dim, kernel_size=1, padding=0, stride=1),\n            torch.nn.AvgPool2d(2, stride=2, padding=0)\n        )\n        \n        # Dense block4\n        block4_L = 48\n        self.block4 = Block(first_dim = block3_dim, L = block4_L)\n        \n        self.linear = torch.nn.Linear(k * block4_L + block3_dim, num_classes)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv1(x)\n        \n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        \n        x = self.block1(x)\n        \n        x = self.transition1(x)\n        \n        x = self.block2(x)\n        \n        x = self.transition2(x)\n        \n        x = self.block3(x)\n        \n        x = self.transition3(x)\n        \n        x = self.block4(x)\n\n        x = F.avg_pool2d(x, [img_height // 32, img_width // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n# train\ndef train():\n    # model\n    model = DensetNet264().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = DensetNet264().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB0_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB0.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB0(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB0, self).__init__()\n\n        width_coefficient=1\n        depth_coefficient=1\n        dropout_ratio=0.2\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB0().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB0().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB1_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB1.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB1(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB1, self).__init__()\n        width_coefficient=1\n        depth_coefficient=1.1\n        dropout_ratio=0.2\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB1().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB1().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB2_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB2.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB2(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB2, self).__init__()\n        width_coefficient=1.1\n        depth_coefficient=1.2\n        dropout_ratio=0.3\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB2().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB2().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB3_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB3.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB3(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB3, self).__init__()\n        width_coefficient=1.2\n        depth_coefficient=1.4\n        dropout_ratio=0.3\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB3().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB3().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB4_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB4.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB4(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB4, self).__init__()\n        width_coefficient=1.2\n        depth_coefficient=1.4\n        dropout_ratio=0.3\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB4().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB4().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n    \n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB5_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB5.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB5(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB5, self).__init__()\n        width_coefficient=1.6\n        depth_coefficient=2.2\n        dropout_ratio=0.4\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB5().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB5().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n        \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB6_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB6.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB6(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB6, self).__init__()\n        width_coefficient=1.8\n        depth_coefficient=2.6\n        dropout_ratio=0.5\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB6().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB6().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/EfficientNetB7_pytorch.py,44,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 96, 96\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'EfficientNetB7.pt\'\n\n# random seed\ntorch.manual_seed(0)\n\n\nclass EfficientNetB7(torch.nn.Module):\n    def __init__(self):\n        super(EfficientNetB7, self).__init__()\n        width_coefficient=2.0\n        depth_coefficient=3.1\n        dropout_ratio=0.5\n        depth_divisor=8\n        drop_connect_rate=0.2\n\n        DEFAULT_BLOCKS_ARGS = [\n            # block 1\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 32, \'filters_out\': 16,\n            \'expand_ratio\': 1, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 2\n            {\'kernel_size\': 3, \'repeats\': 2, \'filters_in\': 16, \'filters_out\': 24,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 3\n            {\'kernel_size\': 5, \'repeats\': 2, \'filters_in\': 24, \'filters_out\': 40,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 4\n            {\'kernel_size\': 3, \'repeats\': 3, \'filters_in\': 40, \'filters_out\': 80,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 5\n            {\'kernel_size\': 5, \'repeats\': 3, \'filters_in\': 80, \'filters_out\': 112,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25},\n            # block 6\n            {\'kernel_size\': 5, \'repeats\': 4, \'filters_in\': 112, \'filters_out\': 192,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 2, \'se_ratio\': 0.25},\n            # block 7\n            {\'kernel_size\': 3, \'repeats\': 1, \'filters_in\': 192, \'filters_out\': 320,\n            \'expand_ratio\': 6, \'id_skip\': True, \'stride\': 1, \'se_ratio\': 0.25}\n        ]\n\n        def round_filters(filters, divisor=depth_divisor):\n            """"""Round number of filters based on depth multiplier.""""""\n            filters *= width_coefficient\n            new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_filters < 0.9 * filters:\n                new_filters += divisor\n            return int(new_filters)\n\n        def round_repeats(repeats):\n            """"""Round number of repeats based on depth multiplier.""""""\n            return int(np.ceil(depth_coefficient * repeats))\n\n            \n        class Reshape(torch.nn.Module):\n            def __init__(self, c, h, w):\n                super(Reshape, self).__init__()\n                self.c = c\n                self.h = h\n                self.w = w\n            \n            def forward(self, x):\n                x = x.view(x.size()[0], self.c, self.h, self.w)\n                return x\n\n        class Flatten(torch.nn.Module):\n            def __init__(self):\n                super(Flatten, self).__init__()\n\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n\n        class Swish(torch.nn.Module):\n            def __init__(self):\n                super(Swish, self).__init__()\n\n            def forward(self, x):\n                return x * torch.sigmoid(x)\n                    \n\n        class Block(torch.nn.Module):\n            def __init__(self, activation_fn=Swish(), drop_rate=0., name=\'\',\n                filters_in=32, filters_out=16, kernel_size=3, stride=1,\n                expand_ratio=1, se_ratio=0., id_skip=True):\n                super(Block, self).__init__()\n\n                # Expansion phase\n                filters = filters_in * expand_ratio\n\n                if expand_ratio != 1:\n                    _modules = OrderedDict()\n                    _modules[name + \'expand_conv\'] = torch.nn.Conv2d(filters_in, filters, kernel_size=1, padding=0, bias=False)\n                    _modules[name + \'expand_bn\'] = torch.nn.BatchNorm2d(filters)\n                    _modules[name + \'expand_activation\'] = activation_fn\n                    self.expansion = torch.nn.Sequential(_modules)\n\n                # Depthwise Convolution\n                _modules = OrderedDict()\n\n                conv_pad = kernel_size // 2\n                \n                _modules[name + \'dw_conv\'] = torch.nn.Conv2d(filters, filters, kernel_size, stride=stride, padding=conv_pad, bias=False, groups=1)\n                \n                _modules[name + \'dw_bn\'] = torch.nn.BatchNorm2d(filters)\n                _modules[name + \'dw_activation\'] = activation_fn\n                self.DW_conv = torch.nn.Sequential(_modules)\n\n\n                # Squeeze and Excitation phase\n                if 0 < se_ratio <= 1:\n                    filters_se = max(1, int(filters_in * se_ratio))\n\n                    _modules = OrderedDict()\n                    _modules[name + \'se_sqeeze\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n                    _modules[name + \'se_reshape\'] = Reshape(c=filters, h=1, w=1)\n                    _modules[name + \'se_reduce_conv\'] = torch.nn.Conv2d(filters, filters_se, kernel_size=1, padding=0)\n                    _modules[name + \'se_reduce_activation\'] = activation_fn\n                    _modules[name + \'se_expand_conv\'] = torch.nn.Conv2d(filters_se, filters, kernel_size=1, padding=0)\n                    _modules[name + \'se_expand_activation\'] = torch.nn.Sigmoid()\n                    self.SE_phase = torch.nn.Sequential(_modules)\n                    \n\n                # Output phase\n                _modules = OrderedDict()\n                _modules[name + \'project_conv\'] = torch.nn.Conv2d(filters, filters_out, kernel_size=1, padding=0, bias=False)\n                _modules[name + \'project_bn\'] = torch.nn.BatchNorm2d(filters_out)\n                self.output_phase = torch.nn.Sequential(_modules)\n\n\n                # \n                self.last_add = False\n                if (id_skip is True and stride == 1 and filters_in == filters_out):\n                    if drop_rate > 0:\n                        self.output_phase_Dropout = torch.nn.Dropout2d(p=drop_rate)\n\n                    self.last_add = True\n\n                \n            def forward(self, input_x):\n                # expansion phase\n                if hasattr(self, \'expansion\'):\n                    x = self.expansion(input_x)\n                else:\n                    x = input_x\n\n                x = self.DW_conv(x)\n\n                # Squeeze and Excitation phase\n                if hasattr(self, \'SE_phase\'):\n                    x_SE_phase = self.SE_phase(x)\n                    x = x * x_SE_phase\n\n                # Output phase\n                x = self.output_phase(x)\n\n                if hasattr(self, \'output_phase_Dropout\'):\n                    x = self.output_phase_Dropout(x)\n\n                if self.last_add:\n                    x = x + input_x\n\n                return x\n\n        # stem\n        _modules = OrderedDict()\n        _modules[\'stem_conv\'] = torch.nn.Conv2d(channel, round_filters(32), kernel_size=3, padding=1, stride=2, bias=False)\n        _modules[\'stem_bn\'] = torch.nn.BatchNorm2d(round_filters(32))\n        _modules[\'stem_activation\'] = Swish()\n        self.stem = torch.nn.Sequential(_modules)\n        \n        # block\n        _modules = []\n\n        b = 0\n        block_Num = float(sum(args[\'repeats\'] for args in DEFAULT_BLOCKS_ARGS))\n\n        for (i, args) in enumerate(DEFAULT_BLOCKS_ARGS):\n            assert args[\'repeats\'] > 0\n            # Update block input and output filters based on depth multiplier.\n            args[\'filters_in\'] = round_filters(args[\'filters_in\'])\n            args[\'filters_out\'] = round_filters(args[\'filters_out\'])\n\n            for j in range(round_repeats(args.pop(\'repeats\'))):\n                # The first block needs to take care of stride and filter size increase.\n                if j > 0:\n                    args[\'stride\'] = 1\n                    args[\'filters_in\'] = args[\'filters_out\']\n\n                _modules.append(\n                    Block(activation_fn=Swish(), drop_rate=drop_connect_rate * b / block_Num, name=\'block{}{}_\'.format(i + 1, chr(j + 97)), **args))\n                b += 1\n\n        self.block = torch.nn.Sequential(*_modules)\n\n\n        # top\n        _modules = OrderedDict()\n        _modules[\'top_conv\'] = torch.nn.Conv2d(DEFAULT_BLOCKS_ARGS[-1][\'filters_out\'], round_filters(1280), kernel_size=1, padding=0, bias=False)\n        _modules[\'top_bn\'] = torch.nn.BatchNorm2d(round_filters(1280))\n        _modules[\'top_activation\'] = Swish()\n        self.top = torch.nn.Sequential(_modules)\n\n        _modules = OrderedDict()\n        _modules[\'top_class_GAP\'] = torch.nn.AdaptiveMaxPool2d((1, 1))\n        if dropout_ratio > 0:\n            _modules[\'top_class_dropout\'] = torch.nn.Dropout2d(p=dropout_ratio)\n        _modules[\'top_class_flatten\'] = Flatten()\n        _modules[\'top_class_linear\'] = torch.nn.Linear(round_filters(1280), class_N)\n        self.top_class = torch.nn.Sequential(_modules)\n        \n        \n    def forward(self, x):\n        # stem\n        x = self.stem(x)\n\n        # blocks\n        x = self.block(x)\n\n        # top\n        x = self.top(x)\n        x = self.top_class(x)\n\n        x = F.softmax(x, dim=1)        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = EfficientNetB7().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = EfficientNetB7().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n        \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/MobileNet_v1_pytorch.py,31,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 96, 96\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\n\nclass MobileNet_v1(torch.nn.Module): \n    def __init__(self):\n        class MobileNetBlock(torch.nn.Module):\n            def __init__(self, in_dim, out_dim, repeat=1, stride=1):\n                super(MobileNetBlock, self).__init__()\n                _module = []\n                for _ in range(repeat):\n                    _module += [\n                        torch.nn.Conv2d(in_dim, in_dim, kernel_size=3, padding=1, stride=stride, groups=in_dim),\n                        torch.nn.BatchNorm2d(in_dim),\n                        torch.nn.ReLU(),\n                        torch.nn.Conv2d(in_dim, out_dim, kernel_size=1, padding=0, stride=1),\n                        torch.nn.BatchNorm2d(out_dim),\n                        torch.nn.ReLU(),\n                    ]\n                    \n                self.module = torch.nn.Sequential(*_module)\n                    \n            def forward(self, x):\n                x = self.module(x)\n                return x\n            \n        class Flatten(torch.nn.Module):\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n        \n        super(MobileNet_v1, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            #-----\n            # 1/1 x 1/1 x 3\n            #-----\n            torch.nn.Conv2d(channel, 32, kernel_size=3, padding=1, stride=2),\n            torch.nn.BatchNorm2d(32),\n            torch.nn.ReLU(),\n\n            #-----\n            # 1/2 x 1/2 x 32\n            #-----\n            MobileNetBlock(32, 64),\n\n            #-----\n            # 1/4 x 1/4 x 64\n            #-----\n            MobileNetBlock(64, 128, stride=2),\n            MobileNetBlock(128, 128),\n\n            #-----\n            # 1/8 x 1/8 x 128\n            #-----\n            MobileNetBlock(128, 256, stride=2),\n            MobileNetBlock(256, 256),\n\n            #-----\n            # 1/16 x 1/16 x 256\n            #-----\n            MobileNetBlock(256, 512, stride=2),\n            MobileNetBlock(512, 512, repeat=5),\n            \n            #-----\n            # 1/32 x 1/32 x 1024\n            #-----\n            MobileNetBlock(512, 1024, stride=2),\n            MobileNetBlock(1024, 1024),\n            #torch.nn.AvgPool2d([img_height // 32, img_width // 32], stride=1, padding=0),\n            torch.nn.AdaptiveAvgPool2d([1,1]),\n            Flatten(),\n            torch.nn.Linear(1024, class_N),\n            torch.nn.Softmax(dim=1)\n        )\n\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\n\n\n    \nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = MobileNet_v1().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.CNLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = MobileNet_v1().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/MobileNet_v2_pytorch.py,34,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 128, 128\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'MobileNetv2.pt\'\n\ntorch.manual_seed(0)\n\n\nclass MobileNet_v2(torch.nn.Module): \n    def __init__(self):\n        \n        # define block\n        class MobileNetBlock(torch.nn.Module):\n            def __init__(self, in_dim, out_dim, stride=1, expansion_t=6, split_division_by=8):\n                super(MobileNetBlock, self).__init__()\n                \n                self.module = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_dim, in_dim * expansion_t, kernel_size=1, padding=0, stride=1, groups=in_dim),\n                    torch.nn.BatchNorm2d(in_dim * expansion_t),\n                    torch.nn.ReLU6(),\n                    torch.nn.Conv2d(in_dim * expansion_t, in_dim * expansion_t, kernel_size=3, padding=1, stride=stride, groups=split_division_by),\n                    torch.nn.BatchNorm2d(in_dim * expansion_t),\n                    torch.nn.ReLU6(),\n                    torch.nn.Conv2d(in_dim * expansion_t, out_dim, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_dim),\n                )\n                    \n            def forward(self, _input):\n                x = self.module(_input)\n                \n                # if shape matches, add skip connection\n                if x.size() == _input.size():\n                    x = x + _input\n                \n                return x\n            \n            \n        # define feature dimension flattening layer\n        class Flatten(torch.nn.Module):\n            def forward(self, x):\n                x = x.view(x.size()[0], -1)\n                return x\n        \n        \n        super(MobileNet_v2, self).__init__()\n        \n        self.module = torch.nn.Sequential(\n            # input\n            # 224 x 224 x 3\n            torch.nn.Conv2d(channel, 32, kernel_size=3, padding=1, stride=2),\n            torch.nn.BatchNorm2d(32),\n            torch.nn.ReLU6(),\n            # 112 x 112 x 32\n            MobileNetBlock(32, 16, expansion_t=1),\n            # 112 x 112 x 16\n            MobileNetBlock(16, 24, stride=2),\n            MobileNetBlock(24, 24),\n            # 56 x 56 x 24\n            MobileNetBlock(24, 32, stride=2),\n            MobileNetBlock(32, 32),\n            MobileNetBlock(32, 32),\n            # 28 x 28 x 32\n            MobileNetBlock(32, 64, stride=2),\n            MobileNetBlock(64, 64),\n            MobileNetBlock(64, 64),\n            MobileNetBlock(64, 64),\n            # 14 x 14 x 64\n            MobileNetBlock(64, 96),\n            MobileNetBlock(96, 96),\n            MobileNetBlock(96, 96),\n            # 14 x 14 x 96\n            MobileNetBlock(96, 160, stride=2),\n            MobileNetBlock(160, 160),\n            MobileNetBlock(160, 160),\n            # 7 x 7 x 160\n            MobileNetBlock(160, 320),\n            # 7 x 7 x 320\n            torch.nn.Conv2d(320, 1280, kernel_size=1, padding=0, stride=1),\n            torch.nn.BatchNorm2d(1280),\n            torch.nn.ReLU6(),\n            # 7 x 7 x 1280\n            torch.nn.AdaptiveAvgPool2d([1,1]),\n            Flatten(),\n            # 1 x 1 x 1280\n            torch.nn.Linear(1280, class_N),\n            torch.nn.Softmax(dim=1)\n        )\n\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = MobileNet_v2().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.CNLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    # model\n    model = MobileNet_v2().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/alexnet_pytorch.py,26,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 227, 227\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass AlexNet(torch.nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(channel, 96, kernel_size=11, padding=0, stride=4)\n        self.conv2 = torch.nn.Conv2d(96, 256, kernel_size=5, padding=1)\n        self.conv3 = torch.nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = torch.nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.fc1 = torch.nn.Linear(6*6*256, 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = x.view(-1, 6*6*256)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = AlexNet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = AlexNet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/api_pytorch.py,11,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nfrom cnn_finetune import make_model\n\nnum_classes = 2\nimg_height, img_width = 64, 64\nGPU = False\ntorch.manual_seed(0)\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = make_model(\'vgg16\', num_classes=num_classes, pretrained=True, input_size=(img_height, img_width))\n    model = model.to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(100):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = make_model(\'vgg16\', num_classes=num_classes, pretrained=True, input_size=(img_height, img_width))\n    model = model.to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    for i in range(len(paths)):\n        x = xs[i]\n        t = ts[i]\n        path = paths[i]\n        \n        x = np.expand_dims(x, axis=0)\n        x = torch.tensor(x, dtype=torch.float).to(device)\n        \n        pred = model(x)\n        pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n    \n        print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/bn_pytorch.py,39,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass VGG16(torch.nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n\n        conv1 = []\n        for i in range(2):\n            f = channel if i == 0 else 64\n            conv1.append(torch.nn.Conv2d(f, 64, kernel_size=3, padding=1, stride=1))\n            conv1.append(torch.nn.BatchNorm2d(64))\n            conv1.append(torch.nn.ReLU())\n        self.conv1 = torch.nn.Sequential(*conv1)\n        \n        conv2 = []\n        for i in range(2):\n            f = 64 if i == 0 else 128\n            conv2.append(torch.nn.Conv2d(f, 128, kernel_size=3, padding=1, stride=1))\n            conv2.append(torch.nn.BatchNorm2d(128))\n            conv2.append(torch.nn.ReLU())\n        self.conv2 = torch.nn.Sequential(*conv2)\n\n        conv3 = []\n        for i in range(3):\n            f = 128 if i == 0 else 256\n            conv3.append(torch.nn.Conv2d(f, 256, kernel_size=3, padding=1, stride=1))\n            conv3.append(torch.nn.BatchNorm2d(256))\n            conv3.append(torch.nn.ReLU())\n        self.conv3 = torch.nn.Sequential(*conv3)\n        \n        conv4 = []\n        for i in range(3):\n            f = 256 if i == 0 else 512\n            conv4.append(torch.nn.Conv2d(f, 512, kernel_size=3, padding=1, stride=1))\n            conv4.append(torch.nn.BatchNorm2d(512))\n            conv4.append(torch.nn.ReLU())\n        self.conv4 = torch.nn.Sequential(*conv4)\n            \n        conv5 = []\n        for i in range(3):\n            conv5.append(torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1))\n            conv5.append(torch.nn.BatchNorm2d(512))\n            conv5.append(torch.nn.ReLU())\n        self.conv5 = torch.nn.Sequential(*conv5)\n        \n        \n        self.fc1 = torch.nn.Linear(25088, 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, num_classes)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        \n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        #x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        #x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = VGG16().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = VGG16().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/gap_pytorch.py,23,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass GAP(torch.nn.Module):\n    def __init__(self):\n        super(GAP, self).__init__()\n        self.conv1 = torch.nn.Conv2d(channel, 96, kernel_size=7, padding=1, stride=2)\n        self.conv2 = torch.nn.Conv2d(96, 256, kernel_size=5, padding=1, stride=2)\n        self.conv3 = torch.nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = torch.nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv_out = torch.nn.Conv2d(256, num_classes, kernel_size=1, padding=1)\n        self.gap = torch.nn.AdaptiveAvgPool2d((1,1))\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.max_pool2d(x, 3, stride=2)\n        # GAP\n        x = self.conv_out(x)\n        x = self.gap(x)\n        x = x.view((x.shape[0], -1))\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = GAP().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = GAP().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/googlenetv1_pytorch.py,51,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\n\nclass InceptionModule(torch.nn.Module):\n    def __init__(self, in_f, f_1, f_2_1, f_2_2, f_3_1, f_3_2, f_4_2):\n        super(InceptionModule, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=1)\n        \n        self.conv2_1 = torch.nn.Conv2d(in_f, f_2_1, kernel_size=1, padding=0, stride=1)\n        self.conv2_2 = torch.nn.Conv2d(f_2_1, f_2_2, kernel_size=3, padding=1, stride=1)\n        \n        self.conv3_1 = torch.nn.Conv2d(in_f, f_3_1, kernel_size=1, padding=0, stride=1)\n        self.conv3_2 = torch.nn.Conv2d(f_3_1, f_3_2, kernel_size=5, padding=2, stride=1)\n\n        self.conv4_2 = torch.nn.Conv2d(in_f, f_4_2, kernel_size=1, padding=0, stride=1)\n\n        \n    def forward(self, x):\n        x1 = torch.nn.ReLU()(self.conv1(x))\n        \n        x2 = torch.nn.ReLU()(self.conv2_1(x))\n        x2 = torch.nn.ReLU()(self.conv2_2(x2))\n\n        x3 = torch.nn.ReLU()(self.conv3_1(x))\n        x3 = torch.nn.ReLU()(self.conv3_2(x3))\n\n        x4 = F.max_pool2d(x, 3, padding=1, stride=1)\n        x4 = torch.nn.ReLU()(self.conv4_2(x4))\n        x = torch.cat([x1, x2, x3, x4], dim=1)\n        return x\n\n        \n\nclass GoogLeNetv1(torch.nn.Module):\n    def __init__(self):\n        super(GoogLeNetv1, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, padding=0, stride=2)\n        self.conv2_1 = torch.nn.Conv2d(64, 64, kernel_size=1, padding=0, stride=1)\n        self.conv2_2 = torch.nn.Conv2d(64, 192, kernel_size=3, padding=1, stride=1)\n\n        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n\n        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n\n        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n\n        self.linear = torch.nn.Linear(1024, num_classes)\n            \n        self.aux1_conv1 = torch.nn.Conv2d(512, 128, kernel_size=1, padding=0, stride=1)\n        self.aux1_linear1 = torch.nn.Linear(25088, 1024)\n        self.aux1_linear2 = torch.nn.Linear(1024, num_classes)\n\n        self.aux2_conv1 = torch.nn.Conv2d(528, 128, kernel_size=1, padding=0, stride=1)\n        self.aux2_linear1 = torch.nn.Linear(25088, 1024)\n        self.aux2_linear2 = torch.nn.Linear(1024, num_classes)\n\n        \n        \n    def forward(self, x):\n        x = torch.nn.ReLU()(self.conv1(x))\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n\n        x = torch.nn.ReLU()(self.conv2_1(x))\n        x = torch.nn.ReLU()(self.conv2_2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.inception4a(x)\n\n        x_aux1 = F.avg_pool2d(x, 5, padding=2, stride=1)\n        x_aux1 = torch.nn.ReLU()(self.aux1_conv1(x_aux1))\n        x_aux1 = x_aux1.view(list(x_aux1.size())[0], -1)\n        x_aux1 = torch.nn.ReLU()(self.aux1_linear1(x_aux1))\n        x_aux1 = torch.nn.Dropout(p=0.7)(x_aux1)\n        x_aux1 = self.aux1_linear2(x_aux1)\n        x_aux1 = F.softmax(x_aux1, dim=1)\n        \n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n\n        x_aux2 = F.avg_pool2d(x, 5, padding=2, stride=1)\n        x_aux2 = torch.nn.ReLU()(self.aux2_conv1(x_aux2))\n        x_aux2 = x_aux2.view(list(x_aux2.size())[0], -1)\n        x_aux2 = torch.nn.ReLU()(self.aux2_linear1(x_aux2))\n        x_aux2 = torch.nn.Dropout(p=0.7)(x_aux2)\n        x_aux2 = self.aux2_linear2(x_aux2)\n        x_aux2 = F.softmax(x_aux2, dim=1)\n\n        x = self.inception4e(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        x = F.avg_pool2d(x, 7, padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x, x_aux1, x_aux2\n\n\n    \nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = GoogLeNetv1().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y, y_aux1, y_aux2 = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        loss_aux1 = loss_fn(torch.log(y_aux1), t)\n        loss_aux2 = loss_fn(torch.log(y_aux2), t)\n\n        loss = loss + loss_aux1 + loss_aux2\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = GoogLeNetv1().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/lenet_pytorch.py,19,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 32, 32\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass LeNet(torch.nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(channel, 6, kernel_size=5, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=5, padding=0)\n        self.fc1 = torch.nn.Linear(5*5*16, 120)\n        self.fc2 = torch.nn.Linear(120, 64)\n        self.fc_out = torch.nn.Linear(64, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.sigmoid(F.max_pool2d(x, 2))\n        x = self.conv2(x)\n        x = F.sigmoid(F.max_pool2d(x, 2))\n        x = x.view(-1, 5*5*16)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = LeNet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\')\n\n    # training\n    mb = 8\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = LeNet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/nin_pytorch.py,26,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 128, 128\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass NIN(torch.nn.Module):\n    def __init__(self):\n        super(NIN, self).__init__()\n        self.conv1 = torch.nn.Conv2d(channel, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp1 = torch.nn.Conv2d(192, 160, kernel_size=1, padding=0, stride=1)\n        self.cccp2 = torch.nn.Conv2d(160, 96, kernel_size=1, padding=0, stride=1)\n        self.conv2 = torch.nn.Conv2d(96, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp3 = torch.nn.Conv2d(192, 192, kernel_size=1, padding=0, stride=1)\n        self.cccp4 = torch.nn.Conv2d(192, 192, kernel_size=1, padding=0, stride=1)\n        self.conv3 = torch.nn.Conv2d(192, 192, kernel_size=5, padding=2, stride=1)\n        self.cccp5 = torch.nn.Conv2d(192, 160, kernel_size=1, padding=0, stride=1)\n        self.out = torch.nn.Conv2d(160, num_classes, kernel_size=1, padding=0, stride=1)\n        self.gap = torch.nn.AdaptiveAvgPool2d((1,1))\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.cccp1(x))\n        x = F.relu(self.cccp2(x))\n        x = F.max_pool2d(x, 3, stride=2, padding=0)\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.cccp3(x))\n        x = F.relu(self.cccp4(x))\n        x = F.max_pool2d(x, 3, stride=2, padding=0)\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.cccp5(x))\n        x = self.out(x)\n        x = self.gap(x)\n        x = x.view((x.shape[0], -1))\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = NIN().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = NIN().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/res101_pytorch.py,31,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 224, 224\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'ResNeXt101.pt\'\n\ntorch.manual_seed(0)\n\n        \nclass Res101(torch.nn.Module):\n    def __init__(self):\n        super(Res101, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n            \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n            x = self.fit_bn(x)\n            x = F.relu(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(22):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n# train\ndef train():\n    # model\n    model = Res101().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_func = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    model = Res101().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/res152_pytorch.py,32,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 224, 224\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'ResNeXt101.pt\'\n\ntorch.manual_seed(0)\n        \n\nclass Res152(torch.nn.Module):\n    def __init__(self):\n        super(Res152, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n        \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n            x = self.fit_bn(x)\n            x = F.relu(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        block = []\n        for _ in range(7):\n            block.append(ResBlock(512, 128, 512))\n        self.resblock3s = torch.nn.Sequential(*block)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(35):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3s(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n# train\ndef train():\n    # model\n    model = Res152().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_func = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    model = Res152().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/res18_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\n\n\nclass ResBlock(torch.nn.Module):\n    def __init__(self, in_f, out_f, stride=1):\n        super(ResBlock, self).__init__()\n\n        self.stride = stride\n        self.fit_dim = False\n\n        self.block = torch.nn.Sequential(\n            torch.nn.Conv2d(in_f, out_f, kernel_size=3, padding=1, stride=stride),\n            torch.nn.BatchNorm2d(out_f),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(out_f, out_f, kernel_size=3, padding=1, stride=1),\n            torch.nn.BatchNorm2d(out_f),\n            torch.nn.ReLU()\n        )\n\n        if in_f != out_f:\n            self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n            self.fit_dim = True\n            \n            \n        \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        \n\nclass Res18(torch.nn.Module):\n    def __init__(self):\n        super(Res18, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64)\n        self.resblock2_2 = ResBlock(64, 64)\n\n        self.resblock3_1 = ResBlock(64, 128, stride=2)\n        self.resblock3_2 = ResBlock(128, 128)\n\n        self.resblock4_1 = ResBlock(128, 256, stride=2)\n        self.resblock4_2 = ResBlock(256, 256)\n\n        self.resblock5_1 = ResBlock(256, 512, stride=2)\n        self.resblock5_2 = ResBlock(512, 512)\n        \n        self.linear = torch.nn.Linear(512, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n\n    \nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Res18().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Res18().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/res34_pytorch.py,27,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\n\n\nclass ResBlock(torch.nn.Module):\n    def __init__(self, in_f, out_f, stride=1):\n        super(ResBlock, self).__init__()\n\n        self.stride = stride\n        self.fit_dim = False\n\n        self.block = torch.nn.Sequential(\n            torch.nn.Conv2d(in_f, out_f, kernel_size=3, padding=1, stride=stride),\n            torch.nn.BatchNorm2d(out_f),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(out_f, out_f, kernel_size=3, padding=1, stride=1),\n            torch.nn.BatchNorm2d(out_f),\n            torch.nn.ReLU()\n        )\n\n        if in_f != out_f:\n            self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n            self.fit_dim = True\n            \n            \n        \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        \n\nclass Res34(torch.nn.Module):\n    def __init__(self):\n        super(Res34, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.resblock2_1 = ResBlock(64, 64)\n        self.resblock2_2 = ResBlock(64, 64)\n        self.resblock2_3 = ResBlock(64, 64)\n\n        self.resblock3_1 = ResBlock(64, 128, stride=2)\n        self.resblock3_2 = ResBlock(128, 128)\n        self.resblock3_3 = ResBlock(128, 128)\n        self.resblock3_4 = ResBlock(128, 128)\n\n        self.resblock4_1 = ResBlock(128, 256, stride=2)\n        self.resblock4_2 = ResBlock(256, 256)\n        self.resblock4_3 = ResBlock(256, 256)\n        self.resblock4_4 = ResBlock(256, 256)\n        self.resblock4_5 = ResBlock(256, 256)\n        self.resblock4_6 = ResBlock(256, 256)\n\n        self.resblock5_1 = ResBlock(256, 512, stride=2)\n        self.resblock5_2 = ResBlock(512, 512)\n        self.resblock5_3 = ResBlock(512, 512)\n        \n        self.linear = torch.nn.Linear(512, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n        x = self.resblock4_3(x)\n        x = self.resblock4_4(x)\n        x = self.resblock4_5(x)\n        x = self.resblock4_6(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n\n    \nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(_x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Res34().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=10)\n\n    # training\n    mb = 32\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Res34().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/res50_pytorch.py,30,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 224, 224\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'ResNeXt101.pt\'\n\ntorch.manual_seed(0)\n\n\nclass Res50(torch.nn.Module):\n    def __init__(self):\n        super(Res50, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n        \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n            x = self.fit_bn(x)\n            x = F.relu(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        self.resblock4_2 = ResBlock(1024, 256, 1024)\n        self.resblock4_3 = ResBlock(1024, 256, 1024)\n        self.resblock4_4 = ResBlock(1024, 256, 1024)\n        self.resblock4_5 = ResBlock(1024, 256, 1024)\n        self.resblock4_6 = ResBlock(1024, 256, 1024)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4_2(x)\n        x = self.resblock4_3(x)\n        x = self.resblock4_4(x)\n        x = self.resblock4_5(x)\n        x = self.resblock4_6(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n# train\ndef train():\n    # model\n    model = Res50().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_func = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    model = Res50().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/resNeXt101_pytorch.py,31,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 128, 128\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'ResNeXt101.pt\'\n\ntorch.manual_seed(0)\n\n        \nclass ResNeXt101(torch.nn.Module):\n    def __init__(self):\n        super(ResNeXt101, self).__init__()\n\n        class ResBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1):\n                super(ResBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n\n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU()\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_bn = torch.nn.BatchNorm2d(out_f)\n                    self.fit_dim = True\n                    \n            def forward(self, x):\n                res_x = self.block(x)\n                \n                if self.fit_dim:\n                    x = self.fit_conv(x)\n                    x = self.fit_bn(x)\n                    x = F.relu(x)\n                \n                if self.stride == 2:\n                    x = F.max_pool2d(x, 2, stride=2)\n                    \n                x = torch.add(res_x, x)\n                x = F.relu(x)\n                return x\n\n        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        self.resblock2_1 = ResBlock(64, 64, 256)\n        self.resblock2_2 = ResBlock(256, 64, 256)\n        self.resblock2_3 = ResBlock(256, 64, 256)\n\n        self.resblock3_1 = ResBlock(256, 128, 512, stride=2)\n        self.resblock3_2 = ResBlock(512, 128, 512)\n        self.resblock3_3 = ResBlock(512, 128, 512)\n        self.resblock3_4 = ResBlock(512, 128, 512)\n\n        self.resblock4_1 = ResBlock(512, 256, 1024, stride=2)\n        block = []\n        for _ in range(22):\n            block.append(ResBlock(1024, 256, 1024))\n        self.resblock4s = torch.nn.Sequential(*block)\n\n        self.resblock5_1 = ResBlock(1024, 512, 2048, stride=2)\n        self.resblock5_2 = ResBlock(2048, 512, 2048)\n        self.resblock5_3 = ResBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, class_N)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.resblock2_1(x)\n        x = self.resblock2_2(x)\n        x = self.resblock2_3(x)\n\n        x = self.resblock3_1(x)\n        x = self.resblock3_2(x)\n        x = self.resblock3_3(x)\n        x = self.resblock3_4(x)\n\n        x = self.resblock4_1(x)\n        x = self.resblock4s(x)\n\n        x = self.resblock5_1(x)\n        x = self.resblock5_2(x)\n        x = self.resblock5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = ResNeXt101().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_func = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = ResNeXt101().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/resNeXt50_pytorch.py,29,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 128, 128\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'ResNeXt50.pt\'\n\ntorch.manual_seed(0)\nclass ResNeXt50(torch.nn.Module):\n    def __init__(self):\n        super(ResNeXt50, self).__init__()\n\n        class ResNeXtBlock(torch.nn.Module):\n            def __init__(self, in_f, f_1, out_f, stride=1, cardinality=32):\n                super(ResNeXtBlock, self).__init__()\n\n                self.stride = stride\n                self.fit_dim = False\n                \n                self.block = torch.nn.Sequential(\n                    torch.nn.Conv2d(in_f, f_1, kernel_size=1, padding=0, stride=stride),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, f_1, kernel_size=3, padding=1, stride=1, groups=cardinality),\n                    torch.nn.BatchNorm2d(f_1),\n                    torch.nn.ReLU(),\n                    torch.nn.Conv2d(f_1, out_f, kernel_size=1, padding=0, stride=1),\n                    torch.nn.BatchNorm2d(out_f),\n                    torch.nn.ReLU(),\n                )\n\n                if in_f != out_f:\n                    self.fit_conv = torch.nn.Conv2d(in_f, out_f, kernel_size=1, padding=0, stride=1)\n                    self.fit_dim = True\n            \n    def forward(self, x):\n        res_x = self.block(x)\n        \n        if self.fit_dim:\n            x = self.fit_conv(x)\n        \n        if self.stride == 2:\n            x = F.max_pool2d(x, 2, stride=2)\n            \n        x = torch.add(res_x, x)\n        x = F.relu(x)\n        return x\n\n        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        \n        \n        self.block2_1 = ResNeXtBlock(64, 64, 256)\n        self.block2_2 = ResNeXtBlock(256, 64, 256)\n        self.block2_3 = ResNeXtBlock(256, 64, 256)\n\n        self.block3_1 = ResNeXtBlock(256, 128, 512, stride=2)\n        self.block3_2 = ResNeXtBlock(512, 128, 512)\n        self.block3_3 = ResNeXtBlock(512, 128, 512)\n        self.block3_4 = ResNeXtBlock(512, 128, 512)\n\n        self.block4_1 = ResNeXtBlock(512, 256, 1024, stride=2)\n        self.block4_2 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_3 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_4 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_5 = ResNeXtBlock(1024, 256, 1024)\n        self.block4_6 = ResNeXtBlock(1024, 256, 1024)\n\n        self.block5_1 = ResNeXtBlock(1024, 512, 2048, stride=2)\n        self.block5_2 = ResNeXtBlock(2048, 512, 2048)\n        self.block5_3 = ResNeXtBlock(2048, 512, 2048)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 3, padding=1, stride=2)\n\n        x = self.block2_1(x)\n        x = self.block2_2(x)\n        x = self.block2_3(x)\n\n        x = self.block3_1(x)\n        x = self.block3_2(x)\n        x = self.block3_3(x)\n        x = self.block3_4(x)\n\n        x = self.block4_1(x)\n        x = self.block4_2(x)\n        x = self.block4_3(x)\n        x = self.block4_4(x)\n        x = self.block4_5(x)\n        x = self.block4_6(x)\n\n        x = self.block5_1(x)\n        x = self.block5_2(x)\n        x = self.block5_3(x)\n\n        x = F.avg_pool2d(x, [img_height//32, img_width//32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n# train\ndef train():\n    # model\n    model = ResNeXt50().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_func = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    model = ResNeXt50().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/vgg16_pytorch.py,34,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        conv1 = []\n        for i in range(2):\n            f = channel if i == 0 else 64\n            conv1.append(torch.nn.Conv2d(f, 64, kernel_size=3, padding=1, stride=1))\n            conv1.append(torch.nn.ReLU())\n        self.conv1 = torch.nn.Sequential(*conv1)\n        \n        conv2 = []\n        for i in range(2):\n            f = 64 if i == 0 else 128\n            conv2.append(torch.nn.Conv2d(f, 128, kernel_size=3, padding=1, stride=1))\n            conv2.append(torch.nn.ReLU())\n        self.conv2 = torch.nn.Sequential(*conv2)\n\n        conv3 = []\n        for i in range(3):\n            f = 128 if i == 0 else 256\n            conv3.append(torch.nn.Conv2d(f, 256, kernel_size=3, padding=1, stride=1))\n            conv3.append(torch.nn.ReLU())\n        self.conv3 = torch.nn.Sequential(*conv3)\n        \n        conv4 = []\n        for i in range(3):\n            f = 256 if i == 0 else 512\n            conv4.append(torch.nn.Conv2d(f, 512, kernel_size=3, padding=1, stride=1))\n            conv4.append(torch.nn.ReLU())\n        self.conv4 = torch.nn.Sequential(*conv4)\n            \n        conv5 = []\n        for i in range(3):\n            conv5.append(torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1))\n            conv5.append(torch.nn.ReLU())\n        self.conv5 = torch.nn.Sequential(*conv5)\n        \n        self.fc1 = torch.nn.Linear(25088, 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, num_classes)\n        \n    def forward(self, x):\n        # block conv1\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv2\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv3\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv4\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n\n        # block conv5\n        x = self.conv5(x)\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        \n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/vgg19_pytorch.py,35,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nchannel = 3\nGPU = False\ntorch.manual_seed(0)\n\nclass VGG19(torch.nn.Module):\n    def __init__(self):\n        super(VGG19, self).__init__()\n        self.conv1_1 = torch.nn.Conv2d(channel, 64, kernel_size=3, padding=1, stride=1)\n        self.conv1_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1)\n        self.conv2_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n        self.conv2_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1)\n        self.conv3_1 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n        self.conv3_2 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n        self.conv3_3 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n        self.conv3_4 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n        self.conv4_1 = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1)\n        self.conv4_2 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv4_3 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv4_4 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv5_1 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv5_2 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv5_3 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.conv5_4 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        \n        self.fc1 = torch.nn.Linear(25088, 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = F.relu(self.conv1_2(x))\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        x = F.relu(self.conv2_1(x))\n        x = F.relu(self.conv2_2(x))\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        x = F.relu(self.conv3_1(x))\n        x = F.relu(self.conv3_2(x))\n        x = F.relu(self.conv3_3(x))\n        x = F.relu(self.conv3_4(x))\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        x = F.relu(self.conv4_1(x))\n        x = F.relu(self.conv4_2(x))\n        x = F.relu(self.conv4_3(x))\n        x = F.relu(self.conv4_4(x))\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n        x = F.relu(self.conv5_1(x))\n        x = F.relu(self.conv5_2(x))\n        x = F.relu(self.conv5_3(x))\n        x = F.relu(self.conv5_4(x))\n        x = F.max_pool2d(x, 2, stride=2, padding=0)\n    \n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = VGG19().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss() #torch.nn.CrossEntropyLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = VGG19().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/xception_pytorch.py,80,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport copy\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\n# class config\nclass_label = [\'akahara\', \'madara\']\nclass_N = len(class_label)\n\n# config\nimg_height, img_width = 128, 128\nchannel = 3\n\n# GPU\nGPU = False\ndevice = torch.device(""cuda"" if GPU and torch.cuda.is_available() else ""cpu"")\n\n# other\nmodel_path = \'Xception.pt\'\n\ntorch.manual_seed(0)\n\n\nclass Block(torch.nn.Module):\n    def __init__(self, dim=728, cardinality=1):\n        super(Block, self).__init__()\n\n        self.block = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n            torch.nn.BatchNorm2d(dim),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=cardinality),\n            torch.nn.BatchNorm2d(dim),\n        )\n        \n    def forward(self, x):\n        res_x = self.block(x)            \n        x = torch.add(res_x, x)\n\n        return x\n\n        \n\nclass Xception(torch.nn.Module):\n    def __init__(self):\n        super(Xception, self).__init__()\n\n        # Entry flow\n        self.conv1 = torch.nn.Conv2d(channel, 32, kernel_size=3, padding=1, stride=2)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        \n        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n        self.bn2 = torch.nn.BatchNorm2d(64)\n        \n        self.conv3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv3_sc = torch.nn.Conv2d(64, 128, kernel_size=1, padding=0, stride=2)\n        self.bn3_sc = torch.nn.BatchNorm2d(128)\n        \n        self.conv4 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv4_sc = torch.nn.Conv2d(128, 256, kernel_size=1, padding=0, stride=2)\n        self.bn4_sc = torch.nn.BatchNorm2d(256)\n        \n        self.conv5 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(256, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv5_sc = torch.nn.Conv2d(256, 728, kernel_size=1, padding=0, stride=2)\n        self.bn5_sc = torch.nn.BatchNorm2d(728)\n        \n        # Middle flow\n        self.middle_flow = torch.nn.Sequential(\n            *[Block() for _ in range(8)]\n        )\n        \n        # Exit flow\n        self.conv_exit1 = torch.nn.Sequential(\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 728, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(728),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(728, 1024, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv_exit1_sc = torch.nn.Conv2d(728, 1024, kernel_size=1, padding=0, stride=2)\n        self.bn_exit1_sc = torch.nn.BatchNorm2d(1024)\n        \n        self.conv_exit2 = torch.nn.Sequential(\n            torch.nn.Conv2d(1024, 1536, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(1536),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(1536, 2048, kernel_size=3, padding=1, stride=1, groups=1),\n            torch.nn.BatchNorm2d(2048),)\n        \n        self.linear = torch.nn.Linear(2048, num_classes)\n        \n        \n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        \n        x_sc = self.conv3_sc(x)\n        x_sc = self.bn3_sc(x_sc)\n        x = self.conv3(x)\n        x = torch.add(x_sc, x)\n        \n        x_sc = self.conv4_sc(x_sc)\n        x_sc = self.bn4_sc(x_sc)\n        x = self.conv4(x)\n        x = torch.add(x_sc, x)\n        \n        x_sc = self.conv5_sc(x_sc)\n        x_sc = self.bn5_sc(x_sc)\n        x = self.conv5(x)\n        x = torch.add(x_sc, x)\n        \n        # Middle flow\n        x = self.middle_flow(x)\n        \n        # Exit flow\n        x_sc = self.conv_exit1_sc(x)\n        x_sc = self.bn_exit1_sc(x_sc)\n        x = self.conv_exit1(x)\n        x = torch.add(x_sc, x)\n        \n        x = self.conv_exit2(x)\n\n        x = F.avg_pool2d(x, [img_height // 32, img_width // 32], padding=0, stride=1)\n        x = x.view(list(x.size())[0], -1)\n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\n\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    if (rot == 0) and (rot != False):\n        raise Exception(\'invalid rot >> \', rot, \'should be [1, 359] or False\')\n\n    paths = []\n    ts = []\n    \n    data_num = 0\n    for dir_path in glob(path + \'/*\'):\n        data_num += len(glob(dir_path + ""/*""))\n            \n    pbar = tqdm(total = data_num)\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            for i, cls in enumerate(class_label):\n                if cls in path:\n                    t = i\n\n            paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': 0})\n            ts.append(t)\n\n            # horizontal flip\n            if hf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': False, \'rot\': 0})\n                ts.append(t)\n            # vertical flip\n            if vf:\n                paths.append({\'path\': path, \'hf\': False, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # horizontal and vertical flip\n            if hf and vf:\n                paths.append({\'path\': path, \'hf\': True, \'vf\': True, \'rot\': 0})\n                ts.append(t)\n            # rotation\n            if rot is not False:\n                angle = rot\n                while angle < 360:\n                    paths.append({\'path\': path, \'hf\': False, \'vf\': False, \'rot\': rot})\n                    angle += rot\n                    ts.append(t)\n                \n            pbar.update(1)\n                    \n    pbar.close()\n    \n    return np.array(paths), np.array(ts)\n\ndef get_image(infos):\n    xs = []\n    \n    for info in infos:\n        path = info[\'path\']\n        hf = info[\'hf\']\n        vf = info[\'vf\']\n        rot = info[\'rot\']\n        x = cv2.imread(path)\n\n        # resize\n        x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n        \n        # channel BGR -> Gray\n        if channel == 1:\n            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n            x = np.expand_dims(x, axis=-1)\n\n        # channel BGR -> RGB\n        if channel == 3:\n            x = x[..., ::-1]\n\n        # normalization [0, 255] -> [-1, 1]\n        x = x / 127.5 - 1\n\n        # horizontal flip\n        if hf:\n            x = x[:, ::-1]\n\n        # vertical flip\n        if vf:\n            x = x[::-1]\n\n        # rotation\n        scale = 1\n        _h, _w, _c = x.shape\n        max_side = max(_h, _w)\n        tmp = np.zeros((max_side, max_side, _c))\n        tx = int((max_side - _w) / 2)\n        ty = int((max_side - _h) / 2)\n        tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n        M = cv2.getRotationMatrix2D((max_side / 2, max_side / 2), rot, scale)\n        _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n        _x = _x[tx:tx+_w, ty:ty+_h]\n\n        xs.append(x)\n                \n    xs = np.array(xs, dtype=np.float32)\n    xs = np.transpose(xs, (0,3,1,2))\n    \n    return xs\n\n\n\n# train\ndef train():\n    # model\n    model = Xception().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n\n    paths, ts = data_load(\'../Dataset/train/images/\', hf=True, vf=True, rot=1)\n\n    # training\n    mb = 32\n    mbi = 0\n    data_N = len(paths)\n    train_ind = np.arange(data_N)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > data_N:\n            mb_ind = copy.copy(train_ind)[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb - (data_N - mbi))]))\n        else:\n            mb_ind = train_ind[mbi : mbi + mb]\n            mbi += mb\n\n        x = torch.tensor(get_image(paths[mb_ind]), dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        #y = F.log_softmax(y, dim=1)\n        loss = loss_fn(torch.log(y), t)\n        \n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n\n        if (i + 1) % 50 == 0:\n            print(""iter >>"", i+1, \', loss >>\', loss.item(), \', accuracy >>\', acc)\n\n    torch.save(model.state_dict(), model_path)\n\n# test\ndef test():\n    # model\n    model = Xception().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n    model.eval()\n\n    paths, ts = data_load(\'../Dataset/test/images/\', hf=False, vf=False, rot=False)\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            path = paths[i]\n            x = get_image(path)\n            t = ts[i]\n            \n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = pred.detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_Model/old/scripts_pytorch/zfnet_pytorch.py,26,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\n\nnum_classes = 2\nimg_height, img_width = 224, 224\nGPU = False\ntorch.manual_seed(0)\n\nclass ZFNet(torch.nn.Module):\n    def __init__(self):\n        super(ZFNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 96, kernel_size=7, padding=1, stride=2)\n        self.conv2 = torch.nn.Conv2d(96, 256, kernel_size=5, padding=1, stride=2)\n        self.conv3 = torch.nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = torch.nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.fc1 = torch.nn.Linear(9216, 4096)\n        self.fc2 = torch.nn.Linear(4096, 4096)\n        self.fc_out = torch.nn.Linear(4096, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv2(x))\n        x = torch.nn.modules.normalization.LocalResponseNorm(size=1)(x)\n        x = F.max_pool2d(x, 3, stride=2, padding=1)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.max_pool2d(x, 3, stride=2)\n        x = x.view(-1, 9216)\n        x = F.relu(self.fc1(x))\n        x = torch.nn.Dropout()(x)\n        x = F.relu(self.fc2(x))\n        x = torch.nn.Dropout()(x)\n        x = self.fc_out(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nCLS = [\'akahara\', \'madara\']\n\n# get train data\ndef data_load(path, hf=False, vf=False, rot=False):\n    xs = []\n    ts = []\n    paths = []\n    \n    for dir_path in glob(path + \'/*\'):\n        for path in glob(dir_path + \'/*\'):\n            x = cv2.imread(path)\n            x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n            x /= 255.\n            x = x[..., ::-1]\n            xs.append(x)\n\n            for i, cls in enumerate(CLS):\n                if cls in path:\n                    t = i\n            \n            ts.append(t)\n\n            paths.append(path)\n\n            if hf:\n                xs.append(x[:, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if vf:\n                xs.append(x[::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if hf and vf:\n                xs.append(x[::-1, ::-1])\n                ts.append(t)\n                paths.append(path)\n\n            if rot != False:\n                angle = rot\n                scale = 1\n\n                # show\n                a_num = 360 // rot\n                w_num = np.ceil(np.sqrt(a_num))\n                h_num = np.ceil(a_num / w_num)\n                count = 1\n                #plt.subplot(h_num, w_num, count)\n                #plt.axis(\'off\')\n                #plt.imshow(x)\n                #plt.title(""angle=0"")\n                \n                while angle < 360:\n                    _h, _w, _c = x.shape\n                    max_side = max(_h, _w)\n                    tmp = np.zeros((max_side, max_side, _c))\n                    tx = int((max_side - _w) / 2)\n                    ty = int((max_side - _h) / 2)\n                    tmp[ty: ty+_h, tx: tx+_w] = x.copy()\n                    M = cv2.getRotationMatrix2D((max_side/2, max_side/2), angle, scale)\n                    _x = cv2.warpAffine(tmp, M, (max_side, max_side))\n                    _x = _x[tx:tx+_w, ty:ty+_h]\n                    xs.append(x)\n                    ts.append(t)\n                    paths.append(path)\n\n                    # show\n                    #count += 1\n                    #plt.subplot(h_num, w_num, count)\n                    #plt.imshow(_x)\n                    #plt.axis(\'off\')\n                    #plt.title(""angle={}"".format(angle))\n\n                    angle += rot\n                #plt.show()\n\n\n    xs = np.array(xs, dtype=np.float32)\n    ts = np.array(ts, dtype=np.int)\n    \n    xs = xs.transpose(0,3,1,2)\n\n    return xs, ts, paths\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = ZFNet().to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    model.train()\n\n    xs, ts, paths = data_load(\'../Dataset/train/images/\', hf=True, vf=True)\n\n    # training\n    mb = 16\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        loss = loss_fn(torch.log(y), t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = ZFNet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    xs, ts, paths = data_load(\'../Dataset/test/images/\')\n\n    with torch.no_grad():\n        for i in range(len(paths)):\n            x = xs[i]\n            t = ts[i]\n            path = paths[i]\n            \n            x = np.expand_dims(x, axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n        \n            print(""in {}, predicted probabilities >> {}"".format(path, pred))\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/HRED_pytorch.py,79,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nmb = 16\n\nopt = ""Adam"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 5000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\n# RNN parameters\nhidden_dim = 256 # d_h in original paper\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.1 # Dropout ratio\nnum_layers = 1\n\n# Attention parameters\nAttention = True\nAttention_dkv = 64\nEncoder_attention_time = 0  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 0  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\nFeedForwardNetwork_dff = 128\nuse_PositionalEncoding = True # Transformer technique 5\nuse_Hard_Attention_Encoder = True # Hard Attention for Self Attention in Encoder\nuse_Hard_Attention_SourceTargetAttention_Decoder = True # Hard Attention for Source Target Attention in Decoder\nuse_Hard_Attention_SelfAttention_Decoder = True # Hard Attention for Self Attention in Decoder\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n# HRED parameters\nHRED_Session = 5\nHRED_hidden_dim = 512 # d_s in original paper\n#HRED_out_dim = HRED_hidden_dim * 2 if HRED_use_Bidirectional else HRED_hidden_dim\n\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, attention_dkv=64, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, attention_dkv=64, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention_SelfAttention=False,\n        use_Hard_Attention_SourceTargetAttention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SelfAttention\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SourceTargetAttention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, RNN_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, \n        hidden_dim, \n        memory_dim, \n        attention_dkv, \n        output_dim, \n        dropout_p=0.1, \n        max_length=MAX_LENGTH, \n        head_N=1, \n        self_Attention_Decoder=False,\n        hard_Attention=False\n        ):\n\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attention_dkv = attention_dkv\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n        self.hard_Attention=hard_Attention\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, attention_dkv)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        #self.attention_dropout = torch.nn.Dropout(dropout_p)\n\n        self.dense_output = torch.nn.Linear(attention_dkv, output_dim)\n        self.dropout_output = torch.nn.Dropout(dropout_p)\n\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.attention_dkv // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.attention_dkv)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n\n        # hard attention\n        if self.hard_Attention:\n            _attention_weights = torch.zeros(attention_weights.size(), dtype=torch.float)\n            argmax = torch.argmax(attention_weights, dim=-1)[:, 0]\n            _attention_weights[[_x for _x in range(argmax.size()[0])], :, argmax] = 1\n            attention_weights = _attention_weights\n        \n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.attention_dkv)\n\n        # one head -> Multi head\n        V = V.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n\n        # attention + Input\n        #attention_x = torch.cat([_input, attention_feature], dim=-1)\n        #print(attention_x.size())\n        # apply attention dense\n        #attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_dropout(attention_output)\n\n        attention_output = self.dense_output(attention_feature)\n        attention_output = self.dropout_output(attention_output)\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, d_ff, d_model, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\nclass HRED(torch.nn.Module):\n    def __init__(self, decoder_dim, hidden_dim, num_layers=1, use_Bidirectional=False):\n        super(HRED, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        # output GRU\n        self.gru = torch.nn.GRU(decoder_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n    def forward(self, x, hidden):\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros([tensor_dim, 1, HRED_hidden_dim], device=device)\n\n    \ndef data_load():\n    session_sentences = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    # each file\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            # read all lines in file\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            # add new vocabrary\n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            # add finish flag\n            lines += [\'<FINISH>\']\n\n            # parse lines to [[s1, s2, ..., sN], [s2, s3, ..., sN+1], ..., ]\n            session_sentences += [[lines[i + j] for j in range(HRED_Session)] for i in range(0, len(lines) - HRED_Session)]\n\n    # vocabrary sort\n    voca.sort()\n\n    print(""sentences num:"", len(session_sentences))\n    \n    session_sentence_index = []\n\n    # each session sentences\n    for sentences in session_sentences:\n        sentence_index = []\n\n        # each sentence\n        for i in range(HRED_Session - 1):\n            # parse to semantic element\n            sentence_parsed = mecab.parse(sentences[i]).strip().split(\' \')\n            # get index of element in vocabrary\n            sentence_voca_index = [voca.index(x) for x in sentence_parsed]\n            sentence_index.append(sentence_voca_index)\n\n        # last sentence\n        last_sentence = sentences[-1]\n        # if session finish flag\n        if last_sentence == \'<FINISH>\':\n            sentence_parsed = [last_sentence, \'<EOS>\']\n        else:\n            sentence_parsed = mecab.parse(last_sentence).strip().split(\' \') + [\'<EOS>\']\n        \n        # get index of element in vocabrary\n        sentence_voca_index = [voca.index(x) for x in sentence_parsed]\n        sentence_index.append(sentence_voca_index)\n        \n        session_sentence_index.append(sentence_index)\n\n    return voca, session_sentence_index\n\n\n# train\ndef train():\n    # data load\n    voca, session_sentences = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        HRED_hidden_dim,\n        voca_num, \n        HRED_hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    hred = HRED(\n        decoder_dim=RNN_dim,\n        hidden_dim=HRED_hidden_dim,\n        num_layers=num_layers,\n        use_Bidirectional=use_Bidirectional\n    ).to(device)\n\n    mbi = 0\n\n    data_num = len(session_sentences)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        # define optimizer\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n            hred_optimizer = torch.optim.SGD(hred.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            hred_optimizer = torch.optim.Adam(hred.parameters(), lr=lr, betas=(0.9, 0.98))\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        # for each iteration\n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            # get minibatch\n            session_sentences_minibatch = [session_sentences[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            # for each minibatch data\n            for mb_index in range(mb):\n                # get session sentences for one minibatch\n                Xs = session_sentences_minibatch[mb_index]\n                #Xs = torch.tensor(session_sentences_minibatch[mb_index]).to(device).view(HRED_Session, -1, 1)\n                #Xs_float = torch.tensor(Xs, dtype=torch.float).to(device)\n\n                #xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                #xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                #ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                # get initiate state for Encoder and HRED\n                encoder_hidden = encoder.initHidden()\n                hred_hidden = hred.initHidden()\n\n                # reset gradient\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n                hred_optimizer.zero_grad()\n\n                # for each session sentence\n                for session_index in range(HRED_Session - 1):\n            \n                    # get sentence sequence for Encoder\n                    X_encoder = torch.tensor(Xs[session_index]).to(device).view(-1, 1)\n                    #X_encoder_float = torch.tensor(X_encoderm dtype=torch.float).to(device)\n\n                    # sample recent decoder output as encoder\'s input\n                    if (session_index > 0) and (np.random.random() < 0.5):\n                        X_encoder = self_memory\n                            \n                    X_encoder_length = X_encoder.size()[0]\n\n                    # get sentence sequence for Decoder\n                    #X_decoder = Xs[session_index + 1]\n                    X_decoder = torch.tensor(Xs[session_index + 1]).to(device).view(-1, 1)\n                    X_decoder_length = X_decoder.size()[0]\n                    total_len += X_decoder_length\n\n                    encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                    # update Encoder\n                    for ei in range(X_encoder_length):\n                        encoder_output, encoder_hidden = encoder(X_encoder[ei], encoder_hidden, X_encoder)\n                        encoder_outputs[ei] = encoder_output[0, 0]\n\n\n                    # initialize HRED input\n                    hred_input = encoder_output\n                    \n                    # update HRED\n                    hred_output, hred_hidden = hred(hred_input, hred_hidden)\n\n                    # input \n                    decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n                    decoder_hidden = hred_hidden\n                    \n                    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                    self_memory = decoder_xs\n                    \n                    # update Decoder\n                    if use_teacher_forcing:\n                        # Teacher forcing: Feed the target (ground-truth word) as the next input\n                        for di in range(X_decoder_length):\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n            \n                            # add loss\n                            loss += loss_fn(torch.log(decoder_ys), X_decoder[di])\n\n                            # count accuracy\n                            if decoder_ys.argmax() == X_decoder[di]:\n                                accuracy += 1.\n                                \n                            # set next decoder\'s input (ground-truth label)\n                            decoder_xs = X_decoder[di].view(1, -1)\n                            self_memory = torch.cat([self_memory, decoder_xs])\n\n                    else:\n                        # Without teacher forcing: use its own predictions as the next input\n                        for di in range(X_decoder_length):\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                            \n                            # Select top 1 word with highest probability\n                            #topv, topi = decoder_ys.topk(1)\n                            # choice argmax\n                            if next_word_mode == ""argmax"":\n                                topv, topi = decoder_ys.data.topk(1)\n\n                            elif next_word_mode == ""prob"":\n                                topi = torch.multinomial(decoder_ys, 1)\n                            \n                            # set next input for decoder training\n                            decoder_xs = topi.squeeze().detach().view(1, -1)\n                            self_memory = torch.cat([self_memory, decoder_xs])\n\n                            # add loss\n                            loss += loss_fn(torch.log(decoder_ys), X_decoder[di])\n\n                            # count accuracy\n                            if decoder_ys.argmax() == X_decoder[di]:\n                                accuracy += 1.\n\n                            if decoder_xs.item() == voca.index(""<EOS>""):\n                                break\n\n                            \n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / total_len\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        HRED_hidden_dim,\n        voca_num, \n        HRED_hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    hred = HRED(\n        decoder_dim=RNN_dim,\n        hidden_dim=HRED_hidden_dim,\n        num_layers=num_layers,\n        use_Bidirectional=use_Bidirectional\n    ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        # get initiate state for Encoder and HRED\n        hred_hidden = hred.initHidden()\n\n        while count < 100:\n            input_length = xs.size()[0]\n            decoded_words = []\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            # update encoder\n            encoder_hidden = encoder.initHidden()\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n\n            # initialize HRED input\n            hred_input = encoder_output\n            \n            # update HRED\n            hred_output, hred_hidden = hred(hred_input, hred_hidden)\n\n            # Decoder input \n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n\n            # Decoder state\n            decoder_hidden = hred_hidden\n            \n            self_memory = decoder_x\n\n            # update Decoder\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                # if EOS or FINISH, finish conversation\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                # next input\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_FFN_pytorch.py,65,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = True\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 2000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 1\n\nEncoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        hidden_dim=hidden_dim, \n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        hidden_dim=hidden_dim, \n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attention_dense = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.attention_dropout = torch.nn.Dropout(dropout_p)\n        #self.attention_BN = torch.nn.BatchNorm1d(hidden_dim)\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.hidden_dim // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.hidden_dim)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.hidden_dim)\n\n        # one head -> Multi head\n        V = V.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n        \n        # attention + Input\n        attention_x = torch.cat([_input, attention_feature], dim=-1)\n        \n        # apply attention dense\n        attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_BN(attention_output)\n        attention_output = self.attention_dropout(attention_output)\n        attention_output = F.relu(attention_output)\n\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, hidden_dim, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim * 4),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(hidden_dim * 4, hidden_dim)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_HardAttention_pytorch.py,69,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nmb = 1\n\nopt = ""Adam"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 4000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nhidden_dim = 512\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.1 # Dropout ratio\nnum_layers = 1\n\nAttention = True\nAttention_dkv = 64\nEncoder_attention_time = 6  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 6  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\nFeedForwardNetwork_dff = 2048\nuse_PositionalEncoding = True # Transformer technique 5\nuse_Hard_Attention_Encoder = True # Hard Attention for Self Attention in Encoder\nuse_Hard_Attention_SourceTargetAttention_Decoder = True # Hard Attention for Source Target Attention in Decoder\nuse_Hard_Attention_SelfAttention_Decoder = True # Hard Attention for Self Attention in Decoder\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, attention_dkv=64, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, attention_dkv=64, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False,\n        use_Hard_Attention_SelfAttention=False,\n        use_Hard_Attention_SourceTargetAttention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SelfAttention\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N,\n                        hard_Attention=use_Hard_Attention_SourceTargetAttention\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, \n        hidden_dim, \n        memory_dim, \n        attention_dkv, \n        output_dim, \n        dropout_p=0.1, \n        max_length=MAX_LENGTH, \n        head_N=1, \n        self_Attention_Decoder=False,\n        hard_Attention=False\n        ):\n\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attention_dkv = attention_dkv\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n        self.hard_Attention=hard_Attention\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, attention_dkv)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        #self.attention_dropout = torch.nn.Dropout(dropout_p)\n\n        self.dense_output = torch.nn.Linear(attention_dkv, output_dim)\n        self.dropout_output = torch.nn.Dropout(dropout_p)\n\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.attention_dkv // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.attention_dkv)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n\n        # hard attention\n        if self.hard_Attention:\n            _attention_weights = torch.zeros(attention_weights.size(), dtype=torch.float)\n            argmax = torch.argmax(attention_weights, dim=-1)[:, 0]\n            _attention_weights[[_x for _x in range(argmax.size()[0])], :, argmax] = 1\n            attention_weights = _attention_weights\n        \n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.attention_dkv)\n\n        # one head -> Multi head\n        V = V.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n\n        # attention + Input\n        #attention_x = torch.cat([_input, attention_feature], dim=-1)\n        #print(attention_x.size())\n        # apply attention dense\n        #attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_dropout(attention_output)\n\n        attention_output = self.dense_output(attention_feature)\n        attention_output = self.dropout_output(attention_output)\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, d_ff, d_model, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(0.9, 0.98))\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention=use_Hard_Attention_Encoder\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding,\n        use_Hard_Attention_SelfAttention=use_Hard_Attention_SelfAttention_Decoder,\n        use_Hard_Attention_SourceTargetAttention=use_Hard_Attention_SourceTargetAttention_Decoder\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_MultiHeadAttention_pytorch.py,59,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = True\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 2000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 1\n\nEncoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N\n                        ))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N\n                        ))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attention_dense = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.attention_dropout = torch.nn.Dropout(dropout_p)\n        #self.attention_BN = torch.nn.BatchNorm1d(hidden_dim)\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.hidden_dim // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.hidden_dim)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.hidden_dim)\n\n        # one head -> Multi head\n        V = V.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n        \n        # attention + Input\n        attention_x = torch.cat([_input, attention_feature], dim=-1)\n        \n        # apply attention dense\n        attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_BN(attention_output)\n        attention_output = self.attention_dropout(attention_output)\n        attention_output = F.relu(attention_output)\n\n        return attention_output\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_PE_pytorch.py,67,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = True\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 2000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 1\n\nEncoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\nuse_PositionalEncoding = True # Transformer technique 5\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        use_PositionalEncoding=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        hidden_dim=hidden_dim, \n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        use_PositionalEncoding=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim, \n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        hidden_dim=hidden_dim, \n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attention_dense = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.attention_dropout = torch.nn.Dropout(dropout_p)\n        #self.attention_BN = torch.nn.BatchNorm1d(hidden_dim)\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.hidden_dim // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.hidden_dim)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.hidden_dim)\n\n        # one head -> Multi head\n        V = V.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n        \n        # attention + Input\n        attention_x = torch.cat([_input, attention_feature], dim=-1)\n        \n        # apply attention dense\n        attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_BN(attention_output)\n        attention_output = self.attention_dropout(attention_output)\n        attention_output = F.relu(attention_output)\n\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, hidden_dim, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim * 4),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(hidden_dim * 4, hidden_dim)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_SelfAttention_pytorch.py,58,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = True\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 2000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 1\n\nEncoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(hidden_dim=hidden_dim, memory_dim=hidden_dim, dropout_p=dropout_p, max_length=max_length, self_Attention_Decoder=True))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(hidden_dim=hidden_dim, memory_dim=hidden_dim, dropout_p=dropout_p, max_length=max_length, self_Attention_Decoder=True))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(hidden_dim=hidden_dim, memory_dim=RNN_dim, dropout_p=dropout_p, max_length=max_length))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attention_dense = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.attention_dropout = torch.nn.Dropout(dropout_p)\n        #self.attention_BN = torch.nn.BatchNorm1d(hidden_dim)\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.hidden_dim // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.hidden_dim)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.hidden_dim)\n\n        # one head -> Multi head\n        V = V.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n        \n        # attention + Input\n        attention_x = torch.cat([_input, attention_feature], dim=-1)\n        \n        # apply attention dense\n        attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_BN(attention_output)\n        attention_output = self.attention_dropout(attention_output)\n        attention_output = F.relu(attention_output)\n\n        return attention_output\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_SourceTargetAttention_pytorch.py,58,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = True\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 2000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.2 # Dropout ratio\nnum_layers = 1\n\nEncoder_attention_time = 0  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 1  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Attention\n        if attention_time > 0:\n            self.attentions = []\n            _attentions = []\n            for i in range(attention_time):\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(hidden_dim=hidden_dim, memory_dim=RNN_dim, dropout_p=dropout_p, max_length=max_length))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, hidden_dim)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attention_dense = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.attention_dropout = torch.nn.Dropout(dropout_p)\n        #self.attention_BN = torch.nn.BatchNorm1d(hidden_dim)\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.hidden_dim // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.hidden_dim)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.hidden_dim)\n\n        # one head -> Multi head\n        V = V.view(-1, self.hidden_dim // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n        \n        # attention + Input\n        attention_x = torch.cat([_input, attention_feature], dim=-1)\n        \n        # apply attention dense\n        attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_BN(attention_output)\n        attention_output = self.attention_dropout(attention_output)\n        attention_output = F.relu(attention_output)\n\n        return attention_output\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, hidden_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/Transformer_pytorch.py,67,"b'import numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\nimport sys\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nmb = 1\n\nopt = ""Adam"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 4000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\nhidden_dim = 512\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nuse_Bidirectional = False # Bi-directional\ndropout_p = 0.1 # Dropout ratio\nnum_layers = 1\n\nAttention = True\nAttention_dkv = 64\nEncoder_attention_time = 6  # Transformer technique 3 : Hopping if > 1\nDecoder_attention_time = 6  # Transformer technique 3 : Hopping if > 1\nuse_Source_Target_Attention = True # use source target attention\nuse_Encoder_Self_Attention = True # self attention of Encoder\nuse_Decoder_Self_Attention = True # self attention of Decoder\nMultiHead_Attention_N = 8 # Multi head attention Transformer technique 1\nuse_FeedForwardNetwork = True # Transformer technique 4\nFeedForwardNetwork_dff = 2048\nuse_PositionalEncoding = True # Transformer technique 5\n\n\n# automatically get RNN hidden dimension from above config\nRNN_dim = hidden_dim * 2 if use_Bidirectional else hidden_dim\ntensor_dim = num_layers * 2 if use_Bidirectional else num_layers\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_dim, attention_dkv=64, max_length=MAX_LENGTH, \n        dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False):\n    \n        super(Encoder, self).__init__()\n        self.max_length = max_length\n\n        # Embedding\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        #self_Attention_Decoder=True, \n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n\n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n\n\n    def forward(self, x, hidden, memory):\n        # Embedding\n        x = self.embedding(x).view(1, 1, -1)\n\n        # Memory embedding\n        memory = self.embedding(memory).permute(1, 0, 2)\n        memory = memory.float()\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):#self.use_PositionalEncoding:\n            x = self.positionalEncoding(x)\n            memory = self.positionalEncoding(memory)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory, memory)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n    def initHidden(self):\n        return torch.zeros(tensor_dim, 1, hidden_dim, device=device)\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, RNN_dim, attention_dkv=64, dropout_p=0.1, num_layers=1,\n        attention_time=1,\n        max_length=MAX_LENGTH,\n        use_Source_Target_Attention=False,\n        use_Self_Attention=False,\n        MultiHead_Attention_N=2,\n        use_FFNetwork=False,\n        FeedForwardNetwork_dff=2048,\n        use_PositionalEncoding=False):\n\n        super(Decoder, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        # Embedding\n        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n\n        # Positional Encoding\n        if use_PositionalEncoding:\n            self.positionalEncoding = PositionalEncoding()\n\n        # step1 : Attention\n        self.attentions = []\n        if attention_time > 0:\n            _attentions = []\n            for i in range(attention_time):\n                # step2 : Self Attention\n                if use_Self_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=hidden_dim, \n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length, \n                        self_Attention_Decoder=True,\n                        head_N=MultiHead_Attention_N\n                        ))\n                \n                # step1 : Source Target Attention\n                if use_Source_Target_Attention:\n                    _attentions.append(Attention(\n                        hidden_dim=hidden_dim, \n                        memory_dim=RNN_dim,\n                        attention_dkv=Attention_dkv,\n                        output_dim=hidden_dim,\n                        dropout_p=dropout_p, \n                        max_length=max_length,\n                        head_N=MultiHead_Attention_N\n                        ))\n\n                # Feed Forward Network\n                if use_FFNetwork:\n                    _attentions.append(FeedForwardNetwork(\n                        d_ff=FeedForwardNetwork_dff,\n                        d_model=hidden_dim,\n                        dropout_p=dropout_p))\n        \n            self.attentions = _attentions\n\n        # output GRU\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=use_Bidirectional)\n        self.out = torch.nn.Linear(RNN_dim, output_dim)\n    \n\n    def forward(self, x, hidden, memory_encoder, memory_decoder):\n        # Embedding\n        x = self.input_embedding(x)\n        x = self.input_embedding_dropout(x)\n\n        # Memory Embedding\n        memory_decoder = self.input_embedding(memory_decoder).permute(1, 0, 2)\n\n        # Positional Encoding\n        if hasattr(self, ""positionalEncoding""):\n            x = self.positionalEncoding(x)\n            memory_decoder = self.positionalEncoding(memory_decoder)\n\n        # Attention\n        for layer in self.attentions:\n            x = layer(x, memory_encoder, memory_decoder)\n\n        # output GRU\n        x, hidden = self.gru(x, hidden)\n        x = self.out(x[0])\n        x = F.softmax(x, dim=-1)\n        return x, hidden, None\n\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, hidden_dim, memory_dim, attention_dkv, output_dim, dropout_p=0.1, max_length=MAX_LENGTH, head_N=1, self_Attention_Decoder=False):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attention_dkv = attention_dkv\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.head_N = head_N\n        self.self_Attention_Decoder = self_Attention_Decoder\n\n        # Attention Query\n        #self.Q_embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        #self.Q_dropout = torch.nn.Dropout(self.dropout_p)\n        self.Q_dense = torch.nn.Linear(hidden_dim, attention_dkv)\n        self.Q_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.Q_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention Key\n        self.K_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.K_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.K_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attetion Value\n        self.V_dense = torch.nn.Linear(memory_dim, attention_dkv)\n        self.V_dense_dropout = torch.nn.Dropout(dropout_p)\n        #self.V_BN = torch.nn.BatchNorm1d(hidden_dim)\n        \n        # Attention mask\n        #self.attention = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        #self.attention_dropout = torch.nn.Dropout(dropout_p)\n\n        self.dense_output = torch.nn.Linear(attention_dkv, output_dim)\n        self.dropout_output = torch.nn.Dropout(dropout_p)\n\n\n\n    def forward(self, _input, memory, memory2):\n        # get Query\n        Q = self.Q_dense(_input.view(1, -1))\n        #Q = self.Q_BN(Q)\n        Q = self.Q_dense_dropout(Q)\n        Q = Q.view(1, 1, -1)\n        \n        # one head -> Multi head\n        Q = Q.view(1, self.attention_dkv // self.head_N, self.head_N)\n        Q = Q.permute([2, 0, 1])\n\n        # Transformer technique 1 : scaled dot product\n        Q *= Q.size()[-1] ** -0.5\n\n\n        if self.self_Attention_Decoder:\n            memory = memory2\n\n        # memory transforme [mb(=1), length, dim] -> [length, dim]\n        if len(memory.size()) > 2:\n            memory = memory[0]\n        \n        # get Key\n        K = self.K_dense(memory)\n        #K = self.K_BN(K)\n        K = self.K_dense_dropout(K)\n        K = K.view(1, -1, self.attention_dkv)\n\n\n        # one head -> Multi head\n        K = K.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        K = K.permute([2, 1, 0])\n\n        # get Query and Key (= attention logits)\n        QK = torch.bmm(Q, K)\n\n\n        # Transformer technique 2 : masking attention weight\n        any_zero = memory.sum(dim=1)\n        pad_mask = torch.ones([1, 1, self.max_length]).to(device)\n        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n\n        _, _, QK_length = QK.size()\n        pad_mask = pad_mask[:, :, :QK_length]\n\n\n        QK += pad_mask * sys.float_info.min\n        \n        # get attention weight\n        attention_weights = F.softmax(QK, dim=-1)\n        \n        # get Value\n        V = self.V_dense(memory)\n        #V = self.V_BN(V)\n        V = self.V_dense_dropout(V)\n        V = V.view(1, -1, self.attention_dkv)\n\n        # one head -> Multi head\n        V = V.view(-1, self.attention_dkv // self.head_N, self.head_N)\n        V = V.permute(2, 0, 1)\n        \n        # Attetion x Value\n        attention_feature = torch.bmm(attention_weights, V)\n\n        # Multi head -> one head\n        attention_feature = attention_feature.permute(1, 2, 0)\n        attention_feature = attention_feature.contiguous().view(1, 1, -1)\n\n        # attention + Input\n        #attention_x = torch.cat([_input, attention_feature], dim=-1)\n        #print(attention_x.size())\n        # apply attention dense\n        #attention_output = self.attention_dense(attention_x)\n        #attention_output = self.attention_dropout(attention_output)\n\n        attention_output = self.dense_output(attention_feature)\n        attention_output = self.dropout_output(attention_output)\n        return attention_output\n\n\nclass FeedForwardNetwork(torch.nn.Module):\n    def __init__(self, d_ff, d_model, dropout_p=0.1):\n        super(FeedForwardNetwork, self).__init__()\n\n        self.module = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_p),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n    def forward(self, x, memory_encoder, decoder):\n        x = self.module(x)\n        return x\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n\n    def forward(self, x):\n        mb, sequence_length, dimension = x.size()\n        positionalEncodingFeature = np.zeros([mb, sequence_length, dimension], dtype=np.float32)\n\n        position_index = np.arange(sequence_length).repeat(dimension).reshape(-1, dimension)\n        dimension_index = np.tile(np.arange(dimension), [sequence_length, 1])\n\n        positionalEncodingFeature[:, :, 0::2] = np.sin(position_index[:, 0::2] / (10000 ** (2 * dimension_index[:, 0::2] / dimension)))\n        positionalEncodingFeature[:, :, 1::2] = np.cos(position_index[:, 1::2] / (10000 ** (2 * dimension_index[:, 1::2] / dimension)))\n\n        positionalEncodingFeature = torch.tensor(positionalEncodingFeature).to(device)\n\n        x += positionalEncodingFeature\n\n        return x\n\n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(0.9, 0.98))\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(0.9, 0.98))\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                xs_float = torch.tensor(x_pairs[mb_index][0], dtype=torch.float).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                self_memory = decoder_xs\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di].view(1, -1)\n                        #self_memory[di] = decoder_xs.clone().detach()[0]\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs, self_memory)\n                        \n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach().view(1, -1)\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n                        self_memory = torch.cat([self_memory, decoder_xs])\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = Encoder(\n        voca_num, \n        hidden_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Encoder_attention_time,\n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device) \n\n    decoder = Decoder(\n        hidden_dim,\n        voca_num, \n        RNN_dim,\n        attention_dkv=Attention_dkv,\n        dropout_p=dropout_p,\n        num_layers=num_layers,\n        attention_time=Decoder_attention_time, \n        use_Source_Target_Attention=use_Source_Target_Attention,\n        use_Self_Attention=use_Encoder_Self_Attention,\n        MultiHead_Attention_N=MultiHead_Attention_N,\n        use_FFNetwork=use_FeedForwardNetwork,\n        FeedForwardNetwork_dff=FeedForwardNetwork_dff,\n        use_PositionalEncoding=use_PositionalEncoding\n        ).to(device)\n\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device).view(-1, 1)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, RNN_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden, xs)\n                encoder_outputs[ei] = encoder_output[0, 0]\n\n            decoder_x = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n\n            self_memory = decoder_x\n\n            for di in range(MAX_LENGTH):\n                decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_x, decoder_hidden, encoder_outputs, self_memory)\n        \n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_ys.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_ys, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_x = topi.squeeze().detach().view(1, -1)\n\n                self_memory = torch.cat([self_memory, decoder_x])\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device).view(-1, 1)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/bdlstm_pytorch.py,15,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = False\ntorch.manual_seed(0)\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        base = 128\n        self.h1 = torch.nn.LSTM(num_classes, base, batch_first=True, bidirectional=True)\n        self.fc_out = torch.nn.Linear(base*2, num_classes)\n        \n    def forward(self, x):\n        x, hn = self.h1(x)\n        x = x[:, -1]\n        x = self.fc_out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    xs, ts = data_load()\n    print(xs.shape)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    with torch.no_grad():\n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n            \n            x = np.expand_dims(np.array(x), axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n\n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n            \n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/gru_pytorch.py,15,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = False\ntorch.manual_seed(0)\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        base = 256\n        self.h1 = torch.nn.GRU(num_classes, base, batch_first=True, bidirectional=True)\n        self.fc_out = torch.nn.Linear(base*2, num_classes)\n        \n    def forward(self, x):\n        x, hn = self.h1(x)\n        x = x[:, -1]\n        x = self.fc_out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    xs, ts = data_load()\n    print(xs.shape)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    with torch.no_grad():\n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n            \n            x = np.expand_dims(np.array(x), axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n\n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n            \n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/lstm_pytorch.py,15,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = False\ntorch.manual_seed(0)\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        base = 128\n        self.h1 = torch.nn.LSTM(num_classes, base, batch_first=True)\n        self.fc_out = torch.nn.Linear(base, num_classes)\n        \n    def forward(self, x):\n        x, hn = self.h1(x)\n        x = x[:, -1]\n        x = self.fc_out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    xs, ts = data_load()\n    print(xs.shape)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    with torch.no_grad():\n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n            \n            x = np.expand_dims(np.array(x), axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n\n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n            \n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/onehot.py,0,"b'_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82""\nchars = [c for c in _chars]\n\ndef data_load():\n    fname = \'sandwitchman.txt\'\n\n    xs = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            l = l.strip()\n            for c in l:\n                x = [0 for _ in range(len(chars))]\n                x[chars.index(c)] = 1\n                xs.append(x)\n\n    return xs\n\nprint(data_load())\n'"
Scripts_NLP/old/scripts_pytorch/rnn_pytorch.py,15,"b'import torch\nimport torch.nn.functional as F\nimport argparse\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport os\n\nGPU = False\ntorch.manual_seed(0)\nn_gram = 10\n\n_chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x83\xbc\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99\xef\xbc\x90\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe3\x80\x82@#""\nchars = [c for c in _chars]\nnum_classes = len(chars)\n\nclass Mynet(torch.nn.Module):\n    def __init__(self):\n        super(Mynet, self).__init__()\n        base = 128\n        self.h1 = torch.nn.RNN(num_classes, base, batch_first=True)\n        self.fc_out = torch.nn.Linear(base, num_classes)\n        \n    def forward(self, x):\n        x, hn = self.h1(x)\n        x = x[:, -1]\n        x = self.fc_out(x)\n        return x\n    \ndef data_load():\n    fname = \'sandwitchman.txt\'\n    xs = []\n    ts = []\n    txt = \'\'\n    for _ in range(n_gram):\n        txt += \'@\'\n    onehots = []\n    \n    with open(fname, \'r\') as f:\n        for l in f.readlines():\n            txt += l.strip() + \'#\'\n        txt = txt[:-1] + \'@\'\n\n        for c in txt:\n            onehot = [0 for _ in range(num_classes)]\n            onehot[chars.index(c)] = 1\n            onehots.append(onehot)\n        \n        for i in range(len(txt) - n_gram - 1):\n            xs.append(onehots[i:i+n_gram])\n            ts.append(chars.index(txt[i+n_gram]))\n\n    xs = np.array(xs)\n    ts = np.array(ts)\n    \n    return xs, ts\n\n\n# train\ndef train():\n    # GPU\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n\n    # model\n    model = Mynet().to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    model.train()\n\n    xs, ts = data_load()\n    print(xs.shape)\n\n    # training\n    mb = 128\n    mbi = 0\n    train_ind = np.arange(len(xs))\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n    \n    for i in range(500):\n        if mbi + mb > len(xs):\n            mb_ind = train_ind[mbi:]\n            np.random.shuffle(train_ind)\n            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n        else:\n            mb_ind = train_ind[mbi: mbi+mb]\n            mbi += mb\n\n        x = torch.tensor(xs[mb_ind], dtype=torch.float).to(device)\n        t = torch.tensor(ts[mb_ind], dtype=torch.long).to(device)\n\n        opt.zero_grad()\n        y = model(x)\n        y = F.log_softmax(y, dim=1)\n\n        loss = torch.nn.CrossEntropyLoss()(y, t)\n        loss.backward()\n        opt.step()\n    \n        pred = y.argmax(dim=1, keepdim=True)\n        acc = pred.eq(t.view_as(pred)).sum().item() / mb\n        \n        print(""iter >>"", i+1, \',loss >>\', loss.item(), \',accuracy >>\', acc)\n\n    torch.save(model.state_dict(), \'cnn.pt\')\n\n# test\ndef test():\n    device = torch.device(""cuda"" if GPU else ""cpu"")\n    model = Mynet().to(device)\n    model.eval()\n    model.load_state_dict(torch.load(\'cnn.pt\'))\n\n    def decode(x):\n        return chars[x.argmax()]\n    \n    gens = \'\'\n\n    for _ in range(n_gram):\n        gens += \'@\'\n\n    pred = 0\n    count = 0\n        \n    with torch.no_grad():\n        while pred != \'@\' and count < 1000:\n            in_txt = gens[-n_gram:]\n            x = []\n            for _in in in_txt:\n                _x = [0 for _ in range(num_classes)]\n                _x[chars.index(_in)] = 1\n                x.append(_x)\n            \n            x = np.expand_dims(np.array(x), axis=0)\n            x = torch.tensor(x, dtype=torch.float).to(device)\n            \n            pred = model(x)\n            pred = F.softmax(pred, dim=1).detach().cpu().numpy()[0]\n\n            # sample random from probability distribution\n            ind = np.random.choice(num_classes, 1, p=pred)\n            \n            pred = chars[ind[0]]\n            gens += pred\n            count += 1\n\n    # pose process\n    gens = gens.replace(\'#\', os.linesep).replace(\'@\', \'\')\n        \n    print(\'--\\ngenerated\')\n    print(gens)\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test()\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/seq2seq_pytorch.py,50,"b'\nimport numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nMAX_LENGTH = 100\nteacher_forcing_ratio = 0.5\nmb = 1\nAttention = False\nopt = ""SGD"" # SGD, Adam\n\n# lr, iteration\ntrain_factors = [[0.001, 100000]] \n\nnext_word_mode = ""prob"" # prob, argmax\n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\n\nclass EncoderRNN(torch.nn.Module):\n    def __init__(self, input_size):\n        super(EncoderRNN, self).__init__()\n\n        self.embedding = torch.nn.Embedding(input_size, hidden_dim)\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim)\n        #self.gru2 = torch.nn.GRU(hidden_dim, hidden_dim)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        #output, 2 = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, hidden_dim, device=device)\n\n\nclass DecoderRNN(torch.nn.Module):\n    def __init__(self, output_size):\n        super(DecoderRNN, self).__init__()\n\n        self.embedding = torch.nn.Embedding(output_size, hidden_dim)\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim)\n        #self.gru2 = torch.nn.GRU(hidden_dim, hidden_dim)\n        self.out = torch.nn.Linear(hidden_dim, output_size)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        #output, hidden = self.gru(output, hidden2)\n        output = F.softmax(self.out(output[0]), dim=1)\n        return output, hidden\n    \n    def initHidden(self):\n        return torch.zeros(1, 1, hidden_dim, device=device)\n\n\nclass AttnDecoderRNN(torch.nn.Module):\n    def __init__(self,  output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = torch.nn.Embedding(self.output_size, hidden_dim)\n        self.attn = torch.nn.Linear(hidden_dim * 2, self.max_length)\n        self.attn_combine = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n        self.dropout = torch.nn.Dropout(self.dropout_p)\n        self.gru = torch.nn.GRU(hidden_dim, hidden_dim)\n        self.out = torch.nn.Linear(hidden_dim, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        # Query\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        # Query + Key\n        QK = torch.cat((embedded[0], hidden[0]), 1)\n\n        # Query + Key -> Attention mask\n        attn_weights = F.softmax(self.attn(QK), dim=1)\n\n        # Attention mask x Value -> Attention\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n\n        # Query + Attention\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n        output = F.relu(output)\n\n        # GRU \n        output, hidden = self.gru(output, hidden)\n\n        # Output (Class predict)\n        output = F.softmax(self.out(output[0]), dim=1)\n\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, hidden_dim, device=device)\n    \n\n    \ndef data_load():\n    sentence_pairs = []\n\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n\n    voca = [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""] + [c for c in _chars]\n\n    for file_path in glob(""./sandwitchman_*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            lines_before = lines\n            lines_after = lines[1:] + [""<FINISH>""]\n            sentence_pairs += [[s1, s2] for (s1, s2) in zip(lines_before, lines_after)]\n\n    voca.sort()\n\n    print(""sentence pairs num:"", len(sentence_pairs))\n    \n    sentence_pairs_index = []\n\n    for s1, s2 in sentence_pairs:\n        s1_parse = mecab.parse(s1).strip().split("" "")\n        if s2 == ""<FINISH>"":\n            s2_parse = [""<BOS>"", s2, ""<EOS>""]\n        else:\n            s2_parse = [""<BOS>""] + mecab.parse(s2).strip().split("" "") + [""<EOS>""]\n        \n        s1_index = [voca.index(x) for x in s1_parse]\n        s2_index = [voca.index(x) for x in s2_parse]\n        \n        sentence_pairs_index += [[s1_index, s2_index]]\n\n\n    #sentence_pairs_index = np.array(sentence_pairs_index)\n\n    return voca, sentence_pairs_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_pairs = data_load()\n    voca_num = len(voca)\n\n    pickle.dump(voca, open(""vocabrary.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    encoder = EncoderRNN(voca_num).to(device)\n    if Attention:\n        decoder = AttnDecoderRNN(voca_num, dropout_p=0.1).to(device)\n    else:\n        decoder = DecoderRNN(voca_num).to(device)\n    \n\n    #encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n    #decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n\n    mbi = 0\n\n    data_num = len(sentence_pairs)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    loss_fn = torch.nn.NLLLoss()\n    \n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        if opt == ""SGD"":\n            encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr, momentum=0.9)\n            decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n            decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        \n        for ite in range(ite):\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            x_pairs = [sentence_pairs[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            for mb_index in range(mb):\n                xs = torch.tensor(x_pairs[mb_index][0]).to(device).view(-1, 1)\n                ts = torch.tensor(x_pairs[mb_index][1]).to(device).view(-1, 1)\n            \n                encoder_hidden = encoder.initHidden()\n                #encoder_hidden2 = encoder.initHidden()\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n            \n                xs_length = xs.size()[0]\n                ts_length = ts.size()[0]\n\n                total_len += ts_length\n\n                encoder_outputs = torch.zeros(MAX_LENGTH, hidden_dim).to(device)\n\n                for ei in range(xs_length):\n                    encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden)\n                    encoder_outputs[ei] = encoder_output[0, 0]\n\n                decoder_xs = torch.tensor([[voca.index(""<BOS>"")]]).to(device)\n            \n                decoder_hidden = encoder_hidden\n                \n                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n                if use_teacher_forcing:\n                    # Teacher forcing: Feed the target (ground-truth word) as the next input\n                    for di in range(ts_length):\n                        if Attention:\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs)\n                        else:\n                            decoder_ys, decoder_hidden = decoder(decoder_xs, decoder_hidden)\n        \n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n                            \n                        # set next decoder\'s input (ground-truth label)\n                        decoder_xs = ts[di]\n\n                else:\n                    # Without teacher forcing: use its own predictions as the next input\n                    for di in range(ts_length):\n                        if Attention:\n                            decoder_ys, decoder_hidden, decoder_attention = decoder(decoder_xs, decoder_hidden, encoder_outputs)\n                        else:\n                            decoder_ys, decoder_hidden = decoder(decoder_xs, decoder_hidden)\n\n                        # Select top 1 word with highest probability\n                        #topv, topi = decoder_ys.topk(1)\n                        # choice argmax\n                        if next_word_mode == ""argmax"":\n                            topv, topi = decoder_ys.data.topk(1)\n\n                        elif next_word_mode == ""prob"":\n                            topi = torch.multinomial(decoder_ys, 1)\n                        \n                        # set next input for decoder training\n                        decoder_xs = topi.squeeze().detach()\n\n                        # add loss\n                        loss += loss_fn(torch.log(decoder_ys), ts[di])\n\n                        # count accuracy\n                        if decoder_ys.argmax() == ts[di]:\n                            accuracy += 1.\n\n                        if decoder_xs.item() == voca.index(""<EOS>""):\n                            break\n\n            loss.backward()\n\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            loss = loss.item() / ts_length\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(encoder.state_dict(), \'encoder.pt\')\n    torch.save(decoder.state_dict(), \'decoder.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x81\xa9\xe3\x81\x86\xe3\x82\x82\xe3\x83\xbc\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3\xe3\x81\xa7\xe3\x81\x99""):\n\n    voca = pickle.load(open(""vocabrary.bn"", ""rb""))\n    voca_num = len(voca)\n    \n    # load trained model\n    encoder = EncoderRNN(voca_num).to(device)\n    if Attention:\n        decoder = AttnDecoderRNN(voca_num, dropout_p=0.1).to(device)\n    else:\n        decoder = DecoderRNN(voca_num).to(device)\n    \n    encoder.load_state_dict(torch.load(\'encoder.pt\'))\n    decoder.load_state_dict(torch.load(\'decoder.pt\'))\n\n    with torch.no_grad():\n        xs = []\n        for x in mecab.parse(first_sentence).strip().split("" ""):\n            if x in voca:\n                xs += [voca.index(x)]\n            else:\n                xs += [voca.index(""<UNKNOWN>"")]\n\n        xs = torch.tensor(xs, dtype=torch.long).to(device)\n\n        count = 0\n\n        print(""A:"", first_sentence)\n\n        while count < 100:\n            input_length = xs.size()[0]\n            encoder_hidden = encoder.initHidden()\n\n            encoder_outputs = torch.zeros(MAX_LENGTH, hidden_dim).to(device)\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(xs[ei], encoder_hidden)\n                encoder_outputs[ei] += encoder_output[0, 0]\n\n            decoder_input = torch.tensor([[voca.index(""<BOS>"")]], device=device)  # SOS\n\n            decoder_hidden = encoder_hidden\n            decoded_words = []\n            decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n\n            for di in range(MAX_LENGTH):\n                if Attention:\n                    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n                    decoder_attentions[di] = decoder_attention.data\n                else:\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n\n                # choice argmax\n                if next_word_mode == ""argmax"":\n                    topv, topi = decoder_output.data.topk(1)\n\n                elif next_word_mode == ""prob"":\n                    topi = torch.multinomial(decoder_output, 1)\n\n                if topi.item() == voca.index(""<EOS>""):\n                    decoded_words.append(\'<EOS>\')\n                    break\n                elif topi.item() == voca.index(""<FINISH>""):\n                    break\n                else:\n                    decoded_words.append(voca[topi.item()])\n\n                decoder_input = topi.squeeze().detach()\n\n            decoded_words = decoded_words[1:-1]\n\n            xs = [voca.index(x) for x in decoded_words]  \n            xs = torch.tensor(xs).to(device)\n\n            sentence = """".join(decoded_words)\n\n            if ""<FINISH>"" in sentence:\n                break\n\n            for key in [""<BOS>"", ""<EOS>"", ""<FINISH>"", ""<UNKNOWN>""]:\n                sentence = sentence.replace(key, """")\n            \n            attention = decoder_attentions[:di + 1]\n\n            \n            \n            if count % 2 == 0:\n                print(""B:"", sentence)\n            else:\n                print(""A:"", sentence)\n\n            count += 1\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x81\xa1\xe3\x82\x87\xe3\x81\xa3\xe3\x81\xa8\xe4\xbd\x95\xe8\xa8\x80\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe5\x88\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x81\xaa\xe3\x81\x84"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
Scripts_NLP/old/scripts_pytorch/word2vec_pytorch.py,21,"b'\nimport numpy as np\nimport argparse\nfrom glob import glob\nfrom copy import copy\nimport random\nimport pickle\n\n# network\nimport torch\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# GPU config\nGPU = False\ndevice = torch.device(""cuda"" if GPU else ""cpu"")\n\nhidden_dim = 128\nmb = 32\nopt = ""Adam"" # SGD, Adam\nC = 3  # word2vec window size satisfying C >= 1\nx_length = 1 + C * 2  # training label length\nTopN = 10 # display N similar word in test\n\n# lr, iteration\ntrain_factors = [[0.01, 1000]] \n\nimport MeCab\nmecab = MeCab.Tagger(""-Owakati"")\n\n\nclass Word2Vec(torch.nn.Module):\n    def __init__(self, input_size, dim=512):\n        super(Word2Vec, self).__init__()\n\n        self.embed = torch.nn.Linear(input_size, dim)\n        self.outs = []\n        for _ in range(C * 2):\n            self.outs.append(torch.nn.Linear(dim, input_size))\n        self.out = torch.nn.Linear(dim, input_size)\n\n    def forward(self, input):\n        embed = self.embed(input)\n\n        xs = []\n        for i in range(C * 2):\n            x = self.outs[i](embed)\n            x = F.softmax(x, dim=1)\n            xs.append(x)\n        #x = self.out(embed)\n        #x = F.softmax(x, dim=1)\n        return xs\n\n    def get_vec(self, input):\n        return self.embed(input)\n\n\n    \ndef data_load():\n    sentences = []\n    # get vocabrary\n    _chars = ""\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x8a\xe3\x81\x88\xe3\x81\x8b\xe3\x81\x8d\xe3\x81\x8f\xe3\x81\x91\xe3\x81\x93\xe3\x81\x95\xe3\x81\x97\xe3\x81\x99\xe3\x81\x9b\xe3\x81\x9d\xe3\x81\x9f\xe3\x81\xa1\xe3\x81\xa4\xe3\x81\xa6\xe3\x81\xa8\xe3\x81\xaa\xe3\x81\xab\xe3\x81\xac\xe3\x81\xad\xe3\x81\xae\xe3\x81\xaf\xe3\x81\xb2\xe3\x81\xb5\xe3\x81\xb8\xe3\x81\xbb\xe3\x81\xbe\xe3\x81\xbf\xe3\x82\x80\xe3\x82\x81\xe3\x82\x82\xe3\x82\x84\xe3\x82\x86\xe3\x82\x88\xe3\x82\x89\xe3\x82\x8a\xe3\x82\x8b\xe3\x82\x8c\xe3\x82\x8d\xe3\x82\x8f\xe3\x82\x92\xe3\x82\x93\xe3\x81\x8c\xe3\x81\x8e\xe3\x81\x90\xe3\x81\x92\xe3\x81\x94\xe3\x81\x96\xe3\x81\x98\xe3\x81\x9a\xe3\x81\x9c\xe3\x81\x9e\xe3\x81\xa0\xe3\x81\xa2\xe3\x81\xa5\xe3\x81\xa7\xe3\x81\xa9\xe3\x81\xb0\xe3\x81\xb3\xe3\x81\xb6\xe3\x81\xb9\xe3\x81\xbc\xe3\x81\xb1\xe3\x81\xb4\xe3\x81\xb7\xe3\x81\xba\xe3\x81\xbd\xe3\x81\x81\xe3\x81\x83\xe3\x81\x85\xe3\x81\x87\xe3\x81\x89\xe3\x82\x83\xe3\x82\x85\xe3\x82\x87\xe3\x81\xa3\xe3\x82\xa2\xe3\x82\xa4\xe3\x82\xa6\xe3\x82\xa8\xe3\x82\xaa\xe3\x82\xab\xe3\x82\xad\xe3\x82\xaf\xe3\x82\xb1\xe3\x82\xb3\xe3\x82\xb5\xe3\x82\xb7\xe3\x82\xb9\xe3\x82\xbb\xe3\x82\xbd\xe3\x82\xbf\xe3\x83\x81\xe3\x83\x84\xe3\x83\x86\xe3\x83\x88\xe3\x83\x8a\xe3\x83\x8b\xe3\x83\x8c\xe3\x83\x8d\xe3\x83\x8e\xe3\x83\x8f\xe3\x83\x92\xe3\x83\x95\xe3\x83\x98\xe3\x83\x9b\xe3\x83\x9e\xe3\x83\x9f\xe3\x83\xa0\xe3\x83\xa1\xe3\x83\xa2\xe3\x83\xa4\xe3\x83\xa6\xe3\x83\xa8\xe3\x83\xa9\xe3\x83\xaa\xe3\x83\xab\xe3\x83\xac\xe3\x83\xad\xe3\x83\xaf\xe3\x83\xb2\xe3\x83\xb3\xe3\x82\xac\xe3\x82\xae\xe3\x82\xb0\xe3\x82\xb2\xe3\x82\xb4\xe3\x82\xb6\xe3\x82\xb8\xe3\x82\xba\xe3\x82\xbc\xe3\x82\xbe\xe3\x83\x80\xe3\x83\x82\xe3\x83\x85\xe3\x83\x87\xe3\x83\x89\xe3\x83\x90\xe3\x83\x93\xe3\x83\x96\xe3\x83\x99\xe3\x83\x9c\xe3\x83\x91\xe3\x83\x94\xe3\x83\x97\xe3\x83\x9a\xe3\x83\x9d\xe3\x82\xa1\xe3\x82\xa3\xe3\x82\xa5\xe3\x82\xa7\xe3\x82\xa9\xe3\x83\xa3\xe3\x83\xa5\xe3\x83\xa7\xe3\x83\x83\xe3\x83\xbc\xe3\x80\x81\xe3\x80\x82\xe3\x80\x8c\xe3\x80\x8d1234567890!?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.@#""\n    voca = [""<BRANK>""] + [c for c in _chars]\n\n    # each file\n    for file_path in glob(""./sandwitchman_*.txt""):\n        print(""read:"", file_path)\n        with open(file_path, \'r\') as f:\n            # get line in 1 file\n            lines = [x.strip() for x in f.read().strip().split(""\\n"")]\n        \n            # get vocabrary from mecab parsed\n            for line in lines:\n                voca = list(set(voca) | set(mecab.parse(line).strip().split("" "")))\n\n            # add sentences\n            sentences += lines\n\n    # vocabrary sort\n    voca.sort()\n\n    # display sentence number\n    print(""sentence pairs num:"", len(sentences))\n    \n    sentence_index = []\n\n    # each sentence\n    for s in sentences:\n        # mecab parse\n        s_parse = mecab.parse(s).strip().split("" "")\n\n        # add brank label first and end\n        _s = [""<BRANK>""] * C + s_parse + [""<BRANK>""] * C\n\n        # make training pairs\n        for i in range(C, len(s_parse) + C):\n            s_index = [voca.index(x) for x in _s[i-C : i+C+1]]\n            sentence_index += [s_index]\n\n    return voca, sentence_index\n\n\n# train\ndef train():\n    # data load\n    voca, sentence_index = data_load()\n    voca_num = len(voca)\n\n    # write vocabrary lists\n    pickle.dump(voca, open(""vocabrary_word2vec.bn"", ""wb""))\n\n    print(""vocabrary num:"", voca_num)\n    print(""e.x."", voca[:5])\n    \n    # model\n    model = Word2Vec(voca_num, dim=hidden_dim).to(device)\n\n    # minibatch index\n    mbi = 0\n\n    data_num = len(sentence_index)\n    train_ind = np.arange(data_num)\n    np.random.seed(0)\n    np.random.shuffle(train_ind)\n\n    # loss function\n    loss_fn = torch.nn.NLLLoss()\n    \n    # each learning rate and iteration\n    for lr, ite in train_factors:\n        print(""lr"", lr, "" ite"", ite)\n\n        # optimizer\n        if opt == ""SGD"":\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        elif opt == ""Adam"":\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        else:\n            raise Exception(""invalid optimizer:"", opt)\n        \n        # each iteration\n        for ite in range(ite):\n            # get minibatch index\n            if mbi + mb > data_num:\n                mb_ind = copy(train_ind[mbi:])\n                np.random.shuffle(train_ind)\n                mb_ind = np.hstack((mb_ind, train_ind[:(mb-(data_num-mbi))]))\n            else:\n                mb_ind = train_ind[mbi: mbi+mb]\n                mbi += mb\n\n            # get minibatch\n            X_inds = [sentence_index[i] for i in mb_ind]\n\n            loss = 0\n            accuracy = 0.\n            total_len = 0\n\n            # each data of minibatch\n            for mb_index in range(mb):\n                # 1 data of minibatch\n                Xs = np.array(X_inds[mb_index]).reshape([-1, 1])\n\n                input_X = np.zeros([1, voca_num])\n                input_X[:, Xs[C]] = 1\n                input_X = torch.tensor(input_X, dtype=torch.float).to(device)\n                \n                # reset graph\n                optimizer.zero_grad()\n            \n                # data length\n                total_len += x_length\n\n                # forward network\n                ys = model(input_X)\n\n                # target label index\n                t_inds = [_i for _i in range(x_length) if _i != C]\n\n                # each target label\n                for i, y in zip(t_inds, ys):\n                    # target label\n                    t = torch.tensor(Xs[i], dtype=torch.long).to(device)\n\n                    # get loss\n                    loss += loss_fn(torch.log(y), t)\n\n                    # count accuracy\n                    if y.argmax() == t:\n                        accuracy += 1\n\n                """"""\n                # each target label\n                for i in range(x_length):\n                    # forward network\n                    y = model(input_X)\n\n                    # target label\n                    t = torch.tensor(Xs[i], dtype=torch.long).to(device)\n                    #t = torch.tensor(Xs[i], dtype=torch.long).to(device).view(-1, voca_num)\n\n                    # get loss\n                    loss += loss_fn(torch.log(y), t)\n\n                    # count accuracy\n                    if y.argmax() == t:\n                        accuracy += 1\n                """"""\n\n            # loss backward\n            loss.backward()\n\n            # update weight\n            optimizer.step()\n            \n            # get loss\n            loss = loss.item() / total_len\n            accuracy = accuracy / total_len\n\n            if (ite + 1) % 10 == 0:\n                print(""iter :"", ite+1, "",loss >>:"", loss, ""accuracy:"", accuracy)\n\n    torch.save(model.state_dict(), \'word2vec.pt\')\n    \n\n# test\ndef test(first_sentence=""\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3""):\n    # get vocabrary\n    voca = pickle.load(open(""vocabrary_word2vec.bn"", ""rb""))\n    voca_num = len(voca)\n\n    print(""vocabrary num:"", voca_num)\n    \n    # load trained model\n    model = Word2Vec(voca_num, dim=hidden_dim).to(device)\n    model.load_state_dict(torch.load(\'word2vec.pt\'))\n\n    xs = []\n\n    # if word not found in vocabrary\n    if first_sentence not in voca:\n        raise Exception(""not found word:"", first_sentence)\n\n    # get vector features of vocabrary\n    mb = 32\n\n    # feature vectors library\n    features = np.ndarray([0, hidden_dim])\n\n    with torch.no_grad():\n        for i in range(0, voca_num, mb):\n            # get minibatch\n            _mb = min(mb, voca_num - i)\n\n            # one hot vector\n            input_X = torch.zeros([_mb, voca_num], dtype=torch.float).to(device)\n            input_X[np.arange(_mb), np.arange(i, min(i + mb, voca_num))] = 1\n\n            # get vector feature\n            vecs = model.get_vec(input_X)\n            vecs = vecs.detach().cpu().numpy()\n\n            # add feature vectors\n            features = np.vstack([features, vecs])\n\n    print(features.shape)\n\n    # make one hot input X\n    input_X = torch.zeros([1, voca_num], dtype=torch.float).to(device)\n    input_X[:, voca.index(first_sentence)] = 1\n\n    # get target feature vector\n    vec = model.get_vec(input_X)\n    vec = vec.detach().cpu().numpy()[0]\n\n    # get similarity\n    #similarity_scores = np.sum(np.abs(features - vec) ** 2, axis=1)\n\n    # get cosine similarity\n    Norm_A = np.linalg.norm(features, axis=1)\n    Norm_B = np.linalg.norm(vec)\n\n    similarity_scores = np.dot(features, vec) / Norm_A / Norm_B\n\n    # get min index,,   Skip first because it is target input word\n    min_inds = similarity_scores.argsort()[::-1]\n\n    print(""Target:"", first_sentence)\n\n    # print\n    for i in range(TopN):\n        ind = min_inds[i]\n        print(""top{}: {} ({:.4f})"".format(i + 1, voca[ind], similarity_scores[ind]))\n\n\n    \n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\'CNN implemented with Keras\')\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\')\n    parser.add_argument(\'--test\', dest=\'test\', action=\'store_true\')\n    parser.add_argument(\'--input\', dest=\'input\', default=""\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x89\xe3\x82\xa6\xe3\x82\xa3\xe3\x83\x83\xe3\x83\x81\xe3\x83\x9e\xe3\x83\xb3"", type=str)\n    args = parser.parse_args()\n    return args\n\n# main\nif __name__ == \'__main__\':\n    args = arg_parse()\n\n    if args.train:\n        train()\n    if args.test:\n        test(args.input)\n\n    if not (args.train or args.test):\n        print(""please select train or test flag"")\n        print(""train: python main.py --train"")\n        print(""test:  python main.py --test"")\n        print(""both:  python main.py --train --test"")\n'"
