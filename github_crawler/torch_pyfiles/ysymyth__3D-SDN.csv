file_path,api_count,code
datasets/vkitti_utils.py,0,"b'import os\n\nworldIds = [\'0001\', \'0002\', \'0006\', \'0018\', \'0020\']\nsceneIds = [\'15-deg-left\', \'15-deg-right\', \'30-deg-left\', \'30-deg-right\', \'clone\', \'fog\',\n            \'morning\', \'overcast\', \'rain\', \'sunset\']\nworldSizes = [446, 232, 269, 338, 836]  # 0-446, including 446\ncategory = [\'Misc\', \'Building\', \'Car\', \'GuardRail\', \'Pole\', \'Road\', \'Sky\', \'Terrain\',\n            \'TrafficLight\', \'TrafficSign\', \'Tree\', \'Truck\', \'Van\', \'Vegetation\']\n\n\ndef get_tables(opt, datadir):\n    """"""\n    Get the mapping from (worldId, sceneId, rgb) to the semantic/instance ID.\n    The instance ID is uniquely assigned to each car and van in the dataset.\n    :param opt: \'segm\' or \'inst\'\n    :param datadir: the dataset root\n    :return:\n    """"""\n    global_obj_id = 0\n    table_inst = {}\n    table_segm = {}\n    for worldId in worldIds:\n        for sceneId in sceneIds:\n            with open(os.path.join(datadir, ""vkitti_1.3.1_scenegt"",\n                                   ""%s_%s_scenegt_rgb_encoding.txt"" % (worldId, sceneId)), \'r\') as fin:\n                first_line = True\n                for line in fin:\n                    if first_line:\n                        first_line = False\n                    else:\n                        name, r, g, b = line.split(\' \')\n                        r, g, b = int(r), int(g), int(b)\n                        if name.find(\':\') == -1:\n                            table_segm[(worldId, sceneId, r, g, b)] = category.index(name)\n                            table_inst[(worldId, sceneId, r, g, b)] = category.index(name)\n                        else:\n                            global_obj_id += 1\n                            table_segm[(worldId, sceneId, r, g, b)] = category.index(name.split(\':\')[0])\n                            table_inst[(worldId, sceneId, r, g, b)] = 5000 * category.index(\n                                name.split(\':\')[0]) + global_obj_id\n\n    return table_segm if opt == \'segm\' else table_inst\n\n\ndef get_lists(opt):\n    """"""\n    Get the training/testing split for Virtual KITTI.\n    :param opt: \'train\' or \'test\'\n    :return:\n    """"""\n    splitRanges = {\'train\': [range(0, 356),     range(0, 185),      range(69, 270),     range(0, 270),      range(167, 837)],\n                   \'test\':  [range(356, 447),   range(185, 233),    range(0, 69),       range(270, 339),    range(0, 167)],\n                   \'all\':   [range(0, 447),     range(0, 233),      range(0, 270),     range(0, 339),      range(0, 837)]}\n    _list = []\n    for worldId in worldIds:\n        for sceneId in sceneIds:\n            for imgId in splitRanges[opt][worldIds.index(worldId)]:\n                _list += [\'%s/%s/%05d.png\' % (worldId, sceneId, imgId)]\n    return _list\n\n'"
semantic/models.py,8,"b'import torch\nimport torch.nn as nn\ntry:\n    from . import resnet\n    from .lib.nn import SynchronizedBatchNorm2d\nexcept Exception:\n    import resnet\n    from lib.nn import SynchronizedBatchNorm2d\n\n\nclass SegmentationModuleBase(nn.Module):\n    def __init__(self):\n        super(SegmentationModuleBase, self).__init__()\n\n    def pixel_acc(self, pred, label):\n        _, preds = torch.max(pred, dim=1)\n        valid = (label >= 0).long()\n        acc_sum = torch.sum(valid * (preds == label).long())\n        pixel_sum = torch.sum(valid)\n        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n        return acc\n\n\nclass SegmentationModule(SegmentationModuleBase):\n    def __init__(self, net_enc, net_dec, crit, deep_sup_scale=None):\n        super(SegmentationModule, self).__init__()\n        self.encoder = net_enc\n        self.decoder = net_dec\n        self.crit = crit\n        self.deep_sup_scale = deep_sup_scale\n\n    def forward(self, feed_dict, *, segSize=None):\n        if segSize is None:  # training\n            if self.deep_sup_scale is not None:  # use deep supervision technique\n                (pred, pred_deepsup) = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True))\n            else:\n                pred = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=False))\n\n            loss = self.crit(pred, feed_dict[\'seg_label\'])\n            if self.deep_sup_scale is not None:\n                loss_deepsup = self.crit(pred_deepsup, feed_dict[\'seg_label\'])\n                loss = loss + loss_deepsup * self.deep_sup_scale\n\n            acc = self.pixel_acc(pred, feed_dict[\'seg_label\'])\n            return loss, acc\n        else:  # inference\n            pred = self.decoder(self.encoder(feed_dict[\'img_data\']), segSize=segSize)\n            return pred\n\n\ndef conv3x3(in_planes, out_planes, stride=1, has_bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=has_bias)\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n        conv3x3(in_planes, out_planes, stride),\n        SynchronizedBatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass ModelBuilder():\n    # custom weights initialization\n    \'\'\'\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            m.weight.data.normal_(0.0, 0.001)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n        elif classname.find(\'Linear\') != -1:\n            m.weight.data.normal_(0.0, 0.0001)\n    \'\'\'\n\n    def build_encoder(self, arch=\'resnet50_dilated8\', fc_dim=512, weights=\'\'):\n        pretrained = True if len(weights) == 0 else False\n        if arch == \'resnet34\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet34_dilated8\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet34_dilated16\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnet50\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet50_dilated8\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet50_dilated16\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # net_encoder.apply(self.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_encoder\')\n            net_encoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_encoder\n\n    def build_decoder(self, arch=\'ppm_bilinear_deepsup\',\n                      fc_dim=512, num_class=150,\n                      weights=\'\', use_softmax=False):\n        if arch == \'c1_bilinear_deepsup\':\n            net_decoder = C1BilinearDeepSup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'c1_bilinear\':\n            net_decoder = C1Bilinear(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear\':\n            net_decoder = PPMBilinear(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear_deepsup\':\n            net_decoder = PPMBilinearDeepsup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # net_decoder.apply(self.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_decoder\')\n            net_decoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_decoder\n\n\nclass Resnet(nn.Module):\n    def __init__(self, orig_resnet):\n        super(Resnet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(\n                partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=2))\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate // 2, dilate // 2)\n                    m.padding = (dilate // 2, dilate // 2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        conv_out.append(x)\n        x = self.layer2(x)\n        conv_out.append(x)\n        x = self.layer3(x)\n        conv_out.append(x)\n        x = self.layer4(x)\n        conv_out.append(x)\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\n# last conv, bilinear upsample\nclass C1BilinearDeepSup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1BilinearDeepSup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# last conv, bilinear upsample\nclass C1Bilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1Bilinear, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, x, segSize=None):\n        conv5 = x[-1]\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinear, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim + len(pool_scales) * 512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            SynchronizedBatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.upsample(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\'))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinearDeepsup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinearDeepsup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim + len(pool_scales) * 512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            SynchronizedBatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.dropout_deepsup = nn.Dropout2d(0.1)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.upsample(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\'))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.dropout_deepsup(_)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n'"
semantic/resnet.py,2,"b'import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from .lib.nn import SynchronizedBatchNorm2d\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\']  # resnet101 is coming soon!\n\n\nmodel_urls = {\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = SynchronizedBatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = SynchronizedBatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\'\'\'\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet34\']))\n    return model\n\'\'\'\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet101\']), strict=False)\n    return model\n\n# def resnet152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNet-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnet152\']))\n#     return model\n\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
semantic/utils.py,0,"b'# import torch\nimport numpy as np\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.initialized = False\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n\n    def initialize(self, val, weight):\n        self.val = val\n        self.avg = val\n        self.sum = val * weight\n        self.count = weight\n        self.initialized = True\n\n    def update(self, val, weight=1):\n        if not self.initialized:\n            self.initialize(val, weight)\n        else:\n            self.add(val, weight)\n\n    def add(self, val, weight):\n        self.val = val\n        self.sum += val * weight\n        self.count += weight\n        self.avg = self.sum / self.count\n\n    def value(self):\n        return self.val\n\n    def average(self):\n        return self.avg\n\n\ndef unique(ar, return_index=False, return_inverse=False, return_counts=False):\n    ar = np.asanyarray(ar).flatten()\n\n    optional_indices = return_index or return_inverse\n    optional_returns = optional_indices or return_counts\n\n    if ar.size == 0:\n        if not optional_returns:\n            ret = ar\n        else:\n            ret = (ar,)\n            if return_index:\n                ret += (np.empty(0, np.bool),)\n            if return_inverse:\n                ret += (np.empty(0, np.bool),)\n            if return_counts:\n                ret += (np.empty(0, np.intp),)\n        return ret\n    if optional_indices:\n        perm = ar.argsort(kind=\'mergesort\' if return_index else \'quicksort\')\n        aux = ar[perm]\n    else:\n        ar.sort()\n        aux = ar\n    flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n\n    if not optional_returns:\n        ret = aux[flag]\n    else:\n        ret = (aux[flag],)\n        if return_index:\n            ret += (perm[flag],)\n        if return_inverse:\n            iflag = np.cumsum(flag) - 1\n            inv_idx = np.empty(ar.shape, dtype=np.intp)\n            inv_idx[perm] = iflag\n            ret += (inv_idx,)\n        if return_counts:\n            idx = np.concatenate(np.nonzero(flag) + ([ar.size],))\n            ret += (np.diff(idx),)\n    return ret\n\n\ndef colorEncode(labelmap, colors, mode=\'BGR\'):\n    labelmap = labelmap.astype(\'int\')\n    labelmap_rgb = np.zeros((labelmap.shape[0], labelmap.shape[1], 3),\n                            dtype=np.uint8)\n    for label in unique(labelmap):\n        if label < 0:\n            continue\n        labelmap_rgb += (labelmap == label)[:, :, np.newaxis] * \\\n            np.tile(colors[label],\n                    (labelmap.shape[0], labelmap.shape[1], 1))\n\n    if mode == \'BGR\':\n        return labelmap_rgb[:, :, ::-1]\n    else:\n        return labelmap_rgb\n\n\ndef accuracy(preds, label):\n    valid = (label >= 0)\n    acc_sum = (valid * (preds == label)).sum()\n    valid_sum = valid.sum()\n    acc = float(acc_sum) / (valid_sum + 1e-10)\n    return acc, valid_sum\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    imPred = np.asarray(imPred).copy()\n    imLab = np.asarray(imLab).copy()\n\n    imPred += 1\n    imLab += 1\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    imPred = imPred * (imLab > 0)\n\n    # Compute area intersection:\n    intersection = imPred * (imPred == imLab)\n    (area_intersection, _) = np.histogram(\n        intersection, bins=numClass, range=(1, numClass))\n\n    # Compute area union:\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n    area_union = area_pred + area_lab - area_intersection\n\n    return (area_intersection, area_union)\n'"
semantic/vkitti_dataset.py,10,"b""import os\nimport json\nimport sys\nimport torch\nimport lib.utils.data as torchdata\nimport cv2\nfrom torchvision import transforms\nfrom scipy.misc import imread, imresize\nimport numpy as np\nfrom PIL import Image\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\nfrom datasets.vkitti_utils import get_tables, get_lists\n\n\n# Round x to the nearest multiple of p and x' >= x\ndef round2nearest_multiple(x, p):\n    return ((x - 1) // p + 1) * p\n\n\nclass TrainDataset(torchdata.Dataset):\n    def __init__(self, opt, max_sample=-1, batch_per_gpu=1):\n        self.root_dataset = opt.root_dataset\n        self.root_img = os.path.join(self.root_dataset, 'vkitti_1.3.1_rgb')\n        self.root_seg = os.path.join(self.root_dataset, 'vkitti_1.3.1_scenegt')\n        self.table_segm = get_tables('segm', opt.root_dataset)\n\n        self.imgSize = opt.imgSize\n        self.imgMaxSize = opt.imgMaxSize\n        self.random_flip = opt.random_flip\n        # max down sampling rate of network to avoid rounding during conv or pooling\n        self.padding_constant = opt.padding_constant\n        # down sampling rate of segm labe\n        self.segm_downsampling_rate = opt.segm_downsampling_rate\n        self.batch_per_gpu = batch_per_gpu\n\n        self.batch_record_list = []\n\n        # override dataset length when trainig with batch_per_gpu > 1\n        self.cur_idx = 0\n\n        # mean and std\n        self.img_transform = transforms.Compose([\n            transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255], std=[0.229, 0.224, 0.225])])\n\n        self.img_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n\n        self.list_sample = get_lists('train')\n\n        self.if_shuffled = False\n        if max_sample > 0:\n            self.list_sample = self.list_sample[0:max_sample]\n        self.num_sample = len(self.list_sample)\n        assert self.num_sample > 0\n        print('# samples: {}'.format(self.num_sample))\n\n    def _get_sub_batch(self):\n        while True:\n            # get a sample record\n            self.batch_record_list.append(self.list_sample[self.cur_idx])\n\n            # update current sample pointer\n            self.cur_idx += 1\n            if self.cur_idx >= self.num_sample:\n                self.cur_idx = 0\n                np.random.shuffle(self.list_sample)\n\n            if len(self.batch_record_list) == self.batch_per_gpu:\n                batch_records = self.batch_record_list\n                self.batch_record_list = []\n                break\n        return batch_records\n\n    def __getitem__(self, index):\n        # NOTE: random shuffle for the first time. shuffle in __init__ is useless\n        if not self.if_shuffled:\n            np.random.shuffle(self.list_sample)\n            self.if_shuffled = True\n\n        # get sub-batch candidates\n        batch_records = self._get_sub_batch()\n\n        # resize all images' short edges to the chosen size\n        if isinstance(self.imgSize, list):\n            this_short_size = np.random.choice(self.imgSize)\n        else:\n            this_short_size = self.imgSize\n\n        # calculate the BATCH's height and width\n        # since we concat more than one samples, the batch's h and w shall be larger than EACH sample\n        batch_resized_size = np.zeros((self.batch_per_gpu, 2), np.int32)\n        for i in range(self.batch_per_gpu):\n            img_height, img_width = 375, 1242  # fixed ...\n            this_scale = min(this_short_size / min(img_height, img_width),\n                             self.imgMaxSize / max(img_height, img_width))\n            img_resized_height, img_resized_width = img_height * this_scale, img_width * this_scale\n            batch_resized_size[i, :] = img_resized_height, img_resized_width\n        batch_resized_height = np.max(batch_resized_size[:, 0])\n        batch_resized_width = np.max(batch_resized_size[:, 1])\n\n        # Here we must pad both input image and segmentation map to size h' and w' so that p | h' and p | w'\n        batch_resized_height = int(round2nearest_multiple(batch_resized_height, self.padding_constant))\n        batch_resized_width = int(round2nearest_multiple(batch_resized_width, self.padding_constant))\n\n        assert self.padding_constant >= self.segm_downsampling_rate,\\\n            'padding constant must be equal or large than segm downsamping rate'\n        batch_images = torch.zeros(self.batch_per_gpu, 3, batch_resized_height, batch_resized_width)\n        batch_segms = torch.zeros(self.batch_per_gpu, batch_resized_height // self.segm_downsampling_rate,\n                                  batch_resized_width // self.segm_downsampling_rate).long()\n\n        for i in range(self.batch_per_gpu):\n            this_record = batch_records[i]\n\n            # load image and label\n            image_path = os.path.join(self.root_img, this_record)\n            segm_path = os.path.join(self.root_seg, this_record)\n            img = Image.open(image_path)\n            segm = imread(segm_path, mode='RGB')\n            worldId, sceneId = this_record.split('/')[:2]\n            segm = np.apply_along_axis(lambda a: self.table_segm[(worldId, sceneId, a[0], a[1], a[2])], 2, segm)\n            segm = segm.astype(np.uint8)\n\n            # add data augmentation: color jittering\n            img = self.img_jitter(img)\n            img = np.array(img)\n\n            assert(img.ndim == 3)\n            assert(segm.ndim == 2)\n            assert(img.shape[0] == segm.shape[0])\n            assert(img.shape[1] == segm.shape[1])\n\n            if self.random_flip:\n                random_flip = np.random.choice([0, 1])\n                if random_flip == 1:\n                    img = cv2.flip(img, 1)\n                    segm = cv2.flip(segm, 1)\n\n            # note that each sample within a mini batch has different scale param\n            img = imresize(img, (batch_resized_size[i, 0], batch_resized_size[i, 1]), interp='bilinear')\n            segm = imresize(segm, (batch_resized_size[i, 0], batch_resized_size[i, 1]), interp='nearest')\n\n            # to avoid seg label misalignment\n            segm_rounded_height = round2nearest_multiple(segm.shape[0], self.segm_downsampling_rate)\n            segm_rounded_width = round2nearest_multiple(segm.shape[1], self.segm_downsampling_rate)\n            segm_rounded = np.zeros((segm_rounded_height, segm_rounded_width), dtype='uint8')\n            segm_rounded[:segm.shape[0], :segm.shape[1]] = segm\n\n            segm = imresize(segm_rounded, (segm_rounded.shape[0] // self.segm_downsampling_rate,\n                                           segm_rounded.shape[1] // self.segm_downsampling_rate),\n                            interp='nearest')\n            # image to float\n            img = img.astype(np.float32)[:, :, ::-1]  # RGB to BGR!!!\n            img = img.transpose((2, 0, 1))\n            img = self.img_transform(torch.from_numpy(img.copy()))\n\n            batch_images[i][:, :img.shape[1], :img.shape[2]] = img\n            batch_segms[i][:segm.shape[0], :segm.shape[1]] = torch.from_numpy(segm.astype(np.int)).long()\n\n        batch_segms = batch_segms - 1  # label from -1 to 149\n        output = dict()\n        output['img_data'] = batch_images\n        output['seg_label'] = batch_segms\n        return output\n\n    def __len__(self):\n        return int(1e6)  # It's a fake length due to the trick that every loader maintains its own list\n        # return self.num_sampleclass\n\n\nclass ValDataset(torchdata.Dataset):\n    def __init__(self, opt, max_sample=-1):\n        self.root_dataset = opt.root_dataset\n        self.root_img = os.path.join(self.root_dataset, 'vkitti_1.3.1_rgb')\n        self.root_seg = os.path.join(self.root_dataset, 'vkitti_1.3.1_scenegt')\n        self.table_segm = get_tables('segm', opt.root_dataset)\n\n        self.imgSize = opt.imgSize\n        self.imgMaxSize = opt.imgMaxSize\n        # max down sampling rate of network to avoid rounding during conv or pooling\n        self.padding_constant = opt.padding_constant\n        # down sampling rate of segm labe\n        self.segm_downsampling_rate = opt.segm_downsampling_rate\n\n        # override dataset length when trainig with batch_per_gpu > 1\n        self.cur_idx = 0\n\n        # mean and std\n        self.img_transform = transforms.Compose([\n            transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255], std=[0.229, 0.224, 0.225])])\n\n        self.list_sample = get_lists(opt.split)\n\n        if max_sample > 0:\n            self.list_sample = self.list_sample[0:max_sample]\n        self.num_sample = len(self.list_sample)\n        assert self.num_sample > 0\n        print('# samples: {}'.format(self.num_sample))\n\n    def __getitem__(self, index):\n        this_record = self.list_sample[index]\n        # load image and label\n        image_path = os.path.join(self.root_img, this_record)\n        segm_path = os.path.join(self.root_seg, this_record)\n        img = imread(image_path, mode='RGB')\n        img = img[:, :, ::-1]  # BGR to RGB!!!\n        segm = imread(segm_path)\n        worldId, sceneId = this_record.split('/')[:2]\n        segm = np.apply_along_axis(lambda a: self.table_segm[(worldId, sceneId, a[0], a[1], a[2])], 2, segm)\n        segm = segm.astype(np.uint8)\n        ori_height, ori_width, _ = img.shape\n\n        img_resized_list = []\n        for this_short_size in self.imgSize:\n            # calculate target height and width\n            scale = min(this_short_size / float(min(ori_height, ori_width)),\n                        self.imgMaxSize / float(max(ori_height, ori_width)))\n            target_height, target_width = int(ori_height * scale), int(ori_width * scale)\n\n            # to avoid rounding in network\n            target_height = round2nearest_multiple(target_height, self.padding_constant)\n            target_width = round2nearest_multiple(target_width, self.padding_constant)\n\n            # resize\n            img_resized = cv2.resize(img.copy(), (target_width, target_height))\n\n            # image to float\n            img_resized = img_resized.astype(np.float32)\n            img_resized = img_resized.transpose((2, 0, 1))\n            img_resized = self.img_transform(torch.from_numpy(img_resized))\n\n            img_resized = torch.unsqueeze(img_resized, 0)\n            img_resized_list.append(img_resized)\n\n        segm = torch.from_numpy(segm.astype(np.int)).long()\n\n        batch_segms = torch.unsqueeze(segm, 0)\n\n        batch_segms = batch_segms - 1  # label from -1 to 149\n        output = dict()\n        output['img_ori'] = img.copy()\n        output['img_data'] = [x.contiguous() for x in img_resized_list]\n        output['seg_label'] = batch_segms.contiguous()\n        output['info'] = this_record\n        return output\n\n    def __len__(self):\n        return self.num_sample\n\n\nclass TestDataset(torchdata.Dataset):\n    def __init__(self, opt, max_sample=-1):\n        self.root_img = opt.root_dataset\n        if opt.test_img in ['train', 'test', 'all', 'benchmark']:\n            self.root_img = os.path.join(self.root_img, 'vkitti_1.3.1_rgb')\n        self.imgSize = opt.imgSize\n        self.imgMaxSize = opt.imgMaxSize\n        # max down sampling rate of network to avoid rounding during conv or pooling\n        self.padding_constant = opt.padding_constant\n        # down sampling rate of segm label\n        self.segm_downsampling_rate = opt.segm_downsampling_rate\n\n        # override dataset length when trainig with batch_per_gpu > 1\n        self.cur_idx = 0\n\n        # mean and std\n        self.img_transform = transforms.Compose([\n            transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255], std=[0.229, 0.224, 0.225])])\n\n        if opt.test_img in ['train', 'test', 'all']:\n            self.list_sample = get_lists(opt.test_img)\n        elif opt.test_img == 'benchmark':\n            self.list_sample = []\n            benchmark_json = json.load(open(opt.benchmark_json))\n            for pair in benchmark_json[:len(benchmark_json) // 2]:\n                world, topic, source = pair['world'], pair['topic'], pair['source']\n                self.list_sample += [os.path.join(world, topic, source + '.png')]\n        else:\n            self.list_sample = [opt.test_img]\n\n        if max_sample > 0:\n            self.list_sample = self.list_sample[0:max_sample]\n        self.num_sample = len(self.list_sample)\n        assert self.num_sample > 0\n        print('# samples: {}'.format(self.num_sample))\n\n    def __getitem__(self, index):\n        this_record = self.list_sample[index]\n        # load image and label\n        image_path = os.path.join(self.root_img, this_record)\n        img = imread(image_path, mode='RGB')\n        img = img[:, :, ::-1]  # BGR to RGB!!!\n        ori_height, ori_width, _ = img.shape\n\n        img_resized_list = []\n        for this_short_size in self.imgSize:\n            # calculate target height and width\n            scale = min(this_short_size / float(min(ori_height, ori_width)),\n                        self.imgMaxSize / float(max(ori_height, ori_width)))\n            target_height, target_width = int(ori_height * scale), int(ori_width * scale)\n\n            # to avoid rounding in network\n            target_height = round2nearest_multiple(target_height, self.padding_constant)\n            target_width = round2nearest_multiple(target_width, self.padding_constant)\n\n            # resize\n            img_resized = cv2.resize(img.copy(), (target_width, target_height))\n\n            # image to float\n            img_resized = img_resized.astype(np.float32)\n            img_resized = img_resized.transpose((2, 0, 1))\n            img_resized = self.img_transform(torch.from_numpy(img_resized))\n\n            img_resized = torch.unsqueeze(img_resized, 0)\n            img_resized_list.append(img_resized)\n\n        output = dict()\n        output['img_ori'] = img.copy()\n        output['img_data'] = [x.contiguous() for x in img_resized_list]\n        output['info'] = this_record\n        return output\n\n    def __len__(self):\n        return self.num_sample\n"""
semantic/vkitti_eval.py,7,"b'# System libs\nimport os\nimport datetime\nimport argparse\n\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom scipy.io import loadmat\n# Our libs\nfrom vkitti_dataset import ValDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion\nfrom lib.nn import user_scattered_collate, async_copy_to\nfrom lib.utils import as_numpy\nimport lib.utils.data as torchdata\nimport cv2\n\n\ndef precompute_result(info, preds, args):\n    img_name = info.split(\'/\')[-1]\n    img_path = os.path.join(args.result, info.replace(img_name, \'\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n    cv2.imwrite(os.path.join(img_path, img_name), preds)\n\n\ndef visualize_result(data, preds, args):\n    colors = loadmat(os.path.join(os.path.dirname(__file__), \'data/color150.mat\'))[\'colors\']\n    (img, seg, info) = data\n\n    # segmentation\n    seg_color = colorEncode(seg, colors)\n\n    # prediction\n    pred_color = colorEncode(preds, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, seg_color, pred_color), axis=0).astype(np.uint8)\n    img_name = info.split(\'/\')[-1]\n    img_path = os.path.join(args.result, info.replace(img_name, \'\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n\n    cv2.imwrite(os.path.join(img_path, img_name.replace(\'.png\', \'_visualize.png\')), im_vis)\n\n\ndef evaluate(segmentation_module, loader, args):\n    acc_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n\n    segmentation_module.eval()\n\n    for i, batch_data in enumerate(loader):\n        # process data\n        batch_data = batch_data[0]\n        seg_label = as_numpy(batch_data[\'seg_label\'][0])\n\n        img_resized_list = batch_data[\'img_data\']\n\n        with torch.no_grad():\n            segSize = (seg_label.shape[0], seg_label.shape[1])\n            pred = torch.zeros(1, args.num_class, segSize[0], segSize[1])\n            pred = Variable(pred).cuda()\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict[\'img_data\'] = img\n                del feed_dict[\'img_ori\']\n                del feed_dict[\'info\']\n                feed_dict = async_copy_to(feed_dict, args.gpu_id)\n\n                # forward pass\n                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n                pred = pred + pred_tmp / len(args.imgSize)\n\n            _, preds = torch.max(pred.data.cpu(), dim=1)\n            preds = as_numpy(preds.squeeze(0))\n\n        # calculate accuracy\n        acc, pix = accuracy(preds, seg_label)\n        intersection, union = intersectionAndUnion(preds, seg_label, args.num_class)\n        acc_meter.update(acc, pix)\n        intersection_meter.update(intersection)\n        union_meter.update(union)\n        print(\'[{}] iter {}, accuracy: {}\'\n              .format(datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),\n                      i, acc))\n\n        # visualization\n        if args.visualize:\n            visualize_result(\n                (batch_data[\'img_ori\'], seg_label, batch_data[\'info\']),\n                preds, args)\n        if args.precompute:\n            precompute_result(batch_data[\'info\'], preds, args)\n\n    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n    for i, _iou in enumerate(iou):\n        print(\'class [{}], IoU: {}\'.format(i, _iou))\n\n    print(\'[Eval Summary]:\')\n    print(\'Mean IoU: {:.4}, Accuracy: {:.2f}%\'\n          .format(iou.mean(), acc_meter.average() * 100))\n\n\ndef main(args):\n    torch.cuda.set_device(args.gpu_id)\n\n    # Network Builders\n    builder = ModelBuilder()\n    net_encoder = builder.build_encoder(arch=args.arch_encoder,\n                                        fc_dim=args.fc_dim,\n                                        weights=args.weights_encoder)\n    net_decoder = builder.build_decoder(arch=args.arch_decoder,\n                                        num_class=args.num_class,\n                                        fc_dim=args.fc_dim,\n                                        weights=args.weights_decoder,\n                                        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_val = ValDataset(\n        args, max_sample=args.num_val)\n    loader_val = torchdata.DataLoader(\n        dataset_val,\n        batch_size=args.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=4,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    evaluate(segmentation_module, loader_val, args)\n\n    print(\'Evaluation Done!\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Model related arguments\n    parser.add_argument(\'--id\', required=True,\n                        help=""a name for identifying the model to load"")\n    parser.add_argument(\'--suffix\', default=\'_epoch_25.pth\',\n                        help=""which snapshot to load"")\n    parser.add_argument(\'--arch_encoder\', default=\'resnet50_dilated8\',\n                        help=""architecture of net_encoder"")\n    parser.add_argument(\'--arch_decoder\', default=\'ppm_bilinear_deepsup\',\n                        help=""architecture of net_decoder"")\n    parser.add_argument(\'--fc_dim\', default=2048, type=int,\n                        help=\'number of features between encoder and decoder\')\n\n    # Path related arguments\n    parser.add_argument(\'--root_dataset\',\n                        default=\'./data/\')\n\n    # Data related arguments\n    parser.add_argument(\'--num_val\', default=-1, type=int,\n                        help=\'number of images to evalutate\')\n    parser.add_argument(\'--num_class\', default=14, type=int,\n                        help=\'number of classes\')\n    parser.add_argument(\'--batch_size\', default=1, type=int,\n                        help=\'batchsize. current only supports 1\')\n    parser.add_argument(\'--imgSize\', default=[100, 150, 200, 300, 375], nargs=\'+\', type=int,\n                        help=\'list of input image sizes.\'\n                             \'for multiscale testing, e.g.  300 400 500 600\')\n    parser.add_argument(\'--imgMaxSize\', default=1242, type=int,\n                        help=\'maximum input image size of long edge\')\n    parser.add_argument(\'--padding_constant\', default=8, type=int,\n                        help=\'maxmimum downsampling rate of the network\')\n    parser.add_argument(\'--segm_downsampling_rate\', default=8, type=int,\n                        help=\'downsampling rate of the segmentation label\')\n\n    # Misc arguments\n    parser.add_argument(\'--ckpt\', default=\'./ckpt\',\n                        help=\'folder to output checkpoints\')\n    parser.add_argument(\'--result\', default=\'./result/eval\',\n                        help=\'folder to output visualization results\')\n    parser.add_argument(\'--gpu_id\', default=0, type=int,\n                        help=\'gpu_id for evaluation\')\n\n    # Added arguments\n    parser.add_argument(\'--split\', default=\'test\', type=str,\n                        help=\'train|test|all for evaluation\')\n    parser.add_argument(\'--visualize\', action=\'store_true\', help=\'generate visualizations of segmentations\')\n    parser.add_argument(\'--precompute\', action=\'store_true\', help=\'generate segmentations\')\n\n    args = parser.parse_args()\n    print(args)\n\n    # torch.cuda.set_device(args.gpu_id)\n\n    # absolute paths of model weights\n    args.weights_encoder = os.path.join(args.ckpt, args.id,\n                                        \'encoder\' + args.suffix)\n    args.weights_decoder = os.path.join(args.ckpt, args.id,\n                                        \'decoder\' + args.suffix)\n    assert os.path.exists(args.weights_encoder) and \\\n        os.path.exists(args.weights_encoder), \'checkpoint does not exist!\'\n\n    # args.result = os.path.join(args.result, args.id)\n    if not os.path.isdir(args.result):\n        os.makedirs(args.result)\n\n    main(args)\n'"
semantic/vkitti_test.py,7,"b'# System libs\nimport os\nimport argparse\n\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom scipy.io import loadmat\n# Our libs\nfrom vkitti_dataset import TestDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import colorEncode\nfrom lib.nn import user_scattered_collate, async_copy_to\nfrom lib.utils import as_numpy\nimport lib.utils.data as torchdata\nimport cv2\n\n\ndef precompute_result(info, preds, args):\n    img_name = info.split(\'/\')[-1]\n    img_path = os.path.join(args.result, info.replace(img_name, \'\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n    cv2.imwrite(os.path.join(img_path, img_name), preds)\n\n\ndef visualize_result(data, preds, args):\n    colors = loadmat(os.path.join(os.path.dirname(__file__), \'data/color150.mat\'))[\'colors\']\n    (img, info) = data\n\n    # prediction\n    pred_color = colorEncode(preds, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, pred_color), axis=0).astype(np.uint8)\n    img_name = info.split(\'/\')[-1]\n    img_path = os.path.join(args.result, info.replace(img_name, \'\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n\n    cv2.imwrite(os.path.join(img_path, img_name.replace(\'.png\', \'_visualize.png\')), im_vis)\n\n\ndef test(segmentation_module, loader, args):\n\n    segmentation_module.eval()\n\n    for i, batch_data in enumerate(loader):\n        # process data\n        batch_data = batch_data[0]\n        img_ori = as_numpy(batch_data[\'img_ori\'])\n        img_resized_list = batch_data[\'img_data\']\n\n        with torch.no_grad():\n            segSize = (img_ori.shape[0], img_ori.shape[1])\n            pred = torch.zeros(1, args.num_class, segSize[0], segSize[1])\n            pred = Variable(pred).cuda()\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict[\'img_data\'] = img\n                del feed_dict[\'img_ori\']\n                del feed_dict[\'info\']\n                feed_dict = async_copy_to(feed_dict, args.gpu_id)\n\n                # forward pass\n                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n                pred = pred + pred_tmp / len(args.imgSize)\n\n            _, preds = torch.max(pred.data.cpu(), dim=1)\n            preds = as_numpy(preds.squeeze(0))\n\n        precompute_result(batch_data[\'info\'], preds, args)\n        if args.visualize:\n            visualize_result(\n                (batch_data[\'img_ori\'], batch_data[\'info\']),\n                preds, args)\n\n\ndef main(args):\n    torch.cuda.set_device(args.gpu_id)\n\n    # Network Builders\n    builder = ModelBuilder()\n    net_encoder = builder.build_encoder(arch=args.arch_encoder,\n                                        fc_dim=args.fc_dim,\n                                        weights=args.weights_encoder)\n    net_decoder = builder.build_decoder(arch=args.arch_decoder,\n                                        num_class=args.num_class,\n                                        fc_dim=args.fc_dim,\n                                        weights=args.weights_decoder,\n                                        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_test = TestDataset(\n        args, max_sample=args.num_val)\n    loader_test = torchdata.DataLoader(\n        dataset_test,\n        batch_size=args.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=4,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    test(segmentation_module, loader_test, args)\n\n    print(\'Test Done!\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Model related arguments\n    parser.add_argument(\'--id\', required=True,\n                        help=""a name for identifying the model to load"")\n    parser.add_argument(\'--suffix\', default=\'_epoch_25.pth\',\n                        help=""which snapshot to load"")\n    parser.add_argument(\'--arch_encoder\', default=\'resnet50_dilated8\',\n                        help=""architecture of net_encoder"")\n    parser.add_argument(\'--arch_decoder\', default=\'ppm_bilinear_deepsup\',\n                        help=""architecture of net_decoder"")\n    parser.add_argument(\'--fc_dim\', default=2048, type=int,\n                        help=\'number of features between encoder and decoder\')\n\n    # Path related arguments\n    parser.add_argument(\'--root_dataset\',\n                        default=\'./data/\')\n\n    # Data related arguments\n    parser.add_argument(\'--num_val\', default=-1, type=int,\n                        help=\'number of images to evalutate\')\n    parser.add_argument(\'--num_class\', default=14, type=int,\n                        help=\'number of classes\')\n    parser.add_argument(\'--batch_size\', default=1, type=int,\n                        help=\'batchsize. current only supports 1\')\n    parser.add_argument(\'--imgSize\', default=[100, 150, 200, 300, 375], nargs=\'+\', type=int,\n                        help=\'list of input image sizes.\'\n                             \'for multiscale testing, e.g.  300 400 500 600\')\n    parser.add_argument(\'--imgMaxSize\', default=1242, type=int,\n                        help=\'maximum input image size of long edge\')\n    parser.add_argument(\'--padding_constant\', default=8, type=int,\n                        help=\'maxmimum downsampling rate of the network\')\n    parser.add_argument(\'--segm_downsampling_rate\', default=8, type=int,\n                        help=\'downsampling rate of the segmentation label\')\n\n    # Misc arguments\n    parser.add_argument(\'--ckpt\', default=\'./ckpt\',\n                        help=\'folder to output checkpoints\')\n    parser.add_argument(\'--result\', default=\'./result\',\n                        help=\'folder to output visualization results\')\n    parser.add_argument(\'--gpu_id\', default=0, type=int,\n                        help=\'gpu_id for evaluation\')\n\n    # Added arguments\n    parser.add_argument(\'--test_img\', default=\'all\',\n                        help=\'if specified a single image, do single image testing; otherwise do all/train/test/benchmark set inference\')\n    parser.add_argument(\'--benchmark_json\', default=\'\', help=\'json path if test_img = benchmark\')\n    parser.add_argument(\'--visualize\', action=\'store_true\', help=\'generate visualizations of segmentations\')\n\n    args = parser.parse_args()\n    print(args)\n\n    # torch.cuda.set_device(args.gpu_id)\n\n    # absolute paths of model weights\n    args.weights_encoder = os.path.join(args.ckpt, args.id,\n                                        \'encoder\' + args.suffix)\n    args.weights_decoder = os.path.join(args.ckpt, args.id,\n                                        \'decoder\' + args.suffix)\n    assert os.path.exists(args.weights_encoder) and \\\n        os.path.exists(args.weights_encoder), \'checkpoint does not exist!\'\n\n    # args.result = os.path.join(args.result, args.id)\n    if not os.path.isdir(args.result):\n        os.makedirs(args.result)\n\n    main(args)\n'"
semantic/vkitti_train.py,7,"b'# System libs\nimport os\nimport time\n# import math\nimport random\nimport argparse\n\n# Numerical libs\nimport torch\nimport torch.nn as nn\n# Our libs\nfrom vkitti_dataset import TrainDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import AverageMeter\nfrom lib.nn import UserScatteredDataParallel, user_scattered_collate, patch_replication_callback\nimport lib.utils.data as torchdata\n\n\n# train one epoch\ndef train(segmentation_module, iterator, optimizers, history, epoch, args):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    ave_total_loss = AverageMeter()\n    ave_acc = AverageMeter()\n\n    segmentation_module.train(not args.fix_bn)\n\n    # main loop\n    tic = time.time()\n    for i in range(args.epoch_iters):\n        batch_data = next(iterator)\n        data_time.update(time.time() - tic)\n\n        segmentation_module.zero_grad()\n\n        # forward pass\n        loss, acc = segmentation_module(batch_data)\n        loss = loss.mean()\n        acc = acc.mean()\n\n        # Backward\n        loss.backward()\n        for optimizer in optimizers:\n            optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - tic)\n        tic = time.time()\n\n        # update average loss and acc\n        ave_total_loss.update(loss.data[0])\n        ave_acc.update(acc.data[0] * 100)\n\n        # calculate accuracy, and display\n        if i % args.disp_iter == 0:\n            print(\'Epoch: [{}][{}/{}], Time: {:.2f}, Data: {:.2f}, \'\n                  \'lr_encoder: {:.6f}, lr_decoder: {:.6f}, \'\n                  \'Accuracy: {:4.2f}, Loss: {:.6f}\'\n                  .format(epoch, i, args.epoch_iters,\n                          batch_time.average(), data_time.average(),\n                          args.running_lr_encoder, args.running_lr_decoder,\n                          ave_acc.average(), ave_total_loss.average()))\n\n            fractional_epoch = epoch - 1 + 1. * i / args.epoch_iters\n            history[\'train\'][\'epoch\'].append(fractional_epoch)\n            history[\'train\'][\'loss\'].append(loss.data[0])\n            history[\'train\'][\'acc\'].append(acc.data[0])\n\n        # adjust learning rate\n        cur_iter = i + (epoch - 1) * args.epoch_iters\n        adjust_learning_rate(optimizers, cur_iter, args)\n\n\ndef checkpoint(nets, history, args, epoch_num):\n    print(\'Saving checkpoints...\')\n    (net_encoder, net_decoder, crit) = nets\n    suffix_latest = \'epoch_{}.pth\'.format(epoch_num)\n\n    dict_encoder = net_encoder.state_dict()\n    dict_decoder = net_decoder.state_dict()\n\n    dict_encoder_save = {k: v for k, v in dict_encoder.items() if not (k.endswith(\'_tmp_running_mean\') or k.endswith(\'tmp_running_var\'))}\n    dict_decoder_save = {k: v for k, v in dict_decoder.items() if not (k.endswith(\'_tmp_running_mean\') or k.endswith(\'tmp_running_var\'))}\n\n    torch.save(history,\n               \'{}/history_{}\'.format(args.ckpt, suffix_latest))\n    torch.save(dict_encoder_save,\n               \'{}/encoder_{}\'.format(args.ckpt, suffix_latest))\n    torch.save(dict_decoder_save,\n               \'{}/decoder_{}\'.format(args.ckpt, suffix_latest))\n\n\ndef create_optimizers(nets, args):\n    (net_encoder, net_decoder, crit) = nets\n    optimizer_encoder = torch.optim.SGD(\n        net_encoder.parameters(),\n        lr=args.lr_encoder,\n        momentum=args.beta1,\n        weight_decay=args.weight_decay)\n    optimizer_decoder = torch.optim.SGD(\n        net_decoder.parameters(),\n        lr=args.lr_decoder,\n        momentum=args.beta1,\n        weight_decay=args.weight_decay)\n    return (optimizer_encoder, optimizer_decoder)\n\n\ndef adjust_learning_rate(optimizers, cur_iter, args):\n    scale_running_lr = ((1. - float(cur_iter) / args.max_iters) ** args.lr_pow)\n    args.running_lr_encoder = args.lr_encoder * scale_running_lr\n    args.running_lr_decoder = args.lr_decoder * scale_running_lr\n\n    (optimizer_encoder, optimizer_decoder) = optimizers\n    for param_group in optimizer_encoder.param_groups:\n        param_group[\'lr\'] = args.running_lr_encoder\n    for param_group in optimizer_decoder.param_groups:\n        param_group[\'lr\'] = args.running_lr_decoder\n\n\ndef main(args):\n    # Network Builders\n    builder = ModelBuilder()\n    net_encoder = builder.build_encoder(\n        arch=args.arch_encoder,\n        fc_dim=args.fc_dim,\n        weights=args.weights_encoder)\n    net_decoder = builder.build_decoder(\n        arch=args.arch_decoder,\n        fc_dim=args.fc_dim,\n        num_class=args.num_class,\n        weights=args.weights_decoder)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    if args.arch_decoder.endswith(\'deepsup\'):\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit, args.deep_sup_scale)\n    else:\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_train = TrainDataset(\n        args, batch_per_gpu=args.batch_size_per_gpu)\n\n    loader_train = torchdata.DataLoader(\n        dataset_train,\n        batch_size=args.num_gpus,  # we have modified data_parallel\n        shuffle=False,  # we do not use this param\n        collate_fn=user_scattered_collate,\n        num_workers=int(args.workers),\n        drop_last=True,\n        pin_memory=True)\n\n    print(\'1 Epoch = {} iters\'.format(args.epoch_iters))\n\n    # create loader iterator\n    iterator_train = iter(loader_train)\n\n    # load nets into gpu\n    if args.num_gpus > 1:\n        segmentation_module = UserScatteredDataParallel(\n            segmentation_module,\n            device_ids=range(args.num_gpus))\n        # For sync bn\n        patch_replication_callback(segmentation_module)\n    segmentation_module.cuda()\n\n    # Set up optimizers\n    nets = (net_encoder, net_decoder, crit)\n    optimizers = create_optimizers(nets, args)\n\n    # Main loop\n    history = {\'train\': {\'epoch\': [], \'loss\': [], \'acc\': []}}\n\n    for epoch in range(args.start_epoch, args.num_epoch + 1):\n        train(segmentation_module, iterator_train, optimizers, history, epoch, args)\n\n        # checkpointing\n        checkpoint(nets, history, args, epoch)\n\n    print(\'Training Done!\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Model related arguments\n    parser.add_argument(\'--id\', default=\'baseline\',\n                        help=""a name for identifying the model"")\n    parser.add_argument(\'--arch_encoder\', default=\'resnet50_dilated8\',\n                        help=""architecture of net_encoder"")\n    parser.add_argument(\'--arch_decoder\', default=\'ppm_bilinear_deepsup\',\n                        help=""architecture of net_decoder"")\n    parser.add_argument(\'--weights_encoder\', default=\'\',\n                        help=""weights to finetune net_encoder"")\n    parser.add_argument(\'--weights_decoder\', default=\'\',\n                        help=""weights to finetune net_decoder"")\n    parser.add_argument(\'--fc_dim\', default=2048, type=int,\n                        help=\'number of features between encoder and decoder\')\n\n    # Path related arguments\n    parser.add_argument(\'--root_dataset\',\n                        default=\'./data/\')\n\n    # optimization related arguments\n    parser.add_argument(\'--num_gpus\', default=2, type=int,\n                        help=\'number of gpus to use\')\n    parser.add_argument(\'--batch_size_per_gpu\', default=1, type=int,\n                        help=\'input batch size\')\n    parser.add_argument(\'--num_epoch\', default=100, type=int,\n                        help=\'epochs to train for\')\n    parser.add_argument(\'--start_epoch\', default=1, type=int,\n                        help=\'epoch to start training. useful if continue from a checkpoint\')\n    parser.add_argument(\'--epoch_iters\', default=5000, type=int,\n                        help=\'iterations of each epoch (irrelevant to batch size)\')\n    parser.add_argument(\'--optim\', default=\'SGD\', help=\'optimizer\')\n    parser.add_argument(\'--lr_encoder\', default=1e-2, type=float, help=\'LR\')\n    parser.add_argument(\'--lr_decoder\', default=1e-2, type=float, help=\'LR\')\n    parser.add_argument(\'--lr_pow\', default=0.9, type=float,\n                        help=\'power in poly to drop LR\')\n    parser.add_argument(\'--beta1\', default=0.9, type=float,\n                        help=\'momentum for sgd, beta1 for adam\')\n    parser.add_argument(\'--weight_decay\', default=1e-4, type=float,\n                        help=\'weights regularizer\')\n    parser.add_argument(\'--deep_sup_scale\', default=0.4, type=float,\n                        help=\'the weight of deep supervision loss\')\n    parser.add_argument(\'--fix_bn\', default=0, type=int,\n                        help=\'fix bn params\')\n\n    # Data related arguments\n    parser.add_argument(\'--num_class\', default=14, type=int,\n                        help=\'number of classes\')\n    parser.add_argument(\'--workers\', default=16, type=int,\n                        help=\'number of data loading workers\')\n    parser.add_argument(\'--imgSize\', default=[100, 150, 200, 300, 375], nargs=\'+\', type=int,\n                        help=\'input image size of short edge (int or list)\')\n    parser.add_argument(\'--imgMaxSize\', default=1274, type=int,\n                        help=\'maximum input image size of long edge\')\n    parser.add_argument(\'--padding_constant\', default=8, type=int,\n                        help=\'maxmimum downsampling rate of the network\')\n    parser.add_argument(\'--segm_downsampling_rate\', default=8, type=int,\n                        help=\'downsampling rate of the segmentation label\')\n    parser.add_argument(\'--random_flip\', default=True, type=bool,\n                        help=\'if horizontally flip images when training\')\n\n    # Misc arguments\n    parser.add_argument(\'--seed\', default=304, type=int, help=\'manual seed\')\n    parser.add_argument(\'--ckpt\', default=\'./ckpt\',\n                        help=\'folder to output checkpoints\')\n    parser.add_argument(\'--disp_iter\', type=int, default=20,\n                        help=\'frequency to display\')\n\n    args = parser.parse_args()\n    print(""Input arguments:"")\n    for key, val in vars(args).items():\n        print(""{:16} {}"".format(key, val))\n\n    args.batch_size = args.num_gpus * args.batch_size_per_gpu\n    args.max_iters = args.epoch_iters * args.num_epoch\n    args.running_lr_encoder = args.lr_encoder\n    args.running_lr_decoder = args.lr_decoder\n\n    args.id += \'-\' + str(args.arch_encoder)\n    args.id += \'-\' + str(args.arch_decoder)\n    args.id += \'-ngpus\' + str(args.num_gpus)\n    args.id += \'-batchSize\' + str(args.batch_size)\n    args.id += \'-imgMaxSize\' + str(args.imgMaxSize)\n    args.id += \'-paddingConst\' + str(args.padding_constant)\n    args.id += \'-segmDownsampleRate\' + str(args.segm_downsampling_rate)\n    args.id += \'-LR_encoder\' + str(args.lr_encoder)\n    args.id += \'-LR_decoder\' + str(args.lr_decoder)\n    args.id += \'-epoch\' + str(args.num_epoch)\n    args.id += \'-decay\' + str(args.weight_decay)\n    args.id += \'-fixBN\' + str(args.fix_bn)\n    print(\'Model ID: {}\'.format(args.id))\n\n    args.ckpt = os.path.join(args.ckpt, args.id)\n    if not os.path.isdir(args.ckpt):\n        os.makedirs(args.ckpt)\n\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    main(args)\n'"
textural/edit_benchmark.py,6,"b""# Edit benchmark\n\n# System libs\nimport os\nimport sys\nfrom math import pi\n\n# Third party libs\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom collections import OrderedDict\nfrom PIL import Image\nimport json\n\n# our libs\nsys.path.insert(0, os.path.dirname(__file__))\nfrom util import util, html\nfrom util.visualizer import Visualizer\nfrom models.models import create_model as create_pix2pix_model\nfrom options.edit_options import EditOptions\nfrom data.base_dataset import get_params, get_transform\n\n# opt and setup\nopt = EditOptions().parse(save=False)\nopt.nThreads = 1\nopt.batchSize = 1\nopt.serial_batches = True\nopt.no_flip = True\nif opt.feat_pose_num_bins:\n    bins = np.array(list(range(-180, 181, 360 // opt.feat_pose_num_bins))) / 180\n\nmodel = create_pix2pix_model(opt)\nvisualizer = Visualizer(opt)\n\n# create website\nweb_dir = os.path.join(opt.results_dir, '%s_%s_%s_%s' % (opt.name, opt.experiment_name, opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, 'Experiment = %s, Experiment name = %s, Phase = %s, Epoch = %s' % (opt.name, opt.experiment_name, opt.phase, opt.which_epoch))\n\nloss = torch.nn.L1Loss()\nlosses = []\npaths = []\n\n# edit\nedit_list = json.load(open(opt.edit_list))\nedit_list = edit_list[:len(edit_list) // 2]  # last half of the edit list is reconstruction\nfor i, edit_item in enumerate(edit_list):\n    # set up base images\n    world, topic, source, target = edit_item['world'], edit_item['topic'], edit_item['source'], edit_item['target']\n    edit_source = world + '/' + topic + '/' + source + '.png'\n    edit_target = world + '/' + topic + '/' + target + '.png'\n\n    base_img = Image.open(os.path.join(opt.dataroot, 'vkitti_1.3.1_rgb', edit_source)).convert('RGB')\n    target_img = Image.open(os.path.join(opt.dataroot, 'vkitti_1.3.1_rgb', edit_target)).convert('RGB')\n    base_segm = Image.open(os.path.join(opt.segm_precomputed_path, edit_source))\n    base_inst_exist = 1\n    try:\n        base_inst = Image.open(os.path.join(opt.edit_dir, edit_source))\n    except FileNotFoundError:\n        print('no inst found at', edit_source)\n        base_inst = base_segm.copy()\n        base_inst_exist = 0\n    print(edit_source, edit_target)\n\n    # set up transforms\n    params = get_params(opt, base_segm.size)\n    transform_A = get_transform(opt, params, method=Image.NEAREST, normalize=False)\n    transform_B = get_transform(opt, params)\n    base_segm, base_inst = transform_A(base_segm) * 255.0, transform_A(base_inst) * 255.0\n    base_img, target_img = transform_B(base_img), transform_B(target_img)\n    base_segm = base_segm + 1\n    if not base_inst_exist:\n        base_inst = base_segm.clone()\n    else:\n        base_inst *= 1000\n        base_segm[(base_inst == 0) & (base_segm == 2)] = 5\n        base_segm[(base_inst == 0) & (base_segm == 12)] = 5\n        base_inst[base_inst == 0] = base_segm[base_inst == 0]\n\n    # obtain instance feat\n    feat_dict = model.netE.generate_feat_dict(Variable(base_img.unsqueeze(0)).cuda(), base_inst.unsqueeze(0).cuda())\n\n    inst = Image.open(os.path.join(opt.edit_dir, edit_target))\n    inst = (transform_A(inst) * 255.0).int()\n    d = json.load(open(os.path.join(opt.edit_dir, edit_target.replace('.png', '.json'))))\n\n    segm = base_segm.int()\n    feat = torch.zeros(opt.feat_num, segm.size(1), segm.size(2))\n    pose = torch.zeros(segm.size()) if opt.feat_pose_num_bins else torch.zeros(2, segm.size(1), segm.size(2))\n    segm[segm == 2] = 5\n    segm[segm == 12] = 5\n\n    for k, v in d.items():\n        k = int(k)\n        alpha, class_id = v['alpha'], v['class_id']\n        inst_id = k * 1000\n        inst[inst == k] = inst_id\n        # process segm: remove original car and add current car\n        segm[inst == inst_id] = {1: 2, 2: 12}[v['class_id']]\n        # process pose\n        if opt.feat_pose_num_bins:\n            pose[inst == inst_id] = int(np.digitize(alpha / pi, bins))\n\n    # process inst: use segm to complete the background\n    inst[inst == 0] = segm[inst == 0]\n\n    # process normal\n    normal = torch.zeros(base_img.size())\n    if opt.feat_normal:\n        try:\n            normal_path = os.path.join(opt.edit_dir, edit_target.replace('.png', '-normal.png'))\n            normal_map = Image.open(normal_path).convert('RGB')\n            normal = transform_B(normal_map) + 1 / 255  # bias caused by 0..256 instead 0..255\n        except FileNotFoundError:  # no cars\n            normal = torch.zeros(base_img.size())\n\n    # process feat\n    inst_list = np.unique(inst.cpu().numpy().astype(int))\n    for inst_id in inst_list:\n        inst_id = int(inst_id)\n        if inst_id not in feat_dict:\n            print('error inst_id = %d !' % inst_id)\n            continue\n        indices = (inst == inst_id).nonzero()  # n x 3\n        for j in range(opt.feat_num):\n            feat[indices[:, 0] + j, indices[:, 1], indices[:, 2]] = feat_dict[inst_id][j]\n\n    generated = model.fake_inference(\n        base_img.unsqueeze(0), segm.unsqueeze_(0), inst.unsqueeze_(0),\n        feat.unsqueeze_(0), pose.unsqueeze_(0), normal.unsqueeze_(0))\n\n    visuals = [('input_label', util.tensor2label(segm[0], opt.label_nc)),\n               ('input_inst', util.tensor2label(inst[0], opt.label_nc))]\n    if opt.feat_pose:\n        visuals += [('input_pose', util.tensor2label(pose[0], opt.feat_pose_num_bins))]\n    if opt.feat_normal:\n        visuals += [('input_normal', util.tensor2im(normal[0]))]\n    visuals += [('synthesized_image', util.tensor2im(generated.data[0])),\n                ('target_image', util.tensor2im(target_img)),\n                ('source_image', util.tensor2im(base_img))]\n    visuals += [('feat', util.tensor2im(feat[0][:3]))]\n    visuals = OrderedDict(visuals)\n    losses.append(loss(generated.cpu().data[0], target_img))\n    img_path = ['%05d.png' % i]\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n"""
textural/edit_vkitti.py,5,"b'# Edit single image\n\n# System libs\nimport os\nimport sys\nfrom math import pi\n\n# Third party libs\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom collections import OrderedDict\nfrom PIL import Image\nimport json\n\n# our libs\nsys.path.insert(0, os.path.dirname(__file__))\nfrom util import util, html\nfrom util.visualizer import Visualizer\nfrom models.models import create_model as create_pix2pix_model\nfrom options.edit_options import EditOptions\nfrom data.base_dataset import get_params, get_transform\n\n# opt and setup\nopt = EditOptions().parse(save=False)\nopt.nThreads = 1\nopt.batchSize = 1\nopt.serial_batches = True\nopt.no_flip = True\nif opt.feat_pose_num_bins:\n    bins = np.array(list(range(-180, 181, 360 // opt.feat_pose_num_bins))) / 180\n\nmodel = create_pix2pix_model(opt)\nvisualizer = Visualizer(opt)\n\n# create website\nweb_dir = os.path.join(opt.results_dir, \'%s_%s_%s_%s\' % (opt.name, opt.experiment_name, opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, \'Experiment = %s, Experiment name = %s, Phase = %s, Epoch = %s\' % (opt.name, opt.experiment_name, opt.phase, opt.which_epoch))\n\n# setup base images\nbase_img = Image.open(opt.edit_source).convert(\'RGB\')\nbase_segm = Image.open(opt.segm_precomputed_path)\nbase_inst = Image.open(os.path.join(opt.edit_dir, ""00000.png""))\n\nparams = get_params(opt, base_segm.size)\ntransform_A = get_transform(opt, params, method=Image.NEAREST, normalize=False)\ntransform_B = get_transform(opt, params)\nbase_segm, base_img, base_inst = transform_A(base_segm) * 255.0, transform_B(base_img), transform_A(base_inst) * 255.0\n\nbase_segm = base_segm + 1\nbase_inst *= 1000\nbase_segm[(base_inst == 0) & (base_segm == 2)] = 5\nbase_segm[(base_inst == 0) & (base_segm == 12)] = 5\nbase_inst[base_inst == 0] = base_segm[base_inst == 0]\n\n# obtain instance feat\nfeat_dict = model.netE.generate_feat_dict(Variable(base_img.unsqueeze(0)).cuda(), base_inst.unsqueeze(0).cuda())\nprint(feat_dict)\nprint(np.unique(base_inst.numpy(), return_counts=True))\n\n# edit\nfor i in range(opt.edit_num):\n    inst = Image.open(os.path.join(opt.edit_dir, \'%05d.png\' % i))\n    inst = (transform_A(inst) * 255.0).int()\n    d = json.load(open(os.path.join(opt.edit_dir, \'%05d.json\' % i)))\n\n    segm = base_segm.int()\n    feat = torch.zeros(opt.feat_num, segm.size(1), segm.size(2))\n    pose = torch.zeros(segm.size()) if opt.feat_pose_num_bins else torch.zeros(2, segm.size(1), segm.size(2))\n    segm[segm == 2] = 5\n    segm[segm == 12] = 5\n\n    for k, v in d.items():\n        k = int(k)\n        alpha, class_id = v[\'alpha\'], v[\'class_id\']\n        inst_id = k * 1000\n        inst[inst == k] = inst_id\n        # process segm: remove original car and add current car\n        segm[inst == inst_id] = {1: 2, 2: 12}[class_id]\n        # process pose\n        if opt.feat_pose_num_bins:\n            pose[inst == inst_id] = int(np.digitize(alpha / pi, bins))\n\n    # process inst: use segm to complete the background\n    inst[inst == 0] = segm[inst == 0]\n\n    # process normal\n    normal = torch.zeros(base_img.size())\n    if opt.feat_normal:\n        try:\n            normal_path = os.path.join(opt.edit_dir, \'%05d-normal.png\' % i)\n            normal_map = Image.open(normal_path).convert(\'RGB\')\n            normal = transform_B(normal_map) + 1 / 255  # bias caused by 0..256 instead 0..255\n        except FileNotFoundError:  # no cars\n            normal = torch.zeros(base_img.size())\n\n    # process feat\n    inst_list = np.unique(inst.cpu().numpy().astype(int))\n    for inst_id in inst_list:\n        inst_id = int(inst_id)\n        indices = (inst == inst_id).nonzero()\n        for j in range(opt.feat_num):\n            feat[indices[:, 0] + j, indices[:, 1], indices[:, 2]] = feat_dict[inst_id][j]\n\n    generated = model.fake_inference(\n        base_img.unsqueeze(0), segm.unsqueeze_(0), inst.unsqueeze_(0),\n        feat.unsqueeze_(0), pose.unsqueeze_(0), normal.unsqueeze_(0))\n\n    visuals = [(\'input_label\', util.tensor2label(segm[0], opt.label_nc)),\n               (\'input_inst\', util.tensor2label(inst[0], opt.label_nc))]\n    if opt.feat_pose:\n        visuals += [(\'input_pose\', util.tensor2label(pose[0], opt.feat_pose_num_bins))]\n    if opt.feat_normal:\n        visuals += [(\'input_normal\', util.tensor2im(normal[0]))]\n    visuals += [(\'synthesized_image\', util.tensor2im(generated.data[0])),\n                (\'real_image\', util.tensor2im(base_img))]\n    visuals += [(\'feat\', util.tensor2im(feat[0][:3]))]\n    visuals = OrderedDict(visuals)\n\n    img_path = [opt.edit_source.replace(\'.png\', \'_%05d.png\' % i)]\n    print(\'process image... %s\' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n'"
textural/test.py,2,"b""# System libs\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(__file__))\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n\n# Third party libs\nimport torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport json\n\n# our libs\nfrom util import util, html\nfrom util.visualizer import Visualizer\nfrom models.models import create_model as create_pix2pix_model\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\n\nopt = TestOptions().parse(save=False)\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\ndata_loader = CreateDataLoader(opt)\nprint('#testing images = %d' % len(data_loader))\n\ndataset = data_loader.load_data()\nmodel = create_pix2pix_model(opt)\nvisualizer = Visualizer(opt)\n\n# create website\nname = '%s_%s_%s' % (opt.phase, opt.which_epoch, opt.experiment_name)\nif opt.how_many > 0:\n    name += '_%d_imgs' % opt.how_many\nif opt.segm_precomputed_path:\n    name += '_predicted'\nweb_dir = os.path.join(opt.checkpoints_dir, opt.name, name)\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch), verbose=True)\n\n# attach results of other experiments, aligned by each line\nexps = []\nif exps:\n    webpage.attach(exps)\n\n# test\nloss = torch.nn.L1Loss()\nlosses = []\npaths = []\nfor i, data in enumerate((dataset)):\n    if i >= opt.how_many:\n        break\n\n    generated = model.fake_inference(data['image'], data['label'], data['inst'], pose=data['pose'], normal=data['normal'], depth=data['depth'])\n\n    visuals = [('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n               ('input_inst', util.tensor2label(data['inst'][0], opt.label_nc))]\n    if opt.feat_pose:\n        visuals += [('input_pose', util.tensor2label(data['pose'][0], opt.feat_pose_num_bins))]\n    if opt.feat_normal:\n        visuals += [('input_normal', util.tensor2im(data['normal'][0]))]\n    if opt.feat_depth:\n        visuals += [('input_depth', util.tensor2im(data['depth'][0]))]\n    visuals += [('real_image', util.tensor2im(data['image'][0])),\n                ('%s_%s_%s' % (opt.phase, opt.which_epoch, opt.experiment_name), util.tensor2im(generated.data[0])), ]\n    visuals = OrderedDict(visuals)\n    losses += [loss(Variable(data['image']), generated.cpu()).data[0]]\n    path = data['path'][0]\n    paths += [path]\n    img_path = [data['path'][0].replace('.png', '-%04d.png' % i)]\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\njson.dump(paths, open(os.path.join(opt.checkpoints_dir, opt.name, 'list.json'), 'w'))\navg_loss = sum(losses) / float(len(losses))\nprint('avg:', avg_loss)\nwebpage.add_header(str(avg_loss))\nwebpage.save()\nprint(sum(losses) / float(len(losses)))\n"""
textural/train.py,2,"b""# System libs\nimport time\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(__file__))\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n\n# Third party libs\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\n\n# our libs\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom util import util\nfrom util.visualizer import Visualizer\nfrom models.models import create_model as create_pix2pix_model\n\n\ndef main():\n    # Dealing with options\n    opt = TrainOptions().parse()\n    iter_path = os.path.join(opt.checkpoints_dir, opt.name, 'iter.txt')\n    if opt.continue_train:\n        try:\n            start_epoch, epoch_iter = np.loadtxt(iter_path, delimiter=',', dtype=int)\n        except:\n            start_epoch, epoch_iter = 1, 0\n        print('Resuming from epoch %d at iteration %d' % (start_epoch, epoch_iter))\n    else:\n        start_epoch, epoch_iter = 1, 0\n\n    if opt.debug:\n        opt.display_freq = 1\n        opt.print_freq = 1\n        opt.niter = 1\n        opt.niter_decay = 0\n        opt.max_dataset_size = 10\n\n    # Data loader\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)\n    print('#training images = %d' % dataset_size)\n\n    # pix2pix model\n    pix2pix_model = create_pix2pix_model(opt)\n    visualizer = Visualizer(opt)\n\n    # Training\n    total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        if epoch != start_epoch:\n            epoch_iter = epoch_iter % dataset_size\n        losses_G = []\n        losses_D = []\n        for i, data in enumerate(dataset, start=epoch_iter):\n            iter_start_time = time.time()\n            total_steps += opt.batchSize\n            epoch_iter += opt.batchSize\n\n            # whether to collect output images\n            save_fake = total_steps % opt.display_freq == 0\n\n        # Forward Pass\n            losses, generated = pix2pix_model(\n                Variable(data['label']), Variable(data['inst']), Variable(data['image']), Variable(data['feat']),\n                Variable(data['pose']), Variable(data['normal']), Variable(data['depth']), infer=save_fake)\n\n            # sum per device losses\n            losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]\n            loss_dict = dict(zip(pix2pix_model.module.loss_names, losses))\n            # print(loss_dict)\n\n            # calculate final loss scalar\n            loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n            loss_G = loss_dict['G_GAN'] + loss_dict['G_GAN_Feat'] + loss_dict['G_VGG'] + loss_dict['G_L1'] + loss_dict['E_VAE']\n            losses_D.append(loss_D.data[0])\n            losses_G.append(loss_G.data[0])\n            loss_dict['d_total'] = loss_D\n            loss_dict['g_total'] = loss_G\n\n            # Backward Pass\n            # update generator weights\n            pix2pix_model.module.optimizer_G.zero_grad()\n            loss_G.backward()\n            pix2pix_model.module.optimizer_G.step()\n\n            # update discriminator weights\n            pix2pix_model.module.optimizer_D.zero_grad()\n            loss_D.backward()\n            pix2pix_model.module.optimizer_D.step()\n\n            # Display results and errors\n            # print out errors\n            if total_steps % opt.print_freq == 0:\n                errors = {k: v.data[0] if not isinstance(v, int) else v for k, v in loss_dict.items()}\n                t = (time.time() - iter_start_time) / opt.batchSize\n                visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n                visualizer.plot_current_errors(errors, total_steps)\n\n            # display output images\n            if save_fake:\n                visuals = [('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n                           ('input_inst', util.tensor2label(data['inst'][0], opt.label_nc))]\n                if opt.feat_pose:\n                    visuals += [('input_pose', util.tensor2label(data['pose'][0], opt.feat_pose_num_bins))]\n                if opt.feat_normal:\n                    visuals += [('input_normal', util.tensor2im(data['normal'][0]))]\n                if opt.feat_depth:\n                    visuals += [('input_depth', util.tensor2im(data['depth'][0]))]\n                visuals += [('synthesized_image', util.tensor2im(generated.data[0])),\n                            ('real_image', util.tensor2im(data['image'][0]))]\n                visuals = OrderedDict(visuals)\n                visualizer.display_current_results(visuals, epoch, total_steps)\n\n            # save latest model\n            if total_steps % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n                pix2pix_model.module.save('latest')\n                np.savetxt(iter_path, (epoch, epoch_iter), delimiter=',', fmt='%d')\n\n        # end of epoch\n        print('End of epoch %d / %d \\t Time Taken: %d sec \\t loss_G: %lf loss_D: %lf' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time, np.mean(losses_G), np.mean(losses_D)))\n        visualizer.print_current_error(epoch, np.mean(losses_G), np.mean(losses_D))\n\n        # save model for this epoch\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n            pix2pix_model.module.save('latest')\n            pix2pix_model.module.save(epoch)\n        np.savetxt(iter_path, (epoch + 1, 0), delimiter=',', fmt='%d')\n\n        # instead of only training the local enhancer, train the entire network after certain iterations\n        if (opt.niter_fix_global != 0) and (epoch == opt.niter_fix_global):\n            pix2pix_model.module.update_fixed_params()\n\n        # linearly decay learning rate after certain iterations\n        if epoch > opt.niter:\n            pix2pix_model.module.update_learning_rate()\n\n\nif __name__ == '__main__':\n    main()\n"""
geometric/derender3d/__init__.py,0,b'class TargetType:\n    geometry = (1 << 0)\n    reproject = (1 << 1)\n    normal = (1 << 2)\n    depth = (1 << 3)\n\n    pretrain = geometry\n    finetune = reproject\n    full = geometry | reproject\n    extend = geometry | reproject | normal | depth\n'
geometric/derender3d/data_loader.py,5,"b""import collections\nimport numpy as np\nimport torch\n\nfrom derender3d import TargetType\nfrom derender3d.datasets import (\n    VKitti,\n    KittiObject,\n    KittiSemantics,\n    KittiSemanticsHybrid,\n    CityscapesSemantics,\n    CityscapesSemanticsHybrid,\n)\n\n\n# collate with missing keys\ndef collate_fn(batch):\n    for d in batch:\n        if d is not None:\n            obj = d\n            break\n\n    if isinstance(obj, torch.Tensor):\n        for (num, d) in enumerate(batch):\n            if d is None:\n                batch[num] = torch.zeros_like(obj)\n\n    elif type(obj).__module__ == 'numpy':\n        for (num, d) in enumerate(batch):\n            if d is None:\n                batch[num] = np.zeros_like(obj)\n\n    elif isinstance(obj, collections.Mapping):\n        keys = np.unique([key for d in batch for key in d.keys()])\n        return {key: collate_fn([d.get(key, None) for d in batch]) for key in keys}\n\n    return torch.utils.data.dataloader.default_collate(batch)\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n    def __init__(self, dataset, mode, batch_size, num_workers, is_train, debug=False):\n        if dataset == 'vkitti':\n            dataset = VKitti(is_train=is_train, debug=debug)\n            shuffle = is_train\n            sampler = None\n\n        elif dataset == 'kitti':\n            if (mode == TargetType.pretrain) or (mode == TargetType.extend):\n                dataset = KittiObject(is_train=is_train, debug=debug)\n                shuffle = is_train\n                sampler = None\n\n            elif mode == TargetType.finetune:\n                dataset = KittiSemantics(is_train=is_train, debug=debug)\n                shuffle = is_train\n                sampler = None\n\n            elif mode == TargetType.full:\n                dataset = KittiSemanticsHybrid(is_train=is_train, debug=debug)\n                shuffle = None\n                sampler = torch.utils.data.sampler.WeightedRandomSampler(dataset.get_weights(), len(dataset))\n\n        elif dataset == 'cityscapes':\n            if mode == TargetType.full:\n                dataset = CityscapesSemanticsHybrid(is_train=is_train, debug=debug)\n                shuffle = is_train\n                sampler = None\n\n            elif mode == TargetType.extend:\n                dataset = CityscapesSemantics(is_train=is_train, debug=debug)\n                shuffle = is_train\n                sampler = None\n\n        super(DataLoader, self).__init__(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler,\n            num_workers=num_workers,\n            collate_fn=collate_fn,\n            pin_memory=True,  # No support for users without GPUs, sorry...\n        )\n"""
geometric/derender3d/datasets.py,6,"b""import json\nimport matplotlib.cm\nimport numpy as np\nimport os\nimport pandas as pd\nimport PIL.Image\nimport PIL.ImageDraw\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport random\nimport scipy.ndimage\nimport scipy.special\n\nfrom derender3d import TargetType\n\n\nclass Transforms(object):\n    pad = torchvision.transforms.functional.pad\n    crop = torchvision.transforms.functional.crop\n    resize = torchvision.transforms.functional.resize\n    normalize = torchvision.transforms.functional.normalize\n    to_tensor = torchvision.transforms.functional.to_tensor\n    to_pil_image = torchvision.transforms.functional.to_pil_image\n    color_jitter = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n\n    # torch.Tensor -> torch.Tensor\n    @staticmethod\n    def pad_like(image, _image, mode='constant', value=0):\n        pad_size_2 = _image.shape[2] - image.shape[2]\n        pad_size_3 = _image.shape[3] - image.shape[3]\n\n        return F.pad(image, (pad_size_3 // 2, pad_size_3 // 2, pad_size_2 // 2, pad_size_2 // 2), mode=mode, value=value)\n\n    # list -> list\n    @staticmethod\n    def roi_jitter(roi, ratio=0.1):\n        droi_y = int(ratio * (roi[2] - roi[0]))\n        droi_x = int(ratio * (roi[3] - roi[1]))\n\n        return [\n            roi[0] + random.randint(- droi_y, droi_y),\n            roi[1] + random.randint(- droi_x, droi_x),\n            roi[2] + random.randint(- droi_y, droi_y),\n            roi[3] + random.randint(- droi_x, droi_x),\n        ]\n\n    # PIL.Image -> PIL.Image\n    @staticmethod\n    def crop_square(image, roi, fill):\n        h = roi[2] - roi[0]\n        w = roi[3] - roi[1]\n\n        s = max(h, w)\n        dh = (s - h) // 2\n        dw = (s - w) // 2\n\n        padding = [\n            - min(0, roi[1] - dw),\n            - min(0, roi[0] - dh),\n            max(0, roi[3] + dw - image.width),\n            max(0, roi[2] + dh - image.height),\n        ]\n\n        _l = roi[1] - dw + padding[0]\n        _t = roi[0] - dh + padding[1]\n\n        image = Transforms.pad(image, padding=tuple(padding), fill=fill)\n        image = Transforms.crop(image, _t, _l, s, s)\n\n        return image\n\n    # np.ndarray -> np.ndarray\n    @staticmethod\n    def scene_to_mask(image_scene, code):\n        return np.all(image_scene == code, axis=2, keepdims=True).astype(np.float32)\n\n    # np.ndarray -> np.ndarray\n    @staticmethod\n    def depth_to_normal(image_depth):\n        delta_v = scipy.ndimage.correlate1d(image_depth, weights=[-0.5, 0, 0.5], axis=0, mode='nearest')\n        delta_u = scipy.ndimage.correlate1d(image_depth, weights=[-0.5, 0, 0.5], axis=1, mode='nearest')\n\n        image_normal = np.stack([\n            delta_u,\n            - delta_v,\n            np.ones_like(image_depth),\n        ], axis=2).astype(np.float32)\n        image_normal /= np.sqrt(np.sum(np.square(image_normal), axis=2, keepdims=True))\n\n        return image_normal\n\n    # np.ndarray -> list\n    @staticmethod\n    def mask_to_roi(image_mask):\n        index_row = np.where(np.any(image_mask, axis=0))[0]\n        index_col = np.where(np.any(image_mask, axis=1))[0]\n        return [\n            int(index_col[0]),\n            int(index_row[0]),\n            int(index_col[-1] + 1),\n            int(index_row[-1] + 1),\n        ]\n\n    # np.ndarray -> np.ndarray\n    @staticmethod\n    def map_to_cm(image_map):\n        image_map = image_map / np.max(image_map)\n        image_cm = matplotlib.cm.jet(image_map).astype(np.float32)\n        image_cm[image_map == 0, :3] = 1.0\n\n        return image_cm\n\n    # np.ndarray -> PIL.Image\n    @staticmethod\n    def visualize(image_rgb, image_map, rois, interests=None, alpha=0.5):\n        image_map_pil = Transforms.to_pil_image(np.uint8(np.transpose(image_map, (1, 2, 0))))\n\n        image_cm = Transforms.map_to_cm(image_map[0])\n        image_cm[..., 3] = alpha * (image_map[0] > 0)\n        image_cm_pil = Transforms.to_pil_image(np.uint8(image_cm * 255))\n\n        image_rgb_pil = Transforms.to_pil_image(image_rgb).convert(mode='RGBA')\n        image_rgb_pil.paste(image_cm_pil.convert('RGB'), (0, 0), image_cm_pil)\n\n        draw = PIL.ImageDraw.Draw(image_rgb_pil)\n        for num in range(len(rois)):\n            roi = rois[num]\n\n            if (interests is not None) and interests[num]:\n                color = 'green'\n            else:\n                color = 'red'\n\n            draw.rectangle([roi[1], roi[0], roi[3], roi[2]], outline=color)\n\n        return (image_map_pil, image_rgb_pil)\n\n\nclass BaseDataset(torch.utils.data.dataset.Dataset):\n    def transform_ignore(self, image_ignore, roi):\n        image_ignore_pil = Transforms.to_pil_image(np.uint8(image_ignore * 255))\n\n        ignore_pil = Transforms.crop_square(image_ignore_pil, roi, fill=255)\n        ignore_pil = Transforms.resize(ignore_pil, (256, 256))\n        ignore = Transforms.to_tensor(ignore_pil)\n        return ignore\n\n    def transform_mask(self, image_mask, roi):\n        image_mask_pil = Transforms.to_pil_image(np.uint8(image_mask * 255))\n\n        mask_pil = Transforms.crop_square(image_mask_pil, roi, fill=0)\n        mask_pil = Transforms.resize(mask_pil, (256, 256))\n        mask = Transforms.to_tensor(mask_pil)\n        return mask\n\n    def transform_rgb(self, image_rgb, roi):\n        image_rgb_pil = Transforms.to_pil_image(image_rgb)\n\n        rgb_pil = Transforms.crop_square(image_rgb_pil, roi, fill=(127, 127, 127))\n        if self.is_train:\n            rgb_pil = Transforms.color_jitter(rgb_pil)\n        rgb_pil = Transforms.resize(rgb_pil, (224, 224))\n        rgb = Transforms.to_tensor(rgb_pil)\n\n        rgb = Transforms.normalize(\n            rgb,\n            mean=self.mean,\n            std=self.std,\n        )\n\n        return rgb\n\n\nclass HybridDataset(torch.utils.data.dataset.ConcatDataset):\n    def __init__(self, datasets, weights=None):\n        super(HybridDataset, self).__init__(datasets)\n\n        if weights is None:\n            weights = [1.0] * len(datasets)\n\n        self.weights = weights\n\n    def get_weights(self):\n        weights = np.concatenate([\n            weight * np.ones(len(dataset)) / len(dataset)\n            for (dataset, weight) in zip(self.datasets, self.weights)\n        ], axis=0)\n\n        return weights\n\n\nclass VKitti(BaseDataset):\n    root_dir = os.getenv('VKITTI_ROOT_DIR')\n\n    worlds = ['0001', '0002', '0006', '0018', '0020']\n    train_frames = [range(0, 356), range(0, 185), range(69, 270), range(0, 270), range(167, 837)]\n    test_frames = [range(356, 447), range(185, 233), range(0, 69), range(270, 339), range(0, 167)]\n    topics = ['15-deg-left', '15-deg-right', '30-deg-left', '30-deg-right', 'clone', 'fog', 'morning', 'overcast', 'rain', 'sunset']\n\n    motgt_df = None\n    scenegt_df = None\n\n    mean = [0.5, 0.5, 0.5]\n    std = [0.25, 0.25, 0.25]\n\n    class Camera:\n        width = 1242\n        height = 375\n\n        focal = 725.0\n        u0 = 620.5\n        v0 = 187.0\n\n    @staticmethod\n    def read_scene(world, topic, frame):\n        path = os.path.join(VKitti.root_dir, 'vkitti_1.3.1_scenegt', world, topic, '{:05d}.png'.format(frame))\n        image_scene_pil = PIL.Image.open(path)\n        return np.asarray(image_scene_pil)\n\n    @staticmethod\n    def read_rgb(world, topic, frame):\n        path = os.path.join(VKitti.root_dir, 'vkitti_1.3.1_rgb', world, topic, '{:05d}.png'.format(frame))\n        image_rgb_pil = PIL.Image.open(path)\n        return np.asarray(image_rgb_pil)\n\n    @staticmethod\n    def read_depth(world, topic, frame):\n        path = os.path.join(VKitti.root_dir, 'vkitti_1.3.1_depthgt', world, topic, '{:05d}.png'.format(frame))\n        image_depth_pil = PIL.Image.open(path)\n        image_depth = np.asarray(image_depth_pil).astype(np.float32) / 100 * VKitti.Camera.focal\n\n        return image_depth\n\n    @staticmethod\n    def _read_motgt(debug=False):\n        motgt_dfs = []\n        for world in VKitti.worlds:\n            for topic in VKitti.topics:\n                motgt_path = os.path.join(VKitti.root_dir, 'vkitti_1.3.1_motgt', '{:s}_{:s}.txt'.format(world, topic))\n                if not os.path.isfile(motgt_path):\n                    continue\n\n                print('Reading {:s}'.format(motgt_path))\n\n                df = pd.read_csv(motgt_path, sep=' ', index_col=False)\n                df['world'] = world\n                df['topic'] = topic\n\n                motgt_dfs.append(df)\n\n        if motgt_dfs:\n            VKitti.motgt_df = pd.concat(motgt_dfs).set_index(['world', 'topic', 'frame'])\n        else:\n            VKitti.motgt_df = pd.DataFrame([], index=['world', 'topic', 'frame'])\n\n    @staticmethod\n    def _read_scenegt(debug=False):\n        scenegt_dfs = []\n        for world in VKitti.worlds:\n            for topic in VKitti.topics:\n                scenegt_path = os.path.join(VKitti.root_dir, 'vkitti_1.3.1_scenegt', '{:s}_{:s}_scenegt_rgb_encoding.txt'.format(world, topic))\n                if not os.path.isfile(scenegt_path):\n                    continue\n\n                print('Reading {:s}'.format(scenegt_path))\n\n                df = pd.read_csv(scenegt_path, sep=' ', index_col=False)\n                df['world'] = world\n                df['topic'] = topic\n\n                scenegt_dfs.append(df)\n\n        if scenegt_dfs:\n            VKitti.scenegt_df = pd.concat(scenegt_dfs).set_index(['world', 'topic', 'Category(:id)'])\n        else:\n            VKitti.scenegt_df = pd.DataFrame([], index=['world', 'topic', 'Category(:id)'])\n\n    def __init__(self, is_train=False, is_evaluate=False, debug=False):\n        self.is_train = is_train\n        self.is_evaluate = is_evaluate\n\n        if VKitti.motgt_df is None:\n            VKitti._read_motgt(debug=debug)\n\n        if VKitti.scenegt_df is None:\n            VKitti._read_scenegt(debug=debug)\n\n        if VKitti.motgt_df.size == 0:\n            self.df = None\n            return\n\n        subsample = []\n        for (num_world, world) in enumerate(VKitti.worlds):\n            if is_train:\n                _frames = VKitti.train_frames[num_world]\n            else:\n                _frames = VKitti.test_frames[num_world]\n\n            for topic in VKitti.topics:\n                frames = VKitti.motgt_df.loc[(world, topic)].index.unique()\n                for frame in frames:\n                    if frame in _frames:\n                        subsample.append((\n                            world,\n                            topic,\n                            frame,\n                        ))\n\n        self.df = VKitti.motgt_df.loc[subsample]\n\n        if not is_evaluate:\n            roi = np.stack([\n                self.df.t,\n                self.df.l,\n                self.df.b,\n                self.df.r,\n            ], axis=1)\n            droi_y = roi[:, 2] - roi[:, 0]\n            droi_x = roi[:, 3] - roi[:, 1]\n            sel = (\n                (droi_y * droi_x > 16 * 16) &\n                (self.df.truncr < 0.7) &\n                (self.df.occupr > 0.3)\n            )\n\n            self.df = self.df[sel]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        (world, topic, frame) = item.name\n\n        motgt_df = VKitti.motgt_df.loc[(world, topic, frame)]\n        scenegt_df = VKitti.scenegt_df.loc[(world, topic)]\n\n        image_scene = VKitti.read_scene(world, topic, frame)\n        if not self.is_evaluate:\n            image_rgb = VKitti.read_rgb(world, topic, frame)\n\n        name = item.orig_label + ':' + item.tid.astype(np.str)\n        code = scenegt_df.loc[name].values\n        image_mask = np.all(image_scene[:, :, None, :] == code, axis=3)\n\n        roi = Transforms.mask_to_roi(image_mask)\n        if self.is_train:\n            roi = Transforms.roi_jitter(roi)\n\n        roi_norm = [\n            (roi[0] - VKitti.Camera.v0) / VKitti.Camera.focal,\n            (roi[1] - VKitti.Camera.u0) / VKitti.Camera.focal,\n            (roi[2] - VKitti.Camera.v0) / VKitti.Camera.focal,\n            (roi[3] - VKitti.Camera.u0) / VKitti.Camera.focal,\n        ]\n        mroi_norm = [\n            (roi_norm[2] + roi_norm[0]) / 2.0,\n            (roi_norm[3] + roi_norm[1]) / 2.0,\n        ]\n        droi_norm = [\n            (roi_norm[2] - roi_norm[0]),\n            (roi_norm[3] - roi_norm[1]),\n        ]\n\n        theta = [- item.ry]\n        rotation = [np.cos(item.ry / 2), 0, - np.sin(item.ry / 2), 0]\n        scale = [item.l3d, item.h3d, 1.2206 * item.w3d]\n        xyz = [item.x3d, - (item.y3d - item.h3d / 2), - item.z3d]\n\n        translation2d = [\n            (xyz[1] / xyz[2] - mroi_norm[0]) / droi_norm[0],\n            (- xyz[0] / xyz[2] - mroi_norm[1]) / droi_norm[1],\n        ]\n        translation2d = np.clip(translation2d, -6, 6)\n        log_scale = np.log(scale)\n\n        depth = np.sum(np.square(xyz), axis=0, keepdims=True)\n        log_depth = (\n            np.log(depth) +\n            np.log(droi_norm[0]) +\n            np.log(droi_norm[1])\n        )\n\n        xyzs = np.stack([motgt_df.x3d, - (motgt_df.y3d - motgt_df.h3d / 2), - motgt_df.z3d], axis=1)\n        depths = np.sum(xyzs ** 2, axis=1)\n\n        names = motgt_df.orig_label + ':' + motgt_df.tid.astype(np.str)\n        codes = scenegt_df.loc[names.values].values\n        image_masks = np.all(image_scene[:, :, None, :] == codes, axis=3)\n        image_ignore = np.sum(image_masks * (depths < depth), axis=2, keepdims=True)\n\n        res = {\n            'targets': TargetType.pretrain | TargetType.finetune,\n            'image_masks': Transforms.to_tensor(255 * image_mask),\n            'image_ignores': Transforms.to_tensor(255 * image_ignore),\n            'widths': np.float32([VKitti.Camera.width]),\n            'heights': np.float32([VKitti.Camera.height]),\n            'focals': np.float32([VKitti.Camera.focal]),\n            'u0s': np.float32([VKitti.Camera.u0]),\n            'v0s': np.float32([VKitti.Camera.v0]),\n            'rois': np.float32(roi),\n            'roi_norms': np.float32(roi_norm),\n            'thetas': np.float32(theta),\n            'rotations': np.float32(rotation),\n            'translations': np.float32(xyz),\n            'translation2ds': np.float32(translation2d),\n            'scales': np.float32(scale),\n            'log_scales': np.float32(log_scale),\n            'log_depths': np.float32(log_depth),\n        }\n\n        if not self.is_evaluate:\n            res.update({\n                'images': self.transform_rgb(image_rgb, roi),\n                'masks': self.transform_mask(image_mask, roi),\n                'ignores': self.transform_ignore(image_ignore, roi),\n            })\n\n        return res\n\n\nclass KittiBaseDataset(BaseDataset):\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    class Camera:\n        focal = 725.0\n        u0 = 610.0\n        v0 = 185.0\n\n\nclass KittiObject(KittiBaseDataset):\n    root_dir = os.getenv('KITTI_OBJECT_ROOT_DIR')\n\n    train_frames = range(0, 6733)\n    validation_frames = range(6733, 7481)\n    debug_train_frames = range(0, 10)\n    debug_validation_frames = range(10, 20)\n    test_frames = range(0, 7518)\n\n    motgt_names = [\n        'type',\n        'truncated',\n        'occluded',\n        'alpha',\n        'left', 'top', 'right', 'bottom',\n        'h', 'w', 'l',\n        'x', 'y', 'z',\n        'ry',\n        'score',\n    ]\n\n    motgt_df = None\n    camera_df = None\n\n    @staticmethod\n    def read_rgb(region, frame):\n        if (region == 'training') or (region == 'validation'):\n            name = 'training'\n\n        path = os.path.join(KittiObject.root_dir, name, 'image_2', '{:06d}.png'.format(frame))\n        image_rgb_pil = PIL.Image.open(path)\n        return np.asarray(image_rgb_pil)\n\n    @staticmethod\n    def _read_motgt(debug=False):\n        motgt_dfs = []\n\n        for region in ['training', 'validation']:\n            if region == 'training':\n                if debug:\n                    frames = KittiObject.debug_train_frames\n                else:\n                    frames = KittiObject.train_frames\n            elif region == 'validation':\n                if debug:\n                    frames = KittiObject.debug_validation_frames\n                else:\n                    frames = KittiObject.validation_frames\n\n            for frame in frames:\n                motgt_path = os.path.join(KittiObject.root_dir, 'training', 'label_2', '{:06d}.txt'.format(frame))\n                print('Reading {:s}'.format(motgt_path))\n\n                df = pd.read_csv(motgt_path, sep=' ', names=KittiObject.motgt_names, header=None)\n                df['region'] = region\n                df['frame'] = frame\n\n                motgt_dfs.append(df)\n\n        KittiObject.motgt_df = pd.concat(motgt_dfs).set_index(['region', 'frame'])\n\n    @staticmethod\n    def _read_camera(debug=False):\n        camera_dfs = []\n\n        for region in ['training', 'validation']:\n            if region == 'training':\n                if debug:\n                    frames = KittiObject.debug_train_frames\n                else:\n                    frames = KittiObject.train_frames\n            elif region == 'validation':\n                if debug:\n                    frames = KittiObject.debug_validation_frames\n                else:\n                    frames = KittiObject.validation_frames\n\n            for frame in frames:\n                camera_path = os.path.join(KittiObject.root_dir, 'training', 'calib', '{:06d}.txt'.format(frame))\n                print('Reading {:s}'.format(camera_path))\n\n                df = pd.read_csv(\n                    camera_path,\n                    sep=' ',\n                    header=None,\n                    names=['name', 'focal', 'u0', 'v0'],\n                    index_col=0,\n                    usecols=[0, 1, 3, 7],\n                )\n                df = df.loc[['P2:']]\n                df['region'] = region\n                df['frame'] = frame\n\n                camera_dfs.append(df)\n\n        KittiObject.camera_df = pd.concat(camera_dfs).set_index(['region', 'frame'])\n\n    def __init__(self, is_train=False, debug=False):\n        self.is_train = is_train\n\n        if KittiObject.motgt_df is None:\n            KittiObject._read_motgt(debug=debug)\n\n        if KittiObject.camera_df is None:\n            KittiObject._read_camera(debug=debug)\n\n        if is_train:\n            self.df = KittiObject.motgt_df.loc[['training']]\n            self.df = self.df[self.df.type.isin(['Car', 'Van', 'Truck'])]\n        else:\n            self.df = KittiObject.motgt_df.loc[['validation']]\n            self.df = self.df[self.df.type.isin(['Car'])]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        (region, frame) = item.name\n\n        camera_item = KittiObject.camera_df.loc[(region, frame)]\n\n        image_rgb = KittiObject.read_rgb(region, frame)\n\n        roi_norm = [\n            (item.top - camera_item.v0) / camera_item.focal,\n            (item.left - camera_item.u0) / camera_item.focal,\n            (item.bottom - camera_item.v0) / camera_item.focal,\n            (item.right - camera_item.u0) / camera_item.focal,\n        ]\n        mroi_norm = [\n            (roi_norm[2] + roi_norm[0]) / 2.0,\n            (roi_norm[3] + roi_norm[1]) / 2.0,\n        ]\n        droi_norm = [\n            (roi_norm[2] - roi_norm[0]),\n            (roi_norm[3] - roi_norm[1]),\n        ]\n\n        image_rgb = self.transform_rgb(image_rgb, [\n            int(item.top),\n            int(item.left),\n            int(item.bottom),\n            int(item.right),\n        ])\n\n        focal = [camera_item.focal]\n        theta = [- item.ry]\n        scale = [item.l, item.h, item.w]\n        xyz = [item.x, - (item.y - item.h / 2), - item.z]\n\n        translation2d = [\n            (xyz[1] / xyz[2] - mroi_norm[0]) / droi_norm[0],\n            (- xyz[0] / xyz[2] - mroi_norm[1]) / droi_norm[1],\n        ]\n        translation2d = np.clip(translation2d, -6, 6)\n        log_scale = np.log(scale)\n        depth = np.sum(np.square(xyz), axis=0, keepdims=True)\n        log_depth = (\n            np.log(depth) +\n            np.log(droi_norm[0]) +\n            np.log(droi_norm[1])\n        )\n\n        return {\n            'targets': TargetType.pretrain,\n            'images': image_rgb,\n            'focals': np.float32(focal),\n            'roi_norms': np.float32(roi_norm),\n            'thetas': np.float32(theta),\n            'translation2ds': np.float32(translation2d),\n            'log_scales': np.float32(log_scale),\n            'log_depths': np.float32(log_depth),\n        }\n\n\nclass KittiSemantics(KittiBaseDataset):\n    root_dir = os.getenv('KITTI_SEMANTICS_ROOT_DIR')\n    cache_dir = os.getenv('KITTI_SEMANTICS_CACHE_DIR')\n\n    train_frames = range(0, 180)\n    validation_frames = range(180, 200)\n    debug_train_frames = range(0, 10)\n    debug_validation_frames = range(10, 20)\n\n    scenegt_df = None\n\n    class Category:\n        car = 66\n\n    @staticmethod\n    def index2cat(index):\n        return index // 100\n\n    @staticmethod\n    def read_rgb(region, frame):\n        if (region == 'training') or (region == 'validation'):\n            name = 'training'\n\n        path = os.path.join(KittiSemantics.root_dir, name, 'image_2', '{:06d}_10.png'.format(frame))\n        image_rgb_pil = PIL.Image.open(path)\n        return np.asarray(image_rgb_pil)\n\n    @staticmethod\n    def read_scene(region, frame):\n        if (region == 'training') or (region == 'validation'):\n            name = 'training'\n\n        path = os.path.join(KittiSemantics.root_dir, name, 'instance', '{:06d}_10.png'.format(frame))\n        image_scene_pil = PIL.Image.open(path)\n        return np.asarray(image_scene_pil)\n\n    @staticmethod\n    def _read_scenegt(debug=False):\n        scenegt_dfs = []\n\n        if not os.path.isdir(KittiSemantics.cache_dir):\n            os.makedirs(KittiSemantics.cache_dir)\n\n        for region in ['training', 'validation']:\n            if region == 'training':\n                if debug:\n                    frames = KittiSemantics.debug_train_frames\n                else:\n                    frames = KittiSemantics.train_frames\n            elif region == 'validation':\n                if debug:\n                    frames = KittiSemantics.debug_validation_frames\n                else:\n                    frames = KittiSemantics.validation_frames\n\n            for frame in frames:\n                json_path = os.path.join(KittiSemantics.cache_dir, '_{:06d}.json'.format(frame))\n                print('Reading {:s}'.format(json_path))\n\n                if os.path.isfile(json_path):\n                    with open(json_path, 'r') as f:\n                        json_objs = json.load(f)\n                else:\n                    image_scene = KittiSemantics.read_scene(region, frame)\n                    obj_indices = np.unique(image_scene)\n\n                    json_objs = []\n                    for obj_index in obj_indices:\n                        image_mask_array = (image_scene == obj_index)\n                        index_row = np.where(np.any(image_mask_array, axis=0))[0]\n                        index_col = np.where(np.any(image_mask_array, axis=1))[0]\n                        roi = np.asarray([\n                            index_col[0],\n                            index_row[0],\n                            index_col[-1] + 1,\n                            index_row[-1] + 1,\n                        ])\n\n                        json_objs.append({\n                            'obj_index': int(obj_index),\n                            'roi': roi.tolist(),\n                        })\n\n                    with open(json_path, 'w') as f:\n                        json.dump(json_objs, f)\n\n                for json_obj in json_objs:\n                    obj_index = json_obj['obj_index']\n                    roi = json_obj['roi']\n\n                    if not KittiSemantics.index2cat(obj_index) == KittiSemantics.Category.car:\n                        continue\n\n                    scenegt_dfs.append({\n                        'region': region,\n                        'frame': frame,\n                        'obj_index': obj_index,\n                        'roi': roi,\n                    })\n\n        KittiSemantics.scenegt_df = pd.DataFrame(scenegt_dfs).set_index(['region', 'frame'])\n\n    def __init__(self, is_train=False, debug=False):\n        self.is_train = is_train\n\n        if KittiSemantics.scenegt_df is None:\n            KittiSemantics._read_scenegt(debug=debug)\n\n        if is_train:\n            self.df = KittiSemantics.scenegt_df.loc[['training']]\n        else:\n            self.df = KittiSemantics.scenegt_df.loc[['validation']]\n\n        roi = np.asarray(self.df.roi.values.tolist())\n\n        droi_y = roi[:, 2] - roi[:, 0]\n        droi_x = roi[:, 3] - roi[:, 1]\n        sel = (\n            (droi_y * droi_x > 32 * 32) *\n            (droi_x / droi_y < 4) *\n            (droi_y / droi_x < 4)\n        )\n\n        self.df = self.df[sel]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        (region, frame) = item.name\n\n        image_scene = KittiSemantics.read_scene(region, frame)\n        image_rgb = KittiSemantics.read_rgb(region, frame)\n\n        u0 = (image_rgb.shape[1] - 1) / 2\n        v0 = (image_rgb.shape[0] - 1) / 2\n\n        obj_index = item.obj_index\n        roi = item.roi\n        if self.is_train:\n            roi = Transforms.roi_jitter(roi)\n\n        roi_norm = [\n            (roi[0] - v0) / KittiSemantics.Camera.focal,\n            (roi[1] - u0) / KittiSemantics.Camera.focal,\n            (roi[2] - v0) / KittiSemantics.Camera.focal,\n            (roi[3] - u0) / KittiSemantics.Camera.focal,\n        ]\n        focal = [KittiSemantics.Camera.focal]\n\n        image_mask = (image_scene == obj_index)[:, :, None]\n\n        return {\n            'targets': TargetType.finetune,\n            'images': self.transform_rgb(image_rgb, roi),\n            'focals': np.float32(focal),\n            'masks': self.transform_mask(image_mask, roi),\n            'ignores': torch.zeros(1, 256, 256),\n            'roi_norms': np.float32(roi_norm),\n        }\n\n\nclass KittiSemanticsHybrid(HybridDataset):\n    def __init__(self, is_train, debug=False):\n        super(KittiSemanticsHybrid, self).__init__([\n            KittiObject(is_train=is_train, debug=debug),\n            KittiSemantics(is_train=is_train, debug=debug),\n        ])\n\n\nclass CityscapesBaseDataset(BaseDataset):\n    root_dir = os.getenv('CITYSCAPES_ROOT_DIR')\n\n    camera_df = None\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    class Camera:\n        focal = 2250.0\n        u0 = 925.0\n        v0 = 460.0\n\n    @staticmethod\n    def read_rgb(split, city, seq, frame):\n        path = os.path.join(CityscapesBaseDataset.root_dir, 'images', 'leftImg8bit', split, city, '{:s}_{:s}_{:s}_leftImg8bit.png'.format(city, seq, frame))\n        image_pil = PIL.Image.open(path)\n        return np.asarray(image_pil)\n\n    @staticmethod\n    def _read_camera(splits, debug=False):\n        camera_dfs = []\n\n        for split in splits:\n            split_dir = os.path.join(CityscapesBaseDataset.root_dir, 'camera', split)\n\n            for city in os.listdir(split_dir):\n                city_dir = os.path.join(split_dir, city)\n\n                for name in sorted(os.listdir(city_dir)):\n                    if not name.endswith('_camera.json'):\n                        continue\n\n                    (_, seq, frame, _) = name.split('_')\n                    path = os.path.join(city_dir, name)\n\n                    print('Reading {:s}'.format(path))\n\n                    with open(path, 'r') as f:\n                        json_objs = json.load(f)\n\n                    camera_dfs.append({\n                        'split': split,\n                        'city': city,\n                        'seq': seq,\n                        'frame': frame,\n                        'f': json_objs['intrinsic']['fx'],\n                        'u0': json_objs['intrinsic']['u0'],\n                        'v0': json_objs['intrinsic']['v0'],\n                    })\n\n                    if debug:\n                        break\n\n        CityscapesBaseDataset.camera_df = pd.DataFrame(camera_dfs).set_index(['split', 'city', 'seq', 'frame'])\n\n\nclass CityscapesSemantics(CityscapesBaseDataset):\n    cache_dir = os.getenv('CITYSCAPES_SEMANTICS_CACHE_DIR')\n\n    splits = ['train', 'val']\n\n    scenegt_df = None\n\n    class Category:\n        car = 26\n\n    @staticmethod\n    def index2cat(index):\n        return index // 1000\n\n    @staticmethod\n    def read_scene(split, city, seq, frame):\n        path = os.path.join(CityscapesBaseDataset.root_dir, 'gtFine', split, city, '{:s}_{:s}_{:s}_gtFine_instanceIds.png'.format(city, seq, frame))\n        image_scene_pil = PIL.Image.open(path)\n        return np.asarray(image_scene_pil)[..., None]\n\n    @staticmethod\n    def read_disparity(split, city, seq, frame):\n        path = os.path.join(CityscapesBaseDataset.root_dir, 'disparity', split, city, '{:s}_{:s}_{:s}_disparity.png'.format(city, seq, frame))\n        image_disparity_pil = PIL.Image.open(path)\n        return np.asarray(image_disparity_pil)[..., None]\n\n    @staticmethod\n    def _read_scenegt(debug=False):\n        scenege_dfs = []\n\n        if not os.path.isdir(CityscapesSemantics.cache_dir):\n            os.makedirs(CityscapesSemantics.cache_dir)\n\n        for split in CityscapesSemantics.splits:\n            split_dir = os.path.join(CityscapesBaseDataset.root_dir, 'gtFine', split)\n\n            for city in sorted(os.listdir(split_dir)):\n                city_dir = os.path.join(split_dir, city)\n\n                for name in sorted(os.listdir(city_dir)):\n                    if not name.endswith('gtFine_instanceIds.png'):\n                        continue\n\n                    (seq, frame) = name.split('_')[1:3]\n                    json_path = os.path.join(CityscapesSemantics.cache_dir, '{:s}_{:s}_{:s}_gtFine.json'.format(city, seq, frame))\n\n                    print('Reading {:s}'.format(json_path))\n\n                    if os.path.isfile(json_path):\n                        with open(json_path, 'r') as f:\n                            json_objs = json.load(f)\n                    else:\n                        image_scene = CityscapesSemantics.read_scene(split, city, seq, frame)\n                        obj_indices = np.unique(image_scene)\n\n                        json_objs = []\n                        for obj_index in obj_indices:\n                            if CityscapesSemantics.index2cat(obj_index) == CityscapesSemantics.Category.car:\n                                json_objs.append({'obj_index': int(obj_index)})\n\n                        with open(json_path, 'w') as f:\n                            json.dump(json_objs, f)\n\n                    for json_obj in json_objs:\n                        obj_index = json_obj['obj_index']\n\n                        scenege_dfs.append({\n                            'split': split,\n                            'city': city,\n                            'seq': seq,\n                            'frame': frame,\n                            'obj_index': obj_index,\n                        })\n\n        CityscapesSemantics.scenegt_df = pd.DataFrame(scenege_dfs).set_index(['split', 'city', 'seq', 'frame'])\n\n    def __init__(self, is_train=False, debug=False):\n        self.is_train = is_train\n\n        if CityscapesBaseDataset.camera_df is None:\n            CityscapesBaseDataset._read_camera(CityscapesSemantics.splits, debug=debug)\n\n        if CityscapesSemantics.scenegt_df is None:\n            CityscapesSemantics._read_scenegt(debug=debug)\n\n        if is_train:\n            self.df = CityscapesSemantics.scenegt_df.loc[['train']]\n        else:\n            self.df = CityscapesSemantics.scenegt_df.loc[['val']]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        (split, city, seq, frame) = item.name\n\n        image_rgb = CityscapesBaseDataset.read_rgb(split, city, seq, frame)\n        image_scene = CityscapesSemantics.read_scene(split, city, seq, frame)\n        image_disparity = CityscapesSemantics.read_disparity(split, city, seq, frame)\n\n        image_mask = (image_scene == item.obj_index)\n        roi = Transforms.mask_to_roi(image_mask)\n        if self.is_train:\n            roi = Transforms.roi_jitter(roi)\n\n        roi_norm = [\n            (roi[0] - CityscapesBaseDataset.Camera.v0) / CityscapesBaseDataset.Camera.focal,\n            (roi[1] - CityscapesBaseDataset.Camera.u0) / CityscapesBaseDataset.Camera.focal,\n            (roi[2] - CityscapesBaseDataset.Camera.v0) / CityscapesBaseDataset.Camera.focal,\n            (roi[3] - CityscapesBaseDataset.Camera.u0) / CityscapesBaseDataset.Camera.focal,\n        ]\n\n        disparity = image_disparity[image_mask]\n        disparity = disparity[disparity != 0]\n        if disparity.size:\n            disparity = np.percentile(disparity, 95)\n        else:\n            disparity = 0\n        image_ignore = (image_disparity > disparity)\n\n        res = {\n            'targets': TargetType.finetune,\n            'images': self.transform_rgb(image_rgb, roi),\n            'masks': self.transform_mask(image_mask, roi),\n            'ignores': self.transform_ignore(image_ignore, roi),\n            'widths': np.float32([image_rgb.shape[1]]),\n            'heights': np.float32([image_rgb.shape[0]]),\n            'focals': np.float32([CityscapesBaseDataset.Camera.focal]),\n            'u0s': np.float32([CityscapesBaseDataset.Camera.u0]),\n            'v0s': np.float32([CityscapesBaseDataset.Camera.v0]),\n            'rois': np.float32(roi),\n            'roi_norms': np.float32(roi_norm),\n        }\n        return res\n\n\nclass CityscapesMaskRCNN(CityscapesBaseDataset):\n    root_dir = os.getenv('CITYSCAPES_MASKRCNN_ROOT_DIR')\n    cache_dir = os.getenv('CITYSCAPES_MASKRCNN_CACHE_DIR')\n\n    splits = ['train', 'train_extra', 'test']\n\n    scenegt_df = None\n\n    @staticmethod\n    def read_scene(split, city, seq, frame):\n        path = os.path.join(CityscapesMaskRCNN.root_dir, split, city, '{:s}_{:s}_{:s}_leftImg8bit.png'.format(city, seq, frame))\n        image_pil = PIL.Image.open(path)\n        return np.asarray(image_pil)\n\n    @staticmethod\n    def _read_scenegt(debug=False):\n        scenegt_dfs = []\n\n        if not os.path.isdir(CityscapesMaskRCNN.cache_dir):\n            os.makedirs(CityscapesMaskRCNN.cache_dir)\n\n        for split in CityscapesMaskRCNN.splits:\n            split_dir = os.path.join(CityscapesMaskRCNN.root_dir, split)\n\n            for city in sorted(os.listdir(split_dir)):\n                city_dir = os.path.join(split_dir, city)\n\n                for name in sorted(os.listdir(city_dir)):\n                    if not name.endswith('_leftImg8bit.png'):\n                        continue\n\n                    (_, seq, frame, _) = name.split('_')\n                    json_path = os.path.join(CityscapesMaskRCNN.cache_dir, '_{:s}_{:s}_{:s}_gtFine.json'.format(city, seq, frame))\n\n                    print('Reading {:s}'.format(json_path))\n\n                    if os.path.isfile(json_path):\n                        with open(json_path, 'r') as f:\n                            json_objs = json.load(f)\n                    else:\n                        image_scene = CityscapesMaskRCNN.read_scene(split, city, seq, frame)\n                        obj_indices = np.unique(image_scene)\n\n                        json_objs = []\n                        for obj_index in obj_indices:\n                            image_mask = (image_scene == obj_index)[:, :, None]\n                            roi = Transforms.mask_to_roi(image_mask)\n\n                            json_objs.append({\n                                'obj_index': int(obj_index),\n                                'roi': roi,\n                            })\n\n                        with open(json_path, 'w') as f:\n                            json.dump(json_objs, f)\n\n                    for json_obj in json_objs:\n                        obj_index = json_obj['obj_index']\n                        roi = json_obj['roi']\n\n                        if obj_index == 0:\n                            continue\n\n                        scenegt_dfs.append({\n                            'split': split,\n                            'city': city,\n                            'seq': seq,\n                            'frame': frame,\n                            'obj_index': obj_index,\n                            'roi': roi,\n                        })\n\n                    if debug:\n                        break\n\n        CityscapesMaskRCNN.scenegt_df = pd.DataFrame(scenegt_dfs).set_index(['split', 'city', 'seq', 'frame'])\n\n    def __init__(self, is_train=False, debug=False):\n        self.is_train = is_train\n\n        if CityscapesMaskRCNN.scenegt_df is None:\n            CityscapesMaskRCNN._read_scenegt(debug=debug)\n\n        if is_train:\n            self.df = CityscapesMaskRCNN.scenegt_df.loc[['train']]\n        else:\n            self.df = CityscapesMaskRCNN.scenegt_df.loc[['test']]\n\n        roi = np.asarray(self.df.roi.values.tolist())\n\n        droi_y = roi[:, 2] - roi[:, 0]\n        droi_x = roi[:, 3] - roi[:, 1]\n        sel = (\n            (droi_y * droi_x > 32 * 32) *\n            (droi_x / droi_y < 4) *\n            (droi_y / droi_x < 4)\n        )\n\n        self.df = self.df[sel]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        (split, city, seq, frame) = item.name\n\n        image_scene = CityscapesMaskRCNN.read_scene(split, city, seq, frame)\n        image_rgb = CityscapesMaskRCNN.read_rgb(split, city, seq, frame)\n\n        camera_item = CityscapesMaskRCNN.camera_df.loc[(split, city, seq, frame)]\n\n        obj_index = item.obj_index\n        roi = item.roi\n        if self.is_train:\n            roi = Transforms.roi_jitter(roi)\n\n        roi_norm = [\n            (roi[0] - camera_item.v0) / camera_item.f,\n            (roi[1] - camera_item.u0) / camera_item.f,\n            (roi[2] - camera_item.v0) / camera_item.f,\n            (roi[3] - camera_item.u0) / camera_item.f,\n        ]\n        # focal = [camera_item.f]\n\n        image_mask = (image_scene == obj_index)[:, :, None]\n\n        return {\n            'targets': TargetType.finetune,\n            'images': self.transform_rgb(image_rgb, roi),\n            'widths': np.float32([image_rgb.width]),\n            'heights': np.float32([image_rgb.height]),\n            'focals': np.float32([camera_item.f]),\n            'u0s': np.float32([camera_item.u0]),\n            'v0s': np.float32([camera_item.v0]),\n            'masks': self.transform_mask(image_mask, roi),\n            'ignores': torch.zeros(1, 256, 256),\n            'roi_norms': np.float32(roi_norm),\n        }\n\n\nclass CityscapesSemanticsHybrid(HybridDataset):\n    def __init__(self, is_train, debug=False):\n        super(CityscapesSemanticsHybrid, self).__init__(\n            datasets=[\n                VKitti(is_train=is_train, debug=debug),\n                CityscapesSemantics(is_train=is_train, debug=debug),\n            ],\n            weights=[0.75, 0.25],\n        )\n"""
geometric/derender3d/utils.py,0,b'def to_cpu(v):\n    return v.detach().cpu()\n\n\ndef to_numpy(v):\n    return v.detach().cpu().numpy()\n'
geometric/maskrcnn/cityscapes.py,0,"b'import torch\nimport os\nimport time\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as maskUtils\nimport zipfile\nimport urllib.request\nimport shutil\nfrom config import Config\nimport utils\nimport model as modellib\nfrom scipy.misc import imread, imsave\nfrom torchvision import transforms\nfrom PIL import Image\nimport json\n\n# Root directory of the project\nROOT_DIR = os.getcwd()\n\n# Path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, ""pretrained/mask_rcnn_coco.pth"")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n\n############################################################\n#  Configurations\n############################################################\n\nclass CityscapesConfig(Config):\n    # Give the configuration a recognizable name\n    NAME = ""cityscapes""\n\n    # We use one GPU with 8GB memory, which can fit one image.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 8\n\n    GPU_COUNT = 1\n\n    # Number of classes (including background)\n    NUM_CLASSES = 2\n\n\n############################################################\n#  Dataset\n############################################################\n\nclass CityscapesDataset(utils.Dataset):\n    def load_cityscapes(self, dataset_dir, subset):\n        """"""Load a subset of the COCO dataset.\n        dataset_dir: The root directory of the COCO dataset.\n        subset: What to load (train, test)\n        """"""\n\n        self.inst_dir = os.path.join(dataset_dir, \'gtFine\', subset)\n\n        # preprocess split list: filter out images without objects\n        self.list = json.load(open(""{}/annotations/instancesonly_gtFine_{}.json"".format(dataset_dir, subset)))[\'images\']\n        havecar_name = ""{}/annotations/instanceonly_gtFine_{}_have_car.json"".format(dataset_dir, subset)\n        if not os.path.exists(havecar_name):\n            havecar = []\n            for image_id in range(len(self.list)):\n                if self.load_mask(image_id):\n                    havecar.append(image_id)\n            json.dump(havecar, open(havecar_name, ""w""))\n        havecar = json.load(open(havecar_name))\n        self.list = [self.list[i] for i in havecar]\n\n        # image jittering\n        self.img_jitter = transforms.ColorJitter(\n            brightness=0.2, contrast=0.2,\n            saturation=0.2, hue=0.2)\n\n        # Add classes\n        self.add_class(""cityscapes"", 1, ""car"")\n\n        # Add images\n        for item in self.list:\n            self.add_image(\n                ""cityscapes"", image_id=item[\'id\'],\n                path=os.path.join(dataset_dir, \'images\', item[\'file_name\']),\n                width=item[\'width\'], height=item[\'height\'])\n\n        print(""#{}_dataset = {}"".format(subset, len(self.list)))\n        self.subset = subset\n\n    def load_mask(self, image_id):\n        """"""Load instance masks for the given image.\n\n        Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        """"""\n        inst_file = self.list[image_id][\'seg_file_name\']\n        inst_map = imread(os.path.join(self.inst_dir, inst_file.split(\'_\')[0], inst_file))\n        ids, counts = np.unique(inst_map, return_counts=True)\n        ids = ids[counts > 50]             # filter out objects with area < 50 pixels\n        ids = ids[ids // 1000 == 26]           # filter out non-cars\n        if len(ids) == 0:\n            return False\n        masks = np.stack([(inst_map == x) for x in ids], axis=-1)\n        class_ids = np.ones(len(ids))\n        return masks, class_ids\n\n    def load_image(self, image_id):\n        """"""Load the specified image and return a [H,W,3] Numpy array.\n        """"""\n        # Load image and jitter\n        image = Image.open(self.image_info[image_id][\'path\'])\n        if self.subset == \'train\':\n            image = self.img_jitter(image)\n        image = np.array(image)\n\n        # per inst augment\n        if self.subset == \'train\':\n            inst_file = self.list[image_id][\'seg_file_name\']\n            inst_map = imread(os.path.join(self.inst_dir, inst_file.split(\'_\')[0], inst_file))\n            noise = np.zeros(image.shape)\n            for inst in np.unique(inst_map):\n                if np.random.rand() < 0.3:  # w.p. 0.3\n                    noise[inst_map == inst, :] = np.random.randint(-20, 20, (3,))\n            image = image + noise\n            # if np.random.rand() < 0.1:\n            #    imsave(os.path.join(\'./tmp\', inst_file), image)\n        return image\n\n############################################################\n#  Training\n############################################################\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\'Train Mask R-CNN on Cityscapes.\')\n    parser.add_argument(""command"",\n                        metavar=""<command>"",\n                        help=""\'train\' or \'test\' on cityscapes"")\n    parser.add_argument(\'--dataset\', default=\'./dataset/cityscapes\',\n                        metavar=""/path/to/cityscapes/"",\n                        help=\'Directory of the cityscapes dataset\')\n    parser.add_argument(\'--model\', required=False,\n                        metavar=""/path/to/weights.pth"",\n                        default="""",\n                        help=""Path to weights .pth file or \'coco\'"")\n    parser.add_argument(\'--logs\', required=False,\n                        default=DEFAULT_LOGS_DIR,\n                        metavar=""/path/to/logs/"",\n                        help=\'Logs and checkpoints directory (default=logs/)\')\n    parser.add_argument(\'--limit\', required=False,\n                        default=500,\n                        metavar=""<image count>"",\n                        help=\'Images to use for evaluation (default=500)\')\n    args = parser.parse_args()\n    print(""Command: "", args.command)\n    print(""Model: "", args.model)\n    print(""Dataset: "", args.dataset)\n    print(""Logs: "", args.logs)\n\n    # Cityscapes: now only support training\n    assert args.command == \'train\'\n\n    # Configurations\n    if args.command == ""train"":\n        config = CityscapesConfig()\n\n    # Create model\n    if args.command == ""train"":\n        model = modellib.MaskRCNN(config=config,\n                                  model_dir=args.logs)\n    if config.GPU_COUNT:\n        model = model.cuda()\n\n    # Select weights file to load\n    args.transfer = 0\n    if args.model:\n        if args.model.lower() == ""coco"":\n            model_path = COCO_MODEL_PATH\n            args.transfer = 1\n        elif args.model.lower() == ""last"":\n            # Find last trained weights\n            model_path = model.find_last()[1]\n        else:\n            model_path = args.model\n    else:\n        model_path = """"\n\n    # Load weights\n    print(""Loading weights "", model_path)\n    model.load_weights(model_path, transfer=args.transfer)\n\n    # Train or evaluate\n    if args.command == ""train"":\n        # Training dataset. Use the training set and 35K from the\n        # validation set, as as in the Mask RCNN paper.\n        dataset_train = CityscapesDataset()\n        dataset_train.load_cityscapes(args.dataset, ""train"")\n        dataset_train.prepare()\n\n        # Validation dataset\n        dataset_val = CityscapesDataset()\n        dataset_val.load_cityscapes(args.dataset, ""val"")\n        dataset_val.prepare()\n\n        # *** This training schedule is an example. Update to your needs ***\n\n        if args.transfer:\n            # Training - Stage 0\n            print(""Training for transferring num_classes"")\n            model.train_model(dataset_train, dataset_val,\n                              learning_rate=1e-5,\n                              epochs=10,\n                              layers=""transfer"")\n            # layers=r""(mask.conv5.*)|(classifier.linear_class.*)|(classifier.linear_bbox.*)"")\n\n        # Training - Stage 1\n        print(""Training network heads"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE,\n                          epochs=40,\n                          layers=\'heads\')\n\n        # Training - Stage 2\n        # Finetune layers from ResNet stage 4 and up\n        print(""Fine tune Resnet stage 4 and up"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE / 2,\n                          epochs=70,\n                          layers=\'4+\')\n\n        # Training - Stage 3\n        # Fine tune all layers\n        print(""Fine tune all layers"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE / 5,\n                          epochs=100,\n                          layers=\'all\')\n'"
geometric/maskrcnn/config.py,0,"b'""""""\nMask R-CNN\nBase Configurations class.\n\nCopyright (c) 2017 Matterport, Inc.\nLicensed under the MIT License (see LICENSE for details)\nWritten by Waleed Abdulla\n""""""\n\nimport math\nimport numpy as np\nimport os\n\n\n# Base Configuration Class\n# Don\'t use this class directly. Instead, sub-class it and override\n# the configurations you need to change.\n\nclass Config(object):\n    """"""Base configuration class. For custom configurations, create a\n    sub-class that inherits from this one and override properties\n    that need to be changed.\n    """"""\n    # Name the configurations. For example, \'COCO\', \'Experiment 3\', ...etc.\n    # Useful if your code needs to do things differently depending on which\n    # experiment is running.\n    NAME = None  # Override in sub-classes\n\n    # Path to pretrained imagenet model\n    IMAGENET_MODEL_PATH = os.path.join(os.getcwd(), ""pretrained/resnet50_imagenet.pth"")\n\n    # Path to pretrained coco model\n    COCO_MODEL_PATH = os.path.join(os.getcwd(), ""pretrained/mask_rcnn_coco.pth"")\n\n    # NUMBER OF GPUs to use. For CPU use 0\n    GPU_COUNT = 1\n\n    # Number of images to train with on each GPU. A 12GB GPU can typically\n    # handle 2 images of 1024x1024px.\n    # Adjust based on your GPU memory and image sizes. Use the highest\n    # number that your GPU can handle for best performance.\n    IMAGES_PER_GPU = 1\n\n    # Number of training steps per epoch\n    # This doesn\'t need to match the size of the training set. Tensorboard\n    # updates are saved at the end of each epoch, so setting this to a\n    # smaller number means getting more frequent TensorBoard updates.\n    # Validation stats are also calculated at each epoch end and they\n    # might take a while, so don\'t set this too small to avoid spending\n    # a lot of time on validation stats.\n    STEPS_PER_EPOCH = 1000\n\n    # Number of validation steps to run at the end of every training epoch.\n    # A bigger number improves accuracy of validation stats, but slows\n    # down the training.\n    VALIDATION_STEPS = 50\n\n    # The strides of each layer of the FPN Pyramid. These values\n    # are based on a Resnet101 backbone.\n    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n\n    # Number of classification classes (including background)\n    NUM_CLASSES = 1  # Override in sub-classes\n\n    # Length of square anchor side in pixels\n    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n\n    # Ratios of anchors at each cell (width/height)\n    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n\n    # Anchor stride\n    # If 1 then anchors are created for each cell in the backbone feature map.\n    # If 2, then anchors are created for every other cell, and so on.\n    RPN_ANCHOR_STRIDE = 1\n\n    # Non-max suppression threshold to filter RPN proposals.\n    # You can reduce this during training to generate more propsals.\n    RPN_NMS_THRESHOLD = 0.7\n\n    # How many anchors per image to use for RPN training\n    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n\n    # ROIs kept after non-maximum supression (training and inference)\n    POST_NMS_ROIS_TRAINING = 2000\n    POST_NMS_ROIS_INFERENCE = 1000\n\n    # If enabled, resizes instance masks to a smaller size to reduce\n    # memory load. Recommended when using high-resolution images.\n    USE_MINI_MASK = True\n    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n\n    # Input image resing\n    # Images are resized such that the smallest side is >= IMAGE_MIN_DIM and\n    # the longest side is <= IMAGE_MAX_DIM. In case both conditions can\'t\n    # be satisfied together the IMAGE_MAX_DIM is enforced.\n    IMAGE_MIN_DIM = 300\n    IMAGE_MAX_DIM = 1024\n    # If True, pad images with zeros such that they\'re (max_dim by max_dim)\n    IMAGE_PADDING = True  # currently, the False option is not supported\n\n    # Image mean (RGB)\n    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n\n    # Number of ROIs per image to feed to classifier/mask heads\n    # The Mask RCNN paper uses 512 but often the RPN doesn\'t generate\n    # enough positive proposals to fill this and keep a positive:negative\n    # ratio of 1:3. You can increase the number of proposals by adjusting\n    # the RPN NMS threshold.\n    TRAIN_ROIS_PER_IMAGE = 200\n\n    # Percent of positive ROIs used to train classifier/mask heads\n    ROI_POSITIVE_RATIO = 0.33\n\n    # Pooled ROIs\n    POOL_SIZE = 7\n    MASK_POOL_SIZE = 14\n    MASK_SHAPE = [28, 28]\n\n    # Maximum number of ground truth instances to use in one image\n    MAX_GT_INSTANCES = 100\n\n    # Bounding box refinement standard deviation for RPN and final detections.\n    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n\n    # Max number of final detections\n    DETECTION_MAX_INSTANCES = 100\n\n    # Minimum probability value to accept a detected instance\n    # ROIs below this threshold are skipped\n    DETECTION_MIN_CONFIDENCE = 0.7\n\n    # Non-maximum suppression threshold for detection\n    DETECTION_NMS_THRESHOLD = 0.3\n\n    # Learning rate and momentum\n    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n    # weights to explode. Likely due to differences in optimzer\n    # implementation.\n    LEARNING_RATE = 1e-3\n    LEARNING_MOMENTUM = 0.9\n\n    # Weight decay regularization\n    WEIGHT_DECAY = 0.0001\n\n    # Use RPN ROIs or externally generated ROIs for training\n    # Keep this True for most situations. Set to False if you want to train\n    # the head branches on ROI generated by code rather than the ROIs from\n    # the RPN. For example, to debug the classifier head without having to\n    # train the RPN.\n    USE_RPN_ROIS = True\n\n    def __init__(self):\n        """"""Set values of computed attributes.""""""\n        # Effective batch size\n        if self.GPU_COUNT > 0:\n            self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n        else:\n            self.BATCH_SIZE = self.IMAGES_PER_GPU\n\n        self.DISPLAY_SIZE = 100\n\n        # Adjust step size based on batch size\n        self.STEPS_PER_EPOCH = self.BATCH_SIZE * self.STEPS_PER_EPOCH\n\n        # Input image size\n        self.IMAGE_SHAPE = np.array(\n            [self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])\n\n        # Compute backbone size from input image size\n        self.BACKBONE_SHAPES = np.array(\n            [[int(math.ceil(self.IMAGE_SHAPE[0] / stride)),\n              int(math.ceil(self.IMAGE_SHAPE[1] / stride))]\n             for stride in self.BACKBONE_STRIDES])\n\n    def display(self):\n        """"""Display Configuration values.""""""\n        print(""\\nConfigurations:"")\n        for a in dir(self):\n            if not a.startswith(""__"") and not callable(getattr(self, a)):\n                print(""{:30} {}"".format(a, getattr(self, a)))\n        print(""\\n"")\n'"
geometric/maskrcnn/convert_from_keras.py,2,"b""import argparse\nimport collections\nimport h5py\nimport torch\n\nalphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\nparser = argparse.ArgumentParser(description='Convert keras-mask-rcnn model to pytorch-mask-rcnn model')\nparser.add_argument('--keras_model',\n                    help='the path of the keras model',\n                    default=None, type=str)\nparser.add_argument('--pytorch_model',\n                    help='the path of the pytorch model',\n                    default=None, type=str)\n\nargs = parser.parse_args()\n\nf = h5py.File(args.keras_model, mode='r')\nstate_dict = collections.OrderedDict()\nfor group_name, group in f.items():\n    if len(group.items()) != 0:\n        for layer_name, layer in group.items():\n            for weight_name, weight in layer.items():\n                state_dict[layer_name + '.' + weight_name] = weight.value\n\nreplace_dict = collections.OrderedDict([\n    ('beta:0', 'bias'),\n    ('gamma:0', 'weight'),\n    ('moving_mean:0', 'running_mean'),\n    ('moving_variance:0', 'running_var'),\n    ('bias:0', 'bias'),\n    ('kernel:0', 'weight'),\n    ('mrcnn_mask_', 'mask.'),\n    ('mrcnn_mask', 'mask.conv5'),\n    ('mrcnn_class_', 'classifier.'),\n    ('logits', 'linear_class'),\n    ('mrcnn_bbox_fc', 'classifier.linear_bbox'),\n    ('rpn_', 'rpn.'),\n    ('class_raw', 'conv_class'),\n    ('bbox_pred', 'conv_bbox'),\n    ('bn_conv1', 'fpn.C1.1'),\n    ('bn2a_branch1', 'fpn.C2.0.downsample.1'),\n    ('res2a_branch1', 'fpn.C2.0.downsample.0'),\n    ('bn3a_branch1', 'fpn.C3.0.downsample.1'),\n    ('res3a_branch1', 'fpn.C3.0.downsample.0'),\n    ('bn4a_branch1', 'fpn.C4.0.downsample.1'),\n    ('res4a_branch1', 'fpn.C4.0.downsample.0'),\n    ('bn5a_branch1', 'fpn.C5.0.downsample.1'),\n    ('res5a_branch1', 'fpn.C5.0.downsample.0'),\n    ('fpn_c2p2', 'fpn.P2_conv1'),\n    ('fpn_c3p3', 'fpn.P3_conv1'),\n    ('fpn_c4p4', 'fpn.P4_conv1'),\n    ('fpn_c5p5', 'fpn.P5_conv1'),\n    ('fpn_p2', 'fpn.P2_conv2.1'),\n    ('fpn_p3', 'fpn.P3_conv2.1'),\n    ('fpn_p4', 'fpn.P4_conv2.1'),\n    ('fpn_p5', 'fpn.P5_conv2.1'),\n])\n\nreplace_exact_dict = collections.OrderedDict([\n    ('conv1.bias', 'fpn.C1.0.bias'),\n    ('conv1.weight', 'fpn.C1.0.weight'),\n])\n\nfor block in range(3):\n    for branch in range(3):\n        replace_dict['bn2' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C2.' + str(block) + '.bn' + str(\n            branch + 1)\n        replace_dict['res2' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C2.' + str(block) + '.conv' + str(branch + 1)\n\nfor block in range(4):\n    for branch in range(3):\n        replace_dict['bn3' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C3.' + str(block) + '.bn' + str(\n            branch + 1)\n        replace_dict['res3' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C3.' + str(block) + '.conv' + str(branch + 1)\n\nfor block in range(23):\n    for branch in range(3):\n        replace_dict['bn4' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C4.' + str(block) + '.bn' + str(\n            branch + 1)\n        replace_dict['res4' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C4.' + str(block) + '.conv' + str(branch + 1)\n\nfor block in range(3):\n    for branch in range(3):\n        replace_dict['bn5' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C5.' + str(block) + '.bn' + str(branch + 1)\n        replace_dict['res5' + alphabet[block] + '_branch2' + alphabet[branch]] = 'fpn.C5.' + str(block) + '.conv' + str(branch + 1)\n\n\nfor orig, repl in replace_dict.items():\n    for key in list(state_dict.keys()):\n        if orig in key:\n            state_dict[key.replace(orig, repl)] = state_dict[key]\n            del state_dict[key]\n\nfor orig, repl in replace_exact_dict.items():\n    for key in list(state_dict.keys()):\n        if orig == key:\n            state_dict[repl] = state_dict[key]\n            del state_dict[key]\n\nfor weight_name in list(state_dict.keys()):\n    if state_dict[weight_name].ndim == 4:\n        state_dict[weight_name] = state_dict[weight_name].transpose((3, 2, 0, 1)).copy(order='C')\n    if state_dict[weight_name].ndim == 2:\n        state_dict[weight_name] = state_dict[weight_name].transpose((1, 0)).copy(order='C')\n\nfor weight_name in list(state_dict.keys()):\n    state_dict[weight_name] = torch.from_numpy(state_dict[weight_name])\n\ntorch.save(state_dict, args.pytorch_model)\n"""
geometric/maskrcnn/demo.py,1,"b'import os\nimport sys\nimport random\nimport math\nimport numpy as np\nimport skimage.io\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport coco\nimport utils\nimport model as modellib\nimport visualize\n\nimport torch\n\n\n# Root directory of the project\nROOT_DIR = os.getcwd()\n\n# Directory to save logs and trained model\nMODEL_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n# Path to trained weights file\n# Download this file and place in the root of your\n# project (See README file for details)\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.pth"")\n\n# Directory of images to run detection on\nIMAGE_DIR = os.path.join(ROOT_DIR, ""images"")\n\n\nclass InferenceConfig(coco.CocoConfig):\n    # Set batch size to 1 since we\'ll be running inference on\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n    # GPU_COUNT = 0 for CPU\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n\nconfig = InferenceConfig()\nconfig.display()\n\n# Create model object.\nmodel = modellib.MaskRCNN(model_dir=MODEL_DIR, config=config)\nif config.GPU_COUNT:\n    model = model.cuda()\n\n# Load weights trained on MS-COCO\nmodel.load_state_dict(torch.load(COCO_MODEL_PATH))\n\n# COCO Class names\n# Index of the class in the list is its ID. For example, to get ID of\n# the teddy bear class, use: class_names.index(\'teddy bear\')\nclass_names = [\'BG\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n               \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\',\n               \'fire hydrant\', \'stop sign\', \'parking meter\', \'bench\', \'bird\',\n               \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\',\n               \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n               \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n               \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n               \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n               \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n               \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n               \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n               \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n               \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\',\n               \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n               \'teddy bear\', \'hair drier\', \'toothbrush\']\n\n# Load a random image from the images folder\nfile_names = next(os.walk(IMAGE_DIR))[2]\nimage = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n\n# Run detection\nresults = model.detect([image])\n\n# Visualize results\nr = results[0]\nvisualize.display_instances(image, r[\'rois\'], r[\'masks\'], r[\'class_ids\'],\n                            class_names, r[\'scores\'])\nplt.show()\n'"
geometric/maskrcnn/model.py,101,"b'""""""\nMask R-CNN\nThe main Mask R-CNN model implemenetation.\n\nCopyright (c) 2017 Matterport, Inc.\nLicensed under the MIT License (see LICENSE for details)\nWritten by Waleed Abdulla\n""""""\n\nimport datetime\nimport math\nimport os\nimport random\nimport re\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\n\nsys.path.insert(0, os.path.dirname(__file__))\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport utils as utils\nimport visualize as visualize\n\nfrom nms.nms_wrapper import nms\nfrom roialign.roi_align.crop_and_resize import CropAndResizeFunction\n\n\n############################################################\n#  Logging Utility Functions\n############################################################\n\ndef log(text, array=None):\n    """"""Prints a text message. And, optionally, if a Numpy array is provided it\n    prints it\'s shape, min, and max values.\n    """"""\n    if array is not None:\n        text = text.ljust(25)\n        text += (""shape: {:20}  min: {:10.5f}  max: {:10.5f}"".format(\n            str(array.shape),\n            array.min() if array.size else """",\n            array.max() if array.size else """"))\n    print(text)\n\n\ndef printProgressBar(iteration, total, prefix=\'\', suffix=\'\', decimals=1, length=100, fill=\'\xe2\x96\x88\'):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        length      - Optional  : character length of bar (Int)\n        fill        - Optional  : bar fill character (Str)\n    """"""\n    percent = (""{0:."" + str(decimals) + ""f}"").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + \'-\' * (length - filledLength)\n    print(\'\\r%s |%s| %s%% %s\' % (prefix, bar, percent, suffix), end=\'\\n\')\n    # Print New Line on Complete\n    if iteration == total:\n        print()\n\n\n############################################################\n#  Pytorch Utility Functions\n############################################################\n\ndef unique1d(tensor):\n    if tensor.size()[0] == 0 or tensor.size()[0] == 1:\n        return tensor\n    tensor = tensor.sort()[0]\n    unique_bool = tensor[1:] != tensor[:-1]\n    first_element = Variable(torch.ByteTensor([True]), requires_grad=False)\n    if tensor.is_cuda:\n        first_element = first_element.cuda()\n    unique_bool = torch.cat((first_element, unique_bool), dim=0)\n    return tensor[unique_bool.data]\n\n\ndef intersect1d(tensor1, tensor2):\n    aux = torch.cat((tensor1, tensor2), dim=0)\n    aux = aux.sort()[0]\n    return aux[:-1][(aux[1:] == aux[:-1]).data]\n\n\ndef log2(x):\n    """"""Implementatin of Log2. Pytorch doesn\'t have a native implemenation.""""""\n    ln2 = Variable(torch.log(torch.FloatTensor([2.0])), requires_grad=False)\n    if x.is_cuda:\n        ln2 = ln2.cuda()\n    return torch.log(x) / ln2\n\n\nclass SamePad2d(nn.Module):\n    """"""Mimics tensorflow\'s \'SAME\' padding.\n    """"""\n\n    def __init__(self, kernel_size, stride):\n        super(SamePad2d, self).__init__()\n        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n        self.stride = torch.nn.modules.utils._pair(stride)\n\n    def forward(self, input):\n        in_width = input.size()[2]\n        in_height = input.size()[3]\n        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n        pad_along_width = ((out_width - 1) * self.stride[0] +\n                           self.kernel_size[0] - in_width)\n        pad_along_height = ((out_height - 1) * self.stride[1] +\n                            self.kernel_size[1] - in_height)\n        pad_left = math.floor(pad_along_width / 2)\n        pad_top = math.floor(pad_along_height / 2)\n        pad_right = pad_along_width - pad_left\n        pad_bottom = pad_along_height - pad_top\n        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), \'constant\', 0)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\n############################################################\n#  FPN Graph\n############################################################\n\nclass TopDownLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(TopDownLayer, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n        self.padding2 = SamePad2d(kernel_size=3, stride=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1)\n\n    def forward(self, x, y):\n        y = F.upsample(y, scale_factor=2)\n        x = self.conv1(x)\n        return self.conv2(self.padding2(x + y))\n\n\nclass FPN(nn.Module):\n    def __init__(self, C1, C2, C3, C4, C5, out_channels):\n        super(FPN, self).__init__()\n        self.out_channels = out_channels\n        self.C1 = C1\n        self.C2 = C2\n        self.C3 = C3\n        self.C4 = C4\n        self.C5 = C5\n        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)\n        self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1)\n        self.P5_conv2 = nn.Sequential(\n            SamePad2d(kernel_size=3, stride=1),\n            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n        )\n        self.P4_conv1 = nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1)\n        self.P4_conv2 = nn.Sequential(\n            SamePad2d(kernel_size=3, stride=1),\n            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n        )\n        self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1)\n        self.P3_conv2 = nn.Sequential(\n            SamePad2d(kernel_size=3, stride=1),\n            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n        )\n        self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1)\n        self.P2_conv2 = nn.Sequential(\n            SamePad2d(kernel_size=3, stride=1),\n            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n        )\n\n    def forward(self, x):\n        x = self.C1(x)\n        x = self.C2(x)\n        c2_out = x\n        x = self.C3(x)\n        c3_out = x\n        x = self.C4(x)\n        c4_out = x\n        x = self.C5(x)\n        p5_out = self.P5_conv1(x)\n        p4_out = self.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)\n        p3_out = self.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)\n        p2_out = self.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)\n\n        p5_out = self.P5_conv2(p5_out)\n        p4_out = self.P4_conv2(p4_out)\n        p3_out = self.P3_conv2(p3_out)\n        p2_out = self.P2_conv2(p2_out)\n\n        # P6 is used for the 5th anchor scale in RPN. Generated by\n        # subsampling from P5 with stride of 2.\n        p6_out = self.P6(p5_out)\n\n        return [p2_out, p3_out, p4_out, p5_out, p6_out]\n\n\n############################################################\n#  Resnet Graph\n############################################################\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes, eps=0.001, momentum=0.01)\n        self.padding2 = SamePad2d(kernel_size=3, stride=1)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3)\n        self.bn2 = nn.BatchNorm2d(planes, eps=0.001, momentum=0.01)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1)\n        self.bn3 = nn.BatchNorm2d(planes * 4, eps=0.001, momentum=0.01)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.padding2(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, architecture, stage5=False):\n        super(ResNet, self).__init__()\n        assert architecture in [""resnet50"", ""resnet101""]\n        self.inplanes = 64\n        self.layers = [3, 4, {""resnet50"": 6, ""resnet101"": 23}[architecture], 3]\n        self.block = Bottleneck\n        self.stage5 = stage5\n\n        self.C1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64, eps=0.001, momentum=0.01),\n            nn.ReLU(inplace=True),\n            SamePad2d(kernel_size=3, stride=2),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.C2 = self.make_layer(self.block, 64, self.layers[0])\n        self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)\n        self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)\n        if self.stage5:\n            self.C5 = self.make_layer(self.block, 512, self.layers[3], stride=2)\n        else:\n            self.C5 = None\n\n    def forward(self, x):\n        x = self.C1(x)\n        x = self.C2(x)\n        x = self.C3(x)\n        x = self.C4(x)\n        x = self.C5(x)\n        return x\n\n    def stages(self):\n        return [self.C1, self.C2, self.C3, self.C4, self.C5]\n\n    def make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=0.001, momentum=0.01),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n\n############################################################\n#  Proposal Layer\n############################################################\n\ndef apply_box_deltas(boxes, deltas):\n    """"""Applies the given deltas to the given boxes.\n    boxes: [N, 4] where each row is y1, x1, y2, x2\n    deltas: [N, 4] where each row is [dy, dx, log(dh), log(dw)]\n    """"""\n    # Convert to y, x, h, w\n    height = boxes[:, 2] - boxes[:, 0]\n    width = boxes[:, 3] - boxes[:, 1]\n    center_y = boxes[:, 0] + 0.5 * height\n    center_x = boxes[:, 1] + 0.5 * width\n    # Apply deltas\n    center_y += deltas[:, 0] * height\n    center_x += deltas[:, 1] * width\n    height *= torch.exp(deltas[:, 2])\n    width *= torch.exp(deltas[:, 3])\n    # Convert back to y1, x1, y2, x2\n    y1 = center_y - 0.5 * height\n    x1 = center_x - 0.5 * width\n    y2 = y1 + height\n    x2 = x1 + width\n    result = torch.stack([y1, x1, y2, x2], dim=1)\n    return result\n\n\ndef clip_boxes(boxes, window):\n    """"""\n    boxes: [N, 4] each col is y1, x1, y2, x2\n    window: [4] in the form y1, x1, y2, x2\n    """"""\n    boxes = torch.stack(\n        [boxes[:, 0].clamp(float(window[0]), float(window[2])),\n         boxes[:, 1].clamp(float(window[1]), float(window[3])),\n         boxes[:, 2].clamp(float(window[0]), float(window[2])),\n         boxes[:, 3].clamp(float(window[1]), float(window[3]))], 1)\n    return boxes\n\n\ndef proposal_layer(inputs, proposal_count, nms_threshold, anchors, config=None):\n    """"""Receives anchor scores and selects a subset to pass as proposals\n    to the second stage. Filtering is done based on anchor scores and\n    non-max suppression to remove overlaps. It also applies bounding\n    box refinment detals to anchors.\n\n    Inputs:\n        rpn_probs: [batch, anchors, (bg prob, fg prob)]\n        rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n\n    Returns:\n        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\n    """"""\n\n    # Currently only supports batchsize 1\n    inputs[0] = inputs[0].squeeze(0)\n    inputs[1] = inputs[1].squeeze(0)\n\n    # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n    scores = inputs[0][:, 1]\n\n    # Box deltas [batch, num_rois, 4]\n    deltas = inputs[1]\n    std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1, 4])).float(), requires_grad=False)\n    if config.GPU_COUNT:\n        std_dev = std_dev.cuda()\n    deltas = deltas * std_dev\n\n    # Improve performance by trimming to top anchors by score\n    # and doing the rest on the smaller subset.\n    pre_nms_limit = min(6000, anchors.size()[0])\n    scores, order = scores.sort(descending=True)\n    order = order[:pre_nms_limit]\n    scores = scores[:pre_nms_limit]\n    deltas = deltas[order.data, :]  # TODO: Support batch size > 1 ff.\n    anchors = anchors[order.data, :]\n\n    # Apply deltas to anchors to get refined anchors.\n    # [batch, N, (y1, x1, y2, x2)]\n    boxes = apply_box_deltas(anchors, deltas)\n    # Clip to image boundaries. [batch, N, (y1, x1, y2, x2)]\n    height, width = config.IMAGE_SHAPE[:2]\n    window = np.array([0, 0, height, width]).astype(np.float32)\n    boxes = clip_boxes(boxes, window)\n\n    # Filter out small boxes\n    # According to Xinlei Chen\'s paper, this reduces detection accuracy\n    # for small objects, so we\'re skipping it.\n\n    # Non-max suppression\n    keep = nms(torch.cat((boxes, scores.unsqueeze(1)), 1).data, nms_threshold)\n    keep = keep[:proposal_count]\n    boxes = boxes[keep, :]\n\n    # Normalize dimensions to range of 0 to 1.\n    norm = Variable(torch.from_numpy(np.array([height, width, height, width])).float(), requires_grad=False)\n    if config.GPU_COUNT:\n        norm = norm.cuda()\n    normalized_boxes = boxes / norm\n\n    # Add back batch dimension\n    normalized_boxes = normalized_boxes.unsqueeze(0)\n\n    return normalized_boxes\n\n\n############################################################\n#  ROIAlign Layer\n############################################################\n\ndef pyramid_roi_align(inputs, pool_size, image_shape):\n    """"""Implements ROI Pooling on multiple levels of the feature pyramid.\n\n    Params:\n    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n    - image_shape: [height, width, channels]. Shape of input image in pixels\n\n    Inputs:\n    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n             coordinates.\n    - Feature maps: List of feature maps from different levels of the pyramid.\n                    Each is [batch, channels, height, width]\n\n    Output:\n    Pooled regions in the shape: [num_boxes, height, width, channels].\n    The width and height are those specific in the pool_shape in the layer\n    constructor.\n    """"""\n\n    # Currently only supports batchsize 1\n    for i in range(len(inputs)):\n        inputs[i] = inputs[i].squeeze(0)\n\n    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n    boxes = inputs[0]\n\n    # Feature Maps. List of feature maps from different level of the\n    # feature pyramid. Each is [batch, height, width, channels]\n    feature_maps = inputs[1:]\n\n    # Assign each ROI to a level in the pyramid based on the ROI area.\n    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n    h = y2 - y1\n    w = x2 - x1\n\n    # Equation 1 in the Feature Pyramid Networks paper. Account for\n    # the fact that our coordinates are normalized here.\n    # e.g. a 224x224 ROI (in pixels) maps to P4\n    image_area = Variable(torch.FloatTensor([float(image_shape[0] * image_shape[1])]), requires_grad=False)\n    if boxes.is_cuda:\n        image_area = image_area.cuda()\n    roi_level = 4 + log2(torch.sqrt(h * w) / (224.0 / torch.sqrt(image_area)))\n    roi_level = roi_level.round().int()\n    roi_level = roi_level.clamp(2, 5)\n\n    # Loop through levels and apply ROI pooling to each. P2 to P5.\n    pooled = []\n    box_to_level = []\n    for i, level in enumerate(range(2, 6)):\n        ix = roi_level == level\n        if not ix.any():\n            continue\n        ix = torch.nonzero(ix)[:, 0]\n        level_boxes = boxes[ix.data, :]\n\n        # Keep track of which box is mapped to which level\n        box_to_level.append(ix.data)\n\n        # Stop gradient propogation to ROI proposals\n        level_boxes = level_boxes.detach()\n\n        # Crop and Resize\n        # From Mask R-CNN paper: ""We sample four regular locations, so\n        # that we can evaluate either max or average pooling. In fact,\n        # interpolating only a single value at each bin center (without\n        # pooling) is nearly as effective.""\n        #\n        # Here we use the simplified approach of a single value per bin,\n        # which is how it\'s done in tf.crop_and_resize()\n        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n        ind = Variable(torch.zeros(level_boxes.size()[0]), requires_grad=False).int()\n        if level_boxes.is_cuda:\n            ind = ind.cuda()\n        feature_maps[i] = feature_maps[i].unsqueeze(0)  # CropAndResizeFunction needs batch dimension\n        pooled_features = CropAndResizeFunction(pool_size, pool_size, 0)(feature_maps[i], level_boxes, ind)\n        pooled.append(pooled_features)\n\n    # Pack pooled features into one tensor\n    pooled = torch.cat(pooled, dim=0)\n\n    # Pack box_to_level mapping into one array and add another\n    # column representing the order of pooled boxes\n    box_to_level = torch.cat(box_to_level, dim=0)\n\n    # Rearrange pooled features to match the order of the original boxes\n    _, box_to_level = torch.sort(box_to_level)\n    pooled = pooled[box_to_level, :, :]\n\n    return pooled\n\n\n############################################################\n#  Detection Target Layer\n############################################################\ndef bbox_overlaps(boxes1, boxes2):\n    """"""Computes IoU overlaps between two sets of boxes.\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n    """"""\n    # 1. Tile boxes2 and repeate boxes1. This allows us to compare\n    # every boxes1 against every boxes2 without loops.\n    # TF doesn\'t have an equivalent to np.repeate() so simulate it\n    # using tf.tile() and tf.reshape.\n    boxes1_repeat = boxes2.size()[0]\n    boxes2_repeat = boxes1.size()[0]\n    boxes1 = boxes1.repeat(1, boxes1_repeat).view(-1, 4)\n    boxes2 = boxes2.repeat(boxes2_repeat, 1)\n\n    # 2. Compute intersections\n    b1_y1, b1_x1, b1_y2, b1_x2 = boxes1.chunk(4, dim=1)\n    b2_y1, b2_x1, b2_y2, b2_x2 = boxes2.chunk(4, dim=1)\n    y1 = torch.max(b1_y1, b2_y1)[:, 0]\n    x1 = torch.max(b1_x1, b2_x1)[:, 0]\n    y2 = torch.min(b1_y2, b2_y2)[:, 0]\n    x2 = torch.min(b1_x2, b2_x2)[:, 0]\n    zeros = Variable(torch.zeros(y1.size()[0]), requires_grad=False)\n    if y1.is_cuda:\n        zeros = zeros.cuda()\n    intersection = torch.max(x2 - x1, zeros) * torch.max(y2 - y1, zeros)\n\n    # 3. Compute unions\n    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n    union = b1_area[:, 0] + b2_area[:, 0] - intersection\n\n    # 4. Compute IoU and reshape to [boxes1, boxes2]\n    iou = intersection / union\n    overlaps = iou.view(boxes2_repeat, boxes1_repeat)\n\n    return overlaps\n\n\ndef detection_target_layer(proposals, gt_class_ids, gt_boxes, gt_masks, config):\n    """"""Subsamples proposals and generates target box refinment, class_ids,\n    and masks for each.\n\n    Inputs:\n    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might\n               be zero padded if there are not enough proposals.\n    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.\n    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized\n              coordinates.\n    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type\n\n    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n    and masks.\n    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized\n          coordinates\n    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\n    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, NUM_CLASSES,\n                    (dy, dx, log(dh), log(dw), class_id)]\n                   Class-specific bbox refinments.\n    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width)\n                 Masks cropped to bbox boundaries and resized to neural\n                 network output size.\n    """"""\n\n    # Currently only supports batchsize 1\n    proposals = proposals.squeeze(0)\n    gt_class_ids = gt_class_ids.squeeze(0)\n    gt_boxes = gt_boxes.squeeze(0)\n    gt_masks = gt_masks.squeeze(0)\n\n    # Handle COCO crowds\n    # A crowd box in COCO is a bounding box around several instances. Exclude\n    # them from training. A crowd box is given a negative class ID.\n    if (gt_class_ids < 0).any():\n        crowd_ix = torch.nonzero(gt_class_ids < 0)[:, 0]\n        non_crowd_ix = torch.nonzero(gt_class_ids > 0)[:, 0]\n        crowd_boxes = gt_boxes[crowd_ix.data, :]\n        crowd_masks = gt_masks[crowd_ix.data, :, :]\n        gt_class_ids = gt_class_ids[non_crowd_ix.data]\n        gt_boxes = gt_boxes[non_crowd_ix.data, :]\n        gt_masks = gt_masks[non_crowd_ix.data, :]\n\n        # Compute overlaps with crowd boxes [anchors, crowds]\n        crowd_overlaps = bbox_overlaps(proposals, crowd_boxes)\n        crowd_iou_max = torch.max(crowd_overlaps, dim=1)[0]\n        no_crowd_bool = crowd_iou_max < 0.001\n    else:\n        no_crowd_bool = Variable(torch.ByteTensor(proposals.size()[0] * [True]), requires_grad=False)\n        if config.GPU_COUNT:\n            no_crowd_bool = no_crowd_bool.cuda()\n\n    # Compute overlaps matrix [proposals, gt_boxes]\n    overlaps = bbox_overlaps(proposals, gt_boxes)\n\n    # Determine postive and negative ROIs\n    roi_iou_max = torch.max(overlaps, dim=1)[0]\n\n    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\n    positive_roi_bool = roi_iou_max >= 0.5\n\n    # Subsample ROIs. Aim for 33% positive\n    # Positive ROIs\n    if (positive_roi_bool).any():\n        positive_indices = torch.nonzero(positive_roi_bool)[:, 0]\n\n        positive_count = int(config.TRAIN_ROIS_PER_IMAGE *\n                             config.ROI_POSITIVE_RATIO)\n        rand_idx = torch.randperm(positive_indices.size()[0])\n        rand_idx = rand_idx[:positive_count]\n        if config.GPU_COUNT:\n            rand_idx = rand_idx.cuda()\n        positive_indices = positive_indices[rand_idx]\n        positive_count = positive_indices.size()[0]\n        positive_rois = proposals[positive_indices.data, :]\n\n        # Assign positive ROIs to GT boxes.\n        positive_overlaps = overlaps[positive_indices.data, :]\n        roi_gt_box_assignment = torch.max(positive_overlaps, dim=1)[1]\n        roi_gt_boxes = gt_boxes[roi_gt_box_assignment.data, :]\n        roi_gt_class_ids = gt_class_ids[roi_gt_box_assignment.data]\n\n        # Compute bbox refinement for positive ROIs\n        deltas = Variable(utils.box_refinement(positive_rois.data, roi_gt_boxes.data), requires_grad=False)\n        std_dev = Variable(torch.from_numpy(config.BBOX_STD_DEV).float(), requires_grad=False)\n        if config.GPU_COUNT:\n            std_dev = std_dev.cuda()\n        deltas /= std_dev\n\n        # Assign positive ROIs to GT masks\n        roi_masks = gt_masks[roi_gt_box_assignment.data, :, :]\n\n        # Compute mask targets\n        boxes = positive_rois\n        if config.USE_MINI_MASK:\n            # Transform ROI corrdinates from normalized image space\n            # to normalized mini-mask space.\n            y1, x1, y2, x2 = positive_rois.chunk(4, dim=1)\n            gt_y1, gt_x1, gt_y2, gt_x2 = roi_gt_boxes.chunk(4, dim=1)\n            gt_h = gt_y2 - gt_y1\n            gt_w = gt_x2 - gt_x1\n            y1 = (y1 - gt_y1) / gt_h\n            x1 = (x1 - gt_x1) / gt_w\n            y2 = (y2 - gt_y1) / gt_h\n            x2 = (x2 - gt_x1) / gt_w\n            boxes = torch.cat([y1, x1, y2, x2], dim=1)\n        box_ids = Variable(torch.arange(roi_masks.size()[0]), requires_grad=False).int()\n        if config.GPU_COUNT:\n            box_ids = box_ids.cuda()\n        masks = Variable(CropAndResizeFunction(config.MASK_SHAPE[0], config.MASK_SHAPE[1], 0)(roi_masks.unsqueeze(1), boxes, box_ids).data, requires_grad=False)\n        masks = masks.squeeze(1)\n\n        # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\n        # binary cross entropy loss.\n        masks = torch.round(masks)\n    else:\n        positive_count = 0\n\n    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n    negative_roi_bool = roi_iou_max < 0.5\n    negative_roi_bool = negative_roi_bool & no_crowd_bool\n    # Negative ROIs. Add enough to maintain positive:negative ratio.\n    if (negative_roi_bool).any() and positive_count > 0:\n        negative_indices = torch.nonzero(negative_roi_bool)[:, 0]\n        r = 1.0 / config.ROI_POSITIVE_RATIO\n        negative_count = int(r * positive_count - positive_count)\n        rand_idx = torch.randperm(negative_indices.size()[0])\n        rand_idx = rand_idx[:negative_count]\n        if config.GPU_COUNT:\n            rand_idx = rand_idx.cuda()\n        negative_indices = negative_indices[rand_idx]\n        negative_count = negative_indices.size()[0]\n        negative_rois = proposals[negative_indices.data, :]\n    else:\n        negative_count = 0\n\n    # Append negative ROIs and pad bbox deltas and masks that\n    # are not used for negative ROIs with zeros.\n    if positive_count > 0 and negative_count > 0:\n        rois = torch.cat((positive_rois, negative_rois), dim=0)\n        zeros = Variable(torch.zeros(negative_count), requires_grad=False).int()\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        roi_gt_class_ids = torch.cat([roi_gt_class_ids.int(), zeros], dim=0)\n        zeros = Variable(torch.zeros(negative_count, 4), requires_grad=False)\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        deltas = torch.cat([deltas, zeros], dim=0)\n        zeros = Variable(torch.zeros(negative_count, config.MASK_SHAPE[0], config.MASK_SHAPE[1]), requires_grad=False)\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        masks = torch.cat([masks, zeros], dim=0)\n    elif positive_count > 0:\n        rois = positive_rois\n    elif negative_count > 0:\n        rois = negative_rois\n        zeros = Variable(torch.zeros(negative_count), requires_grad=False)\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        roi_gt_class_ids = zeros\n        zeros = Variable(torch.zeros(negative_count, 4), requires_grad=False).int()\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        deltas = zeros\n        zeros = Variable(torch.zeros(negative_count, config.MASK_SHAPE[0], config.MASK_SHAPE[1]), requires_grad=False)\n        if config.GPU_COUNT:\n            zeros = zeros.cuda()\n        masks = zeros\n    else:\n        rois = Variable(torch.FloatTensor(), requires_grad=False)\n        roi_gt_class_ids = Variable(torch.IntTensor(), requires_grad=False)\n        deltas = Variable(torch.FloatTensor(), requires_grad=False)\n        masks = Variable(torch.FloatTensor(), requires_grad=False)\n        if config.GPU_COUNT:\n            rois = rois.cuda()\n            roi_gt_class_ids = roi_gt_class_ids.cuda()\n            deltas = deltas.cuda()\n            masks = masks.cuda()\n\n    return rois, roi_gt_class_ids, deltas, masks\n\n\n############################################################\n#  Detection Layer\n############################################################\n\ndef clip_to_window(window, boxes):\n    """"""\n        window: (y1, x1, y2, x2). The window in the image we want to clip to.\n        boxes: [N, (y1, x1, y2, x2)]\n    """"""\n    boxes[:, 0] = boxes[:, 0].clamp(float(window[0]), float(window[2]))\n    boxes[:, 1] = boxes[:, 1].clamp(float(window[1]), float(window[3]))\n    boxes[:, 2] = boxes[:, 2].clamp(float(window[0]), float(window[2]))\n    boxes[:, 3] = boxes[:, 3].clamp(float(window[1]), float(window[3]))\n\n    return boxes\n\n\ndef refine_detections(rois, probs, deltas, window, config):\n    """"""Refine classified proposals and filter overlaps and return final\n    detections.\n\n    Inputs:\n        rois: [N, (y1, x1, y2, x2)] in normalized coordinates\n        probs: [N, num_classes]. Class probabilities.\n        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific\n                bounding box deltas.\n        window: (y1, x1, y2, x2) in image coordinates. The part of the image\n            that contains the image excluding the padding.\n\n    Returns detections shaped: [N, (y1, x1, y2, x2, class_id, score)]\n    """"""\n\n    # Class IDs per ROI\n    _, class_ids = torch.max(probs, dim=1)\n\n    # Class probability of the top class of each ROI\n    # Class-specific bounding box deltas\n    idx = torch.arange(class_ids.size()[0]).long()\n    if config.GPU_COUNT:\n        idx = idx.cuda()\n    class_scores = probs[idx, class_ids.data]\n    deltas_specific = deltas[idx, class_ids.data]\n\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1, 4])).float(), requires_grad=False)\n    if config.GPU_COUNT:\n        std_dev = std_dev.cuda()\n    refined_rois = apply_box_deltas(rois, deltas_specific * std_dev)\n\n    # Convert coordiates to image domain\n    height, width = config.IMAGE_SHAPE[:2]\n    scale = Variable(torch.from_numpy(np.array([height, width, height, width])).float(), requires_grad=False)\n    if config.GPU_COUNT:\n        scale = scale.cuda()\n    refined_rois *= scale\n\n    # Clip boxes to image window\n    refined_rois = clip_to_window(window, refined_rois)\n\n    # Round and cast to int since we\'re deadling with pixels now\n    refined_rois = torch.round(refined_rois)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep_bool = class_ids > 0\n\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        keep_bool = keep_bool & (class_scores >= config.DETECTION_MIN_CONFIDENCE)\n    keep = torch.nonzero(keep_bool)[:, 0]\n\n    # Apply per-class NMS\n    pre_nms_class_ids = class_ids[keep.data]\n    pre_nms_scores = class_scores[keep.data]\n    pre_nms_rois = refined_rois[keep.data]\n\n    for i, class_id in enumerate(unique1d(pre_nms_class_ids)):\n        # Pick detections of this class\n        ixs = torch.nonzero(pre_nms_class_ids == class_id)[:, 0]\n\n        # Sort\n        ix_rois = pre_nms_rois[ixs.data]\n        ix_scores = pre_nms_scores[ixs]\n        ix_scores, order = ix_scores.sort(descending=True)\n        ix_rois = ix_rois[order.data, :]\n\n        class_keep = nms(torch.cat((ix_rois, ix_scores.unsqueeze(1)), dim=1).data, config.DETECTION_NMS_THRESHOLD)\n\n        # Map indicies\n        class_keep = keep[ixs[order[class_keep].data].data]\n\n        if i == 0:\n            nms_keep = class_keep\n        else:\n            nms_keep = unique1d(torch.cat((nms_keep, class_keep)))\n    keep = intersect1d(keep, nms_keep)\n\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    top_ids = class_scores[keep.data].sort(descending=True)[1][:roi_count]\n    keep = keep[top_ids.data]\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are in image domain.\n    result = torch.cat((refined_rois[keep.data],\n                        class_ids[keep.data].unsqueeze(1).float(),\n                        class_scores[keep.data].unsqueeze(1)), dim=1)\n\n    return result\n\n\ndef detection_layer(config, rois, mrcnn_class, mrcnn_bbox, image_meta):\n    """"""Takes classified proposal boxes and their bounding box deltas and\n    returns the final detection boxes.\n\n    Returns:\n    [batch, num_detections, (y1, x1, y2, x2, class_score)] in pixels\n    """"""\n\n    # Currently only supports batchsize 1\n    rois = rois.squeeze(0)\n\n    _, _, window, _ = parse_image_meta(image_meta)\n    window = window[0]\n    detections = refine_detections(rois, mrcnn_class, mrcnn_bbox, window, config)\n\n    return detections\n\n\n############################################################\n#  Region Proposal Network\n############################################################\n\nclass RPN(nn.Module):\n    """"""Builds the model of Region Proposal Network.\n\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n\n    Returns:\n        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    """"""\n\n    def __init__(self, anchors_per_location, anchor_stride, depth):\n        super(RPN, self).__init__()\n        self.anchors_per_location = anchors_per_location\n        self.anchor_stride = anchor_stride\n        self.depth = depth\n\n        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n        self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv_class = nn.Conv2d(512, 2 * anchors_per_location, kernel_size=1, stride=1)\n        self.softmax = nn.Softmax(dim=2)\n        self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        # Shared convolutional base of the RPN\n        x = self.relu(self.conv_shared(self.padding(x)))\n\n        # Anchor Score. [batch, anchors per location * 2, height, width].\n        rpn_class_logits = self.conv_class(x)\n\n        # Reshape to [batch, 2, anchors]\n        rpn_class_logits = rpn_class_logits.permute(0, 2, 3, 1)\n        rpn_class_logits = rpn_class_logits.contiguous()\n        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n\n        # Softmax on last dimension of BG/FG.\n        rpn_probs = self.softmax(rpn_class_logits)\n\n        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n        # where depth is [x, y, log(w), log(h)]\n        rpn_bbox = self.conv_bbox(x)\n\n        # Reshape to [batch, 4, anchors]\n        rpn_bbox = rpn_bbox.permute(0, 2, 3, 1)\n        rpn_bbox = rpn_bbox.contiguous()\n        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n\n        return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n\n############################################################\n#  Feature Pyramid Network Heads\n############################################################\n\nclass Classifier(nn.Module):\n    def __init__(self, depth, pool_size, image_shape, num_classes):\n        super(Classifier, self).__init__()\n        self.depth = depth\n        self.pool_size = pool_size\n        self.image_shape = image_shape\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(self.depth, 1024, kernel_size=self.pool_size, stride=1)\n        self.bn1 = nn.BatchNorm2d(1024, eps=0.001, momentum=0.01)\n        self.conv2 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)\n        self.bn2 = nn.BatchNorm2d(1024, eps=0.001, momentum=0.01)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.linear_class = nn.Linear(1024, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n        self.linear_bbox = nn.Linear(1024, num_classes * 4)\n\n    def forward(self, x, rois):\n        x = pyramid_roi_align([rois] + x, self.pool_size, self.image_shape)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = x.view(-1, 1024)\n        mrcnn_class_logits = self.linear_class(x)\n        mrcnn_probs = self.softmax(mrcnn_class_logits)\n\n        mrcnn_bbox = self.linear_bbox(x)\n        mrcnn_bbox = mrcnn_bbox.view(mrcnn_bbox.size()[0], -1, 4)\n\n        return [mrcnn_class_logits, mrcnn_probs, mrcnn_bbox]\n\n\nclass Mask(nn.Module):\n    def __init__(self, depth, pool_size, image_shape, num_classes):\n        super(Mask, self).__init__()\n        self.depth = depth\n        self.pool_size = pool_size\n        self.image_shape = image_shape\n        self.num_classes = num_classes\n        self.padding = SamePad2d(kernel_size=3, stride=1)\n        self.conv1 = nn.Conv2d(self.depth, 256, kernel_size=3, stride=1)\n        self.bn1 = nn.BatchNorm2d(256, eps=0.001)\n        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n        self.bn2 = nn.BatchNorm2d(256, eps=0.001)\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n        self.bn3 = nn.BatchNorm2d(256, eps=0.001)\n        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n        self.bn4 = nn.BatchNorm2d(256, eps=0.001)\n        self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n        self.conv5 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, rois):\n        x = pyramid_roi_align([rois] + x, self.pool_size, self.image_shape)\n        x = self.conv1(self.padding(x))\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(self.padding(x))\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(self.padding(x))\n        x = self.bn3(x)\n        x = self.relu(x)\n        x = self.conv4(self.padding(x))\n        x = self.bn4(x)\n        x = self.relu(x)\n        x = self.deconv(x)\n        x = self.relu(x)\n        x = self.conv5(x)\n        x = self.sigmoid(x)\n\n        return x\n\n\n############################################################\n#  Loss Functions\n############################################################\n\ndef compute_rpn_class_loss(rpn_match, rpn_class_logits):\n    """"""RPN anchor classifier loss.\n\n    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n               -1=negative, 0=neutral anchor.\n    rpn_class_logits: [batch, anchors, 2]. RPN classifier logits for FG/BG.\n    """"""\n\n    # Squeeze last dim to simplify\n    rpn_match = rpn_match.squeeze(2)\n\n    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\n    anchor_class = (rpn_match == 1).long()\n\n    # Positive and Negative anchors contribute to the loss,\n    # but neutral anchors (match value = 0) don\'t.\n    indices = torch.nonzero(rpn_match != 0)\n\n    # Pick rows that contribute to the loss and filter out the rest.\n    rpn_class_logits = rpn_class_logits[indices.data[:, 0], indices.data[:, 1], :]\n    anchor_class = anchor_class[indices.data[:, 0], indices.data[:, 1]]\n\n    # Crossentropy loss\n    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n\n    return loss\n\n\ndef compute_rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n    """"""Return the RPN bounding box loss graph.\n\n    target_bbox: [batch, max positive anchors, (dy, dx, log(dh), log(dw))].\n        Uses 0 padding to fill in unsed bbox deltas.\n    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n               -1=negative, 0=neutral anchor.\n    rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n    """"""\n\n    # Squeeze last dim to simplify\n    rpn_match = rpn_match.squeeze(2)\n\n    # Positive anchors contribute to the loss, but negative and\n    # neutral anchors (match value of 0 or -1) don\'t.\n    indices = torch.nonzero(rpn_match == 1)\n\n    # Pick bbox deltas that contribute to the loss\n    rpn_bbox = rpn_bbox[indices.data[:, 0], indices.data[:, 1]]\n\n    # Trim target bounding box deltas to the same length as rpn_bbox.\n    target_bbox = target_bbox[0, :rpn_bbox.size()[0], :]\n\n    # Smooth L1 loss\n    loss = F.smooth_l1_loss(rpn_bbox, target_bbox)\n\n    return loss\n\n\ndef compute_mrcnn_class_loss(target_class_ids, pred_class_logits):\n    """"""Loss for the classifier head of Mask RCNN.\n\n    target_class_ids: [batch, num_rois]. Integer class IDs. Uses zero\n        padding to fill in the array.\n    pred_class_logits: [batch, num_rois, num_classes]\n    """"""\n\n    # Loss\n    if target_class_ids.size():\n        loss = F.cross_entropy(pred_class_logits, target_class_ids.long())\n    else:\n        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n        if target_class_ids.is_cuda:\n            loss = loss.cuda()\n\n    return loss\n\n\ndef compute_mrcnn_bbox_loss(target_bbox, target_class_ids, pred_bbox):\n    """"""Loss for Mask R-CNN bounding box refinement.\n\n    target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n    target_class_ids: [batch, num_rois]. Integer class IDs.\n    pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n    """"""\n\n    if target_class_ids.size():\n        # Only positive ROIs contribute to the loss. And only\n        # the right class_id of each ROI. Get their indicies.\n        positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n        positive_roi_class_ids = target_class_ids[positive_roi_ix.data].long()\n        indices = torch.stack((positive_roi_ix, positive_roi_class_ids), dim=1)\n\n        # Gather the deltas (predicted and true) that contribute to loss\n        target_bbox = target_bbox[indices[:, 0].data, :]\n        pred_bbox = pred_bbox[indices[:, 0].data, indices[:, 1].data, :]\n\n        # Smooth L1 loss\n        loss = F.smooth_l1_loss(pred_bbox, target_bbox)\n    else:\n        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n        if target_class_ids.is_cuda:\n            loss = loss.cuda()\n\n    return loss\n\n\ndef compute_mrcnn_mask_loss(target_masks, target_class_ids, pred_masks):\n    """"""Mask binary cross-entropy loss for the masks head.\n\n    target_masks: [batch, num_rois, height, width].\n        A float32 tensor of values 0 or 1. Uses zero padding to fill array.\n    target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n    pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n                with values from 0 to 1.\n    """"""\n    if target_class_ids.size():\n        # Only positive ROIs contribute to the loss. And only\n        # the class specific mask of each ROI.\n        positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n        positive_class_ids = target_class_ids[positive_ix.data].long()\n        indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n\n        # Gather the masks (predicted and true) that contribute to loss\n        y_true = target_masks[indices[:, 0].data, :, :]\n        y_pred = pred_masks[indices[:, 0].data, indices[:, 1].data, :, :]\n\n        # Binary cross entropy\n        loss = F.binary_cross_entropy(y_pred, y_true)\n    else:\n        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n        if target_class_ids.is_cuda:\n            loss = loss.cuda()\n\n    return loss\n\n\ndef compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask):\n\n    rpn_class_loss = compute_rpn_class_loss(rpn_match, rpn_class_logits)\n    rpn_bbox_loss = compute_rpn_bbox_loss(rpn_bbox, rpn_match, rpn_pred_bbox)\n    mrcnn_class_loss = compute_mrcnn_class_loss(target_class_ids, mrcnn_class_logits)\n    mrcnn_bbox_loss = compute_mrcnn_bbox_loss(target_deltas, target_class_ids, mrcnn_bbox)\n    mrcnn_mask_loss = compute_mrcnn_mask_loss(target_mask, target_class_ids, mrcnn_mask)\n\n    return [rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss]\n\n\n############################################################\n#  Data Generator\n############################################################\n\ndef load_image_gt(dataset, config, image_id, augment=False,\n                  use_mini_mask=False):\n    """"""Load and return ground truth data for an image (image, mask, bounding boxes).\n\n    augment: If true, apply random image augmentation. Currently, only\n        horizontal flipping is offered.\n    use_mini_mask: If False, returns full-size masks that are the same height\n        and width as the original image. These can be big, for example\n        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,\n        224x224 and are generated by extracting the bounding box of the\n        object and resizing it to MINI_MASK_SHAPE.\n\n    Returns:\n    image: [height, width, 3]\n    shape: the original shape of the image before resizing and cropping.\n    class_ids: [instance_count] Integer class IDs\n    bbox: [instance_count, (y1, x1, y2, x2)]\n    mask: [height, width, instance_count]. The height and width are those\n        of the image unless use_mini_mask is True, in which case they are\n        defined in MINI_MASK_SHAPE.\n    """"""\n    # Load image and mask\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    shape = image.shape\n    image, window, scale, padding = utils.resize_image(\n        image,\n        min_dim=config.IMAGE_MIN_DIM,\n        max_dim=config.IMAGE_MAX_DIM,\n        padding=config.IMAGE_PADDING)\n    mask = utils.resize_mask(mask, scale, padding)\n\n    # Random horizontal flips.\n    if augment:\n        if random.randint(0, 1):\n            image = np.fliplr(image)\n            mask = np.fliplr(mask)\n\n    # Bounding boxes. Note that some boxes might be all zeros\n    # if the corresponding mask got cropped out.\n    # bbox: [num_instances, (y1, x1, y2, x2)]\n    bbox = utils.extract_bboxes(mask)\n\n    # Active classes\n    # Different datasets have different classes, so track the\n    # classes supported in the dataset of this image.\n    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][""source""]]\n    active_class_ids[source_class_ids] = 1\n\n    # Resize masks to smaller size to reduce memory usage\n    if use_mini_mask:\n        mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n\n    # Image meta data\n    image_meta = compose_image_meta(image_id, shape, window, active_class_ids)\n\n    return image, image_meta, class_ids, bbox, mask\n\n\ndef build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):\n    """"""Given the anchors and GT boxes, compute overlaps and identify positive\n    anchors and deltas to refine them to match their corresponding GT boxes.\n\n    anchors: [num_anchors, (y1, x1, y2, x2)]\n    gt_class_ids: [num_gt_boxes] Integer class IDs.\n    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\n\n    Returns:\n    rpn_match: [N] (int32) matches between anchors and GT boxes.\n               1 = positive anchor, -1 = negative anchor, 0 = neutral\n    rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n    """"""\n    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral\n    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)\n    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]\n    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))\n\n    # Handle COCO crowds\n    # A crowd box in COCO is a bounding box around several instances. Exclude\n    # them from training. A crowd box is given a negative class ID.\n    crowd_ix = np.where(gt_class_ids < 0)[0]\n    if crowd_ix.shape[0] > 0:\n        # Filter out crowds from ground truth class IDs and boxes\n        non_crowd_ix = np.where(gt_class_ids > 0)[0]\n        crowd_boxes = gt_boxes[crowd_ix]\n        gt_class_ids = gt_class_ids[non_crowd_ix]\n        gt_boxes = gt_boxes[non_crowd_ix]\n        # Compute overlaps with crowd boxes [anchors, crowds]\n        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)\n        crowd_iou_max = np.amax(crowd_overlaps, axis=1)\n        no_crowd_bool = (crowd_iou_max < 0.001)\n    else:\n        # All anchors don\'t intersect a crowd\n        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)\n\n    # Compute overlaps [num_anchors, num_gt_boxes]\n    overlaps = utils.compute_overlaps(anchors, gt_boxes)\n\n    # Match anchors to GT Boxes\n    # If an anchor overlaps a GT box with IoU >= 0.7 then it\'s positive.\n    # If an anchor overlaps a GT box with IoU < 0.3 then it\'s negative.\n    # Neutral anchors are those that don\'t match the conditions above,\n    # and they don\'t influence the loss function.\n    # However, don\'t keep any GT box unmatched (rare, but happens). Instead,\n    # match it to the closest anchor (even if its max IoU is < 0.3).\n    #\n    # 1. Set negative anchors first. They get overwritten below if a GT box is\n    # matched to them. Skip boxes in crowd areas.\n    anchor_iou_argmax = np.argmax(overlaps, axis=1)\n    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]\n    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1\n    # 2. Set an anchor for each GT box (regardless of IoU value).\n    # TODO: If multiple anchors have the same IoU match all of them\n    gt_iou_argmax = np.argmax(overlaps, axis=0)\n    rpn_match[gt_iou_argmax] = 1\n    # 3. Set anchors with high overlap as positive.\n    rpn_match[anchor_iou_max >= 0.7] = 1\n\n    # Subsample to balance positive and negative anchors\n    # Don\'t let positives be more than half the anchors\n    ids = np.where(rpn_match == 1)[0]\n    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)\n    if extra > 0:\n        # Reset the extra ones to neutral\n        ids = np.random.choice(ids, extra, replace=False)\n        rpn_match[ids] = 0\n    # Same for negative proposals\n    ids = np.where(rpn_match == -1)[0]\n    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -\n                        np.sum(rpn_match == 1))\n    if extra > 0:\n        # Rest the extra ones to neutral\n        ids = np.random.choice(ids, extra, replace=False)\n        rpn_match[ids] = 0\n\n    # For positive anchors, compute shift and scale needed to transform them\n    # to match the corresponding GT boxes.\n    ids = np.where(rpn_match == 1)[0]\n    ix = 0  # index into rpn_bbox\n    # TODO: use box_refinment() rather than duplicating the code here\n    for i, a in zip(ids, anchors[ids]):\n        # Closest gt box (it might have IoU < 0.7)\n        gt = gt_boxes[anchor_iou_argmax[i]]\n\n        # Convert coordinates to center plus width/height.\n        # GT Box\n        gt_h = gt[2] - gt[0]\n        gt_w = gt[3] - gt[1]\n        gt_center_y = gt[0] + 0.5 * gt_h\n        gt_center_x = gt[1] + 0.5 * gt_w\n        # Anchor\n        a_h = a[2] - a[0]\n        a_w = a[3] - a[1]\n        a_center_y = a[0] + 0.5 * a_h\n        a_center_x = a[1] + 0.5 * a_w\n\n        # Compute the bbox refinement that the RPN should predict.\n        rpn_bbox[ix] = [\n            (gt_center_y - a_center_y) / a_h,\n            (gt_center_x - a_center_x) / a_w,\n            np.log(gt_h / a_h),\n            np.log(gt_w / a_w),\n        ]\n        # Normalize\n        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV\n        ix += 1\n\n    return rpn_match, rpn_bbox\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, config, augment=True):\n        """"""A generator that returns images and corresponding target class ids,\n            bounding box deltas, and masks.\n\n            dataset: The Dataset object to pick data from\n            config: The model config object\n            shuffle: If True, shuffles the samples before every epoch\n            augment: If True, applies image augmentation to images (currently only\n                     horizontal flips are supported)\n\n            Returns a Python generator. Upon calling next() on it, the\n            generator returns two lists, inputs and outputs. The containtes\n            of the lists differs depending on the received arguments:\n            inputs list:\n            - images: [batch, H, W, C]\n            - image_metas: [batch, size of image meta]\n            - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\n            - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n            - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\n            - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\n            - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\n                        are those of the image unless use_mini_mask is True, in which\n                        case they are defined in MINI_MASK_SHAPE.\n\n            outputs list: Usually empty in regular training. But if detection_targets\n                is True then the outputs list contains target class_ids, bbox deltas,\n                and masks.\n            """"""\n        self.b = 0  # batch item index\n        self.image_index = -1\n        self.image_ids = np.copy(dataset.image_ids)\n        self.error_count = 0\n\n        self.dataset = dataset\n        self.config = config\n        self.augment = augment\n\n        # Anchors\n        # [anchor_count, (y1, x1, y2, x2)]\n        self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n                                                      config.RPN_ANCHOR_RATIOS,\n                                                      config.BACKBONE_SHAPES,\n                                                      config.BACKBONE_STRIDES,\n                                                      config.RPN_ANCHOR_STRIDE)\n\n    def __getitem__(self, image_index):\n        # Get GT bounding boxes and masks for image.\n        image_id = self.image_ids[image_index]\n        image, image_metas, gt_class_ids, gt_boxes, gt_masks = \\\n            load_image_gt(self.dataset, self.config, image_id, augment=self.augment,\n                          use_mini_mask=self.config.USE_MINI_MASK)\n\n        # Skip images that have no instances. This can happen in cases\n        # where we train on a subset of classes and the image doesn\'t\n        # have any of the classes we care about.\n        if not np.any(gt_class_ids > 0):\n            return None\n\n        # RPN Targets\n        rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors,\n                                                gt_class_ids, gt_boxes, self.config)\n\n        # If more instances than fits in the array, sub-sample from them.\n        if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:\n            ids = np.random.choice(\n                np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)\n            gt_class_ids = gt_class_ids[ids]\n            gt_boxes = gt_boxes[ids]\n            gt_masks = gt_masks[:, :, ids]\n\n        # Add to batch\n        rpn_match = rpn_match[:, np.newaxis]\n        images = mold_image(image.astype(np.float32), self.config)\n\n        # Convert\n        images = torch.from_numpy(images.transpose(2, 0, 1)).float()\n        image_metas = torch.from_numpy(image_metas)\n        rpn_match = torch.from_numpy(rpn_match)\n        rpn_bbox = torch.from_numpy(rpn_bbox).float()\n        gt_class_ids = torch.from_numpy(gt_class_ids)\n        gt_boxes = torch.from_numpy(gt_boxes).float()\n        gt_masks = torch.from_numpy(gt_masks.astype(int).transpose(2, 0, 1)).float()\n\n        return images, image_metas, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks\n\n    def __len__(self):\n        return self.image_ids.shape[0]\n\n\n############################################################\n#  MaskRCNN Class\n############################################################\n\nclass MaskRCNN(nn.Module):\n    """"""Encapsulates the Mask RCNN model functionality.\n    """"""\n\n    def __init__(self, config, model_dir):\n        """"""\n        config: A Sub-class of the Config class\n        model_dir: Directory to save training logs and trained weights\n        """"""\n        super(MaskRCNN, self).__init__()\n        self.config = config\n        self.model_dir = model_dir\n        self.set_log_dir()\n        self.build(config=config)\n        self.initialize_weights()\n        self.loss_history = []\n        self.val_loss_history = []\n\n    def build(self, config):\n        """"""Build Mask R-CNN architecture.\n        """"""\n\n        # Image size must be dividable by 2 multiple times\n        h, w = config.IMAGE_SHAPE[:2]\n        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n            raise Exception(""Image size must be dividable by 2 at least 6 times ""\n                            ""to avoid fractions when downscaling and upscaling.""\n                            ""For example, use 256, 320, 384, 448, 512, ... etc. "")\n\n        # Build the shared convolutional layers.\n        # Bottom-up Layers\n        # Returns a list of the last layers of each stage, 5 in total.\n        # Don\'t create the thead (stage 5), so we pick the 4th item in the list.\n        resnet = ResNet(""resnet101"", stage5=True)\n        C1, C2, C3, C4, C5 = resnet.stages()\n\n        # Top-down Layers\n        # TODO: add assert to varify feature map sizes match what\'s in config\n        self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n\n        # Generate Anchors\n        self.anchors = Variable(torch.from_numpy(utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n                                                                                config.RPN_ANCHOR_RATIOS,\n                                                                                config.BACKBONE_SHAPES,\n                                                                                config.BACKBONE_STRIDES,\n                                                                                config.RPN_ANCHOR_STRIDE)).float(), requires_grad=False)\n        if self.config.GPU_COUNT:\n            self.anchors = self.anchors.cuda()\n\n        # RPN\n        self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n\n        # FPN Classifier\n        self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n\n        # FPN Mask\n        self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n\n        # Fix batch norm layers\n        def set_bn_fix(m):\n            classname = m.__class__.__name__\n            if classname.find(\'BatchNorm\') != -1:\n                for p in m.parameters():\n                    p.requires_grad = False\n\n        self.apply(set_bn_fix)\n\n    def initialize_weights(self):\n        """"""Initialize model weights.\n        """"""\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    def set_trainable(self, layer_regex, model=None, indent=0, verbose=1):\n        """"""Sets model layers as trainable if their names match\n        the given regular expression.\n        """"""\n\n        for param in self.named_parameters():\n            layer_name = param[0]\n            trainable = bool(re.fullmatch(layer_regex, layer_name))\n            if not trainable:\n                param[1].requires_grad = False\n\n    def set_log_dir(self, model_path=None):\n        """"""Sets the model log directory and epoch counter.\n\n        model_path: If None, or a format different from what this code uses\n            then set a new log directory and start epochs from 0. Otherwise,\n            extract the log directory and the epoch counter from the file\n            name.\n        """"""\n\n        # Set date and epoch counter as if starting a new model\n        self.epoch = 0\n        now = datetime.datetime.now()\n\n        # If we have a model path with date and epochs use them\n        if model_path:\n            # Continue from we left of. Get epoch and date from the file name\n            # A sample model path might look like:\n            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5\n            regex = r"".*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.pth""\n            m = re.match(regex, model_path)\n            if m:\n                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n                                        int(m.group(4)), int(m.group(5)))\n                self.epoch = int(m.group(6))\n\n        # Directory for training logs\n        self.log_dir = os.path.join(self.model_dir, ""{}{:%Y%m%dT%H%M}"".format(\n            self.config.NAME.lower(), now))\n\n        # Path to save after each epoch. Include placeholders that get filled by Keras.\n        self.checkpoint_path = os.path.join(self.log_dir, ""mask_rcnn_{}_*epoch*.pth"".format(\n            self.config.NAME.lower()))\n        self.checkpoint_path = self.checkpoint_path.replace(\n            ""*epoch*"", ""{:04d}"")\n\n    def load_last_model(self):\n        """"""Find the last checkpoint file of the model directory, and load the weights.\n        """"""\n        checkpoints = next(os.walk(self.model_dir))[2]\n        checkpoints = filter(lambda f: f.startswith(""mask_rcnn""), checkpoints)\n        checkpoints = sorted(checkpoints)\n        if checkpoints:\n            checkpoint = os.path.join(self.model_dir, checkpoints[-1])\n            self.load_weights(checkpoint)\n            print(checkpoint)\n\n    def find_last(self):\n        """"""Find the last checkpoint file of the last trained model in the\n        model directory.\n        Returns:\n            log_dir: The directory where events and weights are saved\n            checkpoint_path: the path to the last checkpoint file\n        """"""\n        # Get directory names. Each directory corresponds to a model\n        dir_names = next(os.walk(self.model_dir))[1]\n        key = self.config.NAME.lower()\n        dir_names = filter(lambda f: f.startswith(key), dir_names)\n        dir_names = sorted(dir_names)\n        if not dir_names:\n            return None, None\n        # Pick last directory\n        dir_name = os.path.join(self.model_dir, dir_names[-1])\n        # Find the last checkpoint\n        checkpoints = next(os.walk(dir_name))[2]\n        checkpoints = filter(lambda f: f.startswith(""mask_rcnn""), checkpoints)\n        checkpoints = sorted(checkpoints)\n        if not checkpoints:\n            return dir_name, None\n        checkpoint = os.path.join(dir_name, checkpoints[-1])\n        return dir_name, checkpoint\n\n    def load_weights(self, filepath, transfer=0):\n        """"""Modified version of the correspoding Keras function with\n        the addition of multi-GPU support and the ability to exclude\n        some layers from loading.\n        exlude: list of layer names to excluce\n        """"""\n        if os.path.exists(filepath):\n            load_dict = torch.load(filepath)\n            if transfer:\n                remove_list = [\'classifier.linear_bbox.bias\', \'classifier.linear_bbox.weight\', \'classifier.linear_class.bias\', \'classifier.linear_class.weight\', \'mask.conv5.weight\', \'mask.conv5.bias\']\n                for remove_weight in remove_list:\n                    load_dict.pop(remove_weight)\n                state_dict = self.state_dict()\n                state_dict.update(load_dict)\n                self.load_state_dict(state_dict)\n            else:\n                self.load_state_dict(load_dict)\n        else:\n            print(""Weight file not found ..."")\n\n        # Update the log directory\n        self.set_log_dir(filepath)\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n    def detect(self, images):\n        """"""Runs the detection pipeline.\n\n        images: List of images, potentially of different sizes.\n\n        Returns a list of dicts, one dict per image. The dict contains:\n        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n        class_ids: [N] int class IDs\n        scores: [N] float probability scores for the class IDs\n        masks: [H, W, N] instance binary masks\n        """"""\n\n        # Mold inputs to format expected by the neural network\n        molded_images, image_metas, windows = self.mold_inputs(images)\n\n        # Convert images to torch tensor\n        molded_images = torch.from_numpy(molded_images.transpose(0, 3, 1, 2)).float()\n\n        # To GPU\n        if self.config.GPU_COUNT:\n            molded_images = molded_images.cuda()\n\n        # Wrap in variable\n        molded_images = Variable(molded_images, volatile=True)\n\n        # Run object detection\n        detections, mrcnn_mask = self.predict([molded_images, image_metas], mode=\'inference\')\n\n        # Convert to numpy\n        detections = detections.data.cpu().numpy()\n        mrcnn_mask = mrcnn_mask.permute(0, 1, 3, 4, 2).data.cpu().numpy()\n\n        # Process detections\n        results = []\n        for i, image in enumerate(images):\n            final_rois, final_class_ids, final_scores, final_masks =\\\n                self.unmold_detections(detections[i], mrcnn_mask[i],\n                                       image.shape, windows[i])\n            results.append({\n                ""rois"": final_rois,\n                ""class_ids"": final_class_ids,\n                ""scores"": final_scores,\n                ""masks"": final_masks,\n            })\n        return results\n\n    def generate_inst_maps(self, images):\n        """"""Generate instance maps out of images.\n\n        images: of shape (N, H, W, 3).\n\n        Returns:\n        inst_tensors: (N, H, W) numpy instance maps. Each (H, W)instance map maps pixel to 0 (background), 1, 2, 3, etc.\n        """"""\n        inst_tensors = []\n        for n in range(images.shape[0]):\n            detection = self.detect([images[n]])[0]\n            masks = detection[""masks""]\n            inst_tensor = np.zeros(masks.shape[:2])\n            for i in range(masks.shape[2]):\n                inst_tensor = np.maximum(inst_tensor, (i + 1) * masks[:, :, i].astype(int))\n            inst_tensors.append(inst_tensor)\n        inst_tensors = np.stack(inst_tensors, axis=0)\n        return inst_tensors\n\n    def generate_inst_map(self, images, return_more=False):\n        """"""Generate instance maps out of images.\n\n        images: List of images. Each image is of shape (H, W, 3).\n\n        Returns:\n        inst_tensors: [#images, H, W] numpy instance maps. Each [H, W] instance map maps pixel to 0 (background), 1, 2, 3, etc.\n        """"""\n\n        detection = self.detect(images)  # NOTE: only support 1 iamge!!!\n        inst_tensors = []\n        d = {}\n        for image_dict in detection:\n            masks = image_dict[""masks""]\n            # print(masks.shape)\n            h = masks.shape[0]\n            w = masks.shape[1]\n            inst_tensor = np.zeros((h, w))\n            for i in range(masks.shape[2]):\n                #inst_tensor[masks[:,:,i]] = i+1\n                inst_tensor = np.maximum(inst_tensor, (i + 1) * masks[:, :, i].astype(int))\n                if return_more:\n                    d[int(i + 1)] = {\'rois\': image_dict[\'rois\'][i].tolist(), \'class_id\': int(image_dict[\'class_ids\'][i])}\n            inst_tensors.append(inst_tensor)\n        inst_tensors = np.stack(inst_tensors, axis=0)\n        if return_more:\n            return inst_tensors, d, detection\n        else:\n            return inst_tensors\n\n    def predict(self, input, mode):\n        molded_images = input[0]\n        image_metas = input[1]\n\n        if mode == \'inference\':\n            self.eval()\n        elif mode == \'training\':\n            self.train()\n\n            # Set batchnorm always in eval mode during training\n            def set_bn_eval(m):\n                classname = m.__class__.__name__\n                if classname.find(\'BatchNorm\') != -1:\n                    m.eval()\n\n            self.apply(set_bn_eval)\n\n        # Feature extraction\n        [p2_out, p3_out, p4_out, p5_out, p6_out] = self.fpn(molded_images)\n\n        # Note that P6 is used in RPN, but not in the classifier heads.\n        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]\n        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n\n        # Loop through pyramid layers\n        layer_outputs = []  # list of lists\n        for p in rpn_feature_maps:\n            layer_outputs.append(self.rpn(p))\n\n        # Concatenate layer outputs\n        # Convert from list of lists of level outputs to list of lists\n        # of outputs across levels.\n        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n        outputs = list(zip(*layer_outputs))\n        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n        rpn_class_logits, rpn_class, rpn_bbox = outputs\n\n        # Generate proposals\n        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n        # and zero padded.\n        proposal_count = self.config.POST_NMS_ROIS_TRAINING if mode == ""training"" \\\n            else self.config.POST_NMS_ROIS_INFERENCE\n        rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n                                  proposal_count=proposal_count,\n                                  nms_threshold=self.config.RPN_NMS_THRESHOLD,\n                                  anchors=self.anchors,\n                                  config=self.config)\n\n        if mode == \'inference\':\n            # Network Heads\n            # Proposal classifier and BBox regressor heads\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rpn_rois)\n\n            # Detections\n            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n            detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)\n\n            # Convert boxes to normalized coordinates\n            # TODO: let DetectionLayer return normalized coordinates to avoid\n            #       unnecessary conversions\n            h, w = self.config.IMAGE_SHAPE[:2]\n            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n            if self.config.GPU_COUNT:\n                scale = scale.cuda()\n            detection_boxes = detections[:, :4] / scale\n\n            # Add back batch dimension\n            detection_boxes = detection_boxes.unsqueeze(0)\n\n            # Create masks for detections\n            mrcnn_mask = self.mask(mrcnn_feature_maps, detection_boxes)\n\n            # Add back batch dimension\n            detections = detections.unsqueeze(0)\n            mrcnn_mask = mrcnn_mask.unsqueeze(0)\n\n            return [detections, mrcnn_mask]\n\n        elif mode == \'training\':\n\n            gt_class_ids = input[2]\n            gt_boxes = input[3]\n            gt_masks = input[4]\n\n            # Normalize coordinates\n            h, w = self.config.IMAGE_SHAPE[:2]\n            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n            if self.config.GPU_COUNT:\n                scale = scale.cuda()\n            gt_boxes = gt_boxes / scale\n\n            # Generate detection targets\n            # Subsamples proposals and generates target outputs for training\n            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n            # padded. Equally, returned rois and targets are zero padded.\n            rois, target_class_ids, target_deltas, target_mask = \\\n                detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n\n            if not rois.size():\n                mrcnn_class_logits = Variable(torch.FloatTensor())\n                mrcnn_class = Variable(torch.IntTensor())\n                mrcnn_bbox = Variable(torch.FloatTensor())\n                mrcnn_mask = Variable(torch.FloatTensor())\n                if self.config.GPU_COUNT:\n                    mrcnn_class_logits = mrcnn_class_logits.cuda()\n                    mrcnn_class = mrcnn_class.cuda()\n                    mrcnn_bbox = mrcnn_bbox.cuda()\n                    mrcnn_mask = mrcnn_mask.cuda()\n            else:\n                # Network Heads\n                # Proposal classifier and BBox regressor heads\n                mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rois)\n\n                # Create masks for detections\n                mrcnn_mask = self.mask(mrcnn_feature_maps, rois)\n\n            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask]\n\n    def train_model(self, train_dataset, val_dataset, learning_rate, epochs, layers):\n        """"""Train the model.\n        train_dataset, val_dataset: Training and validation Dataset objects.\n        learning_rate: The learning rate to train with\n        epochs: Number of training epochs. Note that previous training epochs\n                are considered to be done alreay, so this actually determines\n                the epochs to train in total rather than in this particaular\n                call.\n        layers: Allows selecting wich layers to train. It can be:\n            - A regular expression to match layer names to train\n            - One of these predefined values:\n              heaads: The RPN, classifier and mask heads of the network\n              all: All the layers\n              3+: Train Resnet stage 3 and up\n              4+: Train Resnet stage 4 and up\n              5+: Train Resnet stage 5 and up\n        """"""\n\n        # Transfer the model from coco\n\n        transfer = False\n        if layers == \'transfer\':\n            transfer = True\n            layers = \'heads\'\n\n        # Pre-defined layer regular expressions\n        layer_regex = {\n            # all layers but the backbone\n            ""heads"": r""(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)"",\n            # From a specific Resnet stage and up\n            ""3+"": r""(fpn.C3.*)|(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)"",\n            ""4+"": r""(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)"",\n            ""5+"": r""(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)"",\n            # All layers\n            ""all"": "".*"",\n        }\n        if layers in layer_regex.keys():\n            layers = layer_regex[layers]\n\n        # Data generators\n        train_set = Dataset(train_dataset, self.config, augment=True)\n        train_generator = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=16)\n        val_set = Dataset(val_dataset, self.config, augment=True)\n        val_generator = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True, num_workers=16)\n\n        # Train\n        log(""\\nStarting at epoch {}. LR={}\\n"".format(self.epoch + 1, learning_rate))\n        log(""Checkpoint Path: {}"".format(self.checkpoint_path))\n        self.set_trainable(layers)\n\n        # Optimizer object\n        # Add L2 Regularization\n        # Skip gamma and beta weights of batch normalization layers.\n        trainables_wo_bn = [param for name, param in self.named_parameters() if param.requires_grad and not \'bn\' in name]\n        trainables_only_bn = [param for name, param in self.named_parameters() if param.requires_grad and \'bn\' in name]\n        if transfer:\n            transfer_layers = ""(mask.conv5.*)|(classifier.linear_class.*)|(classifier.linear_bbox.*)""\n            trainables_transfer = [param for name, param in self.named_parameters() if param.requires_grad and not \'bn\' in name and re.fullmatch(transfer_layers, name)]\n            trainables_wo_bn = [param for name, param in self.named_parameters() if param.requires_grad and not \'bn\' in name and not re.fullmatch(transfer_layers, name)]\n            optimizer = optim.SGD([\n                {\'params\': trainables_transfer, \'lr\': 1e-2, \'weight_decay\': self.config.WEIGHT_DECAY},\n                {\'params\': trainables_wo_bn, \'lr\': learning_rate, \'weight_decay\': self.config.WEIGHT_DECAY},\n                {\'params\': trainables_only_bn, \'lr\': learning_rate}\n            ], momentum=self.config.LEARNING_MOMENTUM)\n        else:\n            optimizer = optim.SGD([\n                {\'params\': trainables_wo_bn, \'weight_decay\': self.config.WEIGHT_DECAY},\n                {\'params\': trainables_only_bn}\n            ], lr=learning_rate, momentum=self.config.LEARNING_MOMENTUM)\n\n        for epoch in range(self.epoch + 1, epochs + 1):\n            log(""Epoch {}/{}."".format(epoch, epochs))\n\n            # Training\n            loss = self.train_epoch(train_generator, optimizer, self.config.STEPS_PER_EPOCH)\n\n            # Validation\n            val_loss = self.valid_epoch(val_generator, self.config.VALIDATION_STEPS)\n\n            # Statistics\n            self.loss_history.append(loss)\n            self.val_loss_history.append(val_loss)\n            visualize.plot_loss(self.loss_history, self.val_loss_history, save=True, log_dir=self.log_dir)\n\n            # Save model\n            if epoch % 5 == 0:\n                torch.save(self.state_dict(), self.checkpoint_path.format(epoch))\n\n        self.epoch = epochs\n\n    def train_epoch(self, datagenerator, optimizer, steps):\n        batch_count = 0\n        loss_sum = 0\n        step = 0\n\n        for inputs in datagenerator:\n            batch_count += 1\n\n            images = inputs[0]\n            image_metas = inputs[1]\n            rpn_match = inputs[2]\n            rpn_bbox = inputs[3]\n            gt_class_ids = inputs[4]\n            gt_boxes = inputs[5]\n            gt_masks = inputs[6]\n\n            # image_metas as numpy array\n            image_metas = image_metas.numpy()\n\n            # Wrap in variables\n            images = Variable(images)\n            rpn_match = Variable(rpn_match)\n            rpn_bbox = Variable(rpn_bbox)\n            gt_class_ids = Variable(gt_class_ids)\n            gt_boxes = Variable(gt_boxes)\n            gt_masks = Variable(gt_masks)\n\n            # To GPU\n            if self.config.GPU_COUNT:\n                images = images.cuda()\n                rpn_match = rpn_match.cuda()\n                rpn_bbox = rpn_bbox.cuda()\n                gt_class_ids = gt_class_ids.cuda()\n                gt_boxes = gt_boxes.cuda()\n                gt_masks = gt_masks.cuda()\n\n            # Run object detection\n            rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask = \\\n                self.predict([images, image_metas, gt_class_ids, gt_boxes, gt_masks], mode=\'training\')\n\n            # Compute losses\n            rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss = compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask)\n            loss = rpn_class_loss + rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss\n\n            # Backpropagation\n            if (batch_count % self.config.BATCH_SIZE) == 0:\n                optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm(self.parameters(), 5.0)\n            if (batch_count % self.config.BATCH_SIZE) == 0:\n                optimizer.step()\n                batch_count = 0\n\n            # Progress\n            if ((step + 1) % self.config.DISPLAY_SIZE) == 0:\n                printProgressBar(step + 1, steps, prefix=""\\t{}/{}"".format(step + 1, steps),\n                                 suffix=""Complete - loss: {:.5f} - rpn_class_loss: {:.5f} - rpn_bbox_loss: {:.5f} - mrcnn_class_loss: {:.5f} - mrcnn_bbox_loss: {:.5f} - mrcnn_mask_loss: {:.5f}"".format(\n                                 loss.data.cpu()[0], rpn_class_loss.data.cpu()[0], rpn_bbox_loss.data.cpu()[0],\n                                 mrcnn_class_loss.data.cpu()[0], mrcnn_bbox_loss.data.cpu()[0],\n                                 mrcnn_mask_loss.data.cpu()[0]), length=10)\n\n            # Statistics\n            loss_sum += loss.data.cpu()[0] / steps\n\n            # Break after \'steps\' steps\n            if step == steps - 1:\n                break\n            step += 1\n\n        return loss_sum\n\n    def valid_epoch(self, datagenerator, steps):\n\n        step = 0\n        loss_sum = 0\n\n        for inputs in datagenerator:\n            images = inputs[0]\n            image_metas = inputs[1]\n            rpn_match = inputs[2]\n            rpn_bbox = inputs[3]\n            gt_class_ids = inputs[4]\n            gt_boxes = inputs[5]\n            gt_masks = inputs[6]\n\n            # image_metas as numpy array\n            image_metas = image_metas.numpy()\n\n            # Wrap in variables\n            images = Variable(images, volatile=True)\n            rpn_match = Variable(rpn_match, volatile=True)\n            rpn_bbox = Variable(rpn_bbox, volatile=True)\n            gt_class_ids = Variable(gt_class_ids, volatile=True)\n            gt_boxes = Variable(gt_boxes, volatile=True)\n            gt_masks = Variable(gt_masks, volatile=True)\n\n            # To GPU\n            if self.config.GPU_COUNT:\n                images = images.cuda()\n                rpn_match = rpn_match.cuda()\n                rpn_bbox = rpn_bbox.cuda()\n                gt_class_ids = gt_class_ids.cuda()\n                gt_boxes = gt_boxes.cuda()\n                gt_masks = gt_masks.cuda()\n\n            # Run object detection\n            rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask = \\\n                self.predict([images, image_metas, gt_class_ids, gt_boxes, gt_masks], mode=\'training\')\n\n            if not target_class_ids.size():\n                continue\n\n            # Compute losses\n            rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss = compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask)\n            loss = rpn_class_loss + rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss\n\n            # Progress\n            printProgressBar(step + 1, steps, prefix=""\\t{}/{}"".format(step + 1, steps),\n                             suffix=""Complete - loss: {:.5f} - rpn_class_loss: {:.5f} - rpn_bbox_loss: {:.5f} - mrcnn_class_loss: {:.5f} - mrcnn_bbox_loss: {:.5f} - mrcnn_mask_loss: {:.5f}"".format(\n                                 loss.data.cpu()[0], rpn_class_loss.data.cpu()[0], rpn_bbox_loss.data.cpu()[0],\n                                 mrcnn_class_loss.data.cpu()[0], mrcnn_bbox_loss.data.cpu()[0],\n                                 mrcnn_mask_loss.data.cpu()[0]), length=10)\n\n            # Statistics\n            loss_sum += loss.data.cpu()[0] / steps\n\n            # Break after \'steps\' steps\n            if step == steps - 1:\n                break\n            step += 1\n\n        return loss_sum\n\n    def mold_inputs(self, images):\n        """"""Takes a list of images and modifies them to the format expected\n        as an input to the neural network.\n        images: List of image matricies [height,width,depth]. Images can have\n            different sizes.\n\n        Returns 3 Numpy matricies:\n        molded_images: [N, h, w, 3]. Images resized and normalized.\n        image_metas: [N, length of meta data]. Details about each image.\n        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n            original image (padding excluded).\n        """"""\n        molded_images = []\n        image_metas = []\n        windows = []\n        for image in images:\n            # Resize image to fit the model expected size\n            # TODO: move resizing to mold_image()\n            molded_image, window, scale, padding = utils.resize_image(\n                image,\n                min_dim=self.config.IMAGE_MIN_DIM,\n                max_dim=self.config.IMAGE_MAX_DIM,\n                padding=self.config.IMAGE_PADDING)\n            molded_image = mold_image(molded_image, self.config)\n            # Build image_meta\n            image_meta = compose_image_meta(\n                0, image.shape, window,\n                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\n            # Append\n            molded_images.append(molded_image)\n            windows.append(window)\n            image_metas.append(image_meta)\n        # Pack into arrays\n        molded_images = np.stack(molded_images)\n        image_metas = np.stack(image_metas)\n        windows = np.stack(windows)\n        return molded_images, image_metas, windows\n\n    def unmold_detections(self, detections, mrcnn_mask, image_shape, window):\n        """"""Reformats the detections of one image from the format of the neural\n        network output to a format suitable for use in the rest of the\n        application.\n\n        detections: [N, (y1, x1, y2, x2, class_id, score)]\n        mrcnn_mask: [N, height, width, num_classes]\n        image_shape: [height, width, depth] Original size of the image before resizing\n        window: [y1, x1, y2, x2] Box in the image where the real image is\n                excluding the padding.\n\n        Returns:\n        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n        class_ids: [N] Integer class IDs for each bounding box\n        scores: [N] Float probability scores of the class_id\n        masks: [height, width, num_instances] Instance masks\n        """"""\n        # How many detections do we have?\n        # Detections array is padded with zeros. Find the first class_id == 0.\n        zero_ix = np.where(detections[:, 4] == 0)[0]\n        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n\n        # Extract boxes, class_ids, scores, and class-specific masks\n        boxes = detections[:N, :4]\n        class_ids = detections[:N, 4].astype(np.int32)\n        scores = detections[:N, 5]\n        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n\n        # Compute scale and shift to translate coordinates to image domain.\n        h_scale = image_shape[0] / (window[2] - window[0])\n        w_scale = image_shape[1] / (window[3] - window[1])\n        scale = min(h_scale, w_scale)\n        shift = window[:2]  # y, x\n        scales = np.array([scale, scale, scale, scale])\n        shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n\n        # Translate bounding boxes to image domain\n        boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n\n        # Filter out detections with zero area. Often only happens in early\n        # stages of training when the network weights are still a bit random.\n        exclude_ix = np.where(\n            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n        if exclude_ix.shape[0] > 0:\n            boxes = np.delete(boxes, exclude_ix, axis=0)\n            class_ids = np.delete(class_ids, exclude_ix, axis=0)\n            scores = np.delete(scores, exclude_ix, axis=0)\n            masks = np.delete(masks, exclude_ix, axis=0)\n            N = class_ids.shape[0]\n\n        # Resize masks to original image size and set boundary threshold.\n        full_masks = []\n        for i in range(N):\n            # Convert neural network mask to full size mask\n            full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)\n            full_masks.append(full_mask)\n        full_masks = np.stack(full_masks, axis=-1)\\\n            if full_masks else np.empty((0,) + masks.shape[1:3])\n\n        return boxes, class_ids, scores, full_masks\n\n\n############################################################\n#  Data Formatting\n############################################################\n\ndef compose_image_meta(image_id, image_shape, window, active_class_ids):\n    """"""Takes attributes of an image and puts them in one 1D array. Use\n    parse_image_meta() to parse the values back.\n\n    image_id: An int ID of the image. Useful for debugging.\n    image_shape: [height, width, channels]\n    window: (y1, x1, y2, x2) in pixels. The area of the image where the real\n            image is (excluding the padding)\n    active_class_ids: List of class_ids available in the dataset from which\n        the image came. Useful if training on images from multiple datasets\n        where not all classes are present in all datasets.\n    """"""\n    meta = np.array(\n        [image_id] +            # size=1\n        list(image_shape) +     # size=3\n        list(window) +          # size=4 (y1, x1, y2, x2) in image cooredinates\n        list(active_class_ids)  # size=num_classes\n    )\n    return meta\n\n\n# Two functions (for Numpy and TF) to parse image_meta tensors.\ndef parse_image_meta(meta):\n    """"""Parses an image info Numpy array to its components.\n    See compose_image_meta() for more details.\n    """"""\n    image_id = meta[:, 0]\n    image_shape = meta[:, 1:4]\n    window = meta[:, 4:8]   # (y1, x1, y2, x2) window of image in in pixels\n    active_class_ids = meta[:, 8:]\n    return image_id, image_shape, window, active_class_ids\n\n\ndef parse_image_meta_graph(meta):\n    """"""Parses a tensor that contains image attributes to its components.\n    See compose_image_meta() for more details.\n\n    meta: [batch, meta length] where meta length depends on NUM_CLASSES\n    """"""\n    image_id = meta[:, 0]\n    image_shape = meta[:, 1:4]\n    window = meta[:, 4:8]\n    active_class_ids = meta[:, 8:]\n    return [image_id, image_shape, window, active_class_ids]\n\n\ndef mold_image(images, config):\n    """"""Takes RGB images with 0-255 values and subtraces\n    the mean pixel and converts it to float. Expects image\n    colors in RGB order.\n    """"""\n    return images.astype(np.float32) - config.MEAN_PIXEL\n\n\ndef unmold_image(normalized_images, config):\n    """"""Takes a image normalized with mold() and returns the original.""""""\n    return (normalized_images + config.MEAN_PIXEL).astype(np.uint8)\n'"
geometric/maskrcnn/utils.py,3,"b'""""""\nMask R-CNN\nCommon utility functions and classes.\n\nCopyright (c) 2017 Matterport, Inc.\nLicensed under the MIT License (see LICENSE for details)\nWritten by Waleed Abdulla\n""""""\n\nimport sys\nimport os\nimport math\nimport random\nimport numpy as np\nimport scipy.misc\nimport scipy.ndimage\nimport skimage.color\nimport skimage.io\nimport torch\n\n############################################################\n#  Bounding Boxes\n############################################################\n\n\ndef extract_bboxes(mask):\n    """"""Compute bounding boxes from masks.\n    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n\n    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n    """"""\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n\ndef compute_iou(box, boxes, box_area, boxes_area):\n    """"""Calculates IoU of the given box with the array of the given boxes.\n    box: 1D vector [y1, x1, y2, x2]\n    boxes: [boxes_count, (y1, x1, y2, x2)]\n    box_area: float. the area of \'box\'\n    boxes_area: array of length boxes_count.\n\n    Note: the areas are passed in rather than calculated here for\n          efficency. Calculate once in the caller to avoid duplicate work.\n    """"""\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\ndef compute_overlaps(boxes1, boxes2):\n    """"""Computes IoU overlaps between two sets of boxes.\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n\n    For better performance, pass the largest set first and the smaller second.\n    """"""\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n\ndef box_refinement(box, gt_box):\n    """"""Compute refinement needed to transform box to gt_box.\n    box and gt_box are [N, (y1, x1, y2, x2)]\n    """"""\n\n    height = box[:, 2] - box[:, 0]\n    width = box[:, 3] - box[:, 1]\n    center_y = box[:, 0] + 0.5 * height\n    center_x = box[:, 1] + 0.5 * width\n\n    gt_height = gt_box[:, 2] - gt_box[:, 0]\n    gt_width = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = torch.log(gt_height / height)\n    dw = torch.log(gt_width / width)\n\n    result = torch.stack([dy, dx, dh, dw], dim=1)\n    return result\n\n\n############################################################\n#  Dataset\n############################################################\n\nclass Dataset(object):\n    """"""The base class for dataset classes.\n    To use it, create a new class that adds functions specific to the dataset\n    you want to use. For example:\n\n    class CatsAndDogsDataset(Dataset):\n        def load_cats_and_dogs(self):\n            ...\n        def load_mask(self, image_id):\n            ...\n        def image_reference(self, image_id):\n            ...\n\n    See COCODataset and ShapesDataset as examples.\n    """"""\n\n    def __init__(self, class_map=None):\n        self._image_ids = []\n        self.image_info = []\n        # Background is always the first class\n        self.class_info = [{""source"": """", ""id"": 0, ""name"": ""BG""}]\n        self.source_class_ids = {}\n\n    def add_class(self, source, class_id, class_name):\n        assert ""."" not in source, ""Source name cannot contain a dot""\n        # Does the class exist already?\n        for info in self.class_info:\n            if info[\'source\'] == source and info[""id""] == class_id:\n                # source.class_id combination already available, skip\n                return\n        # Add the class\n        self.class_info.append({\n            ""source"": source,\n            ""id"": class_id,\n            ""name"": class_name,\n        })\n\n    def add_image(self, source, image_id, path, **kwargs):\n        image_info = {\n            ""id"": image_id,\n            ""source"": source,\n            ""path"": path,\n        }\n        image_info.update(kwargs)\n        self.image_info.append(image_info)\n\n    def image_reference(self, image_id):\n        """"""Return a link to the image in its source Website or details about\n        the image that help looking it up or debugging it.\n\n        Override for your dataset, but pass to this function\n        if you encounter images not in your dataset.\n        """"""\n        return """"\n\n    def prepare(self, class_map=None):\n        """"""Prepares the Dataset class for use.\n\n        TODO: class map is not supported yet. When done, it should handle mapping\n              classes from different datasets to the same class ID.\n        """"""\n        def clean_name(name):\n            """"""Returns a shorter version of object names for cleaner display.""""""\n            return "","".join(name.split("","")[:1])\n\n        # Build (or rebuild) everything else from the info dicts.\n        self.num_classes = len(self.class_info)\n        self.class_ids = np.arange(self.num_classes)\n        self.class_names = [clean_name(c[""name""]) for c in self.class_info]\n        self.num_images = len(self.image_info)\n        self._image_ids = np.arange(self.num_images)\n\n        self.class_from_source_map = {""{}.{}"".format(info[\'source\'], info[\'id\']): id\n                                      for info, id in zip(self.class_info, self.class_ids)}\n\n        # Map sources to class_ids they support\n        self.sources = list(set([i[\'source\'] for i in self.class_info]))\n        self.source_class_ids = {}\n        # Loop over datasets\n        for source in self.sources:\n            self.source_class_ids[source] = []\n            # Find classes that belong to this dataset\n            for i, info in enumerate(self.class_info):\n                # Include BG class in all datasets\n                if i == 0 or source == info[\'source\']:\n                    self.source_class_ids[source].append(i)\n\n    def map_source_class_id(self, source_class_id):\n        """"""Takes a source class ID and returns the int class ID assigned to it.\n\n        For example:\n        dataset.map_source_class_id(""coco.12"") -> 23\n        """"""\n        return self.class_from_source_map[source_class_id]\n\n    def get_source_class_id(self, class_id, source):\n        """"""Map an internal class ID to the corresponding class ID in the source dataset.""""""\n        info = self.class_info[class_id]\n        assert info[\'source\'] == source\n        return info[\'id\']\n\n    def append_data(self, class_info, image_info):\n        self.external_to_class_id = {}\n        for i, c in enumerate(self.class_info):\n            for ds, id in c[""map""]:\n                self.external_to_class_id[ds + str(id)] = i\n\n        # Map external image IDs to internal ones.\n        self.external_to_image_id = {}\n        for i, info in enumerate(self.image_info):\n            self.external_to_image_id[info[""ds""] + str(info[""id""])] = i\n\n    @property\n    def image_ids(self):\n        return self._image_ids\n\n    def source_image_link(self, image_id):\n        """"""Returns the path or URL to the image.\n        Override this to return a URL to the image if it\'s availble online for easy\n        debugging.\n        """"""\n        return self.image_info[image_id][""path""]\n\n    def load_image(self, image_id):\n        """"""Load the specified image and return a [H,W,3] Numpy array.\n        """"""\n        # Load image\n        image = skimage.io.imread(self.image_info[image_id][\'path\'])\n        # If grayscale. Convert to RGB for consistency.\n        if image.ndim != 3:\n            image = skimage.color.gray2rgb(image)\n        return image\n\n    def load_mask(self, image_id):\n        """"""Load instance masks for the given image.\n\n        Different datasets use different ways to store masks. Override this\n        method to load instance masks and return them in the form of am\n        array of binary masks of shape [height, width, instances].\n\n        Returns:\n            masks: A bool array of shape [height, width, instance count] with\n                a binary mask per instance.\n            class_ids: a 1D array of class IDs of the instance masks.\n        """"""\n        # Override this function to load a mask from your dataset.\n        # Otherwise, it returns an empty mask.\n        mask = np.empty([0, 0, 0])\n        class_ids = np.empty([0], np.int32)\n        return mask, class_ids\n\n\ndef resize_image(image, min_dim=None, max_dim=None, padding=False):\n    """"""\n    Resizes an image keeping the aspect ratio.\n\n    min_dim: if provided, resizes the image such that it\'s smaller\n        dimension == min_dim\n    max_dim: if provided, ensures that the image longest side doesn\'t\n        exceed this value.\n    padding: If true, pads image with zeros so it\'s size is max_dim x max_dim\n\n    Returns:\n    image: the resized image\n    window: (y1, x1, y2, x2). If max_dim is provided, padding might\n        be inserted in the returned image. If so, this window is the\n        coordinates of the image part of the full image (excluding\n        the padding). The x2, y2 pixels are not included.\n    scale: The scale factor used to resize the image\n    padding: Padding added to the image [(top, bottom), (left, right), (0, 0)]\n    """"""\n    # Default window (y1, x1, y2, x2) and default scale == 1.\n    h, w = image.shape[:2]\n    window = (0, 0, h, w)\n    scale = 1\n\n    # Scale?\n    if min_dim:\n        # Scale up but not down\n        scale = max(1, min_dim / min(h, w))\n    # Does it exceed max dim?\n    if max_dim:\n        image_max = max(h, w)\n        if round(image_max * scale) > max_dim:\n            scale = max_dim / image_max\n    # Resize image and mask\n    if scale != 1:\n        image = scipy.misc.imresize(\n            image, (round(h * scale), round(w * scale)))\n    # Need padding?\n    if padding:\n        # Get new height and width\n        h, w = image.shape[:2]\n        top_pad = (max_dim - h) // 2\n        bottom_pad = max_dim - h - top_pad\n        left_pad = (max_dim - w) // 2\n        right_pad = max_dim - w - left_pad\n        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n        image = np.pad(image, padding, mode=\'constant\', constant_values=0)\n        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n    return image, window, scale, padding\n\n\ndef resize_mask(mask, scale, padding):\n    """"""Resizes a mask using the given scale and padding.\n    Typically, you get the scale and padding from resize_image() to\n    ensure both, the image and the mask, are resized consistently.\n\n    scale: mask scaling factor\n    padding: Padding to add to the mask in the form\n            [(top, bottom), (left, right), (0, 0)]\n    """"""\n    h, w = mask.shape[:2]\n    mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)\n    mask = np.pad(mask, padding, mode=\'constant\', constant_values=0)\n    return mask\n\n\ndef minimize_mask(bbox, mask, mini_shape):\n    """"""Resize masks to a smaller version to cut memory load.\n    Mini-masks can then resized back to image scale using expand_masks()\n\n    See inspect_data.ipynb notebook for more details.\n    """"""\n    mini_mask = np.zeros(mini_shape + (mask.shape[-1],), dtype=bool)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        y1, x1, y2, x2 = bbox[i][:4]\n        m = m[y1:y2, x1:x2]\n        if m.size == 0:\n            raise Exception(""Invalid bounding box with area of zero"")\n        m = scipy.misc.imresize(m.astype(float), mini_shape, interp=\'bilinear\')\n        mini_mask[:, :, i] = np.where(m >= 128, 1, 0)\n    return mini_mask\n\n\ndef expand_mask(bbox, mini_mask, image_shape):\n    """"""Resizes mini masks back to image size. Reverses the change\n    of minimize_mask().\n\n    See inspect_data.ipynb notebook for more details.\n    """"""\n    mask = np.zeros(image_shape[:2] + (mini_mask.shape[-1],), dtype=bool)\n    for i in range(mask.shape[-1]):\n        m = mini_mask[:, :, i]\n        y1, x1, y2, x2 = bbox[i][:4]\n        h = y2 - y1\n        w = x2 - x1\n        m = scipy.misc.imresize(m.astype(float), (h, w), interp=\'bilinear\')\n        mask[y1:y2, x1:x2, i] = np.where(m >= 128, 1, 0)\n    return mask\n\n\n# TODO: Build and use this function to reduce code duplication\ndef mold_mask(mask, config):\n    pass\n\n\ndef unmold_mask(mask, bbox, image_shape):\n    """"""Converts a mask generated by the neural network into a format similar\n    to it\'s original shape.\n    mask: [height, width] of type float. A small, typically 28x28 mask.\n    bbox: [y1, x1, y2, x2]. The box to fit the mask in.\n\n    Returns a binary mask with the same size as the original image.\n    """"""\n    threshold = 0.5\n    y1, x1, y2, x2 = bbox\n    mask = scipy.misc.imresize(\n        mask, (y2 - y1, x2 - x1), interp=\'bilinear\').astype(np.float32) / 255.0\n    mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n\n    # Put the mask in the right location.\n    full_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n    full_mask[y1:y2, x1:x2] = mask\n    return full_mask\n\n\n############################################################\n#  Anchors\n############################################################\n\ndef generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n    """"""\n    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n    shape: [height, width] spatial shape of the feature map over which\n            to generate anchors.\n    feature_stride: Stride of the feature map relative to the image in pixels.\n    anchor_stride: Stride of anchors on the feature map. For example, if the\n        value is 2 then generate anchors for every other feature map pixel.\n    """"""\n    # Get all combinations of scales and ratios\n    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n    scales = scales.flatten()\n    ratios = ratios.flatten()\n\n    # Enumerate heights and widths from scales and ratios\n    heights = scales / np.sqrt(ratios)\n    widths = scales * np.sqrt(ratios)\n\n    # Enumerate shifts in feature space\n    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n\n    # Enumerate combinations of shifts, widths, and heights\n    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n\n    # Reshape to get a list of (y, x) and a list of (h, w)\n    box_centers = np.stack(\n        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n\n    # Convert to corner coordinates (y1, x1, y2, x2)\n    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n                            box_centers + 0.5 * box_sizes], axis=1)\n    return boxes\n\n\ndef generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n                             anchor_stride):\n    """"""Generate anchors at different levels of a feature pyramid. Each scale\n    is associated with a level of the pyramid, but each ratio is used in\n    all levels of the pyramid.\n\n    Returns:\n    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n        with the same order of the given scales. So, anchors of scale[0] come\n        first, then anchors of scale[1], and so on.\n    """"""\n    # Anchors\n    # [anchor_count, (y1, x1, y2, x2)]\n    anchors = []\n    for i in range(len(scales)):\n        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n                                        feature_strides[i], anchor_stride))\n    return np.concatenate(anchors, axis=0)\n'"
geometric/maskrcnn/visualize.py,0,"b'""""""\nMask R-CNN\nDisplay and Visualization Functions.\n\nCopyright (c) 2017 Matterport, Inc.\nLicensed under the MIT License (see LICENSE for details)\nWritten by Waleed Abdulla\n""""""\n\nimport os\nimport random\nimport itertools\nimport colorsys\nimport numpy as np\nfrom skimage.measure import find_contours\nimport matplotlib.pyplot as plt\nif ""DISPLAY"" not in os.environ:\n    plt.switch_backend(\'agg\')\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nfrom matplotlib.patches import Polygon\n\nimport utils\n\n\n############################################################\n#  Visualization\n############################################################\n\ndef display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None):\n    """"""Display the given set of images, optionally with titles.\n    images: list or array of image tensors in HWC format.\n    titles: optional. A list of titles to display with each image.\n    cols: number of images per row\n    cmap: Optional. Color map to use. For example, ""Blues"".\n    norm: Optional. A Normalize instance to map values to colors.\n    interpolation: Optional. Image interporlation to use for display.\n    """"""\n    titles = titles if titles is not None else [""""] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis(\'off\')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n\ndef random_colors(N, bright=True):\n    """"""\n    Generate random colors.\n    To get visually distinct colors, generate them in HSV space then\n    convert to RGB.\n    """"""\n    brightness = 1.0 if bright else 0.7\n    hsv = [(i / N, 1, brightness) for i in range(N)]\n    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n    random.shuffle(colors)\n    return colors\n\n\ndef apply_mask(image, mask, color, alpha=0.5):\n    """"""Apply the given mask to the image.\n    """"""\n    for c in range(3):\n        image[:, :, c] = np.where(mask == 1,\n                                  image[:, :, c]\n                                  * (1 - alpha) + alpha * color[c] * 255,\n                                  image[:, :, c])\n    return image\n\n\ndef display_instances(image, boxes, masks, class_ids, class_names,\n                      scores=None, title="""",\n                      figsize=(16, 16), ax=None):\n    """"""\n    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n    masks: [height, width, num_instances]\n    class_ids: [num_instances]\n    class_names: list of class names of the dataset\n    scores: (optional) confidence scores for each box\n    figsize: (optional) the size of the image.\n    """"""\n    # Number of instances\n    N = boxes.shape[0]\n    if not N:\n        print(""\\n*** No instances to display *** \\n"")\n    else:\n        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n\n    if not ax:\n        _, ax = plt.subplots(1, figsize=figsize)\n\n    # Generate random colors\n    colors = random_colors(N)\n\n    # Show area outside image boundaries.\n    height, width = image.shape[:2]\n    ax.set_ylim(height + 10, -10)\n    ax.set_xlim(-10, width + 10)\n    ax.axis(\'off\')\n    ax.set_title(title)\n\n    masked_image = image.astype(np.uint32).copy()\n    for i in range(N):\n        color = colors[i]\n\n        # Bounding box\n        if not np.any(boxes[i]):\n            # Skip this instance. Has no bbox. Likely lost in image cropping.\n            continue\n        y1, x1, y2, x2 = boxes[i]\n        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                              alpha=0.7, linestyle=""dashed"",\n                              edgecolor=color, facecolor=\'none\')\n        ax.add_patch(p)\n\n        # Label\n        class_id = class_ids[i]\n        score = scores[i] if scores is not None else None\n        label = class_names[class_id]\n        x = random.randint(x1, (x1 + x2) // 2)\n        caption = ""{} {:.3f}"".format(label, score) if score else label\n        ax.text(x1, y1 + 8, caption,\n                color=\'w\', size=11, backgroundcolor=""none"")\n\n        # Mask\n        mask = masks[:, :, i]\n        masked_image = apply_mask(masked_image, mask, color)\n\n        # Mask Polygon\n        # Pad to ensure proper polygons for masks that touch image edges.\n        padded_mask = np.zeros(\n            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n        padded_mask[1:-1, 1:-1] = mask\n        contours = find_contours(padded_mask, 0.5)\n        for verts in contours:\n            # Subtract the padding and flip (y, x) to (x, y)\n            verts = np.fliplr(verts) - 1\n            p = Polygon(verts, facecolor=""none"", edgecolor=color)\n            ax.add_patch(p)\n    ax.imshow(masked_image.astype(np.uint8))\n    plt.show()\n\n\ndef draw_rois(image, rois, refined_rois, mask, class_ids, class_names, limit=10):\n    """"""\n    anchors: [n, (y1, x1, y2, x2)] list of anchors in image coordinates.\n    proposals: [n, 4] the same anchors but refined to fit objects better.\n    """"""\n    masked_image = image.copy()\n\n    # Pick random anchors in case there are too many.\n    ids = np.arange(rois.shape[0], dtype=np.int32)\n    ids = np.random.choice(\n        ids, limit, replace=False) if ids.shape[0] > limit else ids\n\n    fig, ax = plt.subplots(1, figsize=(12, 12))\n    if rois.shape[0] > limit:\n        plt.title(""Showing {} random ROIs out of {}"".format(\n            len(ids), rois.shape[0]))\n    else:\n        plt.title(""{} ROIs"".format(len(ids)))\n\n    # Show area outside image boundaries.\n    ax.set_ylim(image.shape[0] + 20, -20)\n    ax.set_xlim(-50, image.shape[1] + 20)\n    ax.axis(\'off\')\n\n    for i, id in enumerate(ids):\n        color = np.random.rand(3)\n        class_id = class_ids[id]\n        # ROI\n        y1, x1, y2, x2 = rois[id]\n        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                              edgecolor=color if class_id else ""gray"",\n                              facecolor=\'none\', linestyle=""dashed"")\n        ax.add_patch(p)\n        # Refined ROI\n        if class_id:\n            ry1, rx1, ry2, rx2 = refined_rois[id]\n            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n                                  edgecolor=color, facecolor=\'none\')\n            ax.add_patch(p)\n            # Connect the top-left corners of the anchor and proposal for easy visualization\n            ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n\n            # Label\n            label = class_names[class_id]\n            ax.text(rx1, ry1 + 8, ""{}"".format(label),\n                    color=\'w\', size=11, backgroundcolor=""none"")\n\n            # Mask\n            m = utils.unmold_mask(mask[id], rois[id]\n                                  [:4].astype(np.int32), image.shape)\n            masked_image = apply_mask(masked_image, m, color)\n\n    ax.imshow(masked_image)\n\n    # Print stats\n    print(""Positive ROIs: "", class_ids[class_ids > 0].shape[0])\n    print(""Negative ROIs: "", class_ids[class_ids == 0].shape[0])\n    print(""Positive Ratio: {:.2f}"".format(\n        class_ids[class_ids > 0].shape[0] / class_ids.shape[0]))\n\n\n# TODO: Replace with matplotlib equivalent?\ndef draw_box(image, box, color):\n    """"""Draw 3-pixel width bounding boxes on the given image array.\n    color: list of 3 int values for RGB.\n    """"""\n    y1, x1, y2, x2 = box\n    image[y1:y1 + 2, x1:x2] = color\n    image[y2:y2 + 2, x1:x2] = color\n    image[y1:y2, x1:x1 + 2] = color\n    image[y1:y2, x2:x2 + 2] = color\n    return image\n\n\ndef display_top_masks(image, mask, class_ids, class_names, limit=4):\n    """"""Display the given image and the top few class masks.""""""\n    to_display = []\n    titles = []\n    to_display.append(image)\n    titles.append(""H x W={}x{}"".format(image.shape[0], image.shape[1]))\n    # Pick top prominent classes in this image\n    unique_class_ids = np.unique(class_ids)\n    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n                 for i in unique_class_ids]\n    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n                                    key=lambda r: r[1], reverse=True) if v[1] > 0]\n    # Generate images and titles\n    for i in range(limit):\n        class_id = top_ids[i] if i < len(top_ids) else -1\n        # Pull masks of instances belonging to the same class.\n        m = mask[:, :, np.where(class_ids == class_id)[0]]\n        m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n        to_display.append(m)\n        titles.append(class_names[class_id] if class_id != -1 else ""-"")\n    display_images(to_display, titles=titles, cols=limit + 1, cmap=""Blues_r"")\n\n\ndef plot_precision_recall(AP, precisions, recalls):\n    """"""Draw the precision-recall curve.\n\n    AP: Average precision at IoU >= 0.5\n    precisions: list of precision values\n    recalls: list of recall values\n    """"""\n    # Plot the Precision-Recall curve\n    _, ax = plt.subplots(1)\n    ax.set_title(""Precision-Recall Curve. AP@50 = {:.3f}"".format(AP))\n    ax.set_ylim(0, 1.1)\n    ax.set_xlim(0, 1.1)\n    _ = ax.plot(recalls, precisions)\n\n\ndef plot_overlaps(gt_class_ids, pred_class_ids, pred_scores,\n                  overlaps, class_names, threshold=0.5):\n    """"""Draw a grid showing how ground truth objects are classified.\n    gt_class_ids: [N] int. Ground truth class IDs\n    pred_class_id: [N] int. Predicted class IDs\n    pred_scores: [N] float. The probability scores of predicted classes\n    overlaps: [pred_boxes, gt_boxes] IoU overlaps of predictins and GT boxes.\n    class_names: list of all class names in the dataset\n    threshold: Float. The prediction probability required to predict a class\n    """"""\n    gt_class_ids = gt_class_ids[gt_class_ids != 0]\n    pred_class_ids = pred_class_ids[pred_class_ids != 0]\n\n    plt.figure(figsize=(12, 10))\n    plt.imshow(overlaps, interpolation=\'nearest\', cmap=plt.cm.Blues)\n    plt.yticks(np.arange(len(pred_class_ids)),\n               [""{} ({:.2f})"".format(class_names[int(id)], pred_scores[i])\n                for i, id in enumerate(pred_class_ids)])\n    plt.xticks(np.arange(len(gt_class_ids)),\n               [class_names[int(id)] for id in gt_class_ids], rotation=90)\n\n    thresh = overlaps.max() / 2.\n    for i, j in itertools.product(range(overlaps.shape[0]),\n                                  range(overlaps.shape[1])):\n        text = """"\n        if overlaps[i, j] > threshold:\n            text = ""match"" if gt_class_ids[j] == pred_class_ids[i] else ""wrong""\n        color = (""white"" if overlaps[i, j] > thresh\n                 else ""black"" if overlaps[i, j] > 0\n                 else ""grey"")\n        plt.text(j, i, ""{:.3f}\\n{}"".format(overlaps[i, j], text),\n                 horizontalalignment=""center"", verticalalignment=""center"",\n                 fontsize=9, color=color)\n\n    plt.tight_layout()\n    plt.xlabel(""Ground Truth"")\n    plt.ylabel(""Predictions"")\n\n\ndef draw_boxes(image, boxes=None, refined_boxes=None,\n               masks=None, captions=None, visibilities=None,\n               title="""", ax=None):\n    """"""Draw bounding boxes and segmentation masks with differnt\n    customizations.\n\n    boxes: [N, (y1, x1, y2, x2, class_id)] in image coordinates.\n    refined_boxes: Like boxes, but draw with solid lines to show\n        that they\'re the result of refining \'boxes\'.\n    masks: [N, height, width]\n    captions: List of N titles to display on each box\n    visibilities: (optional) List of values of 0, 1, or 2. Determine how\n        prominant each bounding box should be.\n    title: An optional title to show over the image\n    ax: (optional) Matplotlib axis to draw on.\n    """"""\n    # Number of boxes\n    assert boxes is not None or refined_boxes is not None\n    N = boxes.shape[0] if boxes is not None else refined_boxes.shape[0]\n\n    # Matplotlib Axis\n    if not ax:\n        _, ax = plt.subplots(1, figsize=(12, 12))\n\n    # Generate random colors\n    colors = random_colors(N)\n\n    # Show area outside image boundaries.\n    margin = image.shape[0] // 10\n    ax.set_ylim(image.shape[0] + margin, -margin)\n    ax.set_xlim(-margin, image.shape[1] + margin)\n    ax.axis(\'off\')\n\n    ax.set_title(title)\n\n    masked_image = image.astype(np.uint32).copy()\n    for i in range(N):\n        # Box visibility\n        visibility = visibilities[i] if visibilities is not None else 1\n        if visibility == 0:\n            color = ""gray""\n            style = ""dotted""\n            alpha = 0.5\n        elif visibility == 1:\n            color = colors[i]\n            style = ""dotted""\n            alpha = 1\n        elif visibility == 2:\n            color = colors[i]\n            style = ""solid""\n            alpha = 1\n\n        # Boxes\n        if boxes is not None:\n            if not np.any(boxes[i]):\n                # Skip this instance. Has no bbox. Likely lost in cropping.\n                continue\n            y1, x1, y2, x2 = boxes[i]\n            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                                  alpha=alpha, linestyle=style,\n                                  edgecolor=color, facecolor=\'none\')\n            ax.add_patch(p)\n\n        # Refined boxes\n        if refined_boxes is not None and visibility > 0:\n            ry1, rx1, ry2, rx2 = refined_boxes[i].astype(np.int32)\n            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n                                  edgecolor=color, facecolor=\'none\')\n            ax.add_patch(p)\n            # Connect the top-left corners of the anchor and proposal\n            if boxes is not None:\n                ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n\n        # Captions\n        if captions is not None:\n            caption = captions[i]\n            # If there are refined boxes, display captions on them\n            if refined_boxes is not None:\n                y1, x1, y2, x2 = ry1, rx1, ry2, rx2\n            x = random.randint(x1, (x1 + x2) // 2)\n            ax.text(x1, y1, caption, size=11, verticalalignment=\'top\',\n                    color=\'w\', backgroundcolor=""none"",\n                    bbox={\'facecolor\': color, \'alpha\': 0.5,\n                          \'pad\': 2, \'edgecolor\': \'none\'})\n\n        # Masks\n        if masks is not None:\n            mask = masks[:, :, i]\n            masked_image = apply_mask(masked_image, mask, color)\n            # Mask Polygon\n            # Pad to ensure proper polygons for masks that touch image edges.\n            padded_mask = np.zeros(\n                (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n            padded_mask[1:-1, 1:-1] = mask\n            contours = find_contours(padded_mask, 0.5)\n            for verts in contours:\n                # Subtract the padding and flip (y, x) to (x, y)\n                verts = np.fliplr(verts) - 1\n                p = Polygon(verts, facecolor=""none"", edgecolor=color)\n                ax.add_patch(p)\n    ax.imshow(masked_image.astype(np.uint8))\n\n\ndef plot_loss(loss, val_loss, save=True, log_dir=None):\n    loss = np.array(loss)\n    val_loss = np.array(val_loss)\n\n    plt.figure(""loss"")\n    plt.gcf().clear()\n    plt.plot(loss, label=\'train\')\n    plt.plot(val_loss, label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n\n    return\n\n    plt.figure(""rpn_class_loss"")\n    plt.gcf().clear()\n    plt.plot(loss[:, 1], label=\'train\')\n    plt.plot(val_loss[:, 1], label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""rpn_class_loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n\n    plt.figure(""rpn_bbox_loss"")\n    plt.gcf().clear()\n    plt.plot(loss[:, 2], label=\'train\')\n    plt.plot(val_loss[:, 2], label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""rpn_bbox_loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n\n    plt.figure(""mrcnn_class_loss"")\n    plt.gcf().clear()\n    plt.plot(loss[:, 3], label=\'train\')\n    plt.plot(val_loss[:, 3], label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""mrcnn_class_loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n\n    plt.figure(""mrcnn_bbox_loss"")\n    plt.gcf().clear()\n    plt.plot(loss[:, 4], label=\'train\')\n    plt.plot(val_loss[:, 4], label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""mrcnn_bbox_loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n\n    plt.figure(""mrcnn_mask_loss"")\n    plt.gcf().clear()\n    plt.plot(loss[:, 5], label=\'train\')\n    plt.plot(val_loss[:, 5], label=\'valid\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.legend()\n    if save:\n        save_path = os.path.join(log_dir, ""mrcnn_mask_loss.png"")\n        plt.savefig(save_path)\n    else:\n        plt.show(block=False)\n        plt.pause(0.1)\n'"
geometric/maskrcnn/vkitti.py,0,"b'import os\nimport sys\nimport numpy as np\nimport json\nfrom config import Config\nimport utils\nimport model as modellib\nfrom scipy.misc import imread\nfrom torchvision import transforms\nfrom PIL import Image\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \'../..\'))\nfrom datasets.vkitti_utils import get_tables, get_lists\n\n# Root directory of the project\nROOT_DIR = os.path.dirname(__file__)\n\n# Path to trained weights file\nVKITTI_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_vkitti.pth"")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n\n############################################################\n#  Configurations\n############################################################\n\nclass VKittiConfig(Config):\n    # Give the configuration a recognizable name\n    NAME = ""vkitti""\n\n    # We use one GPU with 8GB memory, which can fit one image.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 8\n\n    GPU_COUNT = 1\n\n    # Number of classes (including background)\n    NUM_CLASSES = 3\n\n\n############################################################\n#  Dataset\n############################################################\n\nclass VKittiDataset(utils.Dataset):\n    def load_vkitti(self, dataset_dir, subset):\n        """"""Load a subset of the COCO dataset.\n        dataset_dir: The root directory of the COCO dataset.\n        subset: What to load (train, test)\n        """"""\n\n        if subset == \'val\':\n            subset = \'test\'\n\n        self.image_dir = os.path.join(dataset_dir, ""vkitti_1.3.1_rgb"")\n        self.label_dir = os.path.join(dataset_dir, ""vkitti_1.3.1_scenegt"")\n        self.table_inst = get_tables(\'inst\', dataset_dir)\n\n        # preprocess split list: filter out images without objects\n        self.list_images = json.load(open(os.path.join(\n            os.path.dirname(__file__), ""assets/vkitti_maskrcnn_{}.json"".format(subset))))\n\n        self.img_jitter = transforms.ColorJitter(\n            brightness=0.2, contrast=0.2,\n            saturation=0.2, hue=0.1)\n\n        # Add classes\n        for i, vechicle_type in enumerate([""car"", ""van""]):\n            self.add_class(""vkitti"", i + 1, vechicle_type)\n\n        # Add images\n        for i in range(len(self.list_images)):\n            self.add_image(\n                ""vkitti"", image_id=i,\n                path=os.path.join(self.image_dir, self.list_images[i]),\n                width=1242, height=375)\n\n        print(""#{}_dataset = {}"".format(subset, len(self.list_images)))\n\n    def load_mask(self, image_id):\n        """"""Load instance masks for the given image.\n\n        Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        """"""\n        inst_map = imread(os.path.join(self.label_dir, self.list_images[image_id]))\n        worldId, sceneId = self.list_images[image_id].split(\'/\')[:2]\n        inst_map = np.apply_along_axis(\n            lambda a: self.table_inst[(worldId, sceneId, a[0], a[1], a[2])], 2, inst_map)\n        ids, counts = np.unique(inst_map, return_counts=True)\n        ids = ids[counts > 50]          # filter out objects with area < 50 pixels\n        ids = ids[ids > 5000]           # filter out non-vechicles\n        ids = ids[ids // 5000 != 11]      # filter out trucks\n        assert len(ids) > 0, self.list_images[image_id]\n        masks = np.stack([(inst_map == x) for x in ids], axis=-1)\n        class_ids = np.fromiter(map(lambda x: {2: 1, 12: 2}[x // 5000], ids), dtype=np.int)  # dont care about trucks no more\n        return masks, class_ids\n\n    def load_image(self, image_id):\n        """"""Load the specified image and return a [H,W,3] Numpy array.\n        """"""\n        # Load image\n        image = Image.open(self.image_info[image_id][\'path\'])\n        image = self.img_jitter(image)\n        image = np.array(image)\n        return image\n\n############################################################\n#  Training\n############################################################\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\'Train Mask R-CNN on Virtual KITTI.\')\n    parser.add_argument(""command"",\n                        metavar=""<command>"",\n                        help=""\'train\' or \'test\' on vkitti"")\n    parser.add_argument(\'--dataset\', default=\'./dataset\',\n                        metavar=""/path/to/vkitti/"",\n                        help=\'Directory of the vkitti dataset\')\n    parser.add_argument(\'--model\', required=False,\n                        metavar=""/path/to/weights.pth"",\n                        default="""",\n                        help=""Path to weights .pth file or \'vkitti\'"")\n    parser.add_argument(\'--pretrain_dir\', required=False,\n                        metavar=""/path/to/weights/dir"",\n                        default=""./pretrained"",\n                        help=""Path to weights dir"")\n    parser.add_argument(\'--logs\', required=False,\n                        default=DEFAULT_LOGS_DIR,\n                        metavar=""/path/to/logs/"",\n                        help=\'Logs and checkpoints directory (default=logs/)\')\n    parser.add_argument(\'--limit\', required=False,\n                        default=500,\n                        metavar=""<image count>"",\n                        help=\'Images to use for evaluation (default=500)\')\n    # parser.add_argument(\'--transfer\', action=\'store_true\',\n    #                     help=\'transfer coco model to new model\')\n\n    args = parser.parse_args()\n    print(""Command: "", args.command)\n    print(""Model: "", args.model)\n    print(""Dataset: "", args.dataset)\n    print(""Logs: "", args.logs)\n\n    # Vkitti: we only need training\n    assert args.command == \'train\'\n\n    # Configurations\n    if args.command == ""train"":\n        config = VKittiConfig()\n        # Path to pretrained imagenet model\n        config.IMAGENET_MODEL_PATH = os.path.join(args.pretrain_dir, ""resnet50_imagenet.pth"")\n        # Path to pretrained coco model\n        config.COCO_MODEL_PATH = os.path.join(args.pretrain_dir, ""mask_rcnn_coco.pth"")\n\n    # Create model\n    if args.command == ""train"":\n        model = modellib.MaskRCNN(config=config,\n                                  model_dir=args.logs)\n    if config.GPU_COUNT:\n        model = model.cuda()\n\n    # Select weights file to load\n    args.transfer = 0\n    if args.model:\n        if args.model.lower() == ""vkitti"":\n            model_path = VKITTI_MODEL_PATH\n        elif args.model.lower() == ""last"":\n            # Find last trained weights\n            model_path = model.find_last()[1]\n        elif args.model.lower() == ""imagenet"":\n            # Start from ImageNet trained weights\n            model_path = config.IMAGENET_MODEL_PATH\n            args.transfer = 1\n        elif args.model.lower() == ""coco"":\n            # Start from COCO trained weights\n            model_path = config.COCO_MODEL_PATH\n            args.transfer = 1\n        else:\n            model_path = args.model\n    else:\n        model_path = """"\n\n    # Load weights\n    print(""Loading weights "", model_path)\n    model.load_weights(model_path, transfer=args.transfer)\n\n    # Train or evaluate\n    if args.command == ""train"":\n        # Training dataset. Use the training set and 35K from the\n        # validation set, as as in the Mask RCNN paper.\n        dataset_train = VKittiDataset()\n        dataset_train.load_vkitti(args.dataset, ""train"")\n        dataset_train.prepare()\n\n        # Validation dataset\n        dataset_val = VKittiDataset()\n        dataset_val.load_vkitti(args.dataset, ""val"")\n        dataset_val.prepare()\n\n        # *** This training schedule is an example. Update to your needs ***\n\n        if args.transfer:\n            # Training - Stage 0\n            print(""Training for transferring num_classes"")\n            model.train_model(dataset_train, dataset_val,\n                              learning_rate=1e-5,\n                              epochs=10,\n                              layers=""transfer"")\n            # layers=r""(mask.conv5.*)|(classifier.linear_class.*)|(classifier.linear_bbox.*)"")\n\n        # Training - Stage 1\n        print(""Training network heads"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE,\n                          epochs=40,\n                          layers=\'heads\')\n\n        # Training - Stage 2\n        # Finetune layers from ResNet stage 4 and up\n        print(""Fine tune Resnet stage 4 and up"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE / 2,\n                          epochs=70,\n                          layers=\'4+\')\n\n        # Training - Stage 3\n        # Fine tune all layers\n        print(""Fine tune all layers"")\n        model.train_model(dataset_train, dataset_val,\n                          learning_rate=config.LEARNING_RATE / 5,\n                          epochs=100,\n                          layers=\'all\')\n'"
geometric/neural_renderer/__init__.py,0,"b""from __future__ import absolute_import\nfrom .cross import cross\nfrom .get_points_from_angles import get_points_from_angles\nfrom .lighting import lighting\nfrom .load_obj import load_obj\nfrom .look import look\nfrom .look_at import look_at\nfrom .mesh import Mesh\nfrom .optimizers import Adam\nfrom .perspective import perspective\nfrom .rasterize import (\n    rasterize_rgbad, rasterize, rasterize_silhouettes, rasterize_depth, use_unsafe_rasterizer, Rasterize)\nfrom .renderer import Renderer\nfrom .save_obj import save_obj\nfrom .vertices_to_faces import vertices_to_faces\n\n__version__ = '1.1.3'\n"""
geometric/neural_renderer/cross.py,0,"b""from __future__ import division\nfrom past.utils import old_div\nimport chainer\nimport cupy as cp\nimport numpy as np\n\n\nclass Cross(chainer.Function):\n    def check_type_forward(self, in_types):\n        chainer.utils.type_check.expect(\n            in_types[0].dtype.kind == 'f',\n            in_types[0].ndim == 2,\n            in_types[0].shape[1] == 3,\n            in_types[1].dtype.kind == 'f',\n            in_types[1].ndim == 2,\n            in_types[1].shape[1] == 3,\n            in_types[0].shape[0] == in_types[1].shape[0],\n        )\n\n    def forward_cpu(self, inputs):\n        a, b = inputs\n        c = np.cross(a, b)\n        return c,\n\n    def forward_gpu(self, inputs):\n        a, b = inputs\n        c = cp.zeros_like(a, 'float32')\n        chainer.cuda.elementwise(\n            'int32 j, raw T a, raw T b',\n            'raw T c',\n            '''\n                float* ap = &a[j * 3];\n                float* bp = &b[j * 3];\n                float* cp = &c[j * 3];\n                cp[0] = ap[1] * bp[2] - ap[2] * bp[1];\n                cp[1] = ap[2] * bp[0] - ap[0] * bp[2];\n                cp[2] = ap[0] * bp[1] - ap[1] * bp[0];\n            ''',\n            'function',\n        )(\n            cp.arange(old_div(a.size, 3)).astype('int32'), a, b, c,\n        )\n        return c,\n\n    def backward_cpu(self, inputs, gradients):\n        a, b = inputs\n        gc = gradients[0]\n        ga = np.cross(b, gc)\n        gb = np.cross(gc, a)\n        return ga, gb\n\n    def backward_gpu(self, inputs, gradients):\n        a, b = inputs\n        gc = gradients[0]\n        ga = self.forward_gpu((b, gc))[0]\n        gb = self.forward_gpu((gc, a))[0]\n        return ga, gb\n\n\ndef cross(a, b):\n    return Cross()(a, b)\n"""
geometric/neural_renderer/get_points_from_angles.py,0,"b'import math\n\nimport chainer\n\n\ndef get_points_from_angles(distance, elevation, azimuth, degrees=True):\n    if isinstance(distance, float) or isinstance(distance, int):\n        if degrees:\n            elevation = math.radians(elevation)\n            azimuth = math.radians(azimuth)\n        return (\n            distance * math.cos(elevation) * math.sin(azimuth),\n            distance * math.sin(elevation),\n            -distance * math.cos(elevation) * math.cos(azimuth))\n    else:\n        xp = chainer.cuda.get_array_module(distance)\n        if degrees:\n            elevation = xp.radians(elevation)\n            azimuth = xp.radians(azimuth)\n        return xp.stack([\n            distance * xp.cos(elevation) * xp.sin(azimuth),\n            distance * xp.sin(elevation),\n            -distance * xp.cos(elevation) * xp.cos(azimuth),\n        ]).transpose()\n'"
geometric/neural_renderer/lighting.py,0,"b""import chainer\nimport chainer.functions as cf\nimport chainer.functions.math.basic_math as cfmath\n\nimport neural_renderer\n\n\ndef lighting(\n        faces, textures, intensity_ambient=0.5, intensity_directional=0.5, color_ambient=(1, 1, 1),\n        color_directional=(1, 1, 1), direction=(0, 1, 0)):\n    xp = chainer.cuda.get_array_module(faces)\n    bs, nf = faces.shape[:2]\n\n    # arguments\n    if isinstance(color_ambient, tuple) or isinstance(color_ambient, list):\n        color_ambient = xp.array(color_ambient, 'float32')\n    if isinstance(color_directional, tuple) or isinstance(color_directional, list):\n        color_directional = xp.array(color_directional, 'float32')\n    if isinstance(direction, tuple) or isinstance(direction, list):\n        direction = xp.array(direction, 'float32')\n    if color_ambient.ndim == 1:\n        color_ambient = cf.broadcast_to(color_ambient[None, :], (bs, 3))\n    if color_directional.ndim == 1:\n        color_directional = cf.broadcast_to(color_directional[None, :], (bs, 3))\n    if direction.ndim == 1:\n        direction = cf.broadcast_to(direction[None, :], (bs, 3))\n\n    # create light\n    light = xp.zeros((bs, nf, 3), 'float32')\n\n    # ambient light\n    if intensity_ambient != 0:\n        light = light + intensity_ambient * cf.broadcast_to(color_ambient[:, None, :], light.shape)\n\n    # directional light\n    if intensity_directional != 0:\n        faces = faces.reshape((bs * nf, 3, 3))\n        v10 = faces[:, 0] - faces[:, 1]\n        v12 = faces[:, 2] - faces[:, 1]\n        normals = cf.normalize(neural_renderer.cross(v10, v12))\n        normals = normals.reshape((bs, nf, 3))\n\n        if direction.ndim == 2:\n            direction = cf.broadcast_to(direction[:, None, :], normals.shape)\n        cos = cf.relu(cf.sum(normals * direction, axis=2))\n        light = (\n            light + intensity_directional * cfmath.mul(*cf.broadcast(color_directional[:, None, :], cos[:, :, None])))\n\n    # apply\n    light = cf.broadcast_to(light[:, :, None, None, None, :], textures.shape)\n    textures = textures * light\n    return textures\n"""
geometric/neural_renderer/load_obj.py,0,"b'from __future__ import division\nfrom builtins import range\nfrom past.utils import old_div\nimport string\n\nimport chainer\nimport numpy as np\nimport skimage.io\n\n\ndef load_textures(filename_obj, filename_texture, texture_size):\n    """"""\n    WARNING: this function is not well tested.\n    """"""\n\n    # load vertices\n    vertices = []\n    for line in open(filename_obj).readlines():\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'vt\':\n            vertices.append([float(v) for v in line.split()[1:3]])\n    vertices = np.vstack(vertices).astype(\'float32\')\n\n    # load faces for textures\n    faces = []\n    for line in open(filename_obj).readlines():\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'f\':\n            vs = line.split()[1:]\n            nv = len(vs)\n            v0 = int(vs[0].split(\'/\')[1])\n            for i in range(nv - 2):\n                v1 = int(vs[i + 1].split(\'/\')[1])\n                v2 = int(vs[i + 2].split(\'/\')[1])\n                faces.append((v0, v1, v2))\n    faces = np.vstack(faces).astype(\'int32\') - 1\n    faces = vertices[faces]\n    faces = chainer.cuda.to_gpu(faces)\n    faces = faces % 1\n\n    textures = np.zeros((faces.shape[0], texture_size, texture_size, texture_size, 3), \'float32\')\n    textures = chainer.cuda.to_gpu(textures)\n    image = old_div(skimage.io.imread(filename_texture).astype(\'float32\'), 255.)\n    image = chainer.cuda.to_gpu(image)\n    image = image[::-1, ::]\n\n    loop = np.arange(old_div(textures.size, 3)).astype(\'int32\')\n    loop = chainer.cuda.to_gpu(loop)\n    chainer.cuda.elementwise(\n        \'int32 j, raw float32 image, raw float32 faces, raw float32 textures\',\n        \'\',\n        string.Template(\'\'\'\n            const int ts = ${texture_size};\n            const int fn = i / (ts * ts * ts);\n            float dim0 = ((i / (ts * ts)) % ts) / (ts - 1.) ;\n            float dim1 = ((i / ts) % ts) / (ts - 1.);\n            float dim2 = (i % ts) / (ts - 1.);\n            if (1 < dim0 + dim1 + dim2) {\n                float sum = dim0 + dim1 + dim2;\n                dim0 /= sum;\n                dim1 /= sum;\n                dim2 /= sum;\n            }\n            const float* face = &faces[fn * 3 * 2];\n            float* texture = &textures[i * 3];\n\n            const float pos_x = (\n                (face[2 * 0 + 0] * dim0 + face[2 * 1 + 0] * dim1 + face[2 * 2 + 0] * dim2) * (${image_width} - 1));\n            const float pos_y = (\n                (face[2 * 0 + 1] * dim0 + face[2 * 1 + 1] * dim1 + face[2 * 2 + 1] * dim2) * (${image_height} - 1));\n            const float weight_x1 = pos_x - (int)pos_x;\n            const float weight_x0 = 1 - weight_x1;\n            const float weight_y1 = pos_y - (int)pos_y;\n            const float weight_y0 = 1 - weight_y1;\n            for (int k = 0; k < 3; k++) {\n                float c = 0;\n                c += image[((int)pos_y * ${image_width} + (int)pos_x) * 3 + k] * (weight_x0 * weight_y0);\n                c += image[((int)(pos_y + 1) * ${image_width} + (int)pos_x) * 3 + k] * (weight_x0 * weight_y1);\n                c += image[((int)pos_y * ${image_width} + ((int)pos_x) + 1) * 3 + k] * (weight_x1 * weight_y0);\n                c += image[((int)(pos_y + 1)* ${image_width} + ((int)pos_x) + 1) * 3 + k] * (weight_x1 * weight_y1);\n                texture[k] = c;\n            }\n        \'\'\').substitute(\n            texture_size=texture_size,\n            image_height=image.shape[0],\n            image_width=image.shape[1],\n        ),\n        \'function\',\n    )(loop, image, faces, textures)\n    return textures\n\n\ndef load_obj(filename_obj, normalization=True, filename_texture=None, texture_size=4):\n    """"""\n    Load Wavefront .obj file.\n    This function only supports vertices (v x x x) and faces (f x x x).\n    """"""\n\n    # load vertices\n    vertices = []\n    for line in open(filename_obj).readlines():\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'v\':\n            vertices.append([float(v) for v in line.split()[1:4]])\n    vertices = np.vstack(vertices).astype(\'float32\')\n\n    # load faces\n    faces = []\n    for line in open(filename_obj).readlines():\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'f\':\n            vs = line.split()[1:]\n            nv = len(vs)\n            v0 = int(vs[0].split(\'/\')[0])\n            for i in range(nv - 2):\n                v1 = int(vs[i + 1].split(\'/\')[0])\n                v2 = int(vs[i + 2].split(\'/\')[0])\n                faces.append((v0, v1, v2))\n    faces = np.vstack(faces).astype(\'int32\') - 1\n\n    # load textures\n    if filename_texture is not None:\n        textures = load_textures(filename_obj, filename_texture, texture_size)\n    else:\n        textures = None\n\n    # normalize into a unit cube centered zero\n    if normalization:\n        vertices -= vertices.min(0)[None, :]\n        vertices /= np.abs(vertices).max()\n        vertices *= 2\n        vertices -= old_div(vertices.max(0)[None, :], 2)\n\n    if textures is None:\n        return vertices, faces\n    else:\n        return vertices, faces, textures\n'"
geometric/neural_renderer/look.py,0,"b'import chainer\nimport chainer.functions as cf\n\nimport neural_renderer\n\n\ndef look(vertices, eye, direction=None, up=None):\n    """"""\n    ""Look at"" transformation of vertices.\n    """"""\n    assert (vertices.ndim == 3)\n\n    xp = chainer.cuda.get_array_module(vertices)\n    if direction is None:\n        direction = xp.array([0, 0, 1], \'float32\')\n    if up is None:\n        up = xp.array([0, 1, 0], \'float32\')\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = xp.array(eye, \'float32\')\n    if eye.ndim == 1:\n        eye = eye[None, :]\n    if direction.ndim == 1:\n        direction = direction[None, :]\n    if up.ndim == 1:\n        up = up[None, :]\n\n    # create new axes\n    z_axis = cf.normalize(direction)\n    x_axis = cf.normalize(neural_renderer.cross(up, z_axis))\n    y_axis = cf.normalize(neural_renderer.cross(z_axis, x_axis))\n\n    # create rotation matrix: [bs, 3, 3]\n    r = cf.concat((x_axis[:, None, :], y_axis[:, None, :], z_axis[:, None, :]), axis=1)\n    if r.shape[0] != vertices.shape[0]:\n        r = cf.broadcast_to(r, vertices.shape)\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = cf.broadcast_to(eye[:, None, :], vertices.shape)\n    vertices = vertices - eye\n    vertices = cf.matmul(vertices, r, transb=True)\n\n    return vertices\n'"
geometric/neural_renderer/look_at.py,0,"b'import chainer\nimport chainer.functions as cf\n\nimport neural_renderer\n\n\ndef look_at(vertices, eye, at=None, up=None):\n    """"""\n    ""Look at"" transformation of vertices.\n    """"""\n    assert (vertices.ndim == 3)\n\n    xp = chainer.cuda.get_array_module(vertices)\n    batch_size = vertices.shape[0]\n    if at is None:\n        at = xp.array([0, 0, 0], \'float32\')\n    if up is None:\n        up = xp.array([0, 1, 0], \'float32\')\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = xp.array(eye, \'float32\')\n    if eye.ndim == 1:\n        eye = cf.tile(eye[None, :], (batch_size, 1))\n    if at.ndim == 1:\n        at = cf.tile(at[None, :], (batch_size, 1))\n    if up.ndim == 1:\n        up = cf.tile(up[None, :], (batch_size, 1))\n\n    # create new axes\n    z_axis = cf.normalize(at - eye)\n    x_axis = cf.normalize(neural_renderer.cross(up, z_axis))\n    y_axis = cf.normalize(neural_renderer.cross(z_axis, x_axis))\n\n    # create rotation matrix: [bs, 3, 3]\n    r = cf.concat((x_axis[:, None, :], y_axis[:, None, :], z_axis[:, None, :]), axis=1)\n    if r.shape[0] != vertices.shape[0]:\n        r = cf.broadcast_to(r, (vertices.shape[0], 3, 3))\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = cf.broadcast_to(eye[:, None, :], vertices.shape)\n    vertices = vertices - eye\n    vertices = cf.matmul(vertices, r, transb=True)\n\n    return vertices\n'"
geometric/neural_renderer/mesh.py,0,"b'import chainer\nimport chainer.functions as cf\n\nimport neural_renderer\n\n\nclass Mesh(chainer.Link):\n    def __init__(self, filename_obj, texture_size=4, normalization=True):\n        super(Mesh, self).__init__()\n\n        with self.init_scope():\n            # load .obj\n            vertices, faces = neural_renderer.load_obj(filename_obj, normalization)\n            self.vertices = chainer.Parameter(vertices)\n            self.faces = faces\n            self.num_vertices = self.vertices.shape[0]\n            self.num_faces = self.faces.shape[0]\n\n            # create textures\n            init = chainer.initializers.Normal()\n            shape = (self.num_faces, texture_size, texture_size, texture_size, 3)\n            self.textures = chainer.Parameter(init, shape)\n            self.texture_size = texture_size\n\n    def to_gpu(self, device=None):\n        super(Mesh, self).to_gpu(device)\n        self.faces = chainer.cuda.to_gpu(self.faces, device)\n\n    def get_batch(self, batch_size):\n        # broadcast for minibatch\n        vertices = cf.broadcast_to(self.vertices, [batch_size] + list(self.vertices.shape))\n        faces = cf.broadcast_to(self.faces, [batch_size] + list(self.faces.shape)).data\n        textures = cf.sigmoid(cf.broadcast_to(self.textures, [batch_size] + list(self.textures.shape)))\n        return vertices, faces, textures\n\n    def set_lr(self, lr_vertices, lr_textures):\n        self.vertices.lr = lr_vertices\n        self.textures.lr = lr_textures\n'"
geometric/neural_renderer/optimizers.py,0,"b'""""""\nCustom optimizers.\n- do not update a weight if the gradient is zero.\n- use parameter-wise learning rate specified by param.lr.\n""""""\nimport chainer\n\n\nclass AdamRule(chainer.optimizers.adam.AdamRule):\n    def update_core_cpu(self, param):\n        raise NotImplementedError\n\n    def update_core_gpu(self, param):\n        grad = param.grad\n        if grad is None:\n            return\n        lr = self.lr * param.lr if hasattr(param, \'lr\') else self.lr\n        if lr != 0:\n            chainer.cuda.elementwise(\n                \'T grad, T lr, T one_minus_beta1, T one_minus_beta2, T eps\',\n                \'T param, T m, T v\',\n                \'\'\'\n                    if (grad != 0.0) {\n                        m += one_minus_beta1 * (grad - m);\n                        v += one_minus_beta2 * (grad * grad - v);\n                        if (v < 0) v = 0;\n                        param -= lr * m / (sqrt(v) + eps);\n                    }\n                \'\'\',\n                \'adam\',\n            )(\n                grad, lr, 1 - self.hyperparam.beta1, 1 - self.hyperparam.beta2, self.hyperparam.eps, param.data,\n                self.state[\'m\'], self.state[\'v\'],\n            )\n\n\nclass Adam(chainer.optimizers.adam.Adam):\n    def create_update_rule(self):\n        return AdamRule(self.hyperparam)\n'"
geometric/neural_renderer/perspective.py,0,"b""import chainer\nimport chainer.functions as cf\n\n\ndef perspective(vertices, angle=30.):\n    assert (vertices.ndim == 3)\n    xp = chainer.cuda.get_array_module(vertices)\n    if isinstance(angle, float) or isinstance(angle, int):\n        angle = chainer.Variable(xp.array(angle, 'float32'))\n    angle = angle / 180. * 3.1416\n    angle = cf.broadcast_to(angle[None], (vertices.shape[0],))\n\n    width = cf.tan(angle)\n    width = cf.broadcast_to(width[:, None], vertices.shape[:2])\n    z = vertices[:, :, 2]\n    x = vertices[:, :, 0] / z / width\n    y = vertices[:, :, 1] / z / width\n    vertices = cf.concat((x[:, :, None], y[:, :, None], z[:, :, None]), axis=2)\n    return vertices\n"""
geometric/neural_renderer/rasterize.py,0,"b'import os\nimport string\n\nimport chainer\nimport chainer.functions as cf\n\nDEFAULT_IMAGE_SIZE = 256\nDEFAULT_ANTI_ALIASING = True\nDEFAULT_NEAR = 0.1\nDEFAULT_FAR = 100\nDEFAULT_EPS = 1e-4\nDEFAULT_BACKGROUND_COLOR = (0, 0, 0)\nUSE_UNSAFE_IMPLEMENTATION = False\n\nif \'NEURAL_RENDERER_UNSAFE\' in os.environ and int(os.environ[\'NEURAL_RENDERER_UNSAFE\']):\n    USE_UNSAFE_IMPLEMENTATION = True\n\n\nclass Rasterize(chainer.Function):\n    def __init__(\n            self, image_size, near, far, eps, background_color, return_rgb=False, return_alpha=False,\n            return_depth=False):\n        super(Rasterize, self).__init__()\n\n        if not any((return_rgb, return_alpha, return_depth)):\n            # nothing to draw\n            raise Exception\n\n        # arguments\n        self.image_size = image_size\n        self.near = near\n        self.far = far\n        self.eps = eps\n        self.background_color = background_color\n        self.return_rgb = return_rgb\n        self.return_alpha = return_alpha\n        self.return_depth = return_depth\n\n        # input buffers\n        self.faces = None\n        self.textures = None\n        self.grad_rgb_map = None\n        self.grad_alpha_map = None\n        self.grad_depth_map = None\n\n        # output buffers\n        self.rgb_map = None\n        self.alpha_map = None\n        self.depth_map = None\n        self.grad_faces = None\n        self.grad_textures = None\n\n        # intermediate buffers\n        self.face_index_map = None\n        self.weight_map = None\n        self.face_inv_map = None\n        self.sampling_index_map = None\n        self.sampling_weight_map = None\n\n        # input information\n        self.xp = None\n        self.batch_size = None\n        self.num_faces = None\n        self.texture_size = None\n\n    def check_type_forward(self, in_types):\n        assert in_types.size() in [1, 2]\n\n        # faces: [batch size, num of faces, v012, XYZ]\n        faces_type = in_types[0]\n        chainer.utils.type_check.expect(\n            faces_type.dtype.kind == \'f\',\n            faces_type.ndim == 4,\n            faces_type.shape[2] == 3,\n            faces_type.shape[3] == 3,\n        )\n\n        if self.return_rgb:\n            # textures: [batch size, num of faces, texture size, texture size, texture size, RGB]\n            textures_type = in_types[1]\n            chainer.utils.type_check.expect(\n                textures_type.dtype.kind == \'f\',\n                textures_type.ndim == 6,\n                2 <= textures_type.shape[2],\n                textures_type.shape[2] == textures_type.shape[3],\n                textures_type.shape[3] == textures_type.shape[4],\n                textures_type.shape[5] == 3,\n                faces_type.shape[0] == textures_type.shape[0],\n                faces_type.shape[1] == textures_type.shape[1],\n            )\n\n    ####################################################################################################################\n    # forward\n    def forward_face_index_map_gpu(self):\n        # inputs:\n        #   faces\n        # outputs:\n        #   if rgb: face_index_map, weight_map, depth_map\n        #   if alpha: face_index_map, weight_map, depth_map\n        #   if depth: face_index_map, weight_map, depth_map, face_inv_map\n\n        if USE_UNSAFE_IMPLEMENTATION:\n            # very fast, but unable to run on some environments\n\n            # for each face\n            loop = self.xp.arange(self.batch_size * self.num_faces).astype(\'int32\')\n            lock = self.xp.zeros(self.face_index_map.shape, \'int32\')\n            chainer.cuda.elementwise(\n                \'int32 _, raw float32 faces, raw int32 face_index_map, raw float32 weight_map, \' +\n                \'raw float32 depth_map, raw float32 face_inv_map, raw int32 lock\',\n                \'\',\n                string.Template(\'\'\'\n                    /* batch number, face, number, image size, face[v012][RGB] */\n                    const int bn = i / ${num_faces};\n                    const int fn = i % ${num_faces};\n                    const int is = ${image_size};\n                    const float* face = &faces[i * 9];\n\n                    /* return if backside */\n                    if ((face[7] - face[1]) * (face[3] - face[0]) < (face[4] - face[1]) * (face[6] - face[0])) return;\n\n                    /* pi[0], pi[1], pi[2] = leftmost, middle, rightmost points */\n                    int pi[3];\n                    if (face[0] < face[3]) {\n                        if (face[6] < face[0]) pi[0] = 2; else pi[0] = 0;\n                        if (face[3] < face[6]) pi[2] = 2; else pi[2] = 1;\n                    } else {\n                        if (face[6] < face[3]) pi[0] = 2; else pi[0] = 1;\n                        if (face[0] < face[6]) pi[2] = 2; else pi[2] = 0;\n                    }\n                    for (int k = 0; k < 3; k++) if (pi[0] != k && pi[2] != k) pi[1] = k;\n\n                    /* p[num][xyz]: x, y is normalized from [-1, 1] to [0, is - 1]. */\n                    float p[3][3];\n                    for (int num = 0; num < 3; num++) {\n                        for (int dim = 0; dim < 3; dim++) {\n                            if (dim != 2) {\n                                p[num][dim] = 0.5 * (face[3 * pi[num] + dim] * is + is - 1);\n                            } else {\n                                p[num][dim] = face[3 * pi[num] + dim];\n                            }\n                        }\n                    }\n                    if (p[0][0] == p[2][0]) return; // line, not triangle\n\n                    /* compute face_inv */\n                    float face_inv[9] = {\n                        p[1][1] - p[2][1], p[2][0] - p[1][0], p[1][0] * p[2][1] - p[2][0] * p[1][1],\n                        p[2][1] - p[0][1], p[0][0] - p[2][0], p[2][0] * p[0][1] - p[0][0] * p[2][1],\n                        p[0][1] - p[1][1], p[1][0] - p[0][0], p[0][0] * p[1][1] - p[1][0] * p[0][1]};\n                    float face_inv_denominator = (\n                        p[2][0] * (p[0][1] - p[1][1]) +\n                        p[0][0] * (p[1][1] - p[2][1]) +\n                        p[1][0] * (p[2][1] - p[0][1]));\n                    for (int k = 0; k < 9; k++) face_inv[k] /= face_inv_denominator;\n\n                    /* from left to right */\n                    const int xi_min = max(ceil(p[0][0]), 0.);\n                    const int xi_max = min(p[2][0], is - 1.);\n                    for (int xi = xi_min; xi <= xi_max; xi++) {\n                        /* compute yi_min and yi_max */\n                        float yi1, yi2;\n                        if (xi <= p[1][0]) {\n                            if (p[1][0] - p[0][0] != 0) {\n                                yi1 = (p[1][1] - p[0][1]) / (p[1][0] - p[0][0]) * (xi - p[0][0]) + p[0][1];\n                            } else {\n                                yi1 = p[1][1];\n                            }\n                        } else {\n                            if (p[2][0] - p[1][0] != 0) {\n                                yi1 = (p[2][1] - p[1][1]) / (p[2][0] - p[1][0]) * (xi - p[1][0]) + p[1][1];\n                            } else {\n                                yi1 = p[1][1];\n                            }\n                        }\n                        yi2 = (p[2][1] - p[0][1]) / (p[2][0] - p[0][0]) * (xi - p[0][0]) + p[0][1];\n\n                        /* from up to bottom */\n                        int yi_min = max(0., ceil(min(yi1, yi2)));\n                        int yi_max = min(max(yi1, yi2), is - 1.);\n                        for (int yi = yi_min; yi <= yi_max; yi++) {\n                            /* index in output buffers */\n                            int index = bn * is * is + yi * is + xi;\n\n                            /* compute w = face_inv * p */\n                            float w[3];\n                            for (int k = 0; k < 3; k++)\n                                w[k] = face_inv[3 * k + 0] * xi + face_inv[3 * k + 1] * yi + face_inv[3 * k + 2];\n\n                            /* sum(w) -> 1, 0 < w < 1 */\n                            float w_sum = 0;\n                            for (int k = 0; k < 3; k++) {\n                                w[k] = min(max(w[k], 0.), 1.);\n                                w_sum += w[k];\n                            }\n                            for (int k = 0; k < 3; k++) w[k] /= w_sum;\n\n                            /* compute 1 / zp = sum(w / z) */\n                            const float zp = 1. / (w[0] / p[0][2] + w[1] / p[1][2] + w[2] / p[2][2]);\n                            if (zp <= ${near} || ${far} <= zp) continue;\n\n                            /* lock and update */\n                            bool locked = false;\n                            do {\n                                if (locked = atomicCAS(&lock[index], 0, 1) == 0) {\n                                    if (zp < atomicAdd(&depth_map[index], 0)) {\n                                        float record = 0;\n                                        atomicExch(&depth_map[index], zp);\n                                        atomicExch(&face_index_map[index], fn);\n                                        for (int k = 0; k < 3; k++) atomicExch(&weight_map[3 * index + pi[k]], w[k]);\n                                        if (${return_depth}) {\n                                            for (int k = 0; k < 3; k++) for (int l = 0; l < 3; l++)\n                                                atomicExch(\n                                                    &face_inv_map[9 * index + 3 * pi[l] + k], face_inv[3 * l + k]);\n                                        }\n                                        record += atomicAdd(&depth_map[index], 0.);\n                                        record += atomicAdd(&face_index_map[index], 0.);\n                                        if (0 < record) atomicExch(&lock[index], 0);\n                                    } else {\n                                        atomicExch(&lock[index], 0);\n                                    }\n                                }\n                            } while (!locked);\n                        }\n                    }\n                \'\'\').substitute(\n                    num_faces=self.num_faces,\n                    image_size=self.image_size,\n                    near=self.near,\n                    far=self.far,\n                    return_rgb=int(self.return_rgb),\n                    return_alpha=int(self.return_alpha),\n                    return_depth=int(self.return_depth),\n                ),\n                \'function\',\n            )(loop, self.faces, self.face_index_map, self.weight_map, self.depth_map, self.face_inv_map, lock)\n\n        else:\n            # for each face\n            faces_inv = self.xp.zeros_like(self.faces)\n            loop = self.xp.arange(self.batch_size * self.num_faces).astype(\'int32\')\n            chainer.cuda.elementwise(\n                \'int32 _, raw float32 faces, raw float32 faces_inv\',\n                \'\',\n                string.Template(\'\'\'\n                    /* face[v012][RGB] */\n                    const int is = ${image_size};\n                    const float* face = &faces[i * 9];\n                    float* face_inv_g = &faces_inv[i * 9];\n\n                    /* return if backside */\n                    if ((face[7] - face[1]) * (face[3] - face[0]) < (face[4] - face[1]) * (face[6] - face[0]))\n                        continue;\n\n                    /* p[num][xy]: x, y is normalized from [-1, 1] to [0, is - 1]. */\n                    float p[3][2];\n                    for (int num = 0; num < 3; num++) for (int dim = 0; dim < 2; dim++)\n                        p[num][dim] = 0.5 * (face[3 * num + dim] * is + is - 1);\n\n                    /* compute face_inv */\n                    float face_inv[9] = {\n                        p[1][1] - p[2][1], p[2][0] - p[1][0], p[1][0] * p[2][1] - p[2][0] * p[1][1],\n                        p[2][1] - p[0][1], p[0][0] - p[2][0], p[2][0] * p[0][1] - p[0][0] * p[2][1],\n                        p[0][1] - p[1][1], p[1][0] - p[0][0], p[0][0] * p[1][1] - p[1][0] * p[0][1]};\n                    float face_inv_denominator = (\n                        p[2][0] * (p[0][1] - p[1][1]) +\n                        p[0][0] * (p[1][1] - p[2][1]) +\n                        p[1][0] * (p[2][1] - p[0][1]));\n                    for (int k = 0; k < 9; k++) face_inv[k] /= face_inv_denominator;\n\n                    /* set to global memory */\n                    for (int k = 0; k < 9; k++) face_inv_g[k] = face_inv[k];\n                \'\'\').substitute(\n                    image_size=self.image_size,\n                ),\n                \'function\',\n            )(loop, self.faces, faces_inv)\n\n            # for each pixel\n            loop = self.xp.arange(self.batch_size * self.image_size * self.image_size).astype(\'int32\')\n            chainer.cuda.elementwise(\n                \'int32 _, raw float32 faces, raw float32 faces_inv, raw int32 face_index_map, \' +\n                \'raw float32 weight_map, raw float32 depth_map, raw float32 face_inv_map\',\n                \'\',\n                string.Template(\'\'\'\n                    const int is = ${image_size};\n                    const int nf = ${num_faces};\n                    const int bn = i / (is * is);\n                    const int pn = i % (is * is);\n                    const int yi = pn / is;\n                    const int xi = pn % is;\n                    const float yp = (2. * yi + 1 - is) / is;\n                    const float xp = (2. * xi + 1 - is) / is;\n\n                    float* face = &faces[bn * nf * 9] - 9;\n                    float* face_inv = &faces_inv[bn * nf * 9] - 9;\n                    float depth_min = ${far};\n                    int face_index_min = -1;\n                    float weight_min[3];\n                    float face_inv_min[9];\n                    for (int fn = 0; fn < nf; fn++) {\n                        /* go to next face */\n                        face += 9;\n                        face_inv += 9;\n\n                        /* return if backside */\n                        if ((face[7] - face[1]) * (face[3] - face[0]) < (face[4] - face[1]) * (face[6] - face[0]))\n                            continue;\n\n                        /* check [py, px] is inside the face */\n                        if (((yp - face[1]) * (face[3] - face[0]) < (xp - face[0]) * (face[4] - face[1])) ||\n                            ((yp - face[4]) * (face[6] - face[3]) < (xp - face[3]) * (face[7] - face[4])) ||\n                            ((yp - face[7]) * (face[0] - face[6]) < (xp - face[6]) * (face[1] - face[7]))) continue;\n\n                        /* compute w = face_inv * p */\n                        float w[3];\n                        for (int k = 0; k < 3; k++)\n                        w[0] = face_inv[3 * 0 + 0] * xi + face_inv[3 * 0 + 1] * yi + face_inv[3 * 0 + 2];\n                        w[1] = face_inv[3 * 1 + 0] * xi + face_inv[3 * 1 + 1] * yi + face_inv[3 * 1 + 2];\n                        w[2] = face_inv[3 * 2 + 0] * xi + face_inv[3 * 2 + 1] * yi + face_inv[3 * 2 + 2];\n\n                        /* sum(w) -> 1, 0 < w < 1 */\n                        float w_sum = 0;\n                        for (int k = 0; k < 3; k++) {\n                            w[k] = min(max(w[k], 0.), 1.);\n                            w_sum += w[k];\n                        }\n                        for (int k = 0; k < 3; k++) w[k] /= w_sum;\n\n                        /* compute 1 / zp = sum(w / z) */\n                        const float zp = 1. / (w[0] / face[2] + w[1] / face[5] + w[2] / face[8]);\n                        if (zp <= ${near} || ${far} <= zp) continue;\n\n                        /* check z-buffer */\n                        if (zp < depth_min) {\n                            depth_min = zp;\n                            face_index_min = fn;\n                            for (int k = 0; k < 3; k++) weight_min[k] = w[k];\n                            if (${return_depth}) for (int k = 0; k < 9; k++) face_inv_min[k] = face_inv[k];\n                        }\n                    }\n\n                    /* set to global memory */\n                    if (0 <= face_index_min) {\n                        depth_map[i] = depth_min;\n                        face_index_map[i] = face_index_min;\n                        for (int k = 0; k < 3; k++) weight_map[3 * i + k] = weight_min[k];\n                        if (${return_depth}) for (int k = 0; k < 9; k++) face_inv_map[9 * i + k] = face_inv_min[k];\n                    }\n                \'\'\').substitute(\n                    num_faces=self.num_faces,\n                    image_size=self.image_size,\n                    near=self.near,\n                    far=self.far,\n                    return_rgb=int(self.return_rgb),\n                    return_alpha=int(self.return_alpha),\n                    return_depth=int(self.return_depth),\n                ),\n                \'function\',\n            )(loop, self.faces, faces_inv, self.face_index_map, self.weight_map, self.depth_map, self.face_inv_map)\n\n    def forward_texture_sampling(self):\n        # inputs:\n        #   faces, textures, face_index_map, weight_map, depth_map\n        # outputs:\n        #   if rgb: rgb_map, sampling_index_map, sampling_weight_map\n\n        if not self.return_rgb:\n            return\n\n        # for each pixel\n        loop = self.xp.arange(self.batch_size * self.image_size * self.image_size).astype(\'int32\')\n        chainer.cuda.elementwise(\n            \'int32 _, raw float32 faces, raw float32 textures, raw int32 face_index_map, raw float32 weight_map, \' +\n            \'raw float32 depth_map, raw float32 rgb_map, raw int32 sampling_index_map, raw float32 sampling_weight_map\',\n            \'\',\n            string.Template(\'\'\'\n                const int face_index = face_index_map[i];\n\n                if (0 <= face_index) {\n                    /*\n                        from global variables:\n                        batch number, num of faces, image_size, face[v012][RGB], pixel[RGB], weight[v012],\n                        texture[ts][ts][ts][RGB], sampling indices[8], sampling_weights[8];\n                    */\n\n                    const int bn = i / (${image_size} * ${image_size});\n                    const int nf = ${num_faces};\n                    const int ts = ${texture_size};\n                    const float* face = &faces[face_index * 9];\n                    const float* texture = &textures[(bn * nf + face_index) * ts * ts * ts * 3];\n                    float* pixel = &rgb_map[i * 3];\n                    const float* weight = &weight_map[i * 3];\n                    const float depth = depth_map[i];\n                    int* sampling_indices = &sampling_index_map[i * 8];\n                    float* sampling_weights = &sampling_weight_map[i * 8];\n\n                    /* get texture index (float) */\n                    float texture_index_float[3];\n                    for (int k = 0; k < 3; k++)\n                        texture_index_float[k] = weight[k] * (ts - 1 - ${eps}) * (depth / (face[3 * k + 2]));\n\n                    /* blend */\n                    float new_pixel[3] = {0, 0, 0};\n                    for (int pn = 0; pn < 8; pn++) {\n                        float w = 1;                         // weight\n                        int texture_index_int[3];            // index in source (int)\n                        for (int k = 0; k < 3; k++) {\n                            if ((pn >> k) % 2 == 0) {\n                                w *= 1 - (texture_index_float[k] - (int)texture_index_float[k]);\n                                texture_index_int[k] = (int)texture_index_float[k];\n                            } else {\n                                w *= texture_index_float[k] - (int)texture_index_float[k];\n                                texture_index_int[k] = (int)texture_index_float[k] + 1;\n                            }\n                        }\n\n                        int isc = texture_index_int[0] * ts * ts + texture_index_int[1] * ts + texture_index_int[2];\n                        for (int k = 0; k < 3; k++) new_pixel[k] += w * texture[isc * 3 + k];\n                        sampling_indices[pn] = isc;\n                        sampling_weights[pn] = w;\n                    }\n                    for (int k = 0; k < 3; k++) pixel[k] = new_pixel[k];\n                }\n            \'\'\').substitute(\n                image_size=self.image_size,\n                num_faces=self.num_faces,\n                texture_size=self.texture_size,\n                eps=self.eps,\n            ),\n            \'function\',\n        )(\n            loop, self.faces, self.textures, self.face_index_map, self.weight_map, self.depth_map, self.rgb_map,\n            self.sampling_index_map, self.sampling_weight_map,\n        )\n\n    def forward_alpha_map_gpu(self):\n        # inputs:\n        #   face_index_map,\n        # outputs:\n        #   if alpha: alpha_map\n\n        if not self.return_alpha:\n            return\n\n        self.alpha_map[0 <= self.face_index_map] = 1\n\n    def forward_background_gpu(self):\n        # inputs:\n        #   face_index_map, rgb_map, background_color\n        # outputs:\n        #   if rgb: rgb_map\n\n        if not self.return_rgb:\n            return\n\n        background_color = self.xp.array(self.background_color, \'float32\')\n        mask = (0 <= self.face_index_map).astype(\'float32\')[:, :, :, None]\n        if background_color.ndim == 1:\n            self.rgb_map = self.rgb_map * mask + (1 - mask) * background_color[None, None, None, :]\n        elif background_color.ndim == 2:\n            self.rgb_map = self.rgb_map * mask + (1 - mask) * background_color[:, None, None, :]\n\n    def forward_gpu(self, inputs):\n        # get input information\n        self.xp = chainer.cuda.get_array_module(inputs[0])\n        self.faces = self.xp.ascontiguousarray(inputs[0].copy())\n        self.batch_size, self.num_faces = self.faces.shape[:2]\n        if self.return_rgb:\n            textures = self.xp.ascontiguousarray(inputs[1])\n            self.textures = textures\n            self.texture_size = textures.shape[2]\n\n        # initialize outputs\n        self.face_index_map = -1 * self.xp.ones((self.batch_size, self.image_size, self.image_size), \'int32\')\n        self.weight_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size, 3), \'float32\')\n        self.depth_map = self.xp.zeros(self.face_index_map.shape, \'float32\') + self.far\n        if self.return_rgb:\n            self.rgb_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size, 3), \'float32\')\n            self.sampling_index_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size, 8), \'int32\')\n            self.sampling_weight_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size, 8), \'float32\')\n        else:\n            self.rgb_map = self.xp.zeros(1, \'float32\')\n            self.sampling_index_map = self.xp.zeros(1, \'int32\')\n            self.sampling_weight_map = self.xp.zeros(1, \'float32\')\n        if self.return_alpha:\n            self.alpha_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size), \'float32\')\n        else:\n            self.alpha_map = self.xp.zeros(1, \'float32\')\n        if self.return_depth:\n            self.face_inv_map = self.xp.zeros((self.batch_size, self.image_size, self.image_size, 3, 3), \'float32\')\n        else:\n            self.face_inv_map = self.xp.zeros(1, \'float32\')\n\n        # forward pass\n        self.forward_face_index_map_gpu()\n        self.forward_texture_sampling()\n        self.forward_background_gpu()\n        self.forward_alpha_map_gpu()\n\n        # return\n        rgb_r, alpha_r, depth_r = None, None, None\n        if self.return_rgb:\n            rgb_r = self.rgb_map\n        if self.return_alpha:\n            alpha_r = self.alpha_map.copy()\n        if self.return_depth:\n            depth_r = self.depth_map.copy()\n\n        return rgb_r, alpha_r, depth_r\n\n    ####################################################################################################################\n    # backward\n    def backward_pixel_map_gpu(self):\n        # inputs:\n        #   face_index_map, rgb_map, alpha_map, grad_rgb_map, grad_alpha_map\n        # outputs:\n        #   if rgb or alpha: grad_faces\n\n        if (not self.return_rgb) and (not self.return_alpha):\n            return\n\n        # for each face\n        loop = self.xp.arange(self.batch_size * self.num_faces).astype(\'int32\')\n        chainer.cuda.elementwise(\n            \'int32 _, raw float32 faces, raw int32 face_index_map, raw float32 rgb_map, raw float32 alpha_map, \' +\n            \'raw float32 grad_rgb_map, raw float32 grad_alpha_map, raw float32 grad_faces\',\n            \'\',\n            string.Template(\'\'\'\n                const int bn = i / ${num_faces};\n                const int fn = i % ${num_faces};\n                const int is = ${image_size};\n                const float* face = &faces[i * 9];\n                float grad_face[9] = {};\n\n                /* check backside */\n                if ((face[7] - face[1]) * (face[3] - face[0]) < (face[4] - face[1]) * (face[6] - face[0])) return;\n\n                /* for each edge */\n                for (int edge_num = 0; edge_num < 3; edge_num++) {\n                    /* set points of target edge */\n                    int pi[3];\n                    float pp[3][2];\n                    for (int num = 0; num < 3; num++) pi[num] = (edge_num + num) % 3;\n                    for (int num = 0; num < 3; num++) for (int dim = 0; dim < 2; dim++)\n                        pp[num][dim] = 0.5 * (face[3 * pi[num] + dim] * is + is - 1);\n\n                    /* for dy, dx */\n                    for (int axis = 0; axis < 2; axis++) {\n                        /* */\n                        float p[3][2];\n                        for (int num = 0; num < 3; num++) for (int dim = 0; dim < 2; dim++)\n                            p[num][dim] = pp[num][(dim + axis) % 2];\n\n                        /* set direction */\n                        int direction;\n                        if (axis == 0) {\n                            if (p[0][0] < p[1][0]) direction = -1; else direction = 1;\n                        } else {\n                            if (p[0][0] < p[1][0]) direction = 1; else direction = -1;\n                        }\n\n                        /* along edge */\n                        int d0_from, d0_to;\n                        d0_from = max(ceil(min(p[0][0], p[1][0])), 0.);\n                        d0_to = min(max(p[0][0], p[1][0]), is - 1.);\n                        for (int d0 = d0_from; d0 <= d0_to; d0++) {\n                            /* get cross point */\n                            int d1_in, d1_out;\n                            const float d1_cross = (p[1][1] - p[0][1]) / (p[1][0] - p[0][0]) * (d0 - p[0][0]) + p[0][1];\n                            if (0 < direction) d1_in = floor(d1_cross); else d1_in = ceil(d1_cross);\n                            d1_out = d1_in + direction;\n\n                            /* continue if cross point is not shown */\n                            if (d1_in < 0 || is <= d1_in) continue;\n                            if (d1_out < 0 || is <= d1_out) continue;\n\n                            /* get color of in-pixel and out-pixel */\n                            float alpha_in;\n                            float alpha_out;\n                            float *rgb_in;\n                            float *rgb_out;\n                            int map_index_in, map_index_out;\n                            if (axis == 0) {\n                                map_index_in = bn * is * is + d1_in * is + d0;\n                                map_index_out = bn * is * is + d1_out * is + d0;\n                            } else {\n                                map_index_in = bn * is * is + d0 * is + d1_in;\n                                map_index_out = bn * is * is + d0 * is + d1_out;\n                            }\n                            if (${return_alpha}) {\n                                alpha_in = alpha_map[map_index_in];\n                                alpha_out = alpha_map[map_index_out];\n                            }\n                            if (${return_rgb}) {\n                                rgb_in = &rgb_map[map_index_in * 3];\n                                rgb_out = &rgb_map[map_index_out * 3];\n                            }\n\n                            /* out */\n                            bool is_in_fn = (face_index_map[map_index_in] == fn);\n                            if (is_in_fn) {\n                                int d1_limit;\n                                if (0 < direction) d1_limit = is - 1; else d1_limit = 0;\n                                int d1_from = max(min(d1_out, d1_limit), 0);\n                                int d1_to = min(max(d1_out, d1_limit), is - 1);\n                                float* alpha_map_p;\n                                float* grad_alpha_map_p;\n                                float* rgb_map_p;\n                                float* grad_rgb_map_p;\n                                int map_offset, map_index_from;\n                                if (axis == 0) {\n                                    map_offset = is;\n                                    map_index_from = bn * is * is + d1_from * is + d0;\n                                } else {\n                                    map_offset = 1;\n                                    map_index_from = bn * is * is + d0 * is + d1_from;\n                                }\n                                if (${return_alpha}) {\n                                    alpha_map_p = &alpha_map[map_index_from];\n                                    grad_alpha_map_p = &grad_alpha_map[map_index_from];\n                                }\n                                if (${return_rgb}) {\n                                    rgb_map_p = &rgb_map[map_index_from * 3];\n                                    grad_rgb_map_p = &grad_rgb_map[map_index_from * 3];\n                                }\n                                for (int d1 = d1_from; d1 <= d1_to; d1++) {\n                                    float diff_grad = 0;\n                                    if (${return_alpha}) {\n                                        diff_grad += (*alpha_map_p - alpha_in) * *grad_alpha_map_p;\n                                    }\n                                    if (${return_rgb}) {\n                                        for (int k = 0; k < 3; k++)\n                                            diff_grad += (rgb_map_p[k] - rgb_in[k]) * grad_rgb_map_p[k];\n                                    }\n                                    if (${return_alpha}) {\n                                        alpha_map_p += map_offset;\n                                        grad_alpha_map_p += map_offset;\n                                    }\n                                    if (${return_rgb}) {\n                                        rgb_map_p += 3 * map_offset;\n                                        grad_rgb_map_p += 3 * map_offset;\n                                    }\n                                    if (diff_grad <= 0) continue;\n                                    if (p[1][0] != d0) {\n                                        float dist = (p[1][0] - p[0][0]) / (p[1][0] - d0) * (d1 - d1_cross) * 2. / is;\n                                        dist = (0 < dist) ? dist + ${eps} : dist - ${eps};\n                                        grad_face[pi[0] * 3 + (1 - axis)] -= diff_grad / dist;\n                                    }\n                                    if (p[0][0] != d0) {\n                                        float dist = (p[1][0] - p[0][0]) / (d0 - p[0][0]) * (d1 - d1_cross) * 2. / is;\n                                        dist = (0 < dist) ? dist + ${eps} : dist - ${eps};\n                                        grad_face[pi[1] * 3 + (1 - axis)] -= diff_grad / dist;\n                                    }\n                                }\n                            }\n\n                            /* in */\n                            {\n                                int d1_limit;\n                                float d0_cross2;\n                                if ((d0 - p[0][0]) * (d0 - p[2][0]) < 0) {\n                                    d0_cross2 = (p[2][1] - p[0][1]) / (p[2][0] - p[0][0]) * (d0 - p[0][0]) + p[0][1];\n                                } else {\n                                    d0_cross2 = (p[1][1] - p[2][1]) / (p[1][0] - p[2][0]) * (d0 - p[2][0]) + p[2][1];\n                                }\n                                if (0 < direction) d1_limit = ceil(d0_cross2); else d1_limit = floor(d0_cross2);\n                                int d1_from = max(min(d1_in, d1_limit), 0);\n                                int d1_to = min(max(d1_in, d1_limit), is - 1);\n\n                                int* face_index_map_p;\n                                float* alpha_map_p;\n                                float* grad_alpha_map_p;\n                                float* rgb_map_p;\n                                float* grad_rgb_map_p;\n                                int map_index_from;\n                                int map_offset;\n                                if (axis == 0) map_offset = is; else map_offset = 1;\n                                if (axis == 0) {\n                                    map_index_from = bn * is * is + d1_from * is + d0;\n                                } else {\n                                    map_index_from = bn * is * is + d0 * is + d1_from;\n                                }\n                                face_index_map_p = &face_index_map[map_index_from] - map_offset;\n                                if (${return_alpha}) {\n                                    alpha_map_p = &alpha_map[map_index_from] - map_offset;\n                                    grad_alpha_map_p = &grad_alpha_map[map_index_from] - map_offset;\n                                }\n                                if (${return_rgb}) {\n                                    rgb_map_p = &rgb_map[map_index_from * 3] - 3 * map_offset;\n                                    grad_rgb_map_p = &grad_rgb_map[map_index_from * 3] - 3 * map_offset;\n                                }\n\n                                for (int d1 = d1_from; d1 <= d1_to; d1++) {\n                                    face_index_map_p += map_offset;\n                                    if (${return_alpha}) {\n                                        alpha_map_p += map_offset;\n                                        grad_alpha_map_p += map_offset;\n                                    }\n                                    if (${return_rgb}) {\n                                        rgb_map_p += 3 * map_offset;\n                                        grad_rgb_map_p += 3 * map_offset;\n                                    }\n                                    if (*face_index_map_p != fn) continue;\n\n                                    float diff_grad = 0;\n                                    if (${return_alpha}) {\n                                        diff_grad += (*alpha_map_p - alpha_out) * *grad_alpha_map_p;\n                                    }\n                                    if (${return_rgb}) {\n                                        for (int k = 0; k < 3; k++)\n                                            diff_grad += (rgb_map_p[k] - rgb_out[k]) * grad_rgb_map_p[k];\n                                    }\n                                    if (diff_grad <= 0) continue;\n\n                                    if (p[1][0] != d0) {\n                                        float dist = (p[1][0] - p[0][0]) / (p[1][0] - d0) * (d1 - d1_cross) * 2. / is;\n                                        dist = (0 < dist) ? dist + ${eps} : dist - ${eps};\n                                        grad_face[pi[0] * 3 + (1 - axis)] -= diff_grad / dist;\n                                    }\n                                    if (p[0][0] != d0) {\n                                        float dist = (p[1][0] - p[0][0]) / (d0 - p[0][0]) * (d1 - d1_cross) * 2. / is;\n                                        dist = (0 < dist) ? dist + ${eps} : dist - ${eps};\n                                        grad_face[pi[1] * 3 + (1 - axis)] -= diff_grad / dist;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n\n                /* set to global gradient variable */\n                for (int k = 0; k < 9; k++) grad_faces[i * 9 + k] = grad_face[k];\n            \'\'\').substitute(\n                image_size=self.image_size,\n                num_faces=self.num_faces,\n                eps=self.eps,\n                return_rgb=int(self.return_rgb),\n                return_alpha=int(self.return_alpha),\n            ),\n            \'function\',\n        )(\n            loop, self.faces, self.face_index_map, self.rgb_map, self.alpha_map, self.grad_rgb_map, self.grad_alpha_map,\n            self.grad_faces,\n        )\n\n    def backward_textures_gpu(self):\n        # inputs:\n        #   face_index_map, sampling_weight_map, sampling_index_map, grad_rgb_map\n        # outputs:\n        #   if rgb: grad_textures\n\n        if not self.return_rgb:\n            return\n\n        loop = self.xp.arange(self.batch_size * self.image_size * self.image_size).astype(\'int32\')\n        chainer.cuda.elementwise(\n            \'int32 _, raw int32 face_index_map, raw T sampling_weight_map, raw int32 sampling_index_map, \' +\n            \'raw T grad_rgb_map, raw T grad_textures\',\n            \'\',\n            string.Template(\'\'\'\n                const int face_index = face_index_map[i];\n                if (0 <= face_index) {\n                int is = ${image_size};\n                    int nf = ${num_faces};\n                    int ts = ${texture_size};\n                    int bn = i / (is * is);    // batch number [0 -> bs]\n\n                    float* grad_texture = &grad_textures[(bn * nf + face_index) * ts * ts * ts * 3];\n                    float* sampling_weight_map_p = &sampling_weight_map[i * 8];\n                    int* sampling_index_map_p = &sampling_index_map[i * 8];\n                    for (int pn = 0; pn < 8; pn++) {\n                        float w = *sampling_weight_map_p++;\n                        int isc = *sampling_index_map_p++;\n                        float* grad_texture_p = &grad_texture[isc * 3];\n                        float* grad_rgb_map_p = &grad_rgb_map[i * 3];\n                        for (int k = 0; k < 3; k++) atomicAdd(grad_texture_p++, w * *grad_rgb_map_p++);\n                    }\n                }\n            \'\'\').substitute(\n                image_size=self.image_size,\n                num_faces=self.num_faces,\n                texture_size=self.texture_size,\n            ),\n            \'function\',\n        )(\n            loop, self.face_index_map, self.sampling_weight_map, self.sampling_index_map, self.grad_rgb_map,\n            self.grad_textures,\n        )\n\n    def backward_depth_map_gpu(self):\n        # inputs:\n        #   faces, depth_map, face_index_map, face_inv_map, weight_map, grad_depth_map\n        # outputs:\n        #   if depth: grad_faces\n\n        if not self.return_depth:\n            return\n\n        # for each pixel\n        loop = self.xp.arange(self.batch_size * self.image_size * self.image_size).astype(\'int32\')\n        chainer.cuda.elementwise(\n            \'int32 _, raw float32 faces, raw float32 depth_map, raw int32 face_index_map, \' +\n            \'raw float32 face_inv_map, raw float32 weight_map, raw float32 grad_depth_map, raw float32 grad_faces\',\n            \'\',\n            string.Template(\'\'\'\n                const int fn = face_index_map[i];\n                if (0 <= fn) {\n                    const int nf = ${num_faces};\n                    const int is = ${image_size};\n                    const int bn = i / (is * is);\n                    const float* face = &faces[(bn * nf + fn) * 9];\n                    const float depth = depth_map[i];\n                    const float depth2 = depth * depth;\n                    const float* face_inv = &face_inv_map[i * 9];\n                    const float* weight = &weight_map[i * 3];\n                    const float grad_depth = grad_depth_map[i];\n                    float* grad_face = &grad_faces[(bn * nf + fn) * 9];\n\n                    /* derivative wrt z */\n                    for (int k = 0; k < 3; k++) {\n                        const float z_k = face[3 * k + 2];\n                        atomicAdd(&grad_face[3 * k + 2], grad_depth * weight[k] * depth2 / (z_k * z_k));\n                    }\n\n                    /* derivative wrt x, y */\n                    float tmp[3] = {};\n                    for (int k = 0; k < 3; k++) for (int l = 0; l < 3; l++) {\n                        tmp[k] += -face_inv[3 * l + k] / face[3 * l + 2];\n                    }\n                    for (int k = 0; k < 3; k++) for (int l = 0; l < 2; l++) {\n                        // k: point number, l: dimension\n                        atomicAdd(&grad_face[3 * k + l], -grad_depth * tmp[l] * weight[k] * depth2 * is / 2);\n                    }\n                }\n            \'\'\').substitute(\n                num_faces=self.num_faces,\n                image_size=self.image_size,\n            ),\n            \'function\'\n        )(\n            loop, self.faces, self.depth_map, self.face_index_map, self.face_inv_map, self.weight_map,\n            self.grad_depth_map, self.grad_faces,\n        )\n\n    def backward_gpu(self, inputs, grad_outputs):\n        # initialize output buffers\n        self.grad_faces = self.xp.ascontiguousarray(self.xp.zeros_like(self.faces, dtype=\'float32\'))\n        if self.return_rgb:\n            self.grad_textures = self.xp.ascontiguousarray(self.xp.zeros_like(self.textures, dtype=\'float32\'))\n        else:\n            self.grad_textures = self.xp.zeros(1, \'float32\')\n\n        # get grad_outputs\n        if self.return_rgb:\n            if grad_outputs[0] is not None:\n                self.grad_rgb_map = self.xp.ascontiguousarray(grad_outputs[0])\n            else:\n                self.grad_rgb_map = self.xp.zeros_like(self.rgb_map)\n        else:\n            self.grad_rgb_map = self.xp.zeros(1, \'float32\')\n        if self.return_alpha:\n            if grad_outputs[1] is not None:\n                self.grad_alpha_map = self.xp.ascontiguousarray(grad_outputs[1])\n            else:\n                self.grad_alpha_map = self.xp.zeros_like(self.alpha_map)\n        else:\n            self.grad_alpha_map = self.xp.zeros(1, \'float32\')\n        if self.return_depth:\n            if grad_outputs[2] is not None:\n                self.grad_depth_map = self.xp.ascontiguousarray(grad_outputs[2])\n            else:\n                self.grad_depth_map = self.xp.zeros_like(self.depth_map)\n        else:\n            self.grad_depth_map = self.xp.zeros(1, \'float32\')\n\n        # backward pass\n        self.backward_pixel_map_gpu()\n        self.backward_textures_gpu()\n        self.backward_depth_map_gpu()\n\n        # return\n        if len(self.inputs) == 1:\n            return self.grad_faces,\n        else:\n            return self.grad_faces, self.grad_textures\n\n    ####################################################################################################################\n    # CPU\n    def forward_cpu(self, inputs):\n        raise NotImplementedError\n\n    def backward_cpu(self, inputs, grad_outputs):\n        raise NotImplementedError\n\n\ndef rasterize_rgbad(\n        faces,\n        textures=None,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n        return_rgb=True,\n        return_alpha=True,\n        return_depth=True,\n):\n    """"""\n    Generate RGB, alpha channel, and depth images from faces and textures (for RGB).\n\n    Args:\n        faces (chainer.Variable): Faces. The shape is [batch size, number of faces, 3 (vertices), 3 (XYZ)].\n        textures (chainer.Variable): Textures.\n            The shape is [batch size, number of faces, texture size, texture size, texture size, 3 (RGB)].\n        image_size (int): Width and height of rendered images.\n        anti_aliasing (bool): do anti-aliasing by super-sampling.\n        near (float): nearest z-coordinate to draw.\n        far (float): farthest z-coordinate to draw.\n        eps (float): small epsilon for approximated differentiation.\n        background_color (tuple): background color of RGB images.\n        return_rgb (bool): generate RGB images or not.\n        return_alpha (bool): generate alpha channels or not.\n        return_depth (bool): generate depth images or not.\n\n    Returns:\n        dict:\n            {\n                \'rgb\': RGB images. The shape is [batch size, 3, image_size, image_size].\n                \'alpha\': Alpha channels. The shape is [batch size, image_size, image_size].\n                \'depth\': Depth images. The shape is [batch size, image_size, image_size].\n            }\n\n    """"""\n\n    if textures is None:\n        inputs = [faces]\n    else:\n        inputs = [faces, textures]\n\n    if anti_aliasing:\n        # 2x super-sampling\n        rgb, alpha, depth = Rasterize(\n            image_size * 2, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n    else:\n        rgb, alpha, depth = Rasterize(\n            image_size, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n\n    # transpose & vertical flip\n    if return_rgb:\n        rgb = rgb.transpose((0, 3, 1, 2))\n        rgb = rgb[:, :, ::-1, :]\n    if return_alpha:\n        alpha = alpha[:, ::-1, :]\n    if return_depth:\n        depth = depth[:, ::-1, :]\n\n    if anti_aliasing:\n        # 0.5x down-sampling\n        if return_rgb:\n            rgb = cf.average_pooling_2d(rgb, 2, 2)\n        if return_alpha:\n            alpha = cf.average_pooling_2d(alpha[:, None, :, :], 2, 2)[:, 0]\n        if return_depth:\n            depth = cf.average_pooling_2d(depth[:, None, :, :], 2, 2)[:, 0]\n\n    ret = {\n        \'rgb\': rgb if return_rgb else None,\n        \'alpha\': alpha if return_alpha else None,\n        \'depth\': depth if return_depth else None,\n    }\n\n    return ret\n\n\ndef rasterize(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~chainer.Variable: RGB images. The shape is [batch size, 3, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(\n        faces, textures, image_size, anti_aliasing, near, far, eps, background_color, True, False, False)[\'rgb\']\n\n\ndef rasterize_silhouettes(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate alpha channels from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~chainer.Variable: Alpha channels. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None, False, True, False)[\'alpha\']\n\n\ndef rasterize_depth(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate depth images from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~chainer.Variable: Depth images. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None, False, False, True)[\'depth\']\n\n\ndef use_unsafe_rasterizer(flag):\n    global USE_UNSAFE_IMPLEMENTATION\n    USE_UNSAFE_IMPLEMENTATION = flag\n'"
geometric/neural_renderer/renderer.py,0,"b""from __future__ import division\nfrom builtins import object\nfrom past.utils import old_div\nimport math\n\nimport chainer.functions as cf\n\nimport neural_renderer\n\n\nclass Renderer(object):\n    def __init__(self):\n        # rendering\n        self.image_size = 256\n        self.anti_aliasing = True\n        self.background_color = [0, 0, 0]\n        self.fill_back = True\n\n        # camera\n        self.perspective = True\n        self.viewing_angle = 30\n        self.eye = [0, 0, -(old_div(1., math.tan(math.radians(self.viewing_angle))) + 1)]\n        self.camera_mode = 'look_at'\n        self.camera_direction = [0, 0, 1]\n        self.near = 0.1\n        self.far = 100\n\n        # light\n        self.light_intensity_ambient = 0.5\n        self.light_intensity_directional = 0.5\n        self.light_color_ambient = [1, 1, 1]  # white\n        self.light_color_directional = [1, 1, 1]  # white\n        self.light_direction = [0, 1, 0]  # up-to-down\n\n        # rasterization\n        self.rasterizer_eps = 1e-3\n\n    def render_silhouettes(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = neural_renderer.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = neural_renderer.look(vertices, self.eye, self.camera_direction)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = neural_renderer.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = neural_renderer.vertices_to_faces(vertices, faces)\n        images = neural_renderer.rasterize_silhouettes(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_depth(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = neural_renderer.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = neural_renderer.look(vertices, self.eye, self.camera_direction)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = neural_renderer.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = neural_renderer.vertices_to_faces(vertices, faces)\n        images = neural_renderer.rasterize_depth(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render(self, vertices, faces, textures):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n            textures = cf.concat((textures, textures.transpose((0, 1, 4, 3, 2, 5))), axis=1)\n\n        # lighting\n        faces_lighting = neural_renderer.vertices_to_faces(vertices, faces)\n        textures = neural_renderer.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = neural_renderer.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = neural_renderer.look(vertices, self.eye, self.camera_direction)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = neural_renderer.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = neural_renderer.vertices_to_faces(vertices, faces)\n        images = neural_renderer.rasterize(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return images\n"""
geometric/neural_renderer/save_obj.py,0,"b""import os\n\n\ndef save_obj(filename, vertices, faces):\n    assert vertices.ndim == 2\n    assert faces.ndim == 2\n\n    with open(filename, 'w') as f:\n        f.write('# %s\\n' % os.path.basename(filename))\n        f.write('#\\n')\n        f.write('\\n')\n        f.write('g mesh\\n')\n        f.write('\\n')\n        for vertex in vertices:\n            f.write('v  %.4f %.4f %.4f\\n' % (vertex[0], vertex[1], vertex[2]))\n        f.write('\\n')\n        for face in faces:\n            f.write('f  %d %d %d\\n' % (face[0] + 1, face[1] + 1, face[2] + 1))\n"""
geometric/neural_renderer/vertices_to_faces.py,0,"b'import chainer\n\n\ndef vertices_to_faces(vertices, faces):\n    """"""\n    :param vertices: [batch size, number of vertices, 3]\n    :param faces: [batch size, number of faces, 3)\n    :return: [batch size, number of faces, 3, 3]\n    """"""\n    assert (vertices.ndim == 3)\n    assert (faces.ndim == 3)\n    assert (vertices.shape[0] == faces.shape[0])\n    assert (vertices.shape[2] == 3)\n    assert (faces.shape[2] == 3)\n\n    xp = chainer.cuda.get_array_module(faces)\n    bs, nv = vertices.shape[:2]\n    bs, nf = faces.shape[:2]\n    faces = faces + (xp.arange(bs, dtype=\'int32\') * nv)[:, None, None]\n    vertices = vertices.reshape((bs * nv, 3))\n    return vertices[faces]\n'"
geometric/scripts/main.py,70,"b""#!/usr/bin/env python\n\nimport chainer\nimport functools\nimport json\nimport torch\nimport torchvision\nimport numpy as np\nimport os\nimport PIL.Image\nimport sys\n\nfrom absl import flags\nfrom tensorboardX import SummaryWriter\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DataParallel\n\nfrom bulb.net import Net, TrainMixin, TestMixin\nfrom bulb.saver import Saver\nfrom bulb.utils import new_working_dir\n\nfrom derender3d import TargetType\nfrom derender3d.datasets import Transforms\nfrom derender3d.data_loader import DataLoader\nfrom derender3d.models import Derenderer3d\nfrom derender3d.utils import to_numpy\n\nfrom maskrcnn.model import MaskRCNN\nfrom maskrcnn.config import Config\n\nflags.DEFINE_string('do', None, 'do')\nflags.DEFINE_string('_do', '_test', '_do')\nflags.DEFINE_string('input_file', None, 'input_file')\nflags.DEFINE_enum('dataset', None, ['vkitti', 'cityscapes'], 'dataset')\nflags.DEFINE_enum('mode', None, ['pretrain', 'full', 'finetune', 'extend'], 'mode')\nflags.DEFINE_enum('source', 'gt', ['gt', 'maskrcnn'], 'source')\nflags.DEFINE_integer('num_opts', 0, 'num_opts')\nflags.DEFINE_integer('num_epochs', 256, 'num_epochs')\nflags.DEFINE_integer('batch_size', 64, 'batch_size')\nflags.DEFINE_integer('num_grids', 4, 'num_grids')\nflags.DEFINE_float('mask_weight', 0.1, 'mask_weight')\nflags.DEFINE_float('ffd_coeff_reg', 1.0, 'ffd_coeff_reg')\nflags.DEFINE_integer('image_size', 256, 'image_size')\nflags.DEFINE_integer('render_size', 384, 'render_size')\nflags.DEFINE_float('lr', 1e-3, 'lr')\nflags.DEFINE_integer('lr_decay_epochs', 16, 'lr_decay_epochs')\nflags.DEFINE_float('lr_decay_rate', 0.5, 'lr_decay_rate')\nflags.DEFINE_float('weight_decay', 1e-3, 'weight_decay')\nflags.DEFINE_integer('summarize_steps', 1, 'summarize_steps')\nflags.DEFINE_integer('image_steps', 100, 'image_steps')\nflags.DEFINE_integer('save_steps', 5000, 'save_steps')\nflags.DEFINE_string('ckpt_dir', None, 'ckpt_dir')\nflags.DEFINE_string('maskrcnn_path', None, 'maskrcnn_path')\nflags.DEFINE_string('output_dir', None, 'output_dir')\nflags.DEFINE_string('edit_json', None, 'edit_json')\nflags.DEFINE_string('working_dir_root', './models', 'working_dir_root')\nflags.DEFINE_string('name', None, 'name')\nflags.DEFINE_integer('num_workers', 8, 'num_workers')\nflags.DEFINE_bool('debug', False, 'debug')\nFLAGS = flags.FLAGS\n\n\nclass TrainLoader(DataLoader):\n    def __init__(self):\n        super(TrainLoader, self).__init__(\n            dataset=FLAGS.dataset,\n            mode=FLAGS.mode,\n            batch_size=FLAGS.batch_size,\n            num_workers=FLAGS.num_workers,\n            is_train=True,\n            debug=FLAGS.debug,\n        )\n\n\nclass TestLoader(DataLoader):\n    def __init__(self):\n        super(TestLoader, self).__init__(\n            dataset=FLAGS.dataset,\n            mode=FLAGS.mode,\n            batch_size=FLAGS.batch_size,\n            num_workers=FLAGS.num_workers,\n            is_train=False,\n            debug=FLAGS.debug,\n        )\n\n\nclass Model(Derenderer3d):\n    def __init__(self):\n        super(Model, self).__init__(\n            mode=FLAGS.mode,\n            image_size=FLAGS.image_size,\n            render_size=FLAGS.render_size,\n        )\n\n\nclass BaseNet(Net):\n    @staticmethod\n    def partial(f, m):\n        def _f(*args, **kwargs):\n            index = torch.nonzero(m)\n\n            if index.numel():\n                index = index.squeeze(dim=1)\n                v = f(*[arg[index] for arg in args], **kwargs)\n                if torch.isnan(v).any():\n                    import pdb\n                    pdb.set_trace()\n                return v\n            else:\n                return torch.tensor(0.0).cuda()\n\n        return _f\n\n    def step_batch(self):\n        _blob = self.model(self.images, self.roi_norms, self.focals)\n        self._register_vars(_blob)\n\n        loss_dict = {}\n        if FLAGS.mode & TargetType.geometry:\n            is_geometry = self.targets & TargetType.pretrain\n\n            mse_loss = BaseNet.partial(F.mse_loss, is_geometry)\n\n            self.theta_deltas = torch.cat([\n                torch.cos(self.thetas),\n                torch.sin(self.thetas),\n            ], dim=1)\n\n            loss_dict.update({\n                'theta_delta_loss': mse_loss(self._theta_deltas, self.theta_deltas),\n                'translation2d_loss': mse_loss(self._translation2ds, self.translation2ds),\n                'scale_loss': mse_loss(self._log_scales, self.log_scales),\n                'depth_loss': mse_loss(self._log_depths, self.log_depths),\n            })\n\n        if FLAGS.mode & TargetType.reproject:\n            is_reproject = self.targets & TargetType.finetune\n\n            mean = BaseNet.partial(torch.mean, is_reproject)\n            mse_loss = BaseNet.partial(F.mse_loss, is_reproject)\n\n            self.masks = Transforms.pad_like(self.masks, self._masks)\n            self.ignores = Transforms.pad_like(self.ignores, self._masks, mode='replicate')\n\n            mask_losses = (1 - self.ignores) * F.mse_loss(self._masks, self.masks, reduce=False)\n            mask_losses = FLAGS.mask_weight * mask_losses.mean(dim=3).mean(dim=2).mean(dim=1)\n\n            loss_dict.update({\n                'class_reward': mean(self._class_log_probs * mask_losses.detach()),\n                'mask_loss': mean(mask_losses),\n                'ffd_coeff_reg': FLAGS.ffd_coeff_reg * torch.mean(self._ffd_coeffs ** 2),\n            })\n\n        return loss_dict\n\n    def post_batch(self):\n        super(BaseNet, self).post_batch()\n\n        if FLAGS.mode & TargetType.reproject:\n            if (self.num_step % FLAGS.image_steps == 0):\n                masks = torchvision.utils.make_grid(self.masks)\n                self.writer.add_image('{:s}/mask'.format(self.name), masks, self.num_step)\n\n                _masks = torchvision.utils.make_grid(self._masks)\n                self.writer.add_image('{:s}/_mask'.format(self.name), _masks, self.num_step)\n\n                ignores = torchvision.utils.make_grid(self.ignores)\n                self.writer.add_image('{:s}/ignore'.format(self.name), ignores, self.num_step)\n\n\nclass TrainNet(BaseNet, TrainMixin):\n    pass\n\n\nclass TestNet(BaseNet, TestMixin):\n    pass\n\n\ndef train():\n    working_dir = new_working_dir(FLAGS.working_dir_root, FLAGS.name)\n\n    model = DataParallel(Model()).cuda()\n    writer = SummaryWriter(log_dir=working_dir)\n    saver = Saver(working_dir=working_dir)\n    saver.save_meta(FLAGS.flag_values_dict())\n\n    train_net = TrainNet(\n        optimizer=torch.optim.Adam(model.parameters(), lr=FLAGS.lr, weight_decay=FLAGS.weight_decay),\n        model=model,\n        writer=writer,\n        data_loader=TrainLoader(),\n        lr_decay_epochs=FLAGS.lr_decay_epochs,\n        lr_decay_rate=FLAGS.lr_decay_rate,\n        summarize_steps=FLAGS.summarize_steps,\n        save_steps=FLAGS.save_steps,\n        saver=saver,\n    )\n    test_net = TestNet(\n        model=model,\n        writer=writer,\n        data_loader=TestLoader(),\n    )\n\n    if FLAGS.ckpt_dir is not None:\n        train_net.load(ckpt_dir=FLAGS.ckpt_dir)\n\n    for num_epoch in range(FLAGS.num_epochs):\n        train_net.step_epoch()\n        test_net.step_epoch(num_step=train_net.num_step)\n\n    train_net.save()\n    writer.close()\n\n\ndef _test_2d(\n    dataset,\n    image_dir,\n    name,\n    image_rgb,\n    class_ids,\n    image_masks,\n    rois,\n    operations=None,\n    use_ry=False,\n):\n\n    (height, width, _) = image_rgb.shape\n\n    num_objs = len(class_ids)\n    class_ids = torch.tensor(class_ids, dtype=torch.int32, requires_grad=False).cuda()\n    image_masks = torch.tensor(image_masks, dtype=torch.float32, requires_grad=False).cuda()\n    rois = torch.tensor(rois, dtype=torch.int32, requires_grad=False).cuda()\n\n    interests = torch.ones(num_objs).byte().cuda()\n\n    image_instance_map = torch.zeros(1, height, width).cuda()\n    for num_obj in range(num_objs):\n        image_instance_map = (1 - image_masks[num_obj]) * image_instance_map + image_masks[num_obj] * (1 + num_obj)\n\n    (image_instance_map_pil, image_vis_pil) = Transforms.visualize(image_rgb, to_numpy(image_instance_map), to_numpy(rois), to_numpy(interests))\n    image_instance_map_pil.save(os.path.join(image_dir, '{:s}-ref.png'.format(name)))\n    image_vis_pil.save(os.path.join(image_dir, '{:s}-ref-visualize.png'.format(name)))\n\n    mrois = torch.stack([\n        rois[:, 2] + rois[:, 0],\n        rois[:, 3] + rois[:, 1],\n    ], dim=1).float() / 2.0\n    drois = torch.stack([\n        rois[:, 2] - rois[:, 0],\n        rois[:, 3] - rois[:, 1],\n    ], dim=1).float()\n\n    _mrois = mrois.clone()\n    _drois = drois.clone()\n\n    if operations is not None and operations:\n        _mroi_ops = torch.tensor([[\n            float(operation['from']['v']),\n            float(operation['from']['u']),\n        ] for operation in operations]).cuda()\n\n        _mroi_diffs = torch.sum((_mrois[:, None, :] - _mroi_ops[None, :, :]) ** 2, dim=2)\n        if len(_mrois) < len(_mroi_ops):\n            index_ops = torch.argmin(_mroi_diffs, dim=1)\n            indices = [(index_obj, index_op) for (index_obj, index_op) in enumerate(index_ops)]\n        else:\n            index_objs = torch.argmin(_mroi_diffs, dim=0)\n            indices = [(index_obj, index_op) for (index_op, index_obj) in enumerate(index_objs)]\n\n        for (index_obj, index_op) in indices:\n            operation = operations[index_op]\n            print('Object #{:d} and operation {!s}'.format(index_obj, operation))\n\n            u = float(operation['from']['u'])\n            v = float(operation['from']['v'])\n\n            if operation['type'] == 'delete':\n                interests[index_obj] = torch.tensor(0.0).type(torch.ByteTensor).cuda()\n\n            elif operation['type'] == 'modify':\n                _u = float(operation['to']['u'])\n                _v = float(operation['to']['v'])\n                zoom = float(operation['zoom'])\n                ry = float(operation['ry'])\n\n                _mrois[index_obj] = _mrois[index_obj] + torch.tensor([_v - v, _u - u]).cuda()\n\n                if use_ry:\n                    _drois[index_obj] = torch.tensor([zoom * _drois[index_obj, 0], zoom * float(np.cos(ry)) * _drois[index_obj, 1]]).cuda()\n                else:\n                    _drois[index_obj] = zoom * _drois[index_obj]\n\n    json_obj = {}\n    _image_instance_map = torch.zeros(1, height, width).cuda()\n    for index_obj in range(num_objs):\n        if to_numpy(interests[index_obj]):  # interesting obj\n            json_obj[index_obj + 1] = {\n                'class_id': int(class_ids[index_obj]),\n            }\n\n            _mask = image_masks[index_obj, :, rois[index_obj, 0]:rois[index_obj, 2], rois[index_obj, 1]:rois[index_obj, 3]]\n            _mask_pil = Transforms.to_pil_image(_mask.detach().cpu())\n            _mask_pil = Transforms.resize(_mask_pil, (int(_drois[index_obj, 0]), int(_drois[index_obj, 1])))\n            _image_mask_pil = PIL.Image.new(mode='L', size=(width, height))\n            _image_mask_pil.paste(_mask_pil, box=(\n                int(_mrois[index_obj, 1] - _drois[index_obj, 1] / 2),\n                int(_mrois[index_obj, 0] - _drois[index_obj, 0] / 2),\n            ))\n            _image_mask = Transforms.to_tensor(_image_mask_pil).cuda()\n            _image_mask = torch.round(_image_mask)\n\n            _image_instance_map = (1 - _image_mask) * _image_instance_map + _image_mask * (1 + index_obj)\n\n    with open(os.path.join(image_dir, '{:s}.json'.format(name)), 'w') as f:\n        json.dump(json_obj, f, indent=4)\n\n    (_image_instance_map_pil, _image_vis_pil) = Transforms.visualize(image_rgb, to_numpy(_image_instance_map), to_numpy(rois), to_numpy(interests))\n    _image_instance_map_pil.save(os.path.join(image_dir, '{:s}.png'.format(name)))\n    _image_vis_pil.save(os.path.join(image_dir, '{:s}-visualize.png'.format(name)))\n\n\n_test_2d_plus = functools.partial(_test_2d, use_ry=True)\n\n\ndef _test(\n    dataset,\n    model,\n    image_dir,\n    name,\n    image_rgb,\n    class_ids,\n    image_masks,\n    image_ignores,\n    rois,\n    metas=None,\n    operations=None,\n    all_interested=False,\n):\n\n    (height, width, _) = image_rgb.shape\n\n    num_objs = len(class_ids)\n    class_ids = torch.tensor(class_ids, dtype=torch.int32, requires_grad=False).cuda()\n    image_masks = torch.tensor(image_masks, dtype=torch.float32, requires_grad=False).cuda()\n    rois = torch.tensor(rois, dtype=torch.int32, requires_grad=False).cuda()\n\n    mask_sums = image_masks.sum(dim=3).sum(dim=2).squeeze(dim=1)\n    class_sels = torch.tensor([1, 2], dtype=torch.int32, requires_grad=False).cuda()\n\n    if all_interested:\n        interests = torch.ones(num_objs).byte().cuda()\n    else:\n        interests = (\n            torch.sum(class_ids[:, None] == class_sels, dim=1).byte() * (mask_sums > 16 * 16)\n        )\n\n    image_instance_map = torch.zeros(1, height, width).cuda()\n    for num_obj in range(num_objs):\n        image_instance_map = (1 - image_masks[num_obj]) * image_instance_map + image_masks[num_obj] * (1 + num_obj)\n\n    (image_instance_map_pil, image_vis_pil) = Transforms.visualize(image_rgb, to_numpy(image_instance_map), to_numpy(rois), to_numpy(interests))\n    image_instance_map_pil.save(os.path.join(image_dir, '{:s}-ref.png'.format(name)))\n    image_vis_pil.save(os.path.join(image_dir, '{:s}-ref-visualize.png'.format(name)))\n\n    rgbs = []\n    for roi in rois:\n        rgbs.append(dataset.transform_rgb(image_rgb, to_numpy(roi)))\n    rgbs = torch.stack(rgbs, dim=0).cuda()\n\n    masks = []\n    for (image_mask, roi) in zip(image_masks, rois):\n        masks.append(dataset.transform_mask(np.transpose(to_numpy(image_mask), (1, 2, 0)), to_numpy(roi)))\n    masks = torch.stack(masks, dim=0).cuda()\n\n    roi_norms = (rois.float() - torch.tensor([\n        dataset.Camera.v0,\n        dataset.Camera.u0,\n        dataset.Camera.v0,\n        dataset.Camera.u0,\n    ], requires_grad=False).cuda()) / dataset.Camera.focal\n\n    focals = dataset.Camera.focal * torch.ones(num_objs, 1, requires_grad=False).cuda()\n\n    _zoom_tos = FLAGS.render_size / (2.0 * dataset.Camera.focal) * torch.ones(num_objs, 1).cuda()\n\n    _mroi_norms = torch.stack([\n        roi_norms[:, 2] + roi_norms[:, 0],\n        roi_norms[:, 3] + roi_norms[:, 1],\n    ], dim=1) / 2.0\n    _droi_norms = torch.stack([\n        roi_norms[:, 2] - roi_norms[:, 0],\n        roi_norms[:, 3] - roi_norms[:, 1],\n    ], dim=1)\n\n    _blob = {\n        '_roi_norms': roi_norms,\n        '_mroi_norms': _mroi_norms,\n        '_droi_norms': _droi_norms,\n        '_focals': focals,\n    }\n\n    _blob_derendered = model.module.derenderer(rgbs, _mroi_norms, _droi_norms)\n    _blob.update(_blob_derendered)\n\n    if FLAGS.num_opts:\n        if image_ignores is None:\n            _depths = _blob['_log_depths'] - torch.sum(torch.log(_droi_norms), dim=1, keepdim=True)\n            index = torch.sort(_depths, dim=0)[1].squeeze()\n            image_masks_sorted = torch.cat([\n                torch.zeros_like(image_masks[0:1]),\n                torch.index_select(image_masks, dim=0, index=index),\n            ], dim=0)\n            image_masks_sorted = image_masks_sorted[:-1]\n            image_ignores = torch.clamp(torch.cumsum(image_masks_sorted, dim=0), min=0, max=1)\n        else:\n            image_ignores = torch.tensor(image_ignores, dtype=torch.float32, requires_grad=False).cuda()\n\n        ignores = []\n        for (image_ignore, roi) in zip(image_ignores, rois):\n            ignores.append(dataset.transform_ignore(np.transpose(to_numpy(image_ignore), (1, 2, 0)), to_numpy(roi)))\n        ignores = torch.stack(ignores, dim=0).cuda()\n\n        model.train()\n        model.module._force_no_sample = True\n\n        for (key, value) in _blob_derendered.items():\n            value = value.clone().detach()\n            _blob[key] = value\n\n        _blob_derendered_optimize = {}\n        for key in [\n            '_theta_deltas',\n            '_translation2ds',\n            '_log_scales',\n            '_ffd_coeffs',\n        ]:\n            _blob_derendered_optimize[key] = _blob[key].requires_grad_()\n\n        optimizer = torch.optim.Adam(_blob_derendered_optimize.values(), lr=3e-2)\n        for num_opt in range(FLAGS.num_opts):\n            optimizer.zero_grad()\n\n            _blob.update(model.module.render(_blob))\n\n            _masks = _blob['_masks']\n            masks_padded = Transforms.pad_like(masks, _masks)\n            loss = torch.nn.functional.mse_loss(_masks, masks_padded, reduce=False) + 100 * torch.mean(_blob['_ffd_coeffs'] ** 2)\n            if image_ignores is not None:\n                ignores_padded = Transforms.pad_like(ignores, _masks, mode='replicate')\n                loss = loss * (1 - ignores_padded)\n            loss = torch.mean(loss)\n\n            loss.backward()\n            optimizer.step()\n\n            print('Optimizing {:d}/{:d}: loss={:.4e}'.format(num_opt + 1, FLAGS.num_opts, loss.item()))\n\n        model.eval()\n        model.module._force_no_sample = False\n\n    if operations is not None and operations:\n        _mroi_norms = _blob['_mroi_norms']\n        _droi_norms = _blob['_droi_norms']\n        _theta_deltas = _blob['_theta_deltas'].detach()\n        _translation2ds = _blob['_translation2ds'].detach()\n        _log_depths = _blob['_log_depths'].detach()\n\n        _mroi_op_norms = torch.tensor([[\n            (float(operation['from']['v']) - dataset.Camera.v0) / dataset.Camera.focal,\n            (float(operation['from']['u']) - dataset.Camera.u0) / dataset.Camera.focal,\n        ] for operation in operations]).cuda()\n\n        _mroi_norm_diffs = torch.sum((_mroi_norms[:, None, :] - _mroi_op_norms[None, :, :]) ** 2, dim=2)\n        if len(_mroi_norms) < len(_mroi_op_norms):\n            index_ops = torch.argmin(_mroi_norm_diffs, dim=1)\n            indices = [(index_obj, index_op) for (index_obj, index_op) in enumerate(index_ops)]\n        else:\n            index_objs = torch.argmin(_mroi_norm_diffs, dim=0)\n            indices = [(index_obj, index_op) for (index_op, index_obj) in enumerate(index_objs)]\n\n        for (index_obj, index_op) in indices:\n            operation = operations[index_op]\n            print('Object #{:d} and operation {!s}'.format(index_obj, operation))\n\n            u = float(operation['from']['u'])\n            v = float(operation['from']['v'])\n\n            if operation['type'] == 'delete':\n                interests[index_obj] = torch.tensor(0.0).type(torch.ByteTensor).cuda()\n\n            elif operation['type'] == 'modify':\n                u = float(operation['to'].get('u', u))\n                v = float(operation['to'].get('v', v))\n                zoom = float(operation['zoom'])\n                ry = float(operation['ry'])\n\n                _center2d = torch.tensor([\n                    (v - dataset.Camera.v0) / dataset.Camera.focal,\n                    (u - dataset.Camera.u0) / dataset.Camera.focal,\n                ]).cuda()\n                _translation2d = (_center2d - _mroi_norms[index_obj]) / _droi_norms[index_obj]\n                _log_depth = _log_depths[index_obj] - 2 * torch.log(torch.tensor(zoom)).cuda()\n\n                (_theta_cos, _theta_sin) = _theta_deltas[index_obj]\n                _cos = torch.cos(torch.tensor(-ry)).cuda()\n                _sin = torch.sin(torch.tensor(-ry)).cuda()\n                _theta_delta = torch.stack([\n                    _theta_cos * _cos - _theta_sin * _sin,\n                    _theta_sin * _cos + _theta_cos * _sin,\n                ])\n\n                _theta_deltas[index_obj] = _theta_delta\n                _translation2ds[index_obj] = _translation2d\n                _log_depths[index_obj] = _log_depth\n\n    _blob.update(model.module.render(_blob))\n\n    _rotations = _blob['_rotations']\n    # _thetas = _blob['_thetas']\n    _alphas = _blob['_alphas']\n    _scales = _blob['_scales']\n    _depths = _blob['_depths']\n    _center2ds = _blob['_center2ds']\n    _translations = _blob['_translations']\n    _zooms = _blob['_zooms']\n    _masks = _blob['_masks']\n    _normals = _blob['_normals']\n    _depth_maps = _blob['_depth_maps']\n\n    torch.save({\n        'num_objs': num_objs,\n        'image_masks': image_masks,\n        'rois': rois,\n        'interests': interests,\n        '_scales': _scales,\n        '_rotations': _rotations,\n        '_translations': _translations,\n        '_zoom_tos': _zoom_tos,\n    }, os.path.join(image_dir, '{:s}.pkl'.format(name)))\n\n    index_objs = to_numpy(torch.sort(_depths[:, 0], dim=0, descending=True)[1])\n\n    json_obj = {}\n    _image_instance_map = torch.zeros(1, height, width).cuda()\n    _image_normal_map = torch.full((3, height, width), 0.5).cuda()\n    _image_depth_map = torch.full((1, height, width), 1.0).cuda()\n    for index_obj in index_objs.tolist():\n        print('Object #{:d} with depth {:.2f}'.format(index_obj, to_numpy(_depths[index_obj])[0]))\n\n        if to_numpy(interests[index_obj]):  # interesting obj\n            json_obj[index_obj + 1] = {\n                'class_id': int(class_ids[index_obj]),\n                'depth': float(to_numpy(_depths[index_obj])),\n                'alpha': float(to_numpy(_alphas[index_obj])),\n            }\n            if metas is not None:\n                for (key, value) in metas[index_obj].items():\n                    json_obj[index_obj + 1][key] = value\n\n            _image_size = int(FLAGS.render_size / _zooms[index_obj])\n\n            _mask = _masks[index_obj]\n            _mask_pil = Transforms.to_pil_image(_mask.detach().cpu())\n            _mask_pil = Transforms.resize(_mask_pil, (_image_size, _image_size))\n            _image_mask_pil = PIL.Image.new(mode='L', size=(width, height))\n            _image_mask_pil.paste(_mask_pil, box=(\n                int(_center2ds[index_obj, 1] * dataset.Camera.focal + dataset.Camera.u0 - _image_size // 2),\n                int(_center2ds[index_obj, 0] * dataset.Camera.focal + dataset.Camera.v0 - _image_size // 2),\n            ))\n            _image_mask = Transforms.to_tensor(_image_mask_pil).cuda()\n            _image_mask = torch.round(_image_mask)\n\n            _image_instance_map = (1 - _image_mask) * _image_instance_map + _image_mask * (1 + index_obj)\n\n            #\n\n            _normal = _normals[index_obj] / 2 + 0.5\n            _normal_pil = Transforms.to_pil_image(_normal.detach().cpu())\n            _normal_pil = Transforms.resize(_normal_pil, (_image_size, _image_size))\n            _image_normal_pil = PIL.Image.new(mode='RGB', size=(width, height))\n            _image_normal_pil.paste(_normal_pil, box=(\n                int(_center2ds[index_obj, 1] * dataset.Camera.focal + dataset.Camera.u0 - _image_size // 2),\n                int(_center2ds[index_obj, 0] * dataset.Camera.focal + dataset.Camera.v0 - _image_size // 2),\n            ))\n            _image_normal = Transforms.to_tensor(_image_normal_pil).cuda()\n\n            _image_normal_map = (1 - _image_mask) * _image_normal_map + _image_mask * _image_normal\n\n            #\n\n            _depth_map = torch.min(_depth_maps[index_obj] * _zooms[index_obj] / 100.0, torch.tensor(1.0).cuda())\n            _depth_map = _depth_map.detach().cpu().numpy().transpose(1, 2, 0)\n            _depth_pil = Transforms.to_pil_image(_depth_map)\n            _depth_pil = Transforms.resize(_depth_pil, (_image_size, _image_size))\n            _image_depth_pil = PIL.Image.new(mode='F', size=(width, height))\n            _image_depth_pil.paste(_depth_pil, box=(\n                int(_center2ds[index_obj, 1] * dataset.Camera.focal + dataset.Camera.u0 - _image_size // 2),\n                int(_center2ds[index_obj, 0] * dataset.Camera.focal + dataset.Camera.v0 - _image_size // 2),\n            ))\n            _image_depth = Transforms.to_tensor(_image_depth_pil).cuda()\n\n            _image_depth_map = (1 - _image_mask) * _image_depth_map + _image_mask * _image_depth\n\n        elif operations is None:\n            image_mask = image_masks[index_obj]\n\n            _image_instance_map = (1 - image_mask) * _image_instance_map + image_mask * (1 + index_obj)\n\n    with open(os.path.join(image_dir, '{:s}.json'.format(name)), 'w') as f:\n        json.dump(json_obj, f, indent=4)\n\n    (_image_instance_map_pil, _image_vis_pil) = Transforms.visualize(image_rgb, to_numpy(_image_instance_map), to_numpy(rois), to_numpy(interests))\n    _image_instance_map_pil.save(os.path.join(image_dir, '{:s}.png'.format(name)))\n    _image_vis_pil.save(os.path.join(image_dir, '{:s}-visualize.png'.format(name)))\n\n    _image_normal_map_pil = Transforms.to_pil_image(_image_normal_map.detach().cpu())\n    _image_normal_map_pil.save(os.path.join(image_dir, '{:s}-normal.png'.format(name)))\n\n    _image_depth_map = np.uint16(_image_depth_map.detach().cpu().numpy().transpose(1, 2, 0) * 65535)\n    _image_depth_map_pil = PIL.Image.new('I', _image_depth_map.T.shape[1:])\n    _image_depth_map_pil.frombytes(_image_depth_map.tobytes(), 'raw', 'I;16')\n    _image_depth_map_pil.save(os.path.join(image_dir, '{:s}-depth.png'.format(name)))\n\n\ndef test():\n    data_loader = TestLoader()\n    dataset = data_loader.dataset\n    df = dataset.df\n\n    if FLAGS._do == '_test':\n        model = DataParallel(Model().cuda())\n        model.eval()\n        net = TestNet(model=model)\n        net.load(ckpt_dir=FLAGS.ckpt_dir)\n\n    elif FLAGS._do == '_test_2d':\n        model = None\n\n    if FLAGS.source == 'gt':\n        maskrcnn = None\n\n    elif FLAGS.source == 'maskrcnn':\n        if FLAGS.dataset == 'vkitti':\n            num_classes = 3\n        elif FLAGS.dataset == 'cityscapes':\n            num_classes = 2\n\n        class InferenceConfig(Config):\n            NAME = FLAGS.dataset\n            IMAGES_PER_GPU = 1\n            GPU_COUNT = 1\n            NUM_CLASSES = num_classes\n\n        state_dict = torch.load(FLAGS.maskrcnn_path)\n\n        maskrcnn = MaskRCNN(\n            config=InferenceConfig(),\n            model_dir='/tmp',\n        )\n        maskrcnn = maskrcnn.cuda()\n        maskrcnn.load_state_dict(state_dict)\n\n    if FLAGS.edit_json is None:\n        assert FLAGS.input_file is None\n\n        items = np.random.permutation(df.index.unique())\n        operations_list = [None] * len(items)\n        names = [None] * len(items)\n    else:\n        with open(FLAGS.edit_json, 'r') as f:\n            edit_json_objs = json.load(f)\n\n        items = []\n        operations_list = []\n        names = []\n        for edit_json_obj in edit_json_objs:\n            if FLAGS.dataset == 'vkitti':\n                item = (edit_json_obj['world'], edit_json_obj['topic'], int(edit_json_obj['source']))\n                name = edit_json_obj['target']\n            elif FLAGS.dataset == 'cityscapes':\n                item = (edit_json_obj['split'], edit_json_obj['city'], edit_json_obj['seq'], edit_json_obj['source'])\n                name = edit_json_obj['target']\n            else:\n                raise Exception\n\n            items.append(item)\n            operations_list.append(edit_json_obj['operations'])\n            names.append(name)\n\n    for (item, operations, name) in zip(items, operations_list, names):\n        print(item)\n\n        if FLAGS.dataset == 'vkitti':\n            (world, topic, frame) = item\n\n            image_dir = os.path.join(FLAGS.output_dir, FLAGS.dataset, FLAGS.source, world, topic)\n            _name = '{:05d}'.format(frame)\n\n        elif FLAGS.dataset == 'cityscapes':\n            (split, city, seq, frame) = item\n\n            image_dir = os.path.join(FLAGS.output_dir, FLAGS.dataset, FLAGS.source, split, city)\n            _name = '{:s}_{:s}_{:s}'.format(city, seq, frame)\n\n        name = name or _name\n\n        lock_path = os.path.join(image_dir, '{:s}.lock'.format(name))\n        if os.path.isfile(lock_path):\n            print('Skipped')\n            continue\n        else:\n            if not os.path.isdir(image_dir):\n                os.makedirs(image_dir)\n\n            with open(lock_path, 'w') as f:\n                pass\n\n        metas = None\n        if FLAGS.input_file is None:\n            image_rgb = dataset.read_rgb(*item)\n        else:\n            image_rgb = np.asarray(PIL.Image.open(FLAGS.input_file))\n\n        if FLAGS.source == 'gt':\n            class_ids = []\n            image_masks = []\n            image_ignores = []\n            rois = []\n\n            if FLAGS.dataset == 'vkitti':\n                image_scene = dataset.read_scene(world, topic, frame)\n\n                metas = []\n                slice_obj = dataset.motgt_df.index.get_loc(item)\n                indices = range(len(dataset.motgt_df))[slice_obj]\n                item_objs = list(map(dataset.motgt_df.iloc.__getitem__, indices))\n\n                for item_obj in item_objs:\n                    code = dataset.scenegt_df.loc[(\n                        world,\n                        topic,\n                        '{:s}:{:d}'.format(item_obj.orig_label, item_obj.tid),\n                    )].values.astype(np.uint8)\n\n                    image_mask = Transforms.scene_to_mask(image_scene, code)\n                    roi = Transforms.mask_to_roi(image_mask)\n\n                    image_mask = np.transpose(image_mask, (2, 0, 1))\n\n                    if item_obj.orig_label == 'Car':\n                        class_id = 1\n                    elif item_obj.orig_label == 'Van':\n                        class_id = 2\n\n                    class_ids.append(class_id)\n                    image_masks.append(image_mask)\n                    image_ignores = None\n                    rois.append(roi)\n                    metas.append({\n                        'tid': int(item_obj.tid),\n                    })\n\n            elif FLAGS.dataset == 'cityscapes':\n                image_scene = dataset.read_scene(split, city, seq, frame)\n                image_disparity = dataset.read_disparity(split, city, seq, frame)\n\n                obj_indices = np.unique(image_scene)\n                for obj_index in obj_indices:\n                    if not dataset.index2cat(obj_index) == dataset.Category.car:\n                        continue\n\n                    image_mask = Transforms.scene_to_mask(image_scene, [obj_index])\n                    roi = Transforms.mask_to_roi(image_mask)\n\n                    disparity = image_disparity[image_mask.astype(np.bool)]\n                    disparity = disparity[disparity != 0]\n                    if disparity.size:\n                        disparity = np.percentile(disparity, 95)\n                    else:\n                        disparity = 0\n                    image_ignore = Transforms.scene_to_mask(image_disparity > disparity, 1.0)\n\n                    image_mask = np.transpose(image_mask, (2, 0, 1))\n                    image_ignore = np.transpose(image_ignore, (2, 0, 1))\n\n                    class_ids.append(1)\n                    image_masks.append(image_mask)\n                    image_ignores.append(image_ignore)\n                    rois.append(roi)\n\n            class_ids = np.stack(class_ids, axis=0)\n            image_masks = np.stack(image_masks, axis=0)\n            if image_ignores is not None:\n                image_ignores = np.stack(image_ignores, axis=0)\n            rois = np.stack(rois, axis=0)\n\n        elif FLAGS.source == 'maskrcnn':\n            try:\n                detections = maskrcnn.detect([image_rgb])[0]\n\n                class_ids = detections['class_ids']\n                image_masks = detections['masks']\n                rois = detections['rois']\n\n                image_masks = np.transpose(image_masks, (2, 0, 1))\n                image_masks = np.expand_dims(image_masks, axis=1)\n                image_ignores = None\n\n            except Exception:\n                continue\n\n        sels = np.flipud(np.argsort(np.sum(image_masks, axis=(1, 2, 3))))[:min(len(class_ids), 16)]\n\n        class_ids = class_ids[sels]\n        image_masks = image_masks[sels]\n        if image_ignores is not None:\n            image_ignores = image_ignores[sels]\n        rois = rois[sels]\n\n        if FLAGS._do == '_test':\n            _test(\n                dataset,\n                model,\n                image_dir,\n                name,\n                image_rgb,\n                class_ids,\n                image_masks,\n                image_ignores,\n                rois,\n                metas,\n                operations,\n            )\n        elif (FLAGS._do == '_test_2d') or (FLAGS._do == '_test_2d_plus'):\n            globals()[FLAGS._do](\n                dataset,\n                image_dir,\n                name,\n                image_rgb,\n                class_ids,\n                image_masks,\n                rois,\n                operations,\n            )\n\n\nif __name__ == '__main__':\n    argv = FLAGS(sys.argv)\n\n    if FLAGS.mode is not None:\n        FLAGS.mode = TargetType.__dict__[FLAGS.mode]\n\n    locals()[FLAGS.do]()\n"""
textural/data/__init__.py,0,b''
textural/data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n'"
textural/data/base_dataset.py,1,"b""# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.resize_or_crop == 'resize_and_crop':\n        new_h = new_w = opt.loadSize\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        new_w = opt.loadSize\n        new_h = opt.loadSize * h // w\n\n    if opt.isTrain:  # training phase: random crop\n        x = random.randint(0, np.maximum(0, new_w - opt.fineWidth))\n        y = random.randint(0, np.maximum(0, new_h - opt.fineHeight))\n    else:           # non-training phase: central crop\n        x = np.maximum(0, new_w - opt.fineWidth) // 2\n        y = np.maximum(0, new_h - opt.fineHeight) // 2\n    flip = random.random() > 0.5\n    return {'crop_pos': (x, y), 'flip': flip}\n\n\ndef get_transform(opt, params, method=Image.BICUBIC, normalize=True):\n    transform_list = []\n    if 'resize' in opt.resize_or_crop:\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, method))\n    elif 'scale_width' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.loadSize, method)))\n\n    if 'crop' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.fineWidth, opt.fineHeight)))\n\n    if opt.resize_or_crop == 'none':\n        base = float(2 ** opt.n_downsample_global)\n        if opt.netG == 'local':\n            base *= (2 ** opt.n_local_enhancers)\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n\n    transform_list += [transforms.ToTensor()]\n\n    if normalize:\n        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n                                                (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n\ndef normalize():\n    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size\n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n    return img.resize((w, h), method)\n\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    if h == 188:\n        h = 192  # hack, make sure resized to (192, 624) instead of (188, 624) because 192 = 2^6 * 3\n    return img.resize((w, h), method)\n\n\ndef __crop(img, pos, tw, th):\n    ow, oh = img.size\n    x1, y1 = pos\n    if (ow > tw or oh > th):\n        return img.crop((x1, y1, x1 + tw, y1 + th))\n    return img\n\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n"""
textural/data/cityscapes_dataset.py,2,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os.path\nimport random\nimport torch\nimport numpy as np\nfrom data.base_dataset import BaseDataset, get_params, get_transform, normalize\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\nimport data.cityscapes_labels as cityscapes_labels\n\n\nclass CustomDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.phase = \'train\' if opt.isTrain else \'val\'\n\n        # Obtain A_paths, B_paths, inst_paths\n        self.getlists(opt)\n\n        random.Random(20).shuffle(self.A_paths)\n        random.Random(20).shuffle(self.B_paths)\n        random.Random(20).shuffle(self.inst_paths)\n        random.Random(20).shuffle(self.pose_paths)\n        random.Random(20).shuffle(self.normal_paths)\n        self.dataset_size = len(self.A_paths)\n        print(\'{}_set size: {}\'.format(self.phase, self.dataset_size))\n\n    def __getitem__(self, index):\n        # input A (label maps)\n        A_path = self.A_paths[index]\n        A = Image.open(A_path)\n        params = get_params(self.opt, A.size)\n        if self.opt.label_nc == 0:\n            transform_A = get_transform(self.opt, params)\n            A_tensor = transform_A(A.convert(\'RGB\'))\n        else:\n            transform_A = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n            A_tensor = transform_A(A) * 255.0\n\n        B_tensor = inst_tensor = feat_tensor = pose_tensor = normal_tensor = depth_tensor = 0\n\n        # input B (real images)\n        if self.opt.isTrain or True:\n            B_path = self.B_paths[index]\n            B = Image.open(B_path).convert(\'RGB\')\n            #if self.use_augmentation: B = self.img_jitter(B)\n            transform_B = get_transform(self.opt, params)\n            B_tensor = transform_B(B)\n\n        # if using instance maps\n        if not self.opt.no_instance:\n            try:\n                inst_path = self.inst_paths[index]\n                inst = Image.open(inst_path)\n                inst_tensor = transform_A(inst)\n                if self.opt.inst_precomputed_path:\n                    inst_tensor = inst_tensor * 255.0\n                    inst_tensor *= 1000\n                    inst_tensor[inst_tensor == 0] = A_tensor[inst_tensor == 0]\n            except FileNotFoundError:\n                inst_tensor = A_tensor\n\n        # if using pose info\n        if self.opt.feat_pose:  # add pose info to features\n            if self.opt.feat_pose_num_bins > 0:\n                pose_tensor = np.zeros((1, A_tensor.size(1), A_tensor.size(2)))\n            else:\n                pose_tensor = np.zeros((2, A_tensor.size(1), A_tensor.size(2)))\n            try:\n                d = json.load(open(self.pose_paths[index]))\n                inst_map = Image.open(self.pose_paths[index].replace(\'.json\', \'.png\'))\n                inst_map = transform_A(inst_map) * 255.0\n                inst_map = inst_map.numpy()[0]\n                from math import pi, cos, sin\n                if self.opt.feat_pose_num_bins:\n                    bins = np.array(list(range(-180, 181, 360 // self.opt.feat_pose_num_bins))) / 180\n                for inst in np.unique(inst_map):\n                    if inst == 0 or (inst_map == inst).sum() < 256:\n                        continue\n                    alpha = d[str(int(inst))][""alpha""]\n                    if self.opt.feat_pose_num_bins > 0:\n                        pose_tensor[0, inst_map == inst] = np.digitize(alpha / pi, bins)\n                    else:\n                        pose_tensor[0, inst_map == inst] = cos(alpha)\n                        pose_tensor[1, inst_map == inst] = sin(alpha)\n            except FileNotFoundError:  # no cars\n                pass\n            pose_tensor = torch.from_numpy(pose_tensor)\n            pose_tensor = pose_tensor.int() if self.opt.feat_pose_num_bins else pose_tensor.float()\n\n        if self.opt.feat_normal:  # add normal info to features\n            try:\n                normal_map = Image.open(self.normal_paths[index])\n                normal_tensor = transform_B(normal_map) + 1 / 255  # bias caused by 0..256 instead 0..255\n            except FileNotFoundError:  # no cars\n                normal_tensor = torch.zeros(B_tensor.size())\n\n        if not self.opt.segm_precomputed_path:\n            _A_tensor = A_tensor.clone()\n            for label in self.labels:\n                A_tensor[_A_tensor == label.id] = label.trainId + 1 if label.trainId != 255 else 0\n\n        input_dict = {\'label\': A_tensor, \'inst\': inst_tensor, \'image\': B_tensor,\n                      \'feat\': feat_tensor, \'path\': A_path,\n                      \'pose\': pose_tensor, \'normal\': normal_tensor, \'depth\': depth_tensor}\n\n        return input_dict\n\n    def getlists(self, opt):\n        subset = ""train"" if opt.isTrain else ""val""\n\n        annotations = json.load(open(""{}/annotations/instancesonly_gtFine_{}.json"".format(self.root, subset)))[\'images\']\n        self.A_paths, self.B_paths, self.inst_paths = [], [], []\n        self.pose_paths, self.normal_paths = [], []\n        for item in annotations:\n            city = item[\'file_name\'].split(\'_\')[0]\n            name = item[\'file_name\']  # example: darmstadt_000035_000019_leftImg8bit.png\n            if opt.segm_precomputed_path:\n                self.A_paths.append(os.path.join(opt.segm_precomputed_path, city, name))\n            else:\n                self.A_paths.append(os.path.join(self.root, ""gtFine"", subset, city, item[\'seg_file_name\'].replace(\'instance\', \'label\')))\n            self.B_paths.append(os.path.join(self.root, ""images"", name))\n            if opt.inst_precomputed_path:\n                self.inst_paths.append(os.path.join(opt.inst_precomputed_path, city, name.replace(\'_leftImg8bit\', \'\')))\n            else:\n                self.inst_paths.append(os.path.join(self.root, ""gtFine"", subset, city, item[\'seg_file_name\']))\n            if opt.feat_pose:\n                self.pose_paths.append(os.path.join(opt.feat_pose, city, name.replace(\'_leftImg8bit.png\', \'.json\')))\n            if opt.feat_normal:\n                self.normal_paths.append(os.path.join(opt.feat_normal, city, name.replace(\'_leftImg8bit.png\', \'-normal.png\')))\n        self.labels = cityscapes_labels.labels\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return \'Cityscapes CustomDataset\'\n'"
textural/data/cityscapes_labels.py,0,"b'#!/usr/bin/python\n#\n# Cityscapes labels\n#\n\nfrom collections import namedtuple\n\n\n# --------------------------------------------------------------------------------\n# Definitions\n# --------------------------------------------------------------------------------\n\n# a label and all meta information\nLabel = namedtuple(\'Label\', [\n\n    \'name\',  # The identifier of this label, e.g. \'car\', \'person\', ... .\n    # We use them to uniquely name a class\n\n    \'id\',  # An integer ID that is associated with this label.\n    # The IDs are used to represent the label in ground truth images\n    # An ID of -1 means that this label does not have an ID and thus\n    # is ignored when creating ground truth images (e.g. license plate).\n    # Do not modify these IDs, since exactly these IDs are expected by the\n    # evaluation server.\n\n    \'trainId\',  # Feel free to modify these IDs as suitable for your method. Then create\n    # ground truth images with train IDs, using the tools provided in the\n    # \'preparation\' folder. However, make sure to validate or submit results\n    # to our evaluation server using the regular IDs above!\n    # For trainIds, multiple labels might have the same ID. Then, these labels\n    # are mapped to the same class in the ground truth images. For the inverse\n    # mapping, we use the label that is defined first in the list below.\n    # For example, mapping all void-type classes to the same ID in training,\n    # might make sense for some approaches.\n    # Max value is 255!\n\n    \'category\',  # The name of the category that this label belongs to\n\n    \'categoryId\',  # The ID of this category. Used to create ground truth images\n    # on category level.\n\n    \'hasInstances\',  # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\',  # Whether pixels having this class as ground truth label are ignored\n    # during evaluations or not\n\n    \'color\',  # The color of this label\n])\n\n\n# --------------------------------------------------------------------------------\n# A list of all labels\n# --------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for your approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(\'unlabeled\', 0, 255, \'void\', 0, False, True, (0, 0, 0)),\n    Label(\'ego vehicle\', 1, 255, \'void\', 0, False, True, (0, 0, 0)),\n    Label(\'rectification border\', 2, 255, \'void\', 0, False, True, (0, 0, 0)),\n    Label(\'out of roi\', 3, 255, \'void\', 0, False, True, (0, 0, 0)),\n    Label(\'static\', 4, 255, \'void\', 0, False, True, (0, 0, 0)),\n    Label(\'dynamic\', 5, 255, \'void\', 0, False, True, (111, 74, 0)),\n    Label(\'ground\', 6, 255, \'void\', 0, False, True, (81, 0, 81)),\n    Label(\'road\', 7, 0, \'flat\', 1, False, False, (128, 64, 128)),\n    Label(\'sidewalk\', 8, 1, \'flat\', 1, False, False, (244, 35, 232)),\n    Label(\'parking\', 9, 255, \'flat\', 1, False, True, (250, 170, 160)),\n    Label(\'rail track\', 10, 255, \'flat\', 1, False, True, (230, 150, 140)),\n    Label(\'building\', 11, 2, \'construction\', 2, False, False, (70, 70, 70)),\n    Label(\'wall\', 12, 3, \'construction\', 2, False, False, (102, 102, 156)),\n    Label(\'fence\', 13, 4, \'construction\', 2, False, False, (190, 153, 153)),\n    Label(\'guard rail\', 14, 255, \'construction\', 2, False, True, (180, 165, 180)),\n    Label(\'bridge\', 15, 255, \'construction\', 2, False, True, (150, 100, 100)),\n    Label(\'tunnel\', 16, 255, \'construction\', 2, False, True, (150, 120, 90)),\n    Label(\'pole\', 17, 5, \'object\', 3, False, False, (153, 153, 153)),\n    Label(\'polegroup\', 18, 255, \'object\', 3, False, True, (153, 153, 153)),\n    Label(\'traffic light\', 19, 6, \'object\', 3, False, False, (250, 170, 30)),\n    Label(\'traffic sign\', 20, 7, \'object\', 3, False, False, (220, 220, 0)),\n    Label(\'vegetation\', 21, 8, \'nature\', 4, False, False, (107, 142, 35)),\n    Label(\'terrain\', 22, 9, \'nature\', 4, False, False, (152, 251, 152)),\n    Label(\'sky\', 23, 10, \'sky\', 5, False, False, (70, 130, 180)),\n    Label(\'person\', 24, 11, \'human\', 6, True, False, (220, 20, 60)),\n    Label(\'rider\', 25, 12, \'human\', 6, True, False, (255, 0, 0)),\n    Label(\'car\', 26, 13, \'vehicle\', 7, True, False, (0, 0, 142)),\n    Label(\'truck\', 27, 14, \'vehicle\', 7, True, False, (0, 0, 70)),\n    Label(\'bus\', 28, 15, \'vehicle\', 7, True, False, (0, 60, 100)),\n    Label(\'caravan\', 29, 255, \'vehicle\', 7, True, True, (0, 0, 90)),\n    Label(\'trailer\', 30, 255, \'vehicle\', 7, True, True, (0, 0, 110)),\n    Label(\'train\', 31, 16, \'vehicle\', 7, True, False, (0, 80, 100)),\n    Label(\'motorcycle\', 32, 17, \'vehicle\', 7, True, False, (0, 0, 230)),\n    Label(\'bicycle\', 33, 18, \'vehicle\', 7, True, False, (119, 11, 32)),\n    Label(\'license plate\', -1, -1, \'vehicle\', 7, False, True, (0, 0, 142)),\n]\n\n\n# --------------------------------------------------------------------------------\n# Create dictionaries for a fast lookup\n# --------------------------------------------------------------------------------\n\n# Please refer to the main method below for example usages!\n\n# name to label object\nname2label = {label.name: label for label in labels}\n# id to label object\nid2label = {label.id: label for label in labels}\n# trainId to label object\ntrainId2label = {label.trainId: label for label in reversed(labels)}\n# category to list of label objects\ncategory2labels = {}\nfor label in labels:\n    category = label.category\n    if category in category2labels:\n        category2labels[category].append(label)\n    else:\n        category2labels[category] = [label]\n\n# --------------------------------------------------------------------------------\n# Assure single instance name\n# --------------------------------------------------------------------------------\n\n# returns the label name that describes a single instance (if possible)\n# e.g.     input     |   output\n#        ----------------------\n#          car       |   car\n#          cargroup  |   car\n#          foo       |   None\n#          foogroup  |   None\n#          skygroup  |   None\n\n\ndef assureSingleInstanceName(name):\n    # if the name is known, it is not a group\n    if name in name2label:\n        return name\n    # test if the name actually denotes a group\n    if not name.endswith(""group""):\n        return None\n    # remove group\n    name = name[:-len(""group"")]\n    # test if the new name exists\n    if not name in name2label:\n        return None\n    # test if the new name denotes a label that actually has instances\n    if not name2label[name].hasInstances:\n        return None\n    # all good then\n    return name\n\n# --------------------------------------------------------------------------------\n# Main for testing\n# --------------------------------------------------------------------------------\n\n\n# just a dummy main\nif __name__ == ""__main__"":\n    # Print all the labels\n    print(""List of cityscapes labels:"")\n    print("""")\n    print(""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format(\'name\', \'id\', \'trainId\', \'category\', \'categoryId\', \'hasInstances\', \'ignoreInEval\'))\n    print(""    "" + (\'-\' * 98))\n    for label in labels:\n        print(""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format(label.name, label.id, label.trainId, label.category, label.categoryId, label.hasInstances, label.ignoreInEval))\n    print("""")\n\n    print(""Example usages:"")\n\n    # Map from name to label\n    name = \'car\'\n    id = name2label[name].id\n    print(""ID of label \'{name}\': {id}"".format(name=name, id=id))\n\n    # Map from ID to label\n    category = id2label[id].category\n    print(""Category of label with ID \'{id}\': {category}"".format(id=id, category=category))\n\n    # Map from trainID to label\n    trainId = 0\n    name = trainId2label[trainId].name\n    print(""Name of label with trainID \'{id}\': {name}"".format(id=trainId, name=name))\n'"
textural/data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    if opt.name.find(\'cityscapes\') != -1:\n        from data.cityscapes_dataset import CustomDataset\n    else:\n        from data.vkitti_dataset import CustomDataset\n    dataset = CustomDataset()\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
textural/data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
textural/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\', \'.tiff\'\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
textural/data/vkitti_dataset.py,4,"b'import os.path\nimport random\nimport torch\nimport numpy as np\nimport pickle\nfrom data.base_dataset import BaseDataset, get_params, get_transform, normalize\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport json\nfrom torchvision import transforms\nimport time\n\n\nclass CustomDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.phase = \'train\' if opt.isTrain else \'test\'\n        self.root = opt.dataroot\n        self.root_img = os.path.join(self.root, ""vkitti_1.3.1_rgb"")\n        self.root_segm = os.path.join(self.root, ""vkitti_1.3.1_myscenegt"") \\\n            if not opt.segm_precomputed_path else opt.segm_precomputed_path\n        self.root_inst = os.path.join(self.root, ""vkitti_1.3.1_inst"") \\\n            if not opt.inst_precomputed_path else opt.inst_precomputed_path\n\n        worldIds = [\'0001\', \'0002\', \'0006\', \'0018\', \'0020\']\n        sceneIds = [\'15-deg-left\', \'15-deg-right\', \'30-deg-left\', \'30-deg-right\', \'clone\', \'fog\', \'morning\', \'overcast\', \'rain\', \'sunset\']\n        worldSizes = [446, 232, 269, 338, 836]\n        splitRanges = {\'train\': [range(0, 356), range(0, 185), range(69, 270), range(0, 270), range(167, 837)],\n                       \'test\': [range(356, 447), range(185, 233), range(0, 69), range(270, 339), range(0, 167)]}\n        self.list = []\n        for worldId in worldIds:\n            for sceneId in sceneIds:\n                for imgId in splitRanges[self.phase][worldIds.index(worldId)]:\n                    self.list += [\'%s/%s/%05d.png\' % (worldId, sceneId, imgId)]\n\n        self.use_augmentation = opt.isTrain and opt.use_augmentation\n        if self.use_augmentation:\n            self.img_jitter = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)\n\n        random.Random().shuffle(self.list)\n        self.dataset_size = len(self.list)\n        print(\'{}_set size: {}\'.format(self.phase, self.dataset_size))\n\n    def __getitem__(self, index):\n        # input A (label maps)\n        A_path = os.path.join(self.root_segm, self.list[index])\n        A = Image.open(A_path)\n        params = get_params(self.opt, A.size)\n        if self.opt.label_nc == 0:\n            transform_A = get_transform(self.opt, params)\n            A_tensor = transform_A(A.convert(\'RGB\'))\n        else:\n            transform_A = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n            A_tensor = transform_A(A) * 255.0\n        if self.opt.segm_precomputed_path:  # NOTE: due to the model, have to add 1\n            A_tensor = A_tensor + 1\n        B_tensor = inst_tensor = feat_tensor = pose_tensor = normal_tensor = depth_tensor = 0\n\n        # input B (real images)\n        B_path = os.path.join(self.root_img, self.list[index])\n        B = Image.open(B_path).convert(\'RGB\')\n        if self.use_augmentation:\n            B = self.img_jitter(B)\n        transform_B = get_transform(self.opt, params)\n        B_tensor = transform_B(B)\n\n        # if using instance maps\n        if not self.opt.no_instance:\n            try:\n                inst_path = os.path.join(self.root_inst, self.list[index])\n                inst = Image.open(inst_path)\n                inst_tensor = transform_A(inst)\n                if self.opt.inst_precomputed_path:  # need segm to fill in background segments\n                    inst_tensor = inst_tensor * 255.0\n                    inst_tensor *= 1000  # different from semantic labels\n                    if self.opt.segm_precomputed_path:\n                        # use car semantic information from inst map; now only remove\n                        A_tensor[(inst_tensor == 0) & (A_tensor == 2)] = 5\n                        A_tensor[(inst_tensor == 0) & (A_tensor == 12)] = 5\n                    inst_tensor[inst_tensor == 0] = A_tensor[inst_tensor == 0]\n\n                if self.opt.load_features:  # TODO: add load_feature\n                    feat_path = self.feat_paths[index]\n                    feat = Image.open(feat_path).convert(\'RGB\')\n                    norm = normalize()\n                    feat_tensor = norm(transform_A(feat))\n            except FileNotFoundError:\n                inst_tensor = A_tensor\n                #inst_tensor = torch.zeros(A_tensor.size())\n\n        if self.opt.feat_pose:  # add pose info to features\n            if self.opt.feat_pose_num_bins > 0:\n                pose_tensor = np.zeros((1, A_tensor.size(1), A_tensor.size(2)))\n            else:\n                pose_tensor = np.zeros((2, A_tensor.size(1), A_tensor.size(2)))\n            try:\n                dict_path = os.path.join(self.opt.feat_pose, self.list[index])\n                d = json.load(open(dict_path.replace(\'png\', \'json\')))\n                inst_map = Image.open(dict_path)\n                inst_map = transform_A(inst_map) * 255.0\n                inst_map = inst_map.numpy()[0]\n                from math import pi, cos, sin\n                if self.opt.feat_pose_num_bins:\n                    bins = np.array(list(range(-180, 181, 360 // self.opt.feat_pose_num_bins))) / 180\n                for inst in np.unique(inst_map):\n                    if inst == 0:\n                        continue\n                    if not str(int(inst)) in d:\n                        continue\n                    #assert str(int(inst)) in d, (self.list[index], d.keys(), np.unique(inst_map))\n                    alpha = d[str(int(inst))][""alpha""]\n                    if self.opt.feat_pose_num_bins > 0:\n                        pose_tensor[0, inst_map == inst] = np.digitize(alpha / pi, bins)\n                    else:\n                        pose_tensor[0, inst_map == inst] = cos(alpha)\n                        pose_tensor[1, inst_map == inst] = sin(alpha)\n            except FileNotFoundError:  # no cars\n                pass\n            pose_tensor = torch.from_numpy(pose_tensor)\n            pose_tensor = pose_tensor.int() if self.opt.feat_pose_num_bins else pose_tensor.float()\n\n        if self.opt.feat_normal:  # add normal info to features\n            try:\n                normal_map = Image.open(os.path.join(self.opt.feat_normal, self.list[index].replace(\'.png\', \'-normal.png\')))\n                normal_tensor = transform_B(normal_map) + 1 / 255  # bias caused by 0..256 instead of 0..255\n            except FileNotFoundError:  # no cars\n                #print(self.list[index], \'has no car\')\n                normal_tensor = torch.zeros(B_tensor.size())\n\n        if self.opt.feat_depth:  # add depth info to features\n            try:\n                depth_map = Image.open(os.path.join(self.opt.feat_depth, self.list[index].replace(\'.png\', \'-depth.png\')))\n                depth_tensor = transform_A(depth_map)\n                depth_tensor = 1.0 - depth_tensor.float() / 65535.0\n            except FileNotFoundError:\n                depth_tensor = torch.zeros(A_tensor.size())\n\n        input_dict = {\'label\': A_tensor, \'inst\': inst_tensor, \'image\': B_tensor,\n                      \'feat\': feat_tensor, \'path\': self.list[index],\n                      \'pose\': pose_tensor, \'normal\': normal_tensor, \'depth\': depth_tensor}\n\n        return input_dict\n\n    def __len__(self):\n        return self.dataset_size\n\n    def name(self):\n        return \'VKitti CustomDataset\'\n'"
textural/models/__init__.py,0,b''
textural/models/base_model.py,8,"b""# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os\nimport torch\nimport sys\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\n\n\nclass BaseModel(torch.nn.Module):\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda()\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label, save_dir=''):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        if not save_dir:\n            save_dir = self.save_dir\n        save_path = os.path.join(save_dir, save_filename)\n        if not os.path.isfile(save_path):\n            print('%s not exists yet!' % save_path)\n            if network_label == 'G':\n                raise('Generator must exist!')\n        else:\n            print(save_path)\n            pretrained_dict = torch.load(save_path)\n            if next(iter(pretrained_dict.keys()))[:6] == 'module':\n                new_pretrained_dict = OrderedDict()\n                for k, v in pretrained_dict.items():\n                    new_pretrained_dict[k[7:]] = v\n                pretrained_dict = new_pretrained_dict\n            try:\n                network.load_state_dict(pretrained_dict)\n            except:\n                model_dict = network.state_dict()\n                try:\n                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n                    network.load_state_dict(pretrained_dict)\n                    print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n                except:\n                    print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n                    if sys.version_info >= (3, 0):\n                        not_initialized = set()\n                    else:\n                        from sets import Set\n                        not_initialized = Set()\n                    for k, v in pretrained_dict.items():\n                        if v.size() == model_dict[k].size():\n                            model_dict[k] = v\n\n                    for k, v in model_dict.items():\n                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n                            not_initialized.add(k.split('.')[0])\n                    print(sorted(not_initialized))\n                    network.load_state_dict(model_dict)\n\n    def get_z_random(self, batchSize, nz, random_type='gauss'):\n        z = self.Tensor(batchSize, nz)\n        if random_type == 'uni':\n            z.copy_(torch.rand(batchSize, nz) * 2.0 - 1.0)\n        elif random_type == 'gauss':\n            z.copy_(torch.randn(batchSize, nz))\n            z = Variable(z)\n        return z\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    def update_learning_rate():\n        pass\n"""
textural/models/models.py,1,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\n\n\ndef create_model(opt):\n    if opt.model == \'pix2pixHD\':\n        from .pix2pixHD_model import Pix2PixHDModel\n        model = Pix2PixHDModel()\n    else:\n        from .ui_model import UIModel\n        model = UIModel()\n    model.initialize(opt)\n    print(""model [%s] was created"" % (model.name()))\n\n    if opt.isTrain and len(opt.gpu_ids):\n        model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\n\n    return model\n'"
textural/models/networks.py,14,"b""# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\nimport torch.nn as nn\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\n\n###############################################################################\n# Functions\n###############################################################################\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_non_linearity(layer_type='relu'):\n    if layer_type == 'relu':\n        nl_layer = functools.partial(nn.ReLU, inplace=True)\n    elif layer_type == 'lrelu':\n        nl_layer = functools.partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)\n    elif layer_type == 'elu':\n        nl_layer = functools.partial(nn.ELU, inplace=True)\n    else:\n        raise NotImplementedError('nonlinearity activitation [%s] is not found' % layer_type)\n    return nl_layer\n\n\ndef define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1,\n             n_blocks_local=3, norm='instance', gpu_ids=[], isTrain=False):\n    norm_layer = get_norm_layer(norm_type=norm)\n    if netG == 'global':\n        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)\n    elif netG == 'local':\n        netG = LocalEnhancer(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global,\n                             n_local_enhancers, n_blocks_local, norm_layer)\n    elif netG == 'encoder':\n        netG = Encoder(input_nc, output_nc, ngf, n_downsample_global, norm_layer, isTrain=isTrain)\n    else:\n        raise('generator not implemented!')\n    # print(netG)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        netG.cuda(gpu_ids[0])\n        #netG = nn.DataParallel(netG).cuda()\n    netG.apply(weights_init)\n    return netG\n\n\ndef define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1, getIntermFeat=False, gpu_ids=[]):\n    norm_layer = get_norm_layer(norm_type=norm)\n    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)\n    # print(netD)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        netD.cuda(gpu_ids[0])\n        #netD = nn.DataParallel(netD).cuda()\n    netD.apply(weights_init)\n    return netD\n\n\ndef print_network(net):\n    if isinstance(net, list):\n        net = net[0]\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n##############################################################################\n# Losses\n##############################################################################\n\n\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        if isinstance(input[0], list):\n            loss = 0\n            for input_i in input:\n                pred = input_i[-1]\n                target_tensor = self.get_target_tensor(pred, target_is_real)\n                loss += self.loss(pred, target_tensor)\n            return loss\n        else:\n            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n            return self.loss(input[-1], target_tensor)\n\n\nclass VGGLoss(nn.Module):\n    def __init__(self, gpu_ids):\n        super(VGGLoss, self).__init__()\n        self.vgg = Vgg19().cuda()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n\n    def forward(self, x, y):\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n        return loss\n\n##############################################################################\n# Generator\n##############################################################################\n\n\nclass LocalEnhancer(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsample_global=3, n_blocks_global=9,\n                 n_local_enhancers=1, n_blocks_local=3, norm_layer=nn.BatchNorm2d, padding_type='reflect'):\n        super(LocalEnhancer, self).__init__()\n        self.n_local_enhancers = n_local_enhancers\n\n        ###### global generator model #####\n        ngf_global = ngf * (2**n_local_enhancers)\n        model_global = GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model\n        model_global = [model_global[i] for i in range(len(model_global) - 3)]  # get rid of final convolution layers\n        self.model = nn.Sequential(*model_global)\n\n        ###### local enhancer layers #####\n        for n in range(1, n_local_enhancers + 1):\n            # downsample\n            ngf_global = ngf * (2**(n_local_enhancers - n))\n            model_downsample = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size=7, padding=0),\n                                norm_layer(ngf_global), nn.ReLU(True),\n                                nn.Conv2d(ngf_global, ngf_global * 2, kernel_size=3, stride=2, padding=1),\n                                norm_layer(ngf_global * 2), nn.ReLU(True)]\n            # residual blocks\n            model_upsample = []\n            for i in range(n_blocks_local):\n                model_upsample += [ResnetBlock(ngf_global * 2, padding_type=padding_type, norm_layer=norm_layer)]\n\n            # upsample\n            model_upsample += [nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size=3, stride=2, padding=1, output_padding=1),\n                               norm_layer(ngf_global), nn.ReLU(True)]\n\n            # final convolution\n            if n == n_local_enhancers:\n                model_upsample += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n\n            setattr(self, 'model' + str(n) + '_1', nn.Sequential(*model_downsample))\n            setattr(self, 'model' + str(n) + '_2', nn.Sequential(*model_upsample))\n\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def forward(self, input):\n        # create input pyramid\n        input_downsampled = [input]\n        for i in range(self.n_local_enhancers):\n            input_downsampled.append(self.downsample(input_downsampled[-1]))\n\n        # output at coarest level\n        output_prev = self.model(input_downsampled[-1])\n        # build up one layer at a time\n        for n_local_enhancers in range(1, self.n_local_enhancers + 1):\n            model_downsample = getattr(self, 'model' + str(n_local_enhancers) + '_1')\n            model_upsample = getattr(self, 'model' + str(n_local_enhancers) + '_2')\n            input_i = input_downsampled[self.n_local_enhancers - n_local_enhancers]\n            output_prev = model_upsample(model_downsample(input_i) + output_prev)\n        return output_prev\n\n\nclass GlobalGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d,\n                 padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(GlobalGenerator, self).__init__()\n        activation = nn.ReLU(True)\n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        # downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        # resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n\n        # upsample\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                      norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n# Define a resnet block\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       activation]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsampling=4, norm_layer=nn.BatchNorm2d, isTrain=True):\n        super(Encoder, self).__init__()\n        self.isTrain = isTrain\n        self.output_nc = output_nc\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n                 norm_layer(ngf), nn.ReLU(True)]\n        # downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n\n        # upsample\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                      norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]\n\n        model += [nn.ReflectionPad2d(3),\n                  nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input, inst):  # instance-wise average pooling\n        outputs = self.model(input)\n        outputs_mean = outputs.clone()\n        # make sure that same inst in different images won't get mixed\n        batchSize = inst.size(0)\n        for i in range(batchSize):\n            inst[i] = inst[i] * batchSize + i\n        inst_list = np.unique(inst.cpu().numpy().astype(int))\n        for i in inst_list:\n            i = int(i)\n            assert np.any(inst == i), (inst_list, inst)\n            indices = (inst == i).nonzero()  # n x 4\n            for j in range(self.output_nc):\n                output_ins = outputs[indices[:, 0], indices[:, 1] + j, indices[:, 2], indices[:, 3]]\n                mean_feat = torch.mean(output_ins).expand_as(output_ins)\n                outputs_mean[indices[:, 0], indices[:, 1] + j, indices[:, 2], indices[:, 3]] = mean_feat\n        return (outputs_mean[:, :self.output_nc, :, :], 0) if self.isTrain else outputs_mean[:, :self.output_nc, :, :]\n\n    def generate_feat_dict(self, input, inst):\n        outputs = self.model(input)\n        # make sure that same inst in different images won't get mixed\n        batchSize = inst.size(0)\n        for i in range(batchSize):\n            inst[i] = inst[i] * batchSize + i\n\n        feat_dict = {}\n        inst_list = np.unique(inst.cpu().numpy().astype(int))\n        for i in inst_list:\n            i = int(i)\n            feat_dict[i] = []\n            indices = (inst == i).nonzero()  # n x 4\n            for j in range(self.output_nc):\n                output_ins = outputs[indices[:, 0], indices[:, 1] + j, indices[:, 2], indices[:, 3]]\n                mean_feat = torch.mean(output_ins)\n                feat_dict[i] += [float(mean_feat)]\n                # feat_dict[i] += [float(torch.randn(1,))]\n        return feat_dict\n\n\ndef conv3x3(in_planes, out_planes):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, bias=True)\n\n\ndef meanpoolConv(inplanes, outplanes):\n    sequence = []\n    sequence += [nn.AvgPool2d(kernel_size=2, stride=2)]\n    sequence += [nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0, bias=True)]\n    return nn.Sequential(*sequence)\n\n\ndef convMeanpool(inplanes, outplanes):\n    sequence = []\n    sequence += [conv3x3(inplanes, outplanes)]\n    sequence += [nn.AvgPool2d(kernel_size=2, stride=2)]\n    return nn.Sequential(*sequence)\n\n\nclass MultiscaleDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d,\n                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n        super(MultiscaleDiscriminator, self).__init__()\n        self.num_D = num_D\n        self.n_layers = n_layers\n        self.getIntermFeat = getIntermFeat\n\n        for i in range(num_D):\n            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n            if getIntermFeat:\n                for j in range(n_layers + 2):\n                    setattr(self, 'scale' + str(i) + '_layer' + str(j), getattr(netD, 'model' + str(j)))\n            else:\n                setattr(self, 'layer' + str(i), netD.model)\n\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def singleD_forward(self, model, input):\n        if self.getIntermFeat:\n            result = [input]\n            for i in range(len(model)):\n                result.append(model[i](result[-1]))\n            return result[1:]\n        else:\n            return [model(input)]\n\n    def forward(self, input):\n        num_D = self.num_D\n        result = []\n        input_downsampled = input\n        for i in range(num_D):\n            if self.getIntermFeat:\n                model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]\n            else:\n                model = getattr(self, 'layer' + str(num_D - 1 - i))\n            result.append(self.singleD_forward(model, input_downsampled))\n            if i != (num_D - 1):\n                input_downsampled = self.downsample(input_downsampled)\n        return result\n\n# Defines the PatchGAN discriminator with the specified arguments.\n\n\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n        super(NLayerDiscriminator, self).__init__()\n        self.getIntermFeat = getIntermFeat\n        self.n_layers = n_layers\n\n        kw = 4\n        padw = int(np.ceil((kw - 1.0) / 2))\n        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n\n        nf = ndf\n        for n in range(1, n_layers):\n            nf_prev = nf\n            nf = min(nf * 2, 512)\n            sequence += [[\n                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n                norm_layer(nf), nn.LeakyReLU(0.2, True)\n            ]]\n\n        nf_prev = nf\n        nf = min(nf * 2, 512)\n        sequence += [[\n            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n            norm_layer(nf),\n            nn.LeakyReLU(0.2, True)\n        ]]\n\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n        if use_sigmoid:\n            sequence += [[nn.Sigmoid()]]\n\n        if getIntermFeat:\n            for n in range(len(sequence)):\n                setattr(self, 'model' + str(n), nn.Sequential(*sequence[n]))\n        else:\n            sequence_stream = []\n            for n in range(len(sequence)):\n                sequence_stream += sequence[n]\n            self.model = nn.Sequential(*sequence_stream)\n\n    def forward(self, input):\n        if self.getIntermFeat:\n            res = [input]\n            for n in range(self.n_layers + 2):\n                model = getattr(self, 'model' + str(n))\n                res.append(model(res[-1]))\n            return res[1:]\n        else:\n            return self.model(input)\n\n\nfrom torchvision import models\n\n\nclass Vgg19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)\n        h_relu3 = self.slice3(h_relu2)\n        h_relu4 = self.slice4(h_relu3)\n        h_relu5 = self.slice5(h_relu4)\n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n"""
textural/models/pix2pixHD_model.py,31,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport numpy as np\nimport torch\nimport os\nfrom torch.autograd import Variable\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass Pix2PixHDModel(BaseModel):\n    def name(self):\n        return \'Pix2PixHDModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        if opt.resize_or_crop != \'none\':  # when training at full res this causes OOM\n            torch.backends.cudnn.benchmark = True\n        self.isTrain = opt.isTrain\n        self.use_features = opt.instance_feat or opt.label_feat\n        self.gen_features = self.use_features and not self.opt.load_features\n        self.no_global_encoder = opt.no_global_encoder\n        input_nc = opt.label_nc if opt.label_nc != 0 else 3\n\n        # define networks\n        self.model_names = []\n\n        # Generator network\n        netG_input_nc = input_nc\n        if not opt.no_instance:\n            netG_input_nc += 1\n        if self.use_features:\n            netG_input_nc += opt.feat_num\n        if opt.feat_pose:\n            netG_input_nc += opt.feat_pose_num_bins + 1 if opt.feat_pose_num_bins else 2\n        if opt.feat_normal:\n            netG_input_nc += 3\n        if opt.feat_depth:\n            netG_input_nc += 1\n        if not opt.no_global_encoder:\n            netG_input_nc += opt.global_encoder_nz\n        print(\'netG_input_nc:\', netG_input_nc)\n\n        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG,\n                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers,\n                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)\n        self.model_names += [\'G\']\n\n        # Discriminator network\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            netD_input_nc = input_nc + opt.output_nc\n            if not opt.no_instance:\n                netD_input_nc += 1\n            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid,\n                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n            self.model_names += [\'D\']\n\n        # Encoder network\n        if self.gen_features:\n            self.netE = networks.define_G(opt.output_nc, opt.feat_num, opt.nef, \'encoder\',\n                                          opt.n_downsample_E, norm=opt.norm, gpu_ids=self.gpu_ids, isTrain=opt.isTrain)\n            self.model_names += [\'E\']\n\n        self.print_networks(verbose=True)\n\n        # load networks\n        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n            pretrained_path = \'\' if not self.isTrain else opt.load_pretrain\n            # pretrained_path = opt.load_pretrain\n            self.load_network(self.netG, \'G\', opt.which_epoch, pretrained_path)\n            if self.isTrain:\n                self.load_network(self.netD, \'D\', opt.which_epoch, pretrained_path)\n            if self.gen_features:\n                self.load_network(self.netE, \'E\', opt.which_epoch, pretrained_path)\n            if not self.no_global_encoder:\n                self.load_network(self.netGlobalE, \'GlobalE\', opt.which_epoch, pretrained_path)\n\n        # set loss functions and optimizers\n        if self.isTrain:\n            if opt.pool_size > 0 and (len(self.gpu_ids)) > 1:\n                raise NotImplementedError(""Fake Pool Not Implemented for MultiGPU"")\n            self.fake_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionFeat = torch.nn.L1Loss()\n            if not opt.no_vgg_loss:\n                self.criterionVGG = networks.VGGLoss(self.gpu_ids)\n\n            # Names so we can breakout loss\n            self.loss_names = [\'G_GAN\', \'G_GAN_Feat\', \'G_VGG\', \'D_real\', \'D_fake\', \'G_L1\', \'E_VAE\', \'E_regress\']\n\n            # initialize optimizers\n            # optimizer G\n            if opt.niter_fix_global > 0:\n                print(\'------------- Only training the local enhancer network (for %d epochs) ------------\' % opt.niter_fix_global)\n                params_dict = dict(self.netG.named_parameters())\n                params = []\n                for key, value in params_dict.items():\n                    if key.startswith(\'model\' + str(opt.n_local_enhancers)):\n                        params += [{\'params\': [value], \'lr\':opt.lr}]\n                    else:\n                        params += [{\'params\': [value], \'lr\':0.0}]\n            else:\n                params = list(self.netG.parameters())\n            if self.gen_features:\n                params += list(self.netE.parameters())\n            if not opt.no_global_encoder:\n                params += list(self.netGlobalE.parameters())\n            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n\n            # optimizer D\n            params = list(self.netD.parameters())\n            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n\n            # optimizer global E\n            # if not opt.no_global_encoder:\n            #    params = list(self.netGlobalE.parameters())\n            #    self.optimizer_E = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n\n    def encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, pose_map=None, normal_map=None, depth_map=None, infer=False):\n        if self.opt.label_nc == 0:\n            input_label = label_map.data.cuda()\n        else:\n            # create one-hot vector for label map\n            size = label_map.size()\n            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n            input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n            input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n\n        # get edges from instance map\n        if not self.opt.no_instance:\n            inst_map = inst_map.data.cuda()\n            edge_map = self.get_edges(inst_map)\n            input_label = torch.cat((input_label, edge_map), dim=1)\n        with torch.no_grad():\n            input_label = Variable(input_label)\n\n        # real images for training\n        if real_image is not None:\n            real_image = Variable(real_image.data.cuda())\n\n        # instance map for feature encoding\n        if self.use_features:\n            # get precomputed feature maps\n            if self.opt.load_features:\n                feat_map = Variable(feat_map.data.cuda())\n            # create one-hot vector for pose\n            if self.opt.feat_pose:\n                if self.opt.feat_pose_num_bins:  # quantize\n                    size = pose_map.size()\n                    oneHot_size = (size[0], self.opt.feat_pose_num_bins + 1, size[2], size[3])\n                    _pose_map = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                    pose_map = _pose_map.scatter_(1, pose_map.data.long().cuda(), 1.0)\n                    pose_map = Variable(pose_map)\n                else:                           # cos, sin\n                    pose_map = Variable(pose_map.data.cuda())\n            if self.opt.feat_normal:\n                normal_map = Variable(normal_map.data.cuda())\n            if self.opt.feat_depth:\n                depth_map = Variable(depth_map.data.cuda())\n\n        return input_label, inst_map, real_image, feat_map, pose_map, normal_map, depth_map\n\n    def discriminate(self, input_label, test_image, use_pool=False):\n        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n        if use_pool:\n            fake_query = self.fake_pool.query(input_concat)\n            return self.netD.forward(fake_query)\n        else:\n            return self.netD.forward(input_concat)\n\n    def forward(self, label, inst, image, feat, pose=None, normal=None, depth=None, infer=False):\n        # Encode Inputs\n        input_label, inst_map, real_image, feat_map, pose_map, normal_map, depth_map = self.encode_input(label, inst, image, feat, pose, normal, depth)\n\n        # Fake Generation\n        input_concat = input_label\n        if self.use_features:\n            if not self.opt.load_features:          # instance-wise code\n                feat_map, loss_E_VAE = self.netE.forward(real_image, inst_map)\n                input_concat = torch.cat((input_concat, feat_map), dim=1)\n            if self.opt.feat_pose:                  # add pose code\n                input_concat = torch.cat((input_concat, pose_map), dim=1)\n            if self.opt.feat_normal:                # add normal code\n                input_concat = torch.cat((input_concat, normal_map), dim=1)\n            if self.opt.feat_depth:                 # add depth code\n                input_concat = torch.cat((input_concat, depth_map), dim=1)\n            if not self.opt.no_global_encoder:      # global code\n                mu, logvar = self.netGlobalE.forward(real_image)\n                std = logvar.mul(0.5).exp_()\n                eps = self.get_z_random(std.size(0), std.size(1), \'gauss\')\n                z = eps.mul(std).add_(mu)\n                z = z.view(z.size(0), z.size(1), 1, 1).expand(z.size(0), z.size(1), real_image.size(2), real_image.size(3))\n                input_concat = torch.cat((input_concat, z), dim=1)\n        fake_image = self.netG.forward(input_concat)\n\n        # Fake Detection and Loss\n        pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n        loss_D_fake = self.criterionGAN(pred_fake_pool, False)\n\n        # Real Detection and Loss\n        pred_real = self.discriminate(input_label, real_image)\n        loss_D_real = self.criterionGAN(pred_real, True)\n\n        # GAN loss (Fake Passability Loss)\n        pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))\n        loss_G_GAN = self.criterionGAN(pred_fake, True)\n\n        # GAN feature matching loss\n        loss_G_GAN_Feat = 0\n        if not self.opt.no_ganFeat_loss:\n            feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n            D_weights = 1.0 / self.opt.num_D\n            for i in range(self.opt.num_D):\n                for j in range(len(pred_fake[i]) - 1):\n                    loss_G_GAN_Feat += D_weights * feat_weights * \\\n                        self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n\n        # VGG feature matching loss\n        loss_G_VGG = 0\n        if not self.opt.no_vgg_loss:\n            loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n\n        # L1 reconstruction loss\n        loss_G_L1 = 0\n        if self.opt.lambda_L1 > 0:\n            loss_G_L1 = self.criterionFeat(fake_image, real_image) * self.opt.lambda_L1\n\n        # global and instance encoder VAE loss and regress loss\n        loss_E_regress = 0\n        if not self.opt.no_global_encoder:\n            kl_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar).mul_(-0.5)\n            loss_E_VAE = loss_E_VAE + torch.sum(kl_element)\n        loss_E_VAE = loss_E_VAE * self.opt.lambda_KL\n        if isinstance(loss_E_VAE, float):\n            loss_E_VAE = 0\n\n        # loss_E_regress: not implemented\n\n        # Only return the fake_B image if necessary to save BW\n        # Added losses with global encoder: loss_E_VAE, loss_E_regress\n        return [[loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake, loss_G_L1, loss_E_VAE, loss_E_regress], None if not infer else fake_image]\n\n    def fake_inference(self, image, label, inst, feat=None, pose=None, normal=None, depth=None):\n        # Encode Inputs\n        input_label, inst_map, real_image, feat_map, pose_map, normal_map, depth_map = self.encode_input(Variable(label), inst_map=Variable(inst), pose_map=Variable(pose), normal_map=Variable(normal), depth_map=Variable(depth), real_image=Variable(image), infer=True)\n\n        # Fake Generation\n        if self.use_features:\n            if feat is None:                        # instance-wise code\n                feat_map = self.netE.forward(real_image, inst_map)\n            else:\n                feat_map = Variable(feat.cuda())\n            input_concat = torch.cat((input_label.cuda(), feat_map), dim=1)\n            if self.opt.feat_pose:                  # add pose code\n                input_concat = torch.cat((input_concat, pose_map), dim=1)\n            if self.opt.feat_normal:                  # add normal code\n                input_concat = torch.cat((input_concat, normal_map), dim=1)\n            if self.opt.feat_depth:\n                input_concat = torch.cat((input_concat, depth_map), dim=1)\n            if not self.opt.no_global_encoder:      # global code\n                mu, logvar = self.netGlobalE.forward(Variable(image.cuda()))\n                std = logvar.mul(0.5).exp_()\n                eps = self.get_z_random(std.size(0), std.size(1), \'gauss\')\n                z = eps.mul(std).add_(mu)\n                z = z.view(z.size(0), z.size(1), 1, 1).expand(z.size(0), z.size(1), image.size(2), image.size(3))\n                z = mu.view(mu.size(0), mu.size(1), 1, 1).expand(mu.size(0), mu.size(1), image.size(2), image.size(3))\n                input_concat = torch.cat((input_concat, z), dim=1)\n        else:\n            input_concat = input_label\n        if torch.__version__.startswith(\'0.4\'):\n            with torch.no_grad():\n                fake_image = self.netG.forward(input_concat)\n        else:\n            fake_image = self.netG.forward(input_concat)\n        return fake_image\n\n    def inference(self, label, inst):\n        # Encode Inputs\n        input_label, inst_map, _, _ = self.encode_input(Variable(label), Variable(inst), infer=True)\n\n        # Fake Generation\n        if self.use_features:\n            # sample clusters from precomputed features\n            feat_map = self.sample_features(inst_map)\n            feat_map = Variable(feat_map)\n            input_concat = torch.cat((input_label, feat_map), dim=1)\n        else:\n            input_concat = input_label\n        if torch.__version__.startswith(\'0.4\'):\n            with torch.no_grad():\n                fake_image = self.netG.forward(input_concat)\n        else:\n            fake_image = self.netG.forward(input_concat)\n        return fake_image\n\n    def sample_features(self, inst):\n        # read precomputed feature clusters\n        cluster_path = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.cluster_path)\n        features_clustered = np.load(cluster_path).item()\n\n        # randomly sample from the feature clusters\n        inst_np = inst.cpu().numpy().astype(int)\n        feat_map = torch.cuda.FloatTensor(1, self.opt.feat_num, inst.size()[2], inst.size()[3])\n        for i in np.unique(inst_np):\n            label = i if i < 5000 else i // 5000\n            if label in features_clustered:\n                feat = features_clustered[label]\n                cluster_idx = np.random.randint(0, feat.shape[0])\n\n                idx = (inst == int(i)).nonzero()\n                for k in range(self.opt.feat_num):\n                    feat_map[idx[:, 0], idx[:, 1] + k, idx[:, 2], idx[:, 3]] = feat[cluster_idx, k]\n        return feat_map\n\n    def encode_features(self, image, inst):\n        image = Variable(image.cuda(), volatile=True)\n        feat_num = self.opt.feat_num\n        h, w = inst.size()[2], inst.size()[3]\n        block_num = 32\n        feat_map = self.netE.forward(image, inst.cuda())\n        inst_np = inst.cpu().numpy().astype(int)\n        feature = {}\n        for i in range(self.opt.label_nc):\n            feature[i] = np.zeros((0, feat_num + 1))\n        for i in np.unique(inst_np):\n            label = i if i < 5000 else i // 5000\n            i = int(i)\n            idx = (inst == i).nonzero()\n            num = idx.size()[0]\n            idx = idx[num // 2, :]\n            val = np.zeros((1, feat_num + 1))\n            for k in range(feat_num):\n                val[0, k] = feat_map[idx[0], idx[1] + k, idx[2], idx[3]].data[0]\n            val[0, feat_num] = float(num) / (h * w // block_num)\n            feature[label] = np.append(feature[label], val, axis=0)\n        return feature\n\n    def get_edges(self, t):\n        edge = torch.cuda.ByteTensor(t.size()).zero_()\n        edge[:, :, :, 1:] = edge[:, :, :, 1:] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, :, :-1] = edge[:, :, :, :-1] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, 1:, :] = edge[:, :, 1:, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        edge[:, :, :-1, :] = edge[:, :, :-1, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        return edge.float()\n\n    def save(self, which_epoch):\n        self.save_network(self.netG, \'G\', which_epoch, self.gpu_ids)\n        self.save_network(self.netD, \'D\', which_epoch, self.gpu_ids)\n        if self.gen_features:\n            self.save_network(self.netE, \'E\', which_epoch, self.gpu_ids)\n        if not self.no_global_encoder:\n            self.save_network(self.netGlobalE, \'GlobalE\', which_epoch, self.gpu_ids)\n\n    def update_fixed_params(self):\n        # after fixing the global generator for a number of iterations, also start finetuning it\n        params = list(self.netG.parameters())\n        if self.gen_features:\n            params += list(self.netE.parameters())\n        self.optimizer_G = torch.optim.Adam(params, lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n        print(\'------------ Now also finetuning global generator -----------\')\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd\n        for param_group in self.optimizer_D.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
textural/models/ui_model.py,10,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport numpy as np\nfrom PIL import Image\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass UIModel(BaseModel):\n    def name(self):\n        return \'UIModel\'\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n        self.use_features = opt.instance_feat or opt.label_feat\n\n        netG_input_nc = opt.label_nc\n        if not opt.no_instance:\n            netG_input_nc += 1\n        if self.use_features:\n            netG_input_nc += opt.feat_num\n\n        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG,\n                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers,\n                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)\n        self.load_network(self.netG, \'G\', opt.which_epoch)\n\n        print(\'---------- Networks initialized -------------\')\n\n    def toTensor(self, img, normalize=False):\n        tensor = torch.from_numpy(np.array(img, np.int32, copy=False))\n        tensor = tensor.view(1, img.size[1], img.size[0], len(img.mode))\n        tensor = tensor.transpose(1, 2).transpose(1, 3).contiguous()\n        if normalize:\n            return (tensor.float() / 255.0 - 0.5) / 0.5\n        return tensor.float()\n\n    def load_image(self, label_path, inst_path, feat_path):\n        opt = self.opt\n        # read label map\n        label_img = Image.open(label_path)\n        if label_path.find(\'face\') != -1:\n            label_img = label_img.convert(\'L\')\n        ow, oh = label_img.size\n        w = opt.loadSize\n        h = int(w * oh / ow)\n        label_img = label_img.resize((w, h), Image.NEAREST)\n        label_map = self.toTensor(label_img)\n\n        # onehot vector input for label map\n        self.label_map = label_map.cuda()\n        oneHot_size = (1, opt.label_nc, h, w)\n        input_label = self.Tensor(torch.Size(oneHot_size)).zero_()\n        self.input_label = input_label.scatter_(1, label_map.long().cuda(), 1.0)\n\n        # read instance map\n        if not opt.no_instance:\n            inst_img = Image.open(inst_path)\n            inst_img = inst_img.resize((w, h), Image.NEAREST)\n            self.inst_map = self.toTensor(inst_img).cuda()\n            self.edge_map = self.get_edges(self.inst_map)\n            self.net_input = Variable(torch.cat((self.input_label, self.edge_map), dim=1), volatile=True)\n        else:\n            self.net_input = Variable(self.input_label, volatile=True)\n\n        self.features_clustered = np.load(feat_path).item()\n        self.object_map = self.inst_map if opt.instance_feat else self.label_map\n\n        object_np = self.object_map.cpu().numpy().astype(int)\n        self.feat_map = self.Tensor(1, opt.feat_num, h, w).zero_()\n        self.cluster_indices = np.zeros(self.opt.label_nc, np.uint8)\n        for i in np.unique(object_np):\n            label = i if i < 1000 else i // 1000\n            if label in self.features_clustered:\n                feat = self.features_clustered[label]\n                np.random.seed(i + 1)\n                cluster_idx = np.random.randint(0, feat.shape[0])\n                self.cluster_indices[label] = cluster_idx\n                idx = (self.object_map == i).nonzero()\n                self.set_features(idx, feat, cluster_idx)\n\n        self.net_input_original = self.net_input.clone()\n        self.label_map_original = self.label_map.clone()\n        self.feat_map_original = self.feat_map.clone()\n        if not opt.no_instance:\n            self.inst_map_original = self.inst_map.clone()\n\n    def reset(self):\n        self.net_input = self.net_input_prev = self.net_input_original.clone()\n        self.label_map = self.label_map_prev = self.label_map_original.clone()\n        self.feat_map = self.feat_map_prev = self.feat_map_original.clone()\n        if not self.opt.no_instance:\n            self.inst_map = self.inst_map_prev = self.inst_map_original.clone()\n        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map\n\n    def undo(self):\n        self.net_input = self.net_input_prev\n        self.label_map = self.label_map_prev\n        self.feat_map = self.feat_map_prev\n        if not self.opt.no_instance:\n            self.inst_map = self.inst_map_prev\n        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map\n\n    # get boundary map from instance map\n    def get_edges(self, t):\n        edge = torch.cuda.ByteTensor(t.size()).zero_()\n        edge[:, :, :, 1:] = edge[:, :, :, 1:] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, :, :-1] = edge[:, :, :, :-1] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, 1:, :] = edge[:, :, 1:, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        edge[:, :, :-1, :] = edge[:, :, :-1, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        return edge.float()\n\n    # change the label at the source position to the label at the target position\n    def change_labels(self, click_src, click_tgt):\n        y_src, x_src = click_src[0], click_src[1]\n        y_tgt, x_tgt = click_tgt[0], click_tgt[1]\n        label_src = int(self.label_map[0, 0, y_src, x_src])\n        inst_src = self.inst_map[0, 0, y_src, x_src]\n        label_tgt = int(self.label_map[0, 0, y_tgt, x_tgt])\n        inst_tgt = self.inst_map[0, 0, y_tgt, x_tgt]\n\n        idx_src = (self.inst_map == inst_src).nonzero()\n        # need to change 3 things: label map, instance map, and feature map\n        if idx_src.shape:\n            # backup current maps\n            self.backup_current_state()\n\n            # change both the label map and the network input\n            self.label_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = label_tgt\n            self.net_input[idx_src[:, 0], idx_src[:, 1] + label_src, idx_src[:, 2], idx_src[:, 3]] = 0\n            self.net_input[idx_src[:, 0], idx_src[:, 1] + label_tgt, idx_src[:, 2], idx_src[:, 3]] = 1\n\n            # update the instance map (and the network input)\n            if inst_tgt > 1000:\n                # if different instances have different ids, give the new object a new id\n                tgt_indices = (self.inst_map > label_tgt * 1000) & (self.inst_map < (label_tgt + 1) * 1000)\n                inst_tgt = self.inst_map[tgt_indices].max() + 1\n            self.inst_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = inst_tgt\n            self.net_input[:, -1, :, :] = self.get_edges(self.inst_map)\n\n            # also copy the source features to the target position\n            idx_tgt = (self.inst_map == inst_tgt).nonzero()\n            if idx_tgt.shape:\n                self.copy_features(idx_src, idx_tgt[0, :])\n\n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    # add strokes of target label in the image\n    def add_strokes(self, click_src, label_tgt, bw, save):\n        # get the region of the new strokes (bw is the brush width)\n        size = self.net_input.size()\n        h, w = size[2], size[3]\n        idx_src = torch.LongTensor(bw**2, 4).fill_(0)\n        for i in range(bw):\n            idx_src[i * bw:(i + 1) * bw, 2] = min(h - 1, max(0, click_src[0] - bw // 2 + i))\n            for j in range(bw):\n                idx_src[i * bw + j, 3] = min(w - 1, max(0, click_src[1] - bw // 2 + j))\n        idx_src = idx_src.cuda()\n\n        # again, need to update 3 things\n        if idx_src.shape:\n            # backup current maps\n            if save:\n                self.backup_current_state()\n\n            # update the label map (and the network input) in the stroke region\n            self.label_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = label_tgt\n            for k in range(self.opt.label_nc):\n                self.net_input[idx_src[:, 0], idx_src[:, 1] + k, idx_src[:, 2], idx_src[:, 3]] = 0\n            self.net_input[idx_src[:, 0], idx_src[:, 1] + label_tgt, idx_src[:, 2], idx_src[:, 3]] = 1\n\n            # update the instance map (and the network input)\n            self.inst_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = label_tgt\n            self.net_input[:, -1, :, :] = self.get_edges(self.inst_map)\n\n            # also update the features if available\n            if self.opt.instance_feat:\n                feat = self.features_clustered[label_tgt]\n                # np.random.seed(label_tgt+1)\n                # cluster_idx = np.random.randint(0, feat.shape[0])\n                cluster_idx = self.cluster_indices[label_tgt]\n                self.set_features(idx_src, feat, cluster_idx)\n\n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    # add an object to the clicked position with selected style\n    def add_objects(self, click_src, label_tgt, mask, style_id=0):\n        y, x = click_src[0], click_src[1]\n        mask = np.transpose(mask, (2, 0, 1))[np.newaxis, ...]\n        idx_src = torch.from_numpy(mask).cuda().nonzero()\n        idx_src[:, 2] += y\n        idx_src[:, 3] += x\n\n        # backup current maps\n        self.backup_current_state()\n\n        # update label map\n        self.label_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = label_tgt\n        for k in range(self.opt.label_nc):\n            self.net_input[idx_src[:, 0], idx_src[:, 1] + k, idx_src[:, 2], idx_src[:, 3]] = 0\n        self.net_input[idx_src[:, 0], idx_src[:, 1] + label_tgt, idx_src[:, 2], idx_src[:, 3]] = 1\n\n        # update instance map\n        self.inst_map[idx_src[:, 0], idx_src[:, 1], idx_src[:, 2], idx_src[:, 3]] = label_tgt\n        self.net_input[:, -1, :, :] = self.get_edges(self.inst_map)\n\n        # update feature map\n        self.set_features(idx_src, self.feat, style_id)\n\n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    def single_forward(self, net_input, feat_map):\n        net_input = torch.cat((net_input, feat_map), dim=1)\n        fake_image = self.netG.forward(net_input)\n\n        if fake_image.size()[0] == 1:\n            return fake_image.data[0]\n        return fake_image.data\n\n    # generate all outputs for different styles\n    def style_forward(self, click_pt, style_id=-1):\n        if click_pt is None:\n            self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n            self.crop = None\n            self.mask = None\n        else:\n            instToChange = int(self.object_map[0, 0, click_pt[0], click_pt[1]])\n            self.instToChange = instToChange\n            label = instToChange if instToChange < 1000 else instToChange // 1000\n            self.feat = self.features_clustered[label]\n            self.fake_image = []\n            self.mask = self.object_map == instToChange\n            idx = self.mask.nonzero()\n            self.get_crop_region(idx)\n            if idx.size():\n                if style_id == -1:\n                    (min_y, min_x, max_y, max_x) = self.crop\n                    # original\n                    for cluster_idx in range(self.opt.multiple_output):\n                        self.set_features(idx, self.feat, cluster_idx)\n                        fake_image = self.single_forward(self.net_input, self.feat_map)\n                        fake_image = util.tensor2im(fake_image[:, min_y:max_y, min_x:max_x])\n                        self.fake_image.append(fake_image)\n                    """"""### To speed up previewing different style results, either crop or downsample the label maps\n                    if instToChange > 1000:\n                        (min_y, min_x, max_y, max_x) = self.crop\n                        ### crop\n                        _, _, h, w = self.net_input.size()\n                        offset = 512\n                        y_start, x_start = max(0, min_y-offset), max(0, min_x-offset)\n                        y_end, x_end = min(h, (max_y + offset)), min(w, (max_x + offset))\n                        y_region = slice(y_start, y_start+(y_end-y_start)//16*16)\n                        x_region = slice(x_start, x_start+(x_end-x_start)//16*16)\n                        net_input = self.net_input[:,:,y_region,x_region]\n                        for cluster_idx in range(self.opt.multiple_output):\n                            self.set_features(idx, self.feat, cluster_idx)\n                            fake_image = self.single_forward(net_input, self.feat_map[:,:,y_region,x_region])\n                            fake_image = util.tensor2im(fake_image[:,min_y-y_start:max_y-y_start,min_x-x_start:max_x-x_start])\n                            self.fake_image.append(fake_image)\n                    else:\n                        ### downsample\n                        (min_y, min_x, max_y, max_x) = [crop//2 for crop in self.crop]\n                        net_input = self.net_input[:,:,::2,::2]\n                        size = net_input.size()\n                        net_input_batch = net_input.expand(self.opt.multiple_output, size[1], size[2], size[3])\n                        for cluster_idx in range(self.opt.multiple_output):\n                            self.set_features(idx, self.feat, cluster_idx)\n                            feat_map = self.feat_map[:,:,::2,::2]\n                            if cluster_idx == 0:\n                                feat_map_batch = feat_map\n                            else:\n                                feat_map_batch = torch.cat((feat_map_batch, feat_map), dim=0)\n                        fake_image_batch = self.single_forward(net_input_batch, feat_map_batch)\n                        for i in range(self.opt.multiple_output):\n                            self.fake_image.append(util.tensor2im(fake_image_batch[i,:,min_y:max_y,min_x:max_x]))""""""\n\n                else:\n                    self.set_features(idx, self.feat, style_id)\n                    self.cluster_indices[label] = style_id\n                    self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    def backup_current_state(self):\n        self.net_input_prev = self.net_input.clone()\n        self.label_map_prev = self.label_map.clone()\n        self.inst_map_prev = self.inst_map.clone()\n        self.feat_map_prev = self.feat_map.clone()\n\n    # crop the ROI and get the mask of the object\n    def get_crop_region(self, idx):\n        size = self.net_input.size()\n        h, w = size[2], size[3]\n        min_y, min_x = idx[:, 2].min(), idx[:, 3].min()\n        max_y, max_x = idx[:, 2].max(), idx[:, 3].max()\n        crop_min = 128\n        if max_y - min_y < crop_min:\n            min_y = max(0, (max_y + min_y) // 2 - crop_min // 2)\n            max_y = min(h - 1, min_y + crop_min)\n        if max_x - min_x < crop_min:\n            min_x = max(0, (max_x + min_x) // 2 - crop_min // 2)\n            max_x = min(w - 1, min_x + crop_min)\n        self.crop = (min_y, min_x, max_y, max_x)\n        self.mask = self.mask[:, :, min_y:max_y, min_x:max_x]\n\n    # update the feature map once a new object is added or the label is changed\n    def update_features(self, cluster_idx, mask=None, click_pt=None):\n        self.feat_map_prev = self.feat_map.clone()\n        # adding a new object\n        if mask is not None:\n            y, x = click_pt[0], click_pt[1]\n            mask = np.transpose(mask, (2, 0, 1))[np.newaxis, ...]\n            idx = torch.from_numpy(mask).cuda().nonzero()\n            idx[:, 2] += y\n            idx[:, 3] += x\n        # changing the label of an existing object\n        else:\n            idx = (self.object_map == self.instToChange).nonzero()\n\n        # update feature map\n        self.set_features(idx, self.feat, cluster_idx)\n\n    # set the class features to the target feature\n    def set_features(self, idx, feat, cluster_idx):\n        for k in range(self.opt.feat_num):\n            self.feat_map[idx[:, 0], idx[:, 1] + k, idx[:, 2], idx[:, 3]] = feat[cluster_idx, k]\n\n    # copy the features at the target position to the source position\n    def copy_features(self, idx_src, idx_tgt):\n        for k in range(self.opt.feat_num):\n            val = self.feat_map[idx_tgt[0], idx_tgt[1] + k, idx_tgt[2], idx_tgt[3]]\n            self.feat_map[idx_src[:, 0], idx_src[:, 1] + k, idx_src[:, 2], idx_src[:, 3]] = val\n\n    def get_current_visuals(self, getLabel=False):\n        mask = self.mask\n        if self.mask is not None:\n            mask = np.transpose(self.mask[0].cpu().float().numpy(), (1, 2, 0)).astype(np.uint8)\n\n        dict_list = [(\'fake_image\', self.fake_image), (\'mask\', mask)]\n\n        if getLabel:  # only output label map if needed to save bandwidth\n            label = util.tensor2label(self.net_input.data[0], self.opt.label_nc)\n            dict_list += [(\'label\', label)]\n\n        return OrderedDict(dict_list)\n'"
textural/options/__init__.py,0,b''
textural/options/base_options.py,1,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport argparse\nimport os\nimport torch\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        # experiment specifics\n        self.parser.add_argument(\'--name\', type=str, default=\'baseline\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--model\', type=str, default=\'pix2pixHD\', help=\'which model to use\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n        self.parser.add_argument(\'--use_dropout\', action=\'store_true\', help=\'use dropout for the generator\')\n\n        # input/output sizes\n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=800, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineWidth\', type=int, default=624, help=\'then crop to this width\')\n        self.parser.add_argument(\'--fineHeight\', type=int, default=192, help=\'then crop to this height\')\n        self.parser.add_argument(\'--label_nc\', type=int, default=14, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n\n        # for setting inputs\n        self.parser.add_argument(\'--dataroot\', type=str, default=\'./data\')\n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'scale_width_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data argumentation\')\n        self.parser.add_argument(\'--nThreads\', default=4, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n\n        # for displays\n        self.parser.add_argument(\'--display_winsize\', type=int, default=512, help=\'display window size\')\n        self.parser.add_argument(\'--tf_log\', action=\'store_true\', help=\'if specified, use tensorboard logging. Requires tensorflow installed\')\n\n        # for generator\n        self.parser.add_argument(\'--netG\', type=str, default=\'global\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--n_downsample_global\', type=int, default=4, help=\'number of downsampling layers in netG\')\n        self.parser.add_argument(\'--n_blocks_global\', type=int, default=9, help=\'number of residual blocks in the global generator network\')\n        self.parser.add_argument(\'--n_blocks_local\', type=int, default=3, help=\'number of residual blocks in the local enhancer network\')\n        self.parser.add_argument(\'--n_local_enhancers\', type=int, default=0, help=\'number of local enhancers to use\')\n        self.parser.add_argument(\'--niter_fix_global\', type=int, default=0, help=\'number of epochs that we only train the outmost local enhancer\')\n\n        # for global encoder\n        self.parser.add_argument(\'--no_global_encoder\', type=int, default=1, help=\'if specified, do *not* use global encoder\')\n        self.parser.add_argument(\'--global_encoder_which_model\', type=str, default=\'resnet_128\', help=\'selects model to use for global encoder\')\n        self.parser.add_argument(\'--global_encoder_nz\', type=int, default=3, help=\'# of latent vector of global encoder\')\n        self.parser.add_argument(\'--global_encoder_nef\', type=int, default=64, help=\'# of global encoder filters in first conv layer\')\n\n        # for instance-wise features\n        self.parser.add_argument(\'--no_instance\', action=\'store_true\', help=\'if specified, do *not* add instance map as input\')\n        self.parser.add_argument(\'--instance_feat\', type=bool, default=True, help=\'if true, add encoded instance features as input\')\n        self.parser.add_argument(\'--label_feat\', action=\'store_true\', help=\'if specified, add encoded label features as input\')\n        self.parser.add_argument(\'--feat_num\', type=int, default=5, help=\'vector length for encoded features\')\n        self.parser.add_argument(\'--load_features\', action=\'store_true\', help=\'if specified, load precomputed feature maps\')\n        self.parser.add_argument(\'--n_downsample_E\', type=int, default=4, help=\'# of downsampling layers in encoder\')\n        self.parser.add_argument(\'--nef\', type=int, default=16, help=\'# of encoder filters in the first conv layer\')\n        self.parser.add_argument(\'--n_clusters\', type=int, default=10, help=\'number of clusters for features\')\n        self.parser.add_argument(\'--feat_pose\', type=str, default=\'\', help=\'add quantized pose as instance-wise feature\')\n        self.parser.add_argument(\'--feat_pose_num_bins\', type=int, default=24, help=\'#quantized pose bins if >0, else use (cos, sin)\')\n        self.parser.add_argument(\'--feat_normal\', type=str, default=\'\', help=\'add object normal map as instance-wise feature\')\n        self.parser.add_argument(\'--feat_depth\', type=str, default=\'\', help=\'add object depth map as instance-wise feature\')\n\n        # for semantic segmentation\n        self.parser.add_argument(\'--segm_precomputed_path\', default=\'\', type=str, help=""path of precomputed semantic segmentation result"")\n\n        # for instance segmentation\n        self.parser.add_argument(\'--inst_precomputed_path\', default=\'\', type=str, help=""path of precomputed instance segmentation result"")\n\n        self.initialized = True\n\n    def parse(self, save=True):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        # if self.opt.inst_overlap: # add a new class: predicted object\n        #    self.opt.label_nc += 1\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk\n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        mkdirs(expr_dir)\n        if save and not self.opt.continue_train:\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'------------ Options -------------\\n\')\n                for k, v in sorted(args.items()):\n                    opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n                opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
textural/options/edit_options.py,0,"b""# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\n\nclass EditOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n\n        self.parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n        self.parser.add_argument('--phase', type=str, default='edit', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='60', help='which epoch to load? set to latest to use latest cached model')\n        self.isTrain = False\n\n        self.parser.add_argument('--edit_source', type=str, default='', help='relative path of image on which we read features and edit')\n        self.parser.add_argument('--edit_dir', type=str, default='', help='new instance map and json guiding how to manipulate')\n        self.parser.add_argument('--edit_num', type=int, default=5, help='# of manipulation tasks')\n        self.parser.add_argument('--edit_list', type=str, default='', help='edit list for benchmark edit')\n        self.parser.add_argument('--experiment_name', type=str, default='edit', help='experiment name')\n"""
textural/options/test_options.py,0,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=5000, help=\'how many test images to run\')\n        self.parser.add_argument(\'--cluster_path\', type=str, default=\'features_clustered_010.npy\', help=\'the path for clustered results of encoded features\')\n        self.parser.add_argument(\'--experiment_name\', type=str, default=\'test\', help=\'experiment name\')\n        self.isTrain = False\n'"
textural/options/train_options.py,0,"b""# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        # for displays\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n\n        # for training\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--load_pretrain', type=str, default='', help='load the pretrained model from the specified location')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n\n        # for discriminators\n        self.parser.add_argument('--num_D', type=int, default=2, help='number of discriminators to use')\n        self.parser.add_argument('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\n        self.parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')\n        self.parser.add_argument('--lambda_feat', type=float, default=5.0, help='weight for feature matching loss')\n        self.parser.add_argument('--no_ganFeat_loss', action='store_true', help='if specified, do *not* use discriminator feature matching loss')\n        self.parser.add_argument('--no_vgg_loss', action='store_true', help='if specified, do *not* use VGG feature matching loss')\n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n\n        # new losses\n        self.parser.add_argument('--lambda_L1', type=float, default=10.0, help='weight for |B-G(A, E(B))|')\n        self.parser.add_argument('--lambda_KL', type=float, default=0.01, help='weight for KL(p(global_z) || N(0,1))')\n\n        # for augmentation\n        self.parser.add_argument('--use_augmentation', type=bool, default=True, help='use image augmentation')\n\n        self.isTrain = True\n"""
textural/util/__init__.py,0,b''
textural/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, refresh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=512):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % (width), src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.jpg\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.jpg\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
textural/util/image_pool.py,3,"b'import random\nimport torch\nfrom torch.autograd import Variable\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size - 1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
textural/util/util.py,2,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\n\n\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n        return image_numpy\n    image_numpy = image_tensor.cpu().float().numpy()\n    if normalize:\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    else:\n        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0\n    image_numpy = np.clip(image_numpy, 0, 255)\n    if image_numpy.shape[2] == 1:\n        image_numpy = image_numpy[:, :, 0]\n    return image_numpy.astype(imtype)\n\n# Converts a one-hot tensor into a colorful label map\n\n\ndef tensor2label(label_tensor, n_label, imtype=np.uint8):\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()\n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    label_tensor = Colorize(n_label)(label_tensor)\n    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    return label_numpy.astype(imtype)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Citscape label map colors\n###############################################################################\n\n\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])\n\n\ndef labelcolormap(N):\n    if N == 35:  # cityscape\n        cmap = np.array([(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (111, 74, 0), (81, 0, 81),\n                         (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153),\n                         (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n                         (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), (0, 0, 142), (0, 0, 70),\n                         (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)],\n                        dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7 - j))\n                g = g ^ (np.uint8(str_id[-2]) << (7 - j))\n                b = b ^ (np.uint8(str_id[-3]) << (7 - j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n    return cmap\n\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n'"
textural/util/util2.py,8,"b'from __future__ import print_function\nimport numpy as np\nfrom PIL import Image\nimport inspect\nimport re\nimport numpy as np\nimport os\nimport collections\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.interpolation import zoom\nfrom skimage.measure import compare_ssim\nimport torch\nfrom IPython import embed\nimport cv2\nfrom datetime import datetime\n\n\ndef datetime_str():\n    now = datetime.now()\n    return \'%04d-%02d-%02d-%02d-%02d-%02d\' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n\n\ndef read_text_file(in_path):\n    fid = open(in_path, \'r\')\n\n    vals = []\n    cur_line = fid.readline()\n    while(cur_line != \'\'):\n        vals.append(float(cur_line))\n        cur_line = fid.readline()\n\n    fid.close()\n    return np.array(vals)\n\n\ndef bootstrap(in_vec, num_samples=100, bootfunc=np.mean):\n    from astropy import stats\n    return stats.bootstrap(np.array(in_vec), bootnum=num_samples, bootfunc=bootfunc)\n\n\ndef rand_flip(input1, input2):\n    if(np.random.binomial(1, .5) == 1):\n        return (input1, input2)\n    else:\n        return (input2, input1)\n\n\ndef l2(p0, p1, range=255.):\n    return .5 * np.mean((p0 / range - p1 / range)**2)\n\n\ndef psnr(p0, p1, peak=255.):\n    return 10 * np.log10(peak**2 / np.mean((1. * p0 - 1. * p1)**2))\n\n\ndef dssim(p0, p1, range=255.):\n    # embed()\n    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n\n\ndef rgb2lab(in_img, mean_cent=False):\n    from skimage import color\n    img_lab = color.rgb2lab(in_img)\n    if(mean_cent):\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n    return img_lab\n\n\ndef normalize_blob(in_feat, eps=1e-10):\n    norm_factor = np.sqrt(np.sum(in_feat**2, axis=1, keepdims=True))\n    return in_feat / (norm_factor + eps)\n\n\ndef cos_sim_blob(in0, in1):\n    in0_norm = normalize_blob(in0)\n    in1_norm = normalize_blob(in1)\n    (N, C, X, Y) = in0_norm.shape\n\n    return np.mean(np.mean(np.sum(in0_norm * in1_norm, axis=1), axis=1), axis=1)\n\n\ndef normalize_tensor(in_feat, eps=1e-10):\n    # norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1)).view(in_feat.size()[0],1,in_feat.size()[2],in_feat.size()[3]).repeat(1,in_feat.size()[1],1,1)\n    norm_factor = torch.sqrt(torch.sum(in_feat**2, dim=1)).view(in_feat.size()[0], 1, in_feat.size()[2], in_feat.size()[3])\n    return in_feat / (norm_factor.expand_as(in_feat) + eps)\n\n\ndef cos_sim(in0, in1):\n    in0_norm = normalize_tensor(in0)\n    in1_norm = normalize_tensor(in1)\n    N = in0.size()[0]\n    X = in0.size()[2]\n    Y = in0.size()[3]\n\n    return torch.mean(torch.mean(torch.sum(in0_norm * in1_norm, dim=1).view(N, 1, X, Y), dim=2).view(N, 1, 1, Y), dim=3).view(N)\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the conve\n\n\ndef tensor2np(tensor_obj):\n    # change dimension of a tensor object into a numpy array\n    return tensor_obj[0].cpu().float().numpy().transpose((1, 2, 0))\n\n\ndef np2tensor(np_obj):\n     # change dimenion of np array into tensor array\n    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\n\ndef tensor2tensorlab(image_tensor, to_norm=True, mc_only=False):\n    # image tensor to lab tensor\n    from skimage import color\n\n    img = tensor2im(image_tensor)\n    # print(\'img_rgb\',img.flatten())\n    img_lab = color.rgb2lab(img)\n    # print(\'img_lab\',img_lab.flatten())\n    if(mc_only):\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n    if(to_norm and not mc_only):\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n        img_lab = img_lab / 100.\n\n    return np2tensor(img_lab)\n\n\ndef tensorlab2tensor(lab_tensor, return_inbnd=False):\n    from skimage import color\n    import warnings\n    warnings.filterwarnings(""ignore"")\n\n    lab = tensor2np(lab_tensor) * 100.\n    lab[:, :, 0] = lab[:, :, 0] + 50\n    # print(\'lab\',lab)\n\n    rgb_back = 255. * np.clip(color.lab2rgb(lab.astype(\'float\')), 0, 1)\n    # print(\'rgb\',rgb_back)\n    if(return_inbnd):\n        # convert back to lab, see if we match\n        lab_back = color.rgb2lab(rgb_back.astype(\'uint8\'))\n        # print(\'lab_back\',lab_back)\n        # print(\'lab==lab_back\',np.isclose(lab_back,lab,atol=1.))\n        # print(\'lab-lab_back\',np.abs(lab-lab_back))\n        mask = 1. * np.isclose(lab_back, lab, atol=2.)\n        mask = np2tensor(np.prod(mask, axis=2)[:, :, np.newaxis])\n        return (im2tensor(rgb_back), mask)\n    else:\n        return im2tensor(rgb_back)\n\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255. / 2.):\n    # def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=1.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255. / 2.):\n    # def im2tensor(image, imtype=np.uint8, cent=1., factor=1.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\n\ndef tensor2vec(vector_tensor):\n    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef grab_patch(img_in, P, yy, xx):\n    return img_in[yy:yy + P, xx:xx + P, :]\n\n\ndef load_image(path):\n    if(path[-3:] == \'dng\'):\n        import rawpy\n        with rawpy.imread(path) as raw:\n            img = raw.postprocess()\n        # img = plt.imread(path)\n    elif(path[-3:] == \'bmp\' or path[-3:] == \'jpg\' or path[-3:] == \'png\'):\n        import cv2\n        return cv2.imread(path)[:, :, ::-1]\n    else:\n        img = (255 * plt.imread(path)[:, :, :3]).astype(\'uint8\')\n\n    return img\n\n\ndef resize_image(img, max_size=256):\n    [Y, X] = img.shape[:2]\n\n    # resize\n    max_dim = max([Y, X])\n    zoom_factor = 1. * max_size / max_dim\n    img = zoom(img, [zoom_factor, zoom_factor, 1])\n\n    return img\n\n\ndef resize_image_zoom(img, zoom_factor=1., order=3):\n    if(zoom_factor == 1):\n        return img\n    else:\n        return zoom(img, [zoom_factor, zoom_factor, 1], order=order)\n\n\ndef save_image(image_numpy, image_path, ):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef prep_display_image(img, dtype=\'uint8\'):\n    if(dtype == \'uint8\'):\n        return np.clip(img, 0, 255).astype(\'uint8\')\n    else:\n        return np.clip(img, 0, 1.)\n\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [\n        e for e in dir(object) if isinstance(\n            getattr(\n                object,\n                e),\n            collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print(""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]))\n\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\n            \'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' %\n            (np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef rgb2lab(input):\n    from skimage import color\n    return color.rgb2lab(input / 255.)\n\n\ndef montage(\n    imgs,\n    PAD=5,\n    RATIO=16 / 9.,\n    EXTRA_PAD=(\n        False,\n        False),\n        MM=-1,\n        NN=-1,\n        primeDir=0,\n        verbose=False,\n        returnGridPos=False,\n        backClr=np.array(\n            (0,\n             0,\n             0))):\n    # INPUTS\n    #   imgs        YxXxMxN or YxXxN\n    #   PAD         scalar              number of pixels in between\n    #   RATIO       scalar              target ratio of cols/rows\n    #   MM          scalar              # rows, if specified, overrides RATIO\n    #   NN          scalar              # columns, if specified, overrides RATIO\n    #   primeDir    scalar              0 for top-to-bottom, 1 for left-to-right\n    # OUTPUTS\n    #   mont_imgs   MM*Y x NN*X x M     big image with everything montaged\n    # def montage(imgs, PAD=5, RATIO=16/9., MM=-1, NN=-1, primeDir=0,\n    # verbose=False, forceFloat=False):\n    if(imgs.ndim == 3):\n        toExp = True\n        imgs = imgs[:, :, np.newaxis, :]\n    else:\n        toExp = False\n\n    Y = imgs.shape[0]\n    X = imgs.shape[1]\n    M = imgs.shape[2]\n    N = imgs.shape[3]\n\n    PADS = np.array((PAD))\n    if(PADS.flatten().size == 1):\n        PADY = PADS\n        PADX = PADS\n    else:\n        PADY = PADS[0]\n        PADX = PADS[1]\n\n    if(MM == -1 and NN == -1):\n        NN = np.ceil(np.sqrt(1.0 * N * RATIO))\n        MM = np.ceil(1.0 * N / NN)\n        NN = np.ceil(1.0 * N / MM)\n    elif(MM == -1):\n        MM = np.ceil(1.0 * N / NN)\n    elif(NN == -1):\n        NN = np.ceil(1.0 * N / MM)\n\n    if(primeDir == 0):  # write top-to-bottom\n        [grid_mm, grid_nn] = np.meshgrid(\n            np.arange(MM, dtype=\'uint\'), np.arange(NN, dtype=\'uint\'))\n    elif(primeDir == 1):  # write left-to-right\n        [grid_nn, grid_mm] = np.meshgrid(\n            np.arange(NN, dtype=\'uint\'), np.arange(MM, dtype=\'uint\'))\n\n    grid_mm = np.uint(grid_mm.flatten()[0:N])\n    grid_nn = np.uint(grid_nn.flatten()[0:N])\n\n    EXTRA_PADY = EXTRA_PAD[0] * PADY\n    EXTRA_PADX = EXTRA_PAD[0] * PADX\n\n    # mont_imgs = np.zeros(((Y+PAD)*MM-PAD, (X+PAD)*NN-PAD, M), dtype=use_dtype)\n    mont_imgs = np.zeros(\n        (np.uint(\n            (Y + PADY) * MM - PADY + EXTRA_PADY),\n            np.uint(\n            (X + PADX) * NN - PADX + EXTRA_PADX),\n            M),\n        dtype=imgs.dtype)\n    mont_imgs = mont_imgs + \\\n        backClr.flatten()[np.newaxis, np.newaxis, :].astype(mont_imgs.dtype)\n\n    for ii in np.random.permutation(N):\n        # print imgs[:,:,:,ii].shape\n        # mont_imgs[grid_mm[ii]*(Y+PAD):(grid_mm[ii]*(Y+PAD)+Y), grid_nn[ii]*(X+PAD):(grid_nn[ii]*(X+PAD)+X),:]\n        mont_imgs[np.uint(grid_mm[ii] *\n                          (Y\n                           + PADY)):np.uint((grid_mm[ii]\n                                           * (Y\n                                            + PADY)\n                                           + Y)), np.uint(grid_nn[ii]\n                                                        * (X +\n                                                           PADX)):np.uint((grid_nn[ii]\n                                                                         * (X\n                                                                          + PADX) +\n                                                               X)), :] = imgs[:, :, :, ii]\n\n    if(M == 1):\n        imgs = imgs.reshape(imgs.shape[0], imgs.shape[1], imgs.shape[3])\n\n    if(toExp):\n        mont_imgs = mont_imgs[:, :, 0]\n\n    if(returnGridPos):\n        # return (mont_imgs,np.concatenate((grid_mm[:,:,np.newaxis]*(Y+PAD),\n        # grid_nn[:,:,np.newaxis]*(X+PAD)),axis=2))\n        return (mont_imgs, np.concatenate(\n            (grid_mm[:, np.newaxis] * (Y + PADY), grid_nn[:, np.newaxis] * (X + PADX)), axis=1))\n        # return (mont_imgs, (grid_mm,grid_nn))\n    else:\n        return mont_imgs\n\n\nclass zeroClipper(object):\n    def __init__(self, frequency=1):\n        self.frequency = frequency\n\n    def __call__(self, module):\n        embed()\n        if hasattr(module, \'weight\'):\n            # module.weight.data = torch.max(module.weight.data, 0)\n            module.weight.data = torch.max(module.weight.data, 0) + 100\n\n\ndef flatten_nested_list(nested_list):\n    # only works for list of list\n    accum = []\n    for sublist in nested_list:\n        for item in sublist:\n            accum.append(item)\n    return accum\n\n\ndef read_file(in_path, list_lines=False):\n    agg_str = \'\'\n    f = open(in_path, \'r\')\n    cur_line = f.readline()\n    while(cur_line != \'\'):\n        agg_str += cur_line\n        cur_line = f.readline()\n    f.close()\n    if(list_lines == False):\n        return agg_str.replace(\'\\n\', \'\')\n    else:\n        line_list = agg_str.split(\'\\n\')\n        ret_list = []\n        for item in line_list:\n            if(item != \'\'):\n                ret_list.append(item)\n        return ret_list\n\n\ndef read_csv_file_as_text(in_path):\n    agg_str = []\n    f = open(in_path, \'r\')\n    cur_line = f.readline()\n    while(cur_line != \'\'):\n        agg_str.append(cur_line)\n        cur_line = f.readline()\n    f.close()\n    return agg_str\n\n\ndef random_swap(obj0, obj1):\n    if(np.random.rand() < .5):\n        return (obj0, obj1, 0)\n    else:\n        return (obj1, obj0, 1)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n'"
textural/util/visualizer.py,0,"b'# Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nimport scipy.misc\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.tf_log = opt.tf_log\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.tf_log:\n            import tensorflow as tf\n            self.tf = tf\n            self.log_dir = os.path.join(opt.checkpoints_dir, opt.name, \'logs\')\n            self.writer = tf.summary.FileWriter(self.log_dir)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, step):\n        if self.tf_log:  # show images in tensorboard output\n            img_summaries = []\n            for label, image_numpy in visuals.items():\n                # Write the image to a string\n                try:\n                    s = StringIO()\n                except:\n                    s = BytesIO()\n                scipy.misc.toimage(image_numpy).save(s, format=""jpeg"")\n                # Create an Image object\n                img_sum = self.tf.Summary.Image(encoded_image_string=s.getvalue(), height=image_numpy.shape[0], width=image_numpy.shape[1])\n                # Create a Summary value\n                img_summaries.append(self.tf.Summary.Value(tag=label, image=img_sum))\n\n            # Create and write Summary\n            summary = self.tf.Summary(value=img_summaries)\n            self.writer.add_summary(summary, step)\n\n        if self.use_html:  # save images to a html file\n            for label, image_numpy in visuals.items():\n                if isinstance(image_numpy, list):\n                    for i in range(len(image_numpy)):\n                        img_path = os.path.join(self.img_dir, \'epoch%.3d_%s_%d.jpg\' % (epoch, label, i))\n                        util.save_image(image_numpy[i], img_path)\n                else:\n                    img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.jpg\' % (epoch, label))\n                    util.save_image(image_numpy, img_path)\n\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, refresh=60)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    if isinstance(image_numpy, list):\n                        for i in range(len(image_numpy)):\n                            img_path = \'epoch%.3d_%s_%d.jpg\' % (n, label, i)\n                            ims.append(img_path)\n                            txts.append(label + str(i))\n                            links.append(img_path)\n                    else:\n                        img_path = \'epoch%.3d_%s.jpg\' % (n, label)\n                        ims.append(img_path)\n                        txts.append(label)\n                        links.append(img_path)\n                if len(ims) < 10:\n                    webpage.add_images(ims, txts, links, width=self.win_size)\n                else:\n                    num = int(round(len(ims) / 2.0))\n                    webpage.add_images(ims[:num], txts[:num], links[:num], width=self.win_size)\n                    webpage.add_images(ims[num:], txts[num:], links[num:], width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, errors, step):\n        if self.tf_log:\n            for tag, value in errors.items():\n                summary = self.tf.Summary(value=[self.tf.Summary.Value(tag=tag, simple_value=value)])\n                self.writer.add_summary(summary, step)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            if v != 0:\n                message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def print_current_error(self, epoch, loss_G, loss_D):\n        message = \'(epoch: %d) loss_G: %.3f loss_D: %.3f\' % (epoch, loss_G, loss_D)\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.jpg\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(save_path)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
geometric/bulb/bulb/__init__.py,0,b''
geometric/bulb/bulb/net.py,4,"b""import itertools\nimport torch\n\nfrom bulb.saver import Saver\n\n\nclass Net(object):\n    def __init__(self,\n                 model=None,\n                 writer=None,\n                 data_loader=None,\n                 weights_only=True,\n                 **kwargs):\n\n        self.model = model\n        self.writer = writer\n        self.data_loader = data_loader\n        self.weights_only = weights_only\n\n        self.num_epoch = 0\n\n        self._init(**kwargs)\n\n    def _register_vars(self, vars_dict):\n        for (var_name, var) in vars_dict.items():\n            setattr(self, var_name, var)\n\n    def _prepare(self):\n        for var_name in self._var_names:\n            var = getattr(self, var_name)\n            var = torch.tensor(var).cuda()\n            setattr(self, var_name, var)\n\n    def _process(self):\n        for var_name in itertools.chain(self._loss_names, self._metric_names):\n            var = getattr(self, var_name)\n            setattr(self, var_name, var.item())\n\n    def _log(self):\n        strings = [\n            '[{:s}]'.format(self.name),\n            'epoch: {:d}'.format(self.num_epoch + 1),\n            'batch: {:d}/{:d}'.format(self.num_batch + 1, len(self.data_loader)),\n            'loss: {:.4f}'.format(self.loss),\n        ]\n\n        print('\\t'.join(strings))\n\n    def _summarize(self):\n        for var_name in itertools.chain(self._loss_names, self._metric_names):\n            var = getattr(self, var_name)\n\n            if self.writer is not None:\n                self.writer.add_scalar(\n                    '{:s}/{:s}'.format(self.name, var_name),\n                    var,\n                    self.num_step,\n                )\n\n    def save(self):\n        assert self.name == 'train'\n\n        if self.saver is None:\n            return\n\n        if self.weights_only:\n            model = self.model.state_dict()\n            optimizer = self.optimizer.state_dict()\n        else:\n            model = self.model\n            optimizer = self.optimizer\n\n        obj = {\n            'model': model,\n            'optimizer': optimizer,\n        }\n\n        self.saver.save_model(obj, num_step=self.num_step)\n\n    def load(self, ckpt_dir):\n        obj = Saver.load_model(ckpt_dir=ckpt_dir)\n\n        if self.weights_only:\n            self.model.load_state_dict(obj['model'])\n\n            if self.name == 'train':\n                self.optimizer.load_state_dict(obj['optimizer'])\n\n            elif (self.name == 'test') or (self.name == 'online'):\n                pass\n        else:\n            self.model = obj['model']\n\n            if self.name == 'train':\n                self.optimizer = obj['optimizer']\n\n            elif (self.name == 'test') or (self.name == 'online'):\n                pass\n\n    def step_epoch(self, num_step=None):\n        self.pre_epoch()\n\n        for (self.num_batch, vars_dict) in enumerate(self.data_loader):\n            self.num_step = num_step or self.num_epoch * len(self.data_loader) + self.num_batch\n\n            self._var_names = list(vars_dict.keys())\n            self._register_vars(vars_dict)\n\n            self.pre_batch()\n\n            result = self.step_batch()\n\n            metrics_dict = {}\n            if 'loss' in result:\n                losses_dict = result['loss']\n                if 'metrics' in result:\n                    metrics_dict = result['metrics']\n            else:\n                losses_dict = result\n\n            losses_dict['loss'] = torch.tensor(0.0).cuda() + sum(losses_dict.values())\n\n            self._loss_names = list(losses_dict.keys())\n            self._register_vars(losses_dict)\n\n            self._metric_names = list(metrics_dict.keys())\n            self._register_vars(metrics_dict)\n\n            self.post_batch()\n\n        self.post_epoch()\n\n        self.num_epoch += 1\n\n\nclass TrainMixin(object):\n    name = 'train'\n\n    def _init(self,\n              optimizer=None,\n              lr=None,\n              lr_decay_epochs=None,\n              lr_decay_rate=None,\n              log_steps=1,\n              summarize_steps=1,\n              save_steps=None,\n              saver=None):\n\n        if optimizer is not None:\n            self.optimizer = optimizer\n        elif lr is not None:\n            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n\n        if lr_decay_epochs is None:\n            self.scheduler = None\n        else:\n            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=lr_decay_epochs, gamma=lr_decay_rate)\n\n        self.log_steps = log_steps\n        self.summarize_steps = summarize_steps\n        self.save_steps = save_steps\n        self.saver = saver\n\n    def _optimize(self):\n        self.optimizer.zero_grad()\n        self.loss.backward()\n        self.optimizer.step()\n\n    def pre_batch(self):\n        self._prepare()\n\n    def post_batch(self):\n        self._optimize()\n        self._process()\n\n        if (self.num_step % self.log_steps == 0):\n            self._log()\n\n        if (self.num_step % self.summarize_steps == 0):\n            self._summarize()\n\n        if (self.save_steps is not None) and (self.num_step % self.save_steps == 0):\n            self.save()\n\n    def pre_epoch(self):\n        self.model.train()\n\n    def post_epoch(self):\n        if self.scheduler is not None:\n            self.scheduler.step(epoch=self.num_epoch)\n\n        if self.writer is not None:\n            self.writer.add_scalar(\n                '{:s}/lr'.format(self.name),\n                self.optimizer.param_groups[0]['lr'],\n                self.num_step,\n            )\n\n\nclass TestMixin(object):\n    name = 'test'\n\n    def _init(self):\n        pass\n\n    def pre_batch(self):\n        self._prepare()\n\n    def post_batch(self):\n        self._process()\n\n        for var_name in itertools.chain(self._loss_names, self._metric_names):\n            if var_name in self._loss_metrics:\n                _var = self._loss_metrics[var_name]\n            else:\n                _var = 0\n\n            var = getattr(self, var_name)\n            self._loss_metrics[var_name] = _var + (var - _var) / (self.num_batch + 1)\n\n    def pre_epoch(self):\n        self.model.eval()\n        self._loss_metrics = {}\n\n    def post_epoch(self):\n        for var_name in self._loss_metrics.keys():\n            setattr(self, var_name, self._loss_metrics[var_name])\n\n        self._log()\n        self._summarize()\n"""
geometric/bulb/bulb/saver.py,2,"b""import glob\nimport json\nimport os\nimport torch\n\n\nclass Saver(object):\n    @staticmethod\n    def meta_path(dir_):\n        return os.path.join(dir_, 'meta.json')\n\n    @staticmethod\n    def model_path(dir_, num_step=None, model_name=None):\n        model_name = model_name or 'model-*.ckpt'\n\n        if num_step is not None:\n            model_name = model_name.replace('*', '{:d}'.format(num_step))\n\n        return os.path.join(dir_, model_name)\n\n    @staticmethod\n    def load_model(ckpt_dir, num_step=None, model_name=None):\n        if num_step is None:\n            model_paths = glob.glob(Saver.model_path(ckpt_dir, num_step, model_name))\n            model_path = max(model_paths, key=os.path.getmtime)\n        else:\n            model_path = Saver.model_path(ckpt_dir, num_step)\n\n        return torch.load(model_path)\n\n    def __init__(self, working_dir):\n        self.working_dir = working_dir\n\n    def save_meta(self, meta):\n        with open(Saver.meta_path(self.working_dir), 'w') as f:\n            json.dump(meta, f, indent=4)\n\n    def save_model(self, obj, num_step):\n        torch.save(obj, Saver.model_path(self.working_dir, num_step))\n"""
geometric/bulb/bulb/utils.py,0,"b""import os\nimport time\n\n\ndef new_working_dir(working_dir_root, name=None):\n    working_dir = os.path.join(working_dir_root, time.strftime('%Y-%m-%d-%H%M%S'))\n    if name is not None:\n        working_dir += '-' + name\n\n    os.makedirs(working_dir)\n    return working_dir\n\n\nclass Message(object):\n    num_level = 0\n    delim = '  '\n\n    def __init__(self, message=None):\n        self.message = message or ''\n\n    def __enter__(self):\n        self.start = time.time()\n        print('{:s}{:s} --->'.format(Message.delim * Message.num_level, self.message))\n\n        Message.num_level += 1\n        return self\n\n    def __exit__(self, *args):\n        Message.num_level -= 1\n\n        print('{:s}---> {:s} [{:.4f} s]'.format(Message.delim * Message.num_level, self.message, time.time() - self.start))\n"""
geometric/derender3d/models/__init__.py,29,"b""import numpy as np\nimport os\nimport torch\nimport sys\n\nfrom torch import Tensor, IntTensor\nfrom torch.distributions import Categorical\nfrom torch.nn.modules import Module\n\nimport neural_renderer as nr\n\nfrom derender3d import TargetType\nfrom derender3d.models.derenderer import Derenderer\nfrom derender3d.models.renderer import RenderType, Renderer\nfrom derender3d.models.transforms import FFD, PerspectiveTransform\n\n\nclass ShapenetObj(object):\n    root_dir = os.getenv('SHAPENET_ROOT_DIR')\n\n    def __init__(self,\n                 class_id,\n                 obj_id,\n                 root_dir=root_dir):\n\n        path = os.path.join(root_dir, class_id, obj_id, 'models', 'model_normalized.obj')\n        print('Reading {:s}'.format(path))\n\n        (vertices, faces) = nr.load_obj(path)\n        vertices = vertices / np.ptp(vertices, axis=0)\n        vertices = vertices[:, [2, 1, 0]] * np.asarray([-1, 1, 1], dtype=np.float32)\n\n        self.vertices = Tensor(vertices)\n        self.faces = IntTensor(faces)\n\n\nclass Derenderer3d(Module):\n    def __init__(self, mode, image_size, render_size):\n        super(Derenderer3d, self).__init__()\n\n        self.mode = mode\n        self.image_size = image_size\n        self.render_size = render_size\n\n        self.derenderer = Derenderer()\n\n        self._force_no_sample = False\n\n        if mode & TargetType.reproject:\n            self.objs = [\n                ShapenetObj(class_id='02958343', obj_id='137f67657cdc9da5f985cd98f7d73e9a'),\n                ShapenetObj(class_id='02958343', obj_id='5343e944a7753108aa69dfdc5532bb13'),\n                ShapenetObj(class_id='02958343', obj_id='3776e4d1e2587fd3253c03b7df20edd5'),\n                ShapenetObj(class_id='02958343', obj_id='3ba5bce1b29f0be725f689444c7effe2'),\n                ShapenetObj(class_id='02958343', obj_id='53a031dd120e81dc3aa562f24645e326'),\n                ShapenetObj(class_id='02924116', obj_id='7905d83af08a0ca6dafc1d33c05cbcf8'),\n                ShapenetObj(class_id='02958343', obj_id='a0fe4aac120d5f8a5145cad7315443b3'),\n                ShapenetObj(class_id='02958343', obj_id='cd7feedd6041209131ac5fb37e6c8324'),\n            ]\n            self.ffds = [FFD(obj.vertices, constraints=[\n                FFD.Constraint.symmetry(axis=FFD.Constraint.Axis.z),\n                FFD.Constraint.homogeneity(axis=FFD.Constraint.Axis.y, index=[0, 1]),\n            ]) for obj in self.objs]\n            self.perspective_transform = PerspectiveTransform()\n            self.renderer = Renderer(image_size=render_size)\n\n    def forward(self, images, roi_norms, focals):\n        # batch_size = len(images)\n\n        _mroi_norms = torch.stack([\n            roi_norms[:, 2] + roi_norms[:, 0],\n            roi_norms[:, 3] + roi_norms[:, 1],\n        ], dim=1) / 2.0\n        _droi_norms = torch.stack([\n            roi_norms[:, 2] - roi_norms[:, 0],\n            roi_norms[:, 3] - roi_norms[:, 1],\n        ], dim=1)\n\n        _blob = {\n            '_roi_norms': roi_norms,\n            '_mroi_norms': _mroi_norms,\n            '_droi_norms': _droi_norms,\n            '_focals': focals,\n        }\n\n        _blob.update(self.derenderer(images, _mroi_norms, _droi_norms))\n\n        if not (self.mode & TargetType.reproject):\n            return _blob\n\n        _blob.update(self.render(_blob))\n        return _blob\n\n    def render(self, blob):\n        _mroi_norms = blob['_mroi_norms']\n        _droi_norms = blob['_droi_norms']\n        _focals = blob['_focals']\n        _theta_deltas = blob['_theta_deltas']\n        _translation2ds = blob['_translation2ds']\n        _log_scales = blob['_log_scales']\n        _log_depths = blob['_log_depths']\n        _class_probs = blob['_class_probs']\n        _ffd_coeffs = blob['_ffd_coeffs']\n\n        batch_size = len(_focals)\n\n        _thetas = torch.unsqueeze(torch.atan2(_theta_deltas[:, 1], _theta_deltas[:, 0]), dim=1)\n        _rotations = torch.cat([\n            torch.cos(_thetas / 2),\n            torch.zeros(batch_size, 1).cuda(),\n            torch.sin(_thetas / 2),\n            torch.zeros(batch_size, 1).cuda(),\n        ], dim=1)\n        _areas = torch.unsqueeze(_droi_norms[:, 0] * _droi_norms[:, 1], dim=1)\n\n        _scales = torch.exp(_log_scales)\n        _depths = torch.sqrt(torch.exp(_log_depths) / _areas)\n\n        _center2ds = _mroi_norms + _translation2ds * _droi_norms\n        _translation_units = torch.stack([\n            _center2ds[:, 1],\n            - _center2ds[:, 0],\n            - torch.ones(batch_size).cuda(),\n        ], dim=1)\n        _translation_units = _translation_units / torch.norm(_translation_units, p=2, dim=1, keepdim=True)\n        _translations = _depths * _translation_units\n\n        _alphas = - (_thetas - torch.atan(_translations[:, 0:1] / _translations[:, 2:3]))\n        _alphas = torch.remainder(_alphas + np.pi, 2 * np.pi) - np.pi\n\n        if self.training and not self._force_no_sample:\n            _class_dists = Categorical(_class_probs)\n            _class_samples = _class_dists.sample()\n            _class_log_probs = _class_dists.log_prob(_class_samples)\n\n        else:\n            (_class_max_probs, _class_samples) = torch.max(_class_probs, dim=1)\n            # print(_class_max_probs)\n            # print(_class_samples)\n            _class_log_probs = torch.log(_class_max_probs)\n\n        if self.training:\n            _perspective_translation_units = torch.stack([\n                _mroi_norms[:, 1],\n                - _mroi_norms[:, 0],\n                - torch.ones(batch_size).cuda(),\n            ], dim=1)\n            _perspective_translation_units = _perspective_translation_units / torch.norm(_perspective_translation_units, p=2, dim=1, keepdim=True)\n            _perspective_translations = _depths * _perspective_translation_units\n            _zooms = (self.image_size / _focals) / torch.max(_droi_norms, dim=1, keepdim=True)[0]\n\n        else:\n            _zoom_tos = self.render_size / (2.0 * _focals)\n            _zooms = []\n\n        device = torch.cuda.current_device()\n\n        _masks = []\n        _normals = []\n        _depth_maps = []\n        for size in range(batch_size):\n            print(device, end='')\n            sys.stdout.flush()\n\n            _class_sample = int(_class_samples[size])\n            _ffd_coeff = _ffd_coeffs[size][_class_sample]\n\n            vertices = self.ffds[_class_sample](_ffd_coeff)\n            faces = self.objs[_class_sample].faces.cuda()\n\n            __vertices = vertices.unsqueeze(dim=0)\n            __faces = faces.unsqueeze(dim=0)\n\n            __scales = _scales[size].unsqueeze(dim=0)\n            __rotations = _rotations[size].unsqueeze(dim=0)\n            __translations = _translations[size].unsqueeze(dim=0)\n\n            if self.training:\n                __perspective_translations = _perspective_translations[size].unsqueeze(dim=0)\n                __zooms = _zooms[size].unsqueeze(dim=0)\n\n                __vertices = self.perspective_transform(\n                    __vertices,\n                    scales=__scales,\n                    rotations=__rotations,\n                    translations=__translations,\n                    perspective_translations=__perspective_translations,\n                    zooms=__zooms,\n                )\n            else:\n                __zoom_tos = _zoom_tos[size].unsqueeze(dim=0)\n                (__vertices, __zooms) = self.perspective_transform(\n                    __vertices,\n                    scales=__scales,\n                    rotations=__rotations,\n                    translations=__translations,\n                    perspective_translations=__translations,\n                    zoom_tos=__zoom_tos,\n                )\n                _zooms.append(__zooms)\n\n            self.renderer.viewing_angle = np.arctan(self.render_size / (2.0 * _focals[size].item())) / np.pi * 180\n            __masks = self.renderer(\n                vertices=__vertices,\n                faces=__faces,\n                render_type=RenderType.Silhouette,\n            )\n            _masks.append(__masks)\n\n            if self.mode & TargetType.normal:\n                __normals = self.renderer(\n                    vertices=__vertices,\n                    faces=__faces,\n                    render_type=RenderType.Normal,\n                )\n                _normals.append(__normals)\n\n            if self.mode & TargetType.depth:\n                __depth_maps = self.renderer(\n                    vertices=__vertices,\n                    faces=__faces,\n                    render_type=RenderType.Depth,\n                )\n                _depth_maps.append(__depth_maps)\n\n        if not self.training:\n            _zooms = torch.cat(_zooms, dim=0)\n\n        _masks = torch.cat(_masks, dim=0)\n\n        if self.mode & TargetType.normal:\n            _normals = torch.cat(_normals, dim=0)\n\n        if self.mode & TargetType.depth:\n            _depth_maps = torch.cat(_depth_maps, dim=0)\n\n        return {\n            '_thetas': _thetas,\n            '_alphas': _alphas,\n            '_rotations': _rotations,\n            '_scales': _scales,\n            '_depths': _depths,\n            '_center2ds': _center2ds,\n            '_translations': _translations,\n            '_class_log_probs': _class_log_probs,\n            '_zooms': _zooms,\n            '_masks': _masks,\n            '_normals': _normals,\n            '_depth_maps': _depth_maps,\n        }\n"""
geometric/derender3d/models/derenderer.py,11,"b""import torch\nimport torchvision\n\nfrom torch.nn.modules import Module\n\n\nclass Derenderer(Module):\n    in_size = 4\n    hidden_size = 256\n\n    def __init__(self, num_classes=8, grid_size=4):\n        super(Derenderer, self).__init__()\n\n        self.num_classes = num_classes\n        self.grid_size = grid_size\n        self.out_sizes = {\n            '_theta_deltas': 2,\n            '_translation2ds': 2,\n            '_log_scales': 3,\n            '_log_depths': 1,\n            '_class_probs': num_classes,\n            '_ffd_coeffs': num_classes * (grid_size ** 3) * 3\n        }\n\n        self.net = torchvision.models.resnet18(pretrained=True)\n        self.net.avgpool = torch.nn.AdaptiveAvgPool2d(1)  # PATCH\n        self.net.fc = torch.nn.Linear(512, Derenderer.hidden_size)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n        self.fc1 = torch.nn.Linear(self.hidden_size + self.in_size, self.hidden_size)\n        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n        self._fc3 = torch.nn.Linear(self.hidden_size, sum(self.out_sizes.values()))\n\n    def forward(self, images, mroi_norms, droi_norms):\n        x = self.net(images)\n        x = self.relu(x)\n\n        x = torch.cat([x, mroi_norms, droi_norms], dim=1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self._fc3(x)\n\n        (\n            _theta_deltas,\n            _translation2ds,\n            _log_scales,\n            _log_depths,\n            _class_probs,\n            _ffd_coeffs,\n        ) = torch.split(x, list(self.out_sizes.values()), dim=1)\n\n        _theta_deltas = _theta_deltas / torch.norm(_theta_deltas, p=2, dim=1, keepdim=True)\n        _class_probs = torch.nn.functional.softmax(_class_probs, dim=1)\n        _ffd_coeffs = _ffd_coeffs.view(-1, self.num_classes, (self.grid_size ** 3) * 3)\n\n        return {\n            '_theta_deltas': _theta_deltas,\n            '_translation2ds': _translation2ds,\n            '_log_scales': _log_scales,\n            '_log_depths': _log_depths,\n            '_class_probs': _class_probs,\n            '_ffd_coeffs': _ffd_coeffs,\n        }\n"""
geometric/derender3d/models/renderer.py,5,"b""import chainer\nimport chainer.functions as cf\nimport torch\n\nfrom torch import Tensor\nfrom torch.autograd import Function\nfrom torch.nn.modules import Module\n\nimport neural_renderer as nr\n\n\nclass RenderType:\n    RGB = 0\n    Silhouette = 1\n    Depth = 2\n    Normal = 3\n\n\nclass _Renderer(nr.Renderer):\n    def render_silhouettes(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = nr.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = nr.look(vertices, self.eye, self.camera_direction, self.up)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = nr.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize_silhouettes(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_depth(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = nr.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = nr.look(vertices, self.eye, self.camera_direction, self.up)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = nr.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize_depth(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_normal(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n\n        # normal\n        faces_normal = nr.vertices_to_faces(vertices, faces)\n\n        (bs, nf) = faces_normal.shape[:2]\n        faces_normal = faces_normal.reshape((bs * nf, 3, 3))\n        v10 = faces_normal[:, 0] - faces_normal[:, 1]\n        v12 = faces_normal[:, 2] - faces_normal[:, 1]\n        normals = cf.normalize(nr.cross(v10, v12))\n        normals = normals.reshape((bs, nf, 3))\n\n        textures = normals[:, :, None, None, None, :]\n        textures = cf.tile(textures, (1, 1, 2, 2, 2, 1))\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = nr.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = nr.look(vertices, self.eye, self.camera_direction, self.up)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = nr.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return images\n\n    def render(self, vertices, faces, textures):\n        # fill back\n        if self.fill_back:\n            faces = cf.concat((faces, faces[:, :, ::-1]), axis=1).data\n            textures = cf.concat((textures, textures.transpose((0, 1, 4, 3, 2, 5))), axis=1)\n\n        # lighting\n        faces_lighting = nr.vertices_to_faces(vertices, faces)\n        textures = nr.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # viewpoint transformation\n        if self.camera_mode == 'look_at':\n            vertices = nr.look_at(vertices, self.eye)\n        elif self.camera_mode == 'look':\n            vertices = nr.look(vertices, self.eye, self.camera_direction, self.up)\n\n        # perspective transformation\n        if self.perspective:\n            vertices = nr.perspective(vertices, angle=self.viewing_angle)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return images\n\n\nclass RenderFunction(Function):\n    # Unfortunately direct on-GPU transfer from torch to chainer remains a mystery,\n    # so let's settle for a GPU-CPU-GPU transfer scheme\n\n    @staticmethod\n    def torch2numpy(torch_tensor):\n        return torch_tensor.detach().cpu().numpy()\n\n    @staticmethod\n    def chainer2numpy(chainer_tensor):\n        return chainer.cuda.to_cpu(chainer_tensor)\n\n    @staticmethod\n    def torch2chainer(torch_tensor):\n        device = torch_tensor.get_device()\n        return chainer.cuda.to_gpu(RenderFunction.torch2numpy(torch_tensor), device=device)\n\n    @staticmethod\n    def chainer2torch(chainer_tensor):\n        device = chainer.cuda.get_device_from_array(chainer_tensor).id\n        return torch.Tensor(RenderFunction.chainer2numpy(chainer_tensor)).cuda(device=device)\n\n    @staticmethod\n    def forward(ctx,\n                vertices,\n                faces,\n                textures,\n                renderer,\n                render_type,\n                eye,\n                camera_mode,\n                camera_direction,\n                camera_up):\n\n        _vertices = chainer.Variable(RenderFunction.torch2chainer(vertices))\n        _faces = chainer.Variable(RenderFunction.torch2chainer(faces))\n        _textures = None\n        _eye = chainer.Variable(RenderFunction.torch2chainer(eye))\n        _camera_direction = chainer.Variable(RenderFunction.torch2chainer(camera_direction))\n        _camera_up = chainer.Variable(RenderFunction.torch2chainer(camera_up))\n\n        if render_type == RenderType.RGB:\n            _textures = chainer.Variable(RenderFunction.torch2chainer(textures))\n\n        renderer.eye = _eye\n        renderer.camera_mode = camera_mode\n        renderer.camera_direction = _camera_direction\n        renderer.up = _camera_up\n\n        if render_type == RenderType.RGB:\n            _images = renderer.render(_vertices, _faces, _textures)\n        elif render_type == RenderType.Silhouette:\n            _images = renderer.render_silhouettes(_vertices, _faces)\n            _images = chainer.functions.expand_dims(_images, axis=1)\n        elif render_type == RenderType.Depth:\n            _images = renderer.render_depth(_vertices, _faces)\n            _images = chainer.functions.expand_dims(_images, axis=1)\n        elif render_type == RenderType.Normal:\n            _images = renderer.render_normal(_vertices, _faces)\n\n        ctx._vertices = _vertices\n        ctx._textures = _textures\n        ctx._render_type = render_type\n        ctx._images = _images\n\n        images = RenderFunction.chainer2torch(_images.data)\n        return images\n\n    @staticmethod\n    def backward(ctx, grad_images):\n        _grad_images = chainer.Variable(RenderFunction.torch2chainer(grad_images.data))\n\n        ctx._images.grad_var = _grad_images\n        ctx._images.backward()\n\n        grad_vertices = None\n        if ctx.needs_input_grad[0]:\n            grad_vertices = RenderFunction.chainer2torch(ctx._vertices.grad_var.data)\n\n        grad_textures = None\n        if ctx.needs_input_grad[2] and (ctx._render_type == RenderType.RGB):\n            grad_textures = RenderFunction.chainer2torch(ctx._textures.grad_var.data)\n\n        return (grad_vertices, None, grad_textures, None, None, None, None, None, None)\n\n\nclass Renderer(Module):\n    def __init__(self,\n                 image_size=256,\n                 viewing_angle=30):\n\n        super(Renderer, self).__init__()\n\n        self.image_size = image_size\n        self.viewing_angle = viewing_angle\n\n        self.eye = Tensor([0, 0, 0])\n        self.camera_mode = 'look'\n        self.camera_direction = Tensor([0, 0, -1])\n        self.camera_up = Tensor([0, 1, 0])\n\n    def forward(self,\n                vertices,\n                faces,\n                textures=None,\n                render_type=RenderType.RGB):\n\n        _renderer = _Renderer()\n        _renderer.image_size = self.image_size\n        _renderer.viewing_angle = self.viewing_angle\n\n        # HUGE BUG FOR DIZ RENDERER: IT INVERTS X AXIS!!!\n        # WE HENCE FIX IT HERE\n        vertices = vertices * Tensor([-1, 1, 1]).cuda()\n\n        eye = self.eye.cuda()\n        camera_mode = self.camera_mode\n        camera_direction = self.camera_direction.cuda()\n        camera_up = self.camera_up.cuda()\n\n        batch_size = len(vertices)\n\n        eye = eye[None, :].expand(batch_size, -1)\n        camera_direction = camera_direction[None, :].expand(batch_size, -1)\n        camera_up = camera_up[None, :].expand(batch_size, -1)\n\n        images = RenderFunction.apply(\n            vertices,\n            faces,\n            textures,\n            _renderer,\n            render_type,\n            eye,\n            camera_mode,\n            camera_direction,\n            camera_up,\n        )\n\n        if render_type == RenderType.Normal:\n            (x, y, z) = torch.unbind(images, dim=1)\n            images = torch.stack([-x, y, z], dim=1)\n\n        return images\n"""
geometric/derender3d/models/transforms.py,18,"b""import numpy as np\nimport torch\nimport scipy.ndimage\nimport scipy.special\n\nfrom torch import Tensor\nfrom torch.nn.modules import Module\n\n\nclass FFD(Module):\n    class Constraint:\n        class Type:\n            symmetry = 0\n            homogeneity = 1\n\n        class Axis:\n            x = 0\n            y = 1\n            z = 2\n\n        @staticmethod\n        def symmetry(axis):\n            c = FFD.Constraint(FFD.Constraint.Type.symmetry)\n            c.axis = axis\n            return c\n\n        @staticmethod\n        def homogeneity(axis, index):\n            c = FFD.Constraint(FFD.Constraint.Type.homogeneity)\n            c.axis = axis\n            c.index = index\n            return c\n\n        def __init__(self, type):\n            self.type = type\n\n    @staticmethod\n    def flip(x, dim):\n        shape = x.shape\n        index = torch.arange(shape[dim] - 1, -1, -1, dtype=torch.long).cuda()\n        return torch.index_select(x, dim, index)\n\n    def __init__(self, vertices, num_grids=4, constraints=None):\n        super(FFD, self). __init__()\n\n        assert num_grids % 2 == 0\n\n        self.num_grids = num_grids\n        self.constraints = constraints\n\n        grids = np.arange(num_grids)\n\n        binoms = Tensor(scipy.special.binom(num_grids - 1, grids))\n        grid_1ds = Tensor(grids)\n        grid_3ds = Tensor(np.meshgrid(grids, grids, grids, indexing='ij'))\n\n        coeff = (\n            binoms *\n            torch.pow(torch.unsqueeze(0.5 + vertices, dim=2), grid_1ds) *\n            torch.pow(torch.unsqueeze(0.5 - vertices, dim=2), num_grids - 1 - grid_1ds)\n        )\n\n        self.B = torch.einsum('ni,nj,nk->nijk', torch.unbind(coeff, dim=1))\n        self.B = torch.unsqueeze(self.B, dim=1)\n\n        self.P0 = grid_3ds / (num_grids - 1) - 0.5\n\n    def forward(self, ffd_coeff):\n        dP = ffd_coeff.view(3, self.num_grids, self.num_grids, self.num_grids)\n\n        for constraint in self.constraints:\n            if constraint.type == FFD.Constraint.Type.symmetry:\n                _dP = FFD.flip(dP, dim=constraint.axis + 1)\n                (_dPx, _dPy, _dPz) = torch.unbind(_dP, dim=0)\n                _dP = torch.stack([_dPx, _dPy, -_dPz], dim=0)\n\n                dP = (dP + _dP) / 2\n\n            elif constraint.type == FFD.Constraint.Type.homogeneity:\n                dPs = torch.unbind(dP, dim=constraint.axis + 1)\n\n                _dPs = [dPs[index] for index in constraint.index]\n                _dP_mean = sum(_dPs) / len(_dPs)\n\n                _dPs = []\n                for index in range(self.num_grids):\n                    if index in constraint.index:\n                        _dP = _dP_mean.clone()\n                        _dP[constraint.axis] = dPs[index][constraint.axis]\n                    else:\n                        _dP = dPs[index]\n\n                    _dPs.append(_dP)\n\n                dP = torch.stack(_dPs, dim=constraint.axis + 1)\n\n        PB = (self.P0.cuda() + dP) * self.B.cuda()\n        V = PB.view(-1, 3, self.num_grids * self.num_grids * self.num_grids).sum(dim=2)\n        return V\n\n\nclass PerspectiveTransform(Module):\n    def forward(self,\n                vertices,\n                scales=None,\n                rotations=None,\n                translations=None,\n                perspective_translations=None,\n                zooms=None,\n                zoom_tos=None):\n\n        if scales is not None:\n            scales = scales.unsqueeze(dim=1)\n            vertices = vertices * scales\n\n        if rotations is not None:\n            (a, b, c, d) = torch.unbind(rotations, dim=1)\n\n            T = torch.stack([\n                a * a + b * b - c * c - d * d,\n                2 * b * c - 2 * a * d,\n                2 * b * d + 2 * a * c,\n                2 * b * c + 2 * a * d,\n                a * a - b * b + c * c - d * d,\n                2 * c * d - 2 * a * b,\n                2 * b * d - 2 * a * c,\n                2 * c * d + 2 * a * b,\n                a * a - b * b - c * c + d * d,\n            ], dim=1).view(-1, 3, 3)\n\n            vertices = torch.matmul(vertices, torch.transpose(T, dim0=1, dim1=2))\n\n        if translations is not None:\n            translations = translations.unsqueeze(dim=1)\n            vertices = vertices + translations\n\n        if perspective_translations is not None:\n            perspective_translations = perspective_translations.unsqueeze(dim=1)\n        else:\n            perspective_translations = translations\n\n        (x, y, z) = torch.unbind(vertices, dim=2)\n        (x0, y0, z0) = torch.unbind(perspective_translations, dim=2)\n\n        x = x - x0 / z0 * z\n        y = y - y0 / z0 * z\n\n        if zoom_tos is not None:\n            zooms = torch.min(torch.abs(z) / torch.max(torch.abs(x), torch.abs(y)), dim=1, keepdim=True)[0] * zoom_tos\n\n        z = z / zooms\n\n        vertices = torch.stack([x, y, z], dim=2)\n\n        if zoom_tos is None:\n            return vertices\n        else:\n            return (vertices, zooms)\n"""
geometric/maskrcnn/nms/__init__.py,0,b''
geometric/maskrcnn/nms/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/nms.c']\nheaders = ['src/nms.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/nms_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
geometric/maskrcnn/nms/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nms.pth_nms import pth_nms\n\n\ndef nms(dets, thresh):\n    """"""Dispatch to either CPU or GPU NMS implementations.\n    Accept dets as tensor""""""\n    return pth_nms(dets, thresh)\n'"
geometric/maskrcnn/nms/pth_nms.py,9,"b'import torch\nfrom ._ext import nms\nimport numpy as np\n\n\ndef pth_nms(dets, thresh):\n    """"""\n    dets has to be a tensor\n    """"""\n    if not dets.is_cuda:\n        x1 = dets[:, 1]\n        y1 = dets[:, 0]\n        x2 = dets[:, 3]\n        y2 = dets[:, 2]\n        scores = dets[:, 4]\n\n        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n        order = scores.sort(0, descending=True)[1]\n        # order = torch.from_numpy(np.ascontiguousarray(scores.numpy().argsort()[::-1])).long()\n\n        keep = torch.LongTensor(dets.size(0))\n        num_out = torch.LongTensor(1)\n        nms.cpu_nms(keep, num_out, dets, order, areas, thresh)\n\n        return keep[:num_out[0]]\n    else:\n        x1 = dets[:, 1]\n        y1 = dets[:, 0]\n        x2 = dets[:, 3]\n        y2 = dets[:, 2]\n        scores = dets[:, 4]\n\n        dets_temp = torch.FloatTensor(dets.size()).cuda()\n        dets_temp[:, 0] = dets[:, 1]\n        dets_temp[:, 1] = dets[:, 0]\n        dets_temp[:, 2] = dets[:, 3]\n        dets_temp[:, 3] = dets[:, 2]\n        dets_temp[:, 4] = dets[:, 4]\n\n        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n        order = scores.sort(0, descending=True)[1]\n        # order = torch.from_numpy(np.ascontiguousarray(scores.cpu().numpy().argsort()[::-1])).long().cuda()\n\n        dets = dets[order].contiguous()\n\n        keep = torch.LongTensor(dets.size(0))\n        num_out = torch.LongTensor(1)\n        # keep = torch.cuda.LongTensor(dets.size(0))\n        # num_out = torch.cuda.LongTensor(1)\n        nms.gpu_nms(keep, num_out, dets_temp, thresh)\n\n        return order[keep[:num_out[0]].cuda()].contiguous()\n        # return order[keep[:num_out[0]]].contiguous()\n'"
geometric/maskrcnn/roialign/__init__.py,0,b''
semantic/lib/nn/__init__.py,0,"b'from .modules import *\nfrom .parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
semantic/lib/utils/__init__.py,0,b'from .th import *\n'
semantic/lib/utils/th.py,3,"b""import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport collections\n\n__all__ = ['as_variable', 'as_numpy', 'mark_volatile']\n\n\ndef as_variable(obj):\n    if isinstance(obj, Variable):\n        return obj\n    if isinstance(obj, collections.Sequence):\n        return [as_variable(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_variable(v) for k, v in obj.items()}\n    else:\n        return Variable(obj)\n\n\ndef as_numpy(obj):\n    if isinstance(obj, collections.Sequence):\n        return [as_numpy(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_numpy(v) for k, v in obj.items()}\n    elif isinstance(obj, Variable):\n        return obj.data.cpu().numpy()\n    elif torch.is_tensor(obj):\n        return obj.cpu().numpy()\n    else:\n        return np.array(obj)\n\n\ndef mark_volatile(obj):\n    if torch.is_tensor(obj):\n        obj = Variable(obj)\n    if isinstance(obj, Variable):\n        obj.no_grad = True\n        return obj\n    elif isinstance(obj, collections.Mapping):\n        return {k: mark_volatile(o) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [mark_volatile(o) for o in obj]\n    else:\n        return obj\n"""
geometric/maskrcnn/roialign/roi_align/__init__.py,0,b''
geometric/maskrcnn/roialign/roi_align/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/crop_and_resize.c']\nheaders = ['src/crop_and_resize.h']\ndefines = []\nwith_cuda = False\n\nextra_objects = []\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/crop_and_resize_gpu.c']\n    headers += ['src/crop_and_resize_gpu.h']\n    defines += [('WITH_CUDA', None)]\n    extra_objects += ['src/cuda/crop_and_resize_kernel.cu.o']\n    with_cuda = True\n\nextra_compile_args = ['-fopenmp', '-std=c99']\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nsources = [os.path.join(this_file, fname) for fname in sources]\nheaders = [os.path.join(this_file, fname) for fname in headers]\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.crop_and_resize',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=extra_compile_args\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
geometric/maskrcnn/roialign/roi_align/crop_and_resize.py,5,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nfrom ._ext import crop_and_resize as _backend\n\n\nclass CropAndResizeFunction(Function):\n\n    def __init__(self, crop_height, crop_width, extrapolation_value=0):\n        self.crop_height = crop_height\n        self.crop_width = crop_width\n        self.extrapolation_value = extrapolation_value\n\n    def forward(self, image, boxes, box_ind):\n        crops = torch.zeros_like(image)\n\n        if image.is_cuda:\n            _backend.crop_and_resize_gpu_forward(\n                image, boxes, box_ind,\n                self.extrapolation_value, self.crop_height, self.crop_width, crops)\n        else:\n            _backend.crop_and_resize_forward(\n                image, boxes, box_ind,\n                self.extrapolation_value, self.crop_height, self.crop_width, crops)\n\n        # save for backward\n        self.im_size = image.size()\n        self.save_for_backward(boxes, box_ind)\n\n        return crops\n\n    def backward(self, grad_outputs):\n        boxes, box_ind = self.saved_tensors\n\n        grad_outputs = grad_outputs.contiguous()\n        grad_image = torch.zeros_like(grad_outputs).resize_(*self.im_size)\n\n        if grad_outputs.is_cuda:\n            _backend.crop_and_resize_gpu_backward(\n                grad_outputs, boxes, box_ind, grad_image\n            )\n        else:\n            _backend.crop_and_resize_backward(\n                grad_outputs, boxes, box_ind, grad_image\n            )\n\n        return grad_image, None, None\n\n\nclass CropAndResize(nn.Module):\n    """"""\n    Crop and resize ported from tensorflow\n    See more details on https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize\n    """"""\n\n    def __init__(self, crop_height, crop_width, extrapolation_value=0):\n        super(CropAndResize, self).__init__()\n\n        self.crop_height = crop_height\n        self.crop_width = crop_width\n        self.extrapolation_value = extrapolation_value\n\n    def forward(self, image, boxes, box_ind):\n        return CropAndResizeFunction(self.crop_height, self.crop_width, self.extrapolation_value)(image, boxes, box_ind)\n'"
geometric/maskrcnn/roialign/roi_align/roi_align.py,3,"b'import torch\nfrom torch import nn\n\nfrom .crop_and_resize import CropAndResizeFunction, CropAndResize\n\n\nclass RoIAlign(nn.Module):\n\n    def __init__(self, crop_height, crop_width, extrapolation_value=0, transform_fpcoor=True):\n        super(RoIAlign, self).__init__()\n\n        self.crop_height = crop_height\n        self.crop_width = crop_width\n        self.extrapolation_value = extrapolation_value\n        self.transform_fpcoor = transform_fpcoor\n\n    def forward(self, featuremap, boxes, box_ind):\n        """"""\n        RoIAlign based on crop_and_resize.\n        See more details on https://github.com/ppwwyyxx/tensorpack/blob/6d5ba6a970710eaaa14b89d24aace179eb8ee1af/examples/FasterRCNN/model.py#L301\n        :param featuremap: NxCxHxW\n        :param boxes: Mx4 float box with (x1, y1, x2, y2) **without normalization**\n        :param box_ind: M\n        :return: MxCxoHxoW\n        """"""\n        x1, y1, x2, y2 = torch.split(boxes, 1, dim=1)\n        image_height, image_width = featuremap.size()[2:4]\n\n        if self.transform_fpcoor:\n            spacing_w = (x2 - x1) / float(self.crop_width)\n            spacing_h = (y2 - y1) / float(self.crop_height)\n\n            nx0 = (x1 + spacing_w / 2 - 0.5) / float(image_width - 1)\n            ny0 = (y1 + spacing_h / 2 - 0.5) / float(image_height - 1)\n            nw = spacing_w * float(self.crop_width - 1) / float(image_width - 1)\n            nh = spacing_h * float(self.crop_height - 1) / float(image_height - 1)\n\n            boxes = torch.cat((ny0, nx0, ny0 + nh, nx0 + nw), 1)\n        else:\n            x1 = x1 / float(image_width - 1)\n            x2 = x2 / float(image_width - 1)\n            y1 = y1 / float(image_height - 1)\n            y2 = y2 / float(image_height - 1)\n            boxes = torch.cat((y1, x1, y2, x2), 1)\n\n        boxes = boxes.detach().contiguous()\n        box_ind = box_ind.detach()\n        return CropAndResizeFunction(self.crop_height, self.crop_width, self.extrapolation_value)(featuremap, boxes, box_ind)\n'"
semantic/lib/nn/modules/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
semantic/lib/nn/modules/batchnorm.py,9,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dementions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n        # customed batch norm statistics\n        self._moving_average_fraction = 1. - momentum\n        self.register_buffer(\'_tmp_running_mean\', torch.zeros(self.num_features))\n        self.register_buffer(\'_tmp_running_var\', torch.ones(self.num_features))\n        self.register_buffer(\'_running_iter\', torch.ones(1))\n        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n        self._tmp_running_var = self.running_var.clone() * self._running_iter\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n\n        return outputs\n\n    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n        """"""return *dest* by `dest := dest*alpha + delta*beta + bias`""""""\n        return dest * alpha + delta * beta + bias\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n\n        self.running_mean = self._tmp_running_mean / self._running_iter\n        self.running_var = self._tmp_running_var / self._running_iter\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n'"
semantic/lib/nn/modules/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
semantic/lib/nn/modules/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
semantic/lib/nn/modules/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n            np.allclose(npa, npb, atol=atol),\n            'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
semantic/lib/nn/parallel/__init__.py,0,"b'from .data_parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
semantic/lib/nn/parallel/data_parallel.py,5,"b'# -*- coding: utf8 -*-\n\nimport torch.cuda as cuda\nimport torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport collections\nfrom torch.nn.parallel._functions import Gather\n\n__all__ = [\'UserScatteredDataParallel\', \'user_scattered_collate\', \'async_copy_to\']\n\n\ndef async_copy_to(obj, dev, main_stream=None):\n    if torch.is_tensor(obj):\n        obj = Variable(obj)\n    if isinstance(obj, Variable):\n        v = obj.cuda(dev, non_blocking=True)\n        if main_stream is not None:\n            v.data.record_stream(main_stream)\n        return v\n    elif isinstance(obj, collections.Mapping):\n        return {k: async_copy_to(o, dev, main_stream) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [async_copy_to(o, dev, main_stream) for o in obj]\n    else:\n        return obj\n\n\ndef dict_gather(outputs, target_device, dim=0):\n    """"""\n    Gathers variables from different GPUs on a specified device\n      (-1 means the CPU), with dictionary support.\n    """"""\n    def gather_map(outputs):\n        out = outputs[0]\n        if isinstance(out, Variable):\n            # MJY(20180330) HACK:: force nr_dims > 0\n            if out.dim() == 0:\n                outputs = [o.unsqueeze(0) for o in outputs]\n            return Gather.apply(target_device, dim, *outputs)\n        elif out is None:\n            return None\n        elif isinstance(out, collections.Mapping):\n            return {k: gather_map([o[k] for o in outputs]) for k in out}\n        elif isinstance(out, collections.Sequence):\n            return type(out)(map(gather_map, zip(*outputs)))\n    return gather_map(outputs)\n\n\nclass DictGatherDataParallel(nn.DataParallel):\n    def gather(self, outputs, output_device):\n        return dict_gather(outputs, output_device, dim=self.dim)\n\n\nclass UserScatteredDataParallel(DictGatherDataParallel):\n    def scatter(self, inputs, kwargs, device_ids):\n        assert len(inputs) == 1\n        inputs = inputs[0]\n        inputs = _async_copy_stream(inputs, device_ids)\n        inputs = [[i] for i in inputs]\n        assert len(kwargs) == 0\n        kwargs = [{} for _ in range(len(inputs))]\n\n        return inputs, kwargs\n\n\ndef user_scattered_collate(batch):\n    return batch\n\n\ndef _async_copy(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    for i, dev in zip(inputs, device_ids):\n        with cuda.device(dev):\n            outputs.append(async_copy_to(i, dev))\n\n    return tuple(outputs)\n\n\ndef _async_copy_stream(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    streams = [_get_stream(d) for d in device_ids]\n    for i, dev, stream in zip(inputs, device_ids, streams):\n        with cuda.device(dev):\n            main_stream = cuda.current_stream()\n            with cuda.stream(stream):\n                outputs.append(async_copy_to(i, dev, main_stream=main_stream))\n            main_stream.wait_stream(stream)\n\n    return outputs\n\n\n""""""Adapted from: torch/nn/parallel/_functions.py""""""\n# background streams used for copying\n_streams = None\n\n\ndef _get_stream(device):\n    """"""Gets a background stream for copying between CPU and GPU""""""\n    global _streams\n    if device == -1:\n        return None\n    if _streams is None:\n        _streams = [None] * cuda.device_count()\n    if _streams[device] is None:\n        _streams[device] = cuda.Stream(device)\n    return _streams[device]\n'"
semantic/lib/utils/data/__init__.py,0,"b'from .dataset import Dataset, TensorDataset, ConcatDataset\nfrom .dataloader import DataLoader\n'"
semantic/lib/utils/data/dataloader.py,24,"b'import torch\nimport torch.multiprocessing as multiprocessing\nfrom torch._C import _set_worker_signal_handlers, _update_worker_pids, \\\n    _remove_worker_pids, _error_if_any_worker_fails\nfrom .sampler import SequentialSampler, RandomSampler, BatchSampler\nimport signal\nimport functools\nimport collections\nimport re\nimport sys\nimport threading\nimport traceback\nfrom torch._six import string_classes, int_classes\nimport numpy as np\n\nif sys.version_info[0] == 2:\n    import Queue as queue\nelse:\n    import queue\n\n\nclass ExceptionWrapper(object):\n    r""Wraps an exception plus traceback to communicate across threads""\n\n    def __init__(self, exc_info):\n        self.exc_type = exc_info[0]\n        self.exc_msg = """".join(traceback.format_exception(*exc_info))\n\n\n_use_shared_memory = False\n""""""Whether to use shared memory in default_collate""""""\n\n\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    global _use_shared_memory\n    _use_shared_memory = True\n\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\n    # module\'s handlers are executed after Python returns from C low-level\n    # handlers, likely when the same fatal signal happened again already.\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\n    _set_worker_signal_handlers()\n\n    torch.set_num_threads(1)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    if init_fn is not None:\n        init_fn(worker_id)\n\n    while True:\n        r = index_queue.get()\n        if r is None:\n            break\n        idx, batch_indices = r\n        try:\n            samples = collate_fn([dataset[i] for i in batch_indices])\n        except Exception:\n            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            data_queue.put((idx, samples))\n\n\ndef _worker_manager_loop(in_queue, out_queue, done_event, pin_memory, device_id):\n    if pin_memory:\n        torch.cuda.set_device(device_id)\n\n    while True:\n        try:\n            r = in_queue.get()\n        except Exception:\n            if done_event.is_set():\n                return\n            raise\n        if r is None:\n            break\n        if isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n            continue\n        idx, batch = r\n        try:\n            if pin_memory:\n                batch = pin_memory_batch(batch)\n        except Exception:\n            out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            out_queue.put((idx, batch))\n\n\nnumpy_type_map = {\n    \'float64\': torch.DoubleTensor,\n    \'float32\': torch.FloatTensor,\n    \'float16\': torch.HalfTensor,\n    \'int64\': torch.LongTensor,\n    \'int32\': torch.IntTensor,\n    \'int16\': torch.ShortTensor,\n    \'int8\': torch.CharTensor,\n    \'uint8\': torch.ByteTensor,\n}\n\n\ndef default_collate(batch):\n    ""Puts each data field into a tensor with outer dimension batch size""\n\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if torch.is_tensor(batch[0]):\n        out = None\n        if _use_shared_memory:\n            # If we\'re in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = batch[0].storage()._new_shared(numel)\n            out = batch[0].new(storage)\n        return torch.stack(batch, 0, out=out)\n    elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n            and elem_type.__name__ != \'string_\':\n        elem = batch[0]\n        if elem_type.__name__ == \'ndarray\':\n            # array of string classes and object\n            if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                raise TypeError(error_msg.format(elem.dtype))\n\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(\'float\') else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int_classes):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\ndef pin_memory_batch(batch):\n    if torch.is_tensor(batch):\n        return batch.pin_memory()\n    elif isinstance(batch, string_classes):\n        return batch\n    elif isinstance(batch, collections.Mapping):\n        return {k: pin_memory_batch(sample) for k, sample in batch.items()}\n    elif isinstance(batch, collections.Sequence):\n        return [pin_memory_batch(sample) for sample in batch]\n    else:\n        return batch\n\n\n_SIGCHLD_handler_set = False\n""""""Whether SIGCHLD handler is set for DataLoader worker failures. Only one\nhandler needs to be set for all DataLoaders in a process.""""""\n\n\ndef _set_SIGCHLD_handler():\n    # Windows doesn\'t support SIGCHLD handler\n    if sys.platform == \'win32\':\n        return\n    # can\'t set signal in child threads\n    if not isinstance(threading.current_thread(), threading._MainThread):\n        return\n    global _SIGCHLD_handler_set\n    if _SIGCHLD_handler_set:\n        return\n    previous_handler = signal.getsignal(signal.SIGCHLD)\n    if not callable(previous_handler):\n        previous_handler = None\n\n    def handler(signum, frame):\n        # This following call uses `waitid` with WNOHANG from C side. Therefore,\n        # Python can still get and update the process status successfully.\n        _error_if_any_worker_fails()\n        if previous_handler is not None:\n            previous_handler(signum, frame)\n\n    signal.signal(signal.SIGCHLD, handler)\n    _SIGCHLD_handler_set = True\n\n\nclass DataLoaderIter(object):\n    ""Iterates once over the DataLoader\'s dataset, as specified by the sampler""\n\n    def __init__(self, loader):\n        self.dataset = loader.dataset\n        self.collate_fn = loader.collate_fn\n        self.batch_sampler = loader.batch_sampler\n        self.num_workers = loader.num_workers\n        self.pin_memory = loader.pin_memory and torch.cuda.is_available()\n        self.timeout = loader.timeout\n        self.done_event = threading.Event()\n\n        self.sample_iter = iter(self.batch_sampler)\n\n        if self.num_workers > 0:\n            self.worker_init_fn = loader.worker_init_fn\n            self.index_queue = multiprocessing.SimpleQueue()\n            self.worker_result_queue = multiprocessing.SimpleQueue()\n            self.batches_outstanding = 0\n            self.worker_pids_set = False\n            self.shutdown = False\n            self.send_idx = 0\n            self.rcvd_idx = 0\n            self.reorder_dict = {}\n\n            base_seed = torch.LongTensor(1).random_(0, 2**31 - 1)[0]\n            self.workers = [\n                multiprocessing.Process(\n                    target=_worker_loop,\n                    args=(self.dataset, self.index_queue, self.worker_result_queue, self.collate_fn,\n                          base_seed + i, self.worker_init_fn, i))\n                for i in range(self.num_workers)]\n\n            if self.pin_memory or self.timeout > 0:\n                self.data_queue = queue.Queue()\n                if self.pin_memory:\n                    maybe_device_id = torch.cuda.current_device()\n                else:\n                    # do not initialize cuda context if not necessary\n                    maybe_device_id = None\n                self.worker_manager_thread = threading.Thread(\n                    target=_worker_manager_loop,\n                    args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n                          maybe_device_id))\n                self.worker_manager_thread.daemon = True\n                self.worker_manager_thread.start()\n            else:\n                self.data_queue = self.worker_result_queue\n\n            for w in self.workers:\n                w.daemon = True  # ensure that the worker exits on process exit\n                w.start()\n\n            _update_worker_pids(id(self), tuple(w.pid for w in self.workers))\n            _set_SIGCHLD_handler()\n            self.worker_pids_set = True\n\n            # prime the prefetch loop\n            for _ in range(2 * self.num_workers):\n                self._put_indices()\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n    def _get_batch(self):\n        if self.timeout > 0:\n            try:\n                return self.data_queue.get(timeout=self.timeout)\n            except queue.Empty:\n                raise RuntimeError(\'DataLoader timed out after {} seconds\'.format(self.timeout))\n        else:\n            return self.data_queue.get()\n\n    def __next__(self):\n        if self.num_workers == 0:  # same-process loading\n            indices = next(self.sample_iter)  # may raise StopIteration\n            batch = self.collate_fn([self.dataset[i] for i in indices])\n            if self.pin_memory:\n                batch = pin_memory_batch(batch)\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self._get_batch()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n\n    next = __next__  # Python 2 compatibility\n\n    def __iter__(self):\n        return self\n\n    def _put_indices(self):\n        assert self.batches_outstanding < 2 * self.num_workers\n        indices = next(self.sample_iter, None)\n        if indices is None:\n            return\n        self.index_queue.put((self.send_idx, indices))\n        self.batches_outstanding += 1\n        self.send_idx += 1\n\n    def _process_next_batch(self, batch):\n        self.rcvd_idx += 1\n        self._put_indices()\n        if isinstance(batch, ExceptionWrapper):\n            raise batch.exc_type(batch.exc_msg)\n        return batch\n\n    def __getstate__(self):\n        # TODO: add limited pickling support for sharing an iterator\n        # across multiple threads for HOGWILD.\n        # Probably the best way to do this is by moving the sample pushing\n        # to a separate thread and then just sharing the data queue\n        # but signalling the end is tricky without a non-blocking API\n        raise NotImplementedError(""DataLoaderIterator cannot be pickled"")\n\n    def _shutdown_workers(self):\n        try:\n            if not self.shutdown:\n                self.shutdown = True\n                self.done_event.set()\n                # if worker_manager_thread is waiting to put\n                while not self.data_queue.empty():\n                    self.data_queue.get()\n                for _ in self.workers:\n                    self.index_queue.put(None)\n                # done_event should be sufficient to exit worker_manager_thread,\n                # but be safe here and put another None\n                self.worker_result_queue.put(None)\n        finally:\n            # removes pids no matter what\n            if self.worker_pids_set:\n                _remove_worker_pids(id(self))\n                self.worker_pids_set = False\n\n    def __del__(self):\n        if self.num_workers > 0:\n            self._shutdown_workers()\n\n\nclass DataLoader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: 1).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: False).\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset. If specified, ``shuffle`` must be False.\n        batch_sampler (Sampler, optional): like sampler, but returns a batch of\n            indices at a time. Mutually exclusive with batch_size, shuffle,\n            sampler, and drop_last.\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process.\n            (default: 0)\n        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n            into CUDA pinned memory before returning them.\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: False)\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: 0)\n        worker_init_fn (callable, optional): If not None, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: None)\n\n    .. note:: By default, each worker will have its PyTorch seed set to\n              ``base_seed + worker_id``, where ``base_seed`` is a long generated\n              by main process using its RNG. You may use ``torch.initial_seed()`` to access\n              this value in :attr:`worker_init_fn`, which can be used to set other seeds\n              (e.g. NumPy) before data loading.\n\n    .. warning:: If ``spawn\'\' start method is used, :attr:`worker_init_fn` cannot be an\n                 unpicklable object, e.g., a lambda function.\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,\n                 timeout=0, worker_init_fn=None):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.timeout = timeout\n        self.worker_init_fn = worker_init_fn\n\n        if timeout < 0:\n            raise ValueError(\'timeout option should be non-negative\')\n\n        if batch_sampler is not None:\n            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n                raise ValueError(\'batch_sampler is mutually exclusive with \'\n                                 \'batch_size, shuffle, sampler, and drop_last\')\n\n        if sampler is not None and shuffle:\n            raise ValueError(\'sampler is mutually exclusive with shuffle\')\n\n        if self.num_workers < 0:\n            raise ValueError(\'num_workers cannot be negative; \'\n                             \'use num_workers=0 to disable multiprocessing.\')\n\n        if batch_sampler is None:\n            if sampler is None:\n                if shuffle:\n                    sampler = RandomSampler(dataset)\n                else:\n                    sampler = SequentialSampler(dataset)\n            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n\n        self.sampler = sampler\n        self.batch_sampler = batch_sampler\n\n    def __iter__(self):\n        return DataLoaderIter(self)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n'"
semantic/lib/utils/data/dataset.py,1,"b'import bisect\nimport warnings\n\nfrom torch._utils import _accumulate\nfrom torch import randperm\n\n\nclass Dataset(object):\n    """"""An abstract class representing a Dataset.\n\n    All other datasets should subclass it. All subclasses should override\n    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n    supporting integer indexing in range from 0 to len(self) exclusive.\n    """"""\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __add__(self, other):\n        return ConcatDataset([self, other])\n\n\nclass TensorDataset(Dataset):\n    """"""Dataset wrapping data and target tensors.\n\n    Each sample will be retrieved by indexing both tensors along the first\n    dimension.\n\n    Arguments:\n        data_tensor (Tensor): contains sample data.\n        target_tensor (Tensor): contains sample targets (labels).\n    """"""\n\n    def __init__(self, data_tensor, target_tensor):\n        assert data_tensor.size(0) == target_tensor.size(0)\n        self.data_tensor = data_tensor\n        self.target_tensor = target_tensor\n\n    def __getitem__(self, index):\n        return self.data_tensor[index], self.target_tensor[index]\n\n    def __len__(self):\n        return self.data_tensor.size(0)\n\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n\n    Arguments:\n        datasets (iterable): List of datasets to be concatenated\n    """"""\n\n    @staticmethod\n    def cumsum(sequence):\n        r, s = [], 0\n        for e in sequence:\n            l = len(e)\n            r.append(l + s)\n            s += l\n        return r\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.datasets = list(datasets)\n        self.cumulative_sizes = self.cumsum(self.datasets)\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.datasets[dataset_idx][sample_idx]\n\n    @property\n    def cummulative_sizes(self):\n        warnings.warn(""cummulative_sizes attribute is renamed to ""\n                      ""cumulative_sizes"", DeprecationWarning, stacklevel=2)\n        return self.cumulative_sizes\n\n\nclass Subset(Dataset):\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self):\n        return len(self.indices)\n\n\ndef random_split(dataset, lengths):\n    """"""\n    Randomly split a dataset into non-overlapping new datasets of given lengths\n    ds\n\n    Arguments:\n        dataset (Dataset): Dataset to be split\n        lengths (iterable): lengths of splits to be produced\n    """"""\n    if sum(lengths) != len(dataset):\n        raise ValueError(""Sum of input lengths does not equal the length of the input dataset!"")\n\n    indices = randperm(sum(lengths))\n    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n'"
semantic/lib/utils/data/distributed.py,4,"b'import math\nimport torch\nfrom .sampler import Sampler\nfrom torch.distributed import get_world_size, get_rank\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            num_replicas = get_world_size()\n        if rank is None:\n            rank = get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = list(torch.randperm(len(self.dataset), generator=g))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
semantic/lib/utils/data/sampler.py,4,"b'import torch\n\n\nclass Sampler(object):\n    """"""Base class for all Samplers.\n\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    """"""\n\n    def __init__(self, data_source):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    """"""Samples elements sequentially, always in the same order.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(range(len(self.data_source)))\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass RandomSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(torch.randperm(len(self.data_source)).long())\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass SubsetRandomSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices, without replacement.\n\n    Arguments:\n        indices (list): a list of indices\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.randperm(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass WeightedRandomSampler(Sampler):\n    """"""Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n\n    Arguments:\n        weights (list)   : a list of weights, not necessary summing up to one\n        num_samples (int): number of samples to draw\n        replacement (bool): if ``True``, samples are drawn with replacement.\n            If not, they are drawn without replacement, which means that when a\n            sample index is drawn for a row, it cannot be drawn again for that row.\n    """"""\n\n    def __init__(self, weights, num_samples, replacement=True):\n        self.weights = torch.DoubleTensor(weights)\n        self.num_samples = num_samples\n        self.replacement = replacement\n\n    def __iter__(self):\n        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement))\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass BatchSampler(object):\n    """"""Wraps another sampler to yield a mini-batch of indices.\n\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n\n    Example:\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    """"""\n\n    def __init__(self, sampler, batch_size, drop_last):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n'"
semantic/lib/nn/modules/tests/test_numeric_batchnorm.py,5,"b""# -*- coding: utf-8 -*-\n# File   : test_numeric_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\nclass NumericTestCase(TorchTestCase):\n    def testNumericBatchNorm(self):\n        a = torch.rand(16, 10)\n        bn = nn.BatchNorm2d(10, momentum=1, eps=1e-5, affine=False)\n        bn.train()\n\n        a_var1 = Variable(a, requires_grad=True)\n        b_var1 = bn(a_var1)\n        loss1 = b_var1.sum()\n        loss1.backward()\n\n        a_var2 = Variable(a, requires_grad=True)\n        a_mean2 = a_var2.mean(dim=0, keepdim=True)\n        a_std2 = torch.sqrt(handy_var(a_var2, unbias=False).clamp(min=1e-5))\n        # a_std2 = torch.sqrt(a_var2.var(dim=0, keepdim=True, unbiased=False) + 1e-5)\n        b_var2 = (a_var2 - a_mean2) / a_std2\n        loss2 = b_var2.sum()\n        loss2.backward()\n\n        self.assertTensorClose(bn.running_mean, a.mean(dim=0))\n        self.assertTensorClose(bn.running_var, handy_var(a))\n        self.assertTensorClose(a_var1.data, a_var2.data)\n        self.assertTensorClose(b_var1.data, b_var2.data)\n        self.assertTensorClose(a_var1.grad, a_var2.grad)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
semantic/lib/nn/modules/tests/test_sync_batchnorm.py,7,"b'# -*- coding: utf-8 -*-\n# File   : test_sync_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, DataParallelWithCallback\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\ndef _find_bn(module):\n    for m in module.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, SynchronizedBatchNorm1d, SynchronizedBatchNorm2d)):\n            return m\n\n\nclass SyncTestCase(TorchTestCase):\n    def _syncParameters(self, bn1, bn2):\n        bn1.reset_parameters()\n        bn2.reset_parameters()\n        if bn1.affine and bn2.affine:\n            bn2.weight.data.copy_(bn1.weight.data)\n            bn2.bias.data.copy_(bn1.bias.data)\n\n    def _checkBatchNormResult(self, bn1, bn2, input, is_train, cuda=False):\n        """"""Check the forward and backward for the customized batch normalization.""""""\n        bn1.train(mode=is_train)\n        bn2.train(mode=is_train)\n\n        if cuda:\n            input = input.cuda()\n\n        self._syncParameters(_find_bn(bn1), _find_bn(bn2))\n\n        input1 = Variable(input, requires_grad=True)\n        output1 = bn1(input1)\n        output1.sum().backward()\n        input2 = Variable(input, requires_grad=True)\n        output2 = bn2(input2)\n        output2.sum().backward()\n\n        self.assertTensorClose(input1.data, input2.data)\n        self.assertTensorClose(output1.data, output2.data)\n        self.assertTensorClose(input1.grad, input2.grad)\n        self.assertTensorClose(_find_bn(bn1).running_mean, _find_bn(bn2).running_mean)\n        self.assertTensorClose(_find_bn(bn1).running_var, _find_bn(bn2).running_var)\n\n    def testSyncBatchNormNormalTrain(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True)\n\n    def testSyncBatchNormNormalEval(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False)\n\n    def testSyncBatchNormSyncTrain(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True, cuda=True)\n\n    def testSyncBatchNormSyncEval(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False, cuda=True)\n\n    def testSyncBatchNorm2DSyncTrain(self):\n        bn = nn.BatchNorm2d(10)\n        sync_bn = SynchronizedBatchNorm2d(10)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10, 16, 16), True, cuda=True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
