file_path,api_count,code
data_loader.py,5,"b'""""""\n   CIFAR-10 CIFAR-100, Tiny-ImageNet data loader\n""""""\n\nimport random\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\ndef fetch_dataloader(types, params):\n    """"""\n    Fetch and return train/dev dataloader with hyperparameters (params.subset_percent = 1.)\n    """"""\n    # using random crops and horizontal flip for train set\n    if params.augmentation == ""yes"":\n        train_transformer = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5070751592371323, 0.48654887331495095, 0.4409178433670343), (0.2673342858792401, 0.2564384629170883, 0.27615047132568404))])\n        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.240, 0.243, 0.261))\n\n    # data augmentation can be turned off\n    else:\n        train_transformer = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5070751592371323, 0.48654887331495095, 0.4409178433670343), (0.2673342858792401, 0.2564384629170883, 0.27615047132568404))])\n\n    # transformer for dev set\n    dev_transformer = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5070751592371323, 0.48654887331495095, 0.4409178433670343), (0.2673342858792401, 0.2564384629170883, 0.27615047132568404))])\n\n    if params.dataset == \'cifar10\':\n        trainset = torchvision.datasets.CIFAR10(root=\'./data/data-cifar10\', train=True,\n                                                download=True, transform=train_transformer)\n        devset = torchvision.datasets.CIFAR10(root=\'./data/data-cifar10\', train=False,\n                                              download=True, transform=dev_transformer)\n    elif params.dataset == \'cifar100\':\n        trainset = torchvision.datasets.CIFAR100(root=\'./data/data-cifar100\', train=True,\n                                                download=True, transform=train_transformer)\n        devset = torchvision.datasets.CIFAR100(root=\'./data/data-cifar100\', train=False,\n                                              download=True, transform=dev_transformer)\n    elif params.dataset == \'tiny_imagenet\':\n        data_dir = \'./data/tiny-imagenet-200/\'\n        data_transforms = {\n            \'train\': transforms.Compose([\n                transforms.RandomRotation(20),\n                transforms.RandomHorizontalFlip(0.5),\n                transforms.ToTensor(),\n                transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n            ]),\n            \'val\': transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n            ])\n        }\n        train_dir = data_dir + \'train/\'\n        test_dir = data_dir + \'val/images/\'\n        trainset = torchvision.datasets.ImageFolder(train_dir, data_transforms[\'train\'])\n        devset = torchvision.datasets.ImageFolder(test_dir, data_transforms[\'val\'])\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params.batch_size,\n                                              shuffle=True, num_workers=params.num_workers)\n\n    devloader = torch.utils.data.DataLoader(devset, batch_size=params.batch_size,\n                                            shuffle=False, num_workers=params.num_workers)\n\n    if types == \'train\':\n        dl = trainloader\n    else:\n        dl = devloader\n\n    return dl\n\n\ndef fetch_subset_dataloader(types, params):\n    """"""\n    Use only a subset of dataset for KD training, depending on params.subset_percent\n    """"""\n\n    # using random crops and horizontal flip for train set\n    if params.augmentation == ""yes"":\n        train_transformer = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n\n    # data augmentation can be turned off\n    else:\n        train_transformer = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n\n    # transformer for dev set\n    dev_transformer = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n\n    if params.dataset==\'cifar10\':\n        trainset = torchvision.datasets.CIFAR10(root=\'./data-cifar10\', train=True,\n                                                download=True, transform=train_transformer)\n        devset = torchvision.datasets.CIFAR10(root=\'./data-cifar10\', train=False,\n                                              download=True, transform=dev_transformer)\n    elif params.dataset==\'cifar100\':\n        trainset = torchvision.datasets.CIFAR10(root=\'./data-cifar10\', train=True,\n                                                download=True, transform=train_transformer)\n        devset = torchvision.datasets.CIFAR10(root=\'./data-cifar10\', train=False,\n                                              download=True, transform=dev_transformer)\n    elif params.dataset == \'tiny_imagenet\':\n        data_dir = \'./data/tiny-imagenet-200/\'\n        data_transforms = {\n            \'train\': transforms.Compose([\n                transforms.RandomRotation(20),\n                transforms.RandomHorizontalFlip(0.5),\n                transforms.ToTensor(),\n                transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n            ]),\n            \'val\': transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n            ])\n        }\n        train_dir = data_dir + \'train/\'\n        test_dir = data_dir + \'val/images/\'\n        trainset = torchvision.datasets.ImageFolder(train_dir, data_transforms[\'train\'])\n        devset = torchvision.datasets.ImageFolder(test_dir, data_transforms[\'val\'])\n\n    trainset_size = len(trainset)\n    indices = list(range(trainset_size))\n    split = int(np.floor(params.subset_percent * trainset_size))\n    np.random.seed(230)\n    np.random.shuffle(indices)\n\n    train_sampler = SubsetRandomSampler(indices[:split])\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params.batch_size,\n        sampler=train_sampler, num_workers=params.num_workers, pin_memory=params.cuda)\n\n    devloader = torch.utils.data.DataLoader(devset, batch_size=params.batch_size,\n        shuffle=False, num_workers=params.num_workers, pin_memory=params.cuda)\n\n    if types == \'train\':\n        dl = trainloader\n    else:\n        dl = devloader\n\n    return dl'"
evaluate.py,5,"b'""""""Evaluates the model""""""\n\nimport argparse\nimport logging\n\nfrom torch.autograd import Variable\nimport utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\', help=""Directory of params.json"")\nparser.add_argument(\'--restore_file\', default=\'best\', help=""name of the file in --model_dir \\\n                     containing weights to load"")\n\n\ndef evaluate(model, loss_fn, dataloader, params, args):\n    """"""Evaluate the model on `num_steps` batches.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to evaluation mode\n    model.eval()\n    losses = utils.AverageMeter()\n    total = 0\n    correct = 0\n\n    # compute metrics over the dataset\n    for data_batch, labels_batch in dataloader:\n\n        data_batch, labels_batch = data_batch.cuda(async=True), labels_batch.cuda(async=True)\n\n        data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n        # compute model output\n        output_batch = model(data_batch)\n        if args.regularization:\n            loss = loss_fn(output_batch, labels_batch, params)\n        else:\n            loss = loss_fn(output_batch, labels_batch)\n\n        losses.update(loss.data, data_batch.size(0))\n        _, predicted = output_batch.max(1)\n        total += labels_batch.size(0)\n        correct += predicted.eq(labels_batch).sum().item()\n\n    loss_avg = losses.avg\n    acc = 100.*correct/total\n    logging.info(""- Eval metrics, acc:{acc:.4f}, loss: {loss_avg:.4f}"".format(acc=acc, loss_avg=loss_avg))\n    my_metric = {\'accuracy\': acc, \'loss\': loss_avg}\n    return my_metric\n\n\n""""""\nThis function duplicates ""evaluate()"" but ignores ""loss_fn"" simply for speedup purpose.\nValidation loss during KD mode would display \'0\' all the time.\nOne can bring that info back by using the fetched teacher outputs during evaluation (refer to train.py)\n""""""\ndef evaluate_kd(model, dataloader, params):\n    """"""Evaluate the model on `num_steps` batches.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to evaluation mode\n    model.eval()\n    total = 0\n    correct = 0\n\n    # compute metrics over the dataset\n    for i, (data_batch, labels_batch) in enumerate(dataloader):\n\n        # move to GPU if available\n        data_batch, labels_batch = data_batch.cuda(async=True), labels_batch.cuda(async=True)\n        # fetch the next evaluation batch\n        data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n        \n        # compute model output\n        output_batch = model(data_batch)\n\n        # loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n        loss = 0.0  #force validation loss to zero to reduce computation time\n        _, predicted = output_batch.max(1)\n        total += labels_batch.size(0)\n        correct += predicted.eq(labels_batch).sum().item()\n\n    acc = 100. * correct / total\n    logging.info(""- Eval metrics, acc:{acc:.4f}, loss: {loss:.4f}"".format(acc=acc, loss=loss))\n    my_metric = {\'accuracy\': acc, \'loss\': loss}\n    #my_metric[\'accuracy\'] = acc\n    return my_metric\n'"
main.py,4,"b'""""""\nTeacher free KD, main.py\n""""""\nimport argparse\nimport logging\nimport os\nimport random\nimport warnings\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport utils\nimport model.net as net\nimport data_loader as data_loader\nimport model.resnet as resnet\nimport model.mobilenetv2 as mobilenet\nimport model.densenet as densenet\nimport model.resnext as resnext\nimport model.shufflenetv2 as shufflenet\nimport model.alexnet as alexnet\nimport model.googlenet as googlenet\nimport torchvision.models as models\nfrom my_loss_function import loss_label_smoothing, loss_kd_regularization, loss_kd, loss_kd_self\nfrom train_kd import train_and_evaluate, train_and_evaluate_kd\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_experiments/base_resnet18/\', help=""Directory containing params.json"")\nparser.add_argument(\'--restore_file\', default=None, help=""Optional, name of the file in --model_dir \\\n                    containing weights to reload before training"")  # \'best\' or \'train\'\nparser.add_argument(\'--num_class\', default=100, type=int, help=""number of classes"")\nparser.add_argument(\'-warm\', type=int, default=1, help=\'warm up training phase\')\nparser.add_argument(\'--regularization\', action=\'store_true\', default=False, help=""flag for regulization"")\nparser.add_argument(\'--label_smoothing\', action=\'store_true\', default=False, help=""flag for label smoothing"")\nparser.add_argument(\'--double_training\', action=\'store_true\', default=False, help=""flag for double training"")\nparser.add_argument(\'--self_training\', action=\'store_true\', default=False, help=""flag for self training"")\nparser.add_argument(\'--pt_teacher\', action=\'store_true\', default=False, help=""flag for Defective KD"")\n\n\ndef main():\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # Set the random seed for reproducible experiments\n    random.seed(230)\n    torch.manual_seed(230)\n    np.random.seed(230)\n    torch.cuda.manual_seed(230)\n    warnings.filterwarnings(""ignore"")\n\n    # Set the logger\n    utils.set_logger(os.path.join(args.model_dir, \'train.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Loading the datasets..."")\n\n    # fetch dataloaders, considering full-set vs. sub-set scenarios\n    if params.subset_percent < 1.0:\n        train_dl = data_loader.fetch_subset_dataloader(\'train\', params)\n    else:\n        train_dl = data_loader.fetch_dataloader(\'train\', params)\n\n    dev_dl = data_loader.fetch_dataloader(\'dev\', params)\n\n    logging.info(""- done."")\n\n    """"""\n    Load student and teacher model\n    """"""\n    if ""distill"" in params.model_version:\n\n        # Specify the student models\n        if params.model_version == ""cnn_distill"":  # 5-layers Plain CNN\n            print(""Student model: {}"".format(params.model_version))\n            model = net.Net(params).cuda()\n\n        elif params.model_version == ""shufflenet_v2_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()\n\n        elif params.model_version == ""mobilenet_v2_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()\n\n        elif params.model_version == \'resnet18_distill\':\n            print(""Student model: {}"".format(params.model_version))\n            model = resnet.ResNet18(num_classes=args.num_class).cuda()\n\n        elif params.model_version == \'resnet50_distill\':\n            print(""Student model: {}"".format(params.model_version))\n            model = resnet.ResNet50(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""alexnet_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = alexnet.alexnet(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""vgg19_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = models.vgg19_bn(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""googlenet_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = googlenet.GoogleNet(num_class=args.num_class).cuda()\n\n        elif params.model_version == ""resnext29_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""densenet121_distill"":\n            print(""Student model: {}"".format(params.model_version))\n            model = densenet.densenet121(num_class=args.num_class).cuda()\n\n        # optimizer\n        if params.model_version == ""cnn_distill"":\n            optimizer = optim.Adam(model.parameters(), lr=params.learning_rate * (params.batch_size / 128))\n        else:\n            optimizer = optim.SGD(model.parameters(), lr=params.learning_rate * (params.batch_size / 128), momentum=0.9,\n                                  weight_decay=5e-4)\n\n        iter_per_epoch = len(train_dl)\n        warmup_scheduler = utils.WarmUpLR(optimizer,\n                                          iter_per_epoch * args.warm)  # warmup the learning rate in the first epoch\n\n        # specify loss function\n        if args.self_training:\n            print(\'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>self training>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\')\n            loss_fn_kd = loss_kd_self\n        else:\n            loss_fn_kd = loss_kd\n\n        """""" \n            Specify the pre-trained teacher models for knowledge distillation\n            Checkpoints can be obtained by regular training or downloading our pretrained models\n            For model which is pretrained in multi-GPU, use ""nn.DaraParallel"" to correctly load the model weights.\n        """"""\n        if params.teacher == ""resnet18"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = resnet.ResNet18(num_classes=args.num_class)\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnet18/best.pth.tar\'\n            if args.pt_teacher:  # poorly-trained teacher for Defective KD experiments\n                teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnet18/0.pth.tar\'\n            teacher_model = teacher_model.cuda()\n\n        elif params.teacher == ""alexnet"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = alexnet.alexnet(num_classes=args.num_class)\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_alexnet/best.pth.tar\'\n            teacher_model = teacher_model.cuda()\n\n        elif params.teacher == ""googlenet"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = googlenet.GoogleNet(num_class=args.num_class)\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_googlenet/best.pth.tar\'\n            teacher_model = teacher_model.cuda()\n\n        elif params.teacher == ""vgg19"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = models.vgg19_bn(num_classes=args.num_class)\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_vgg19/best.pth.tar\'\n            teacher_model = teacher_model.cuda()\n\n        elif params.teacher == ""resnet50"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = resnet.ResNet50(num_classes=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnet50/best.pth.tar\'\n            if args.pt_teacher:  # poorly-trained teacher for Defective KD experiments\n                teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnet50/50.pth.tar\'\n\n        elif params.teacher == ""resnet101"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = resnet.ResNet101(num_classes=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnet101/best.pth.tar\'\n            teacher_model = teacher_model.cuda()\n\n        elif params.teacher == ""densenet121"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = densenet.densenet121(num_class=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_densenet121/best.pth.tar\'\n            # teacher_model = nn.DataParallel(teacher_model).cuda()\n\n        elif params.teacher == ""resnext29"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnext29/best.pth.tar\'\n            if args.pt_teacher:  # poorly-trained teacher for Defective KD experiments\n                teacher_checkpoint = \'experiments/pretrained_teacher_models/base_resnext29/50.pth.tar\'\n                teacher_model = nn.DataParallel(teacher_model).cuda()\n\n        elif params.teacher == ""mobilenet_v2"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_mobilenet_v2/best.pth.tar\'\n\n        elif params.teacher == ""shufflenet_v2"":\n            print(""Teacher model: {}"".format(params.teacher))\n            teacher_model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()\n            teacher_checkpoint = \'experiments/pretrained_teacher_models/base_shufflenet_v2/best.pth.tar\'\n\n        utils.load_checkpoint(teacher_checkpoint, teacher_model)\n\n        # Train the model with KD\n        logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n        train_and_evaluate_kd(model, teacher_model, train_dl, dev_dl, optimizer, loss_fn_kd,\n                              warmup_scheduler, params, args, args.restore_file)\n\n    # non-KD mode: regular training to obtain a baseline model\n    else:\n        print(""Train base model"")\n        if params.model_version == ""cnn"":\n            model = net.Net(params).cuda()\n\n        elif params.model_version == ""mobilenet_v2"":\n            print(""model: {}"".format(params.model_version))\n            model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()\n\n        elif params.model_version == ""shufflenet_v2"":\n            print(""model: {}"".format(params.model_version))\n            model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()\n\n        elif params.model_version == ""alexnet"":\n            print(""model: {}"".format(params.model_version))\n            model = alexnet.alexnet(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""vgg19"":\n            print(""model: {}"".format(params.model_version))\n            model = models.vgg19_bn(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""googlenet"":\n            print(""model: {}"".format(params.model_version))\n            model = googlenet.GoogleNet(num_class=args.num_class).cuda()\n\n        elif params.model_version == ""densenet121"":\n            print(""model: {}"".format(params.model_version))\n            model = densenet.densenet121(num_class=args.num_class).cuda()\n\n        elif params.model_version == ""resnet18"":\n            model = resnet.ResNet18(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""resnet50"":\n            model = resnet.ResNet50(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""resnet101"":\n            model = resnet.ResNet101(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""resnet152"":\n            model = resnet.ResNet152(num_classes=args.num_class).cuda()\n\n        elif params.model_version == ""resnext29"":\n            model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()\n            # model = nn.DataParallel(model).cuda()\n\n        if args.regularization:\n            print("">>>>>>>>>>>>>>>>>>>>>>>>Loss of Regularization>>>>>>>>>>>>>>>>>>>>>>>>"")\n            loss_fn = loss_kd_regularization\n        elif args.label_smoothing:\n            print("">>>>>>>>>>>>>>>>>>>>>>>>Label Smoothing>>>>>>>>>>>>>>>>>>>>>>>>"")\n            loss_fn = loss_label_smoothing\n        else:\n            print("">>>>>>>>>>>>>>>>>>>>>>>>Normal Training>>>>>>>>>>>>>>>>>>>>>>>>"")\n            loss_fn = nn.CrossEntropyLoss()\n            if args.double_training:  # double training, compare to self-KD\n                print("">>>>>>>>>>>>>>>>>>>>>>>>Double Training>>>>>>>>>>>>>>>>>>>>>>>>"")\n                checkpoint = \'experiments/pretrained_teacher_models/base_\' + str(params.model_version) + \'/best.pth.tar\'\n                utils.load_checkpoint(checkpoint, model)\n\n        if params.model_version == ""cnn"":\n            optimizer = optim.Adam(model.parameters(), lr=params.learning_rate * (params.batch_size / 128))\n        else:\n            optimizer = optim.SGD(model.parameters(), lr=params.learning_rate * (params.batch_size / 128), momentum=0.9,\n                                  weight_decay=5e-4)\n\n        iter_per_epoch = len(train_dl)\n        warmup_scheduler = utils.WarmUpLR(optimizer, iter_per_epoch * args.warm)\n\n        # Train the model\n        logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n        train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, params,\n                           args.model_dir, warmup_scheduler, args, args.restore_file)\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
my_loss_function.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef loss_kd(outputs, labels, teacher_outputs, params):\n    """"""\n    loss function for Knowledge Distillation (KD)\n    """"""\n    alpha = params.alpha\n    T = params.temperature\n\n    loss_CE = F.cross_entropy(outputs, labels)\n    D_KL = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (T * T)\n    KD_loss =  (1. - alpha)*loss_CE + alpha*D_KL\n\n    return KD_loss\n\ndef loss_kd_self(outputs, labels, teacher_outputs, params):\n    """"""\n    loss function for self training: Tf-KD_{self}\n    """"""\n    alpha = params.alpha\n    T = params.temperature\n\n    loss_CE = F.cross_entropy(outputs, labels)\n    D_KL = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (T * T) * params.multiplier  # multiple is 1.0 in most of cases, some cases are 10 or 50\n    KD_loss =  (1. - alpha)*loss_CE + alpha*D_KL\n\n    return KD_loss\n\n\ndef loss_kd_regularization(outputs, labels, params):\n    """"""\n    loss function for mannually-designed regularization: Tf-KD_{reg}\n    """"""\n    alpha = params.reg_alpha\n    T = params.reg_temperature\n    correct_prob = 0.99    # the probability for correct class in u(k)\n    loss_CE = F.cross_entropy(outputs, labels)\n    K = outputs.size(1)\n\n    teacher_soft = torch.ones_like(outputs).cuda()\n    teacher_soft = teacher_soft*(1-correct_prob)/(K-1)  # p^d(k)\n    for i in range(outputs.shape[0]):\n        teacher_soft[i ,labels[i]] = correct_prob\n    loss_soft_regu = nn.KLDivLoss()(F.log_softmax(outputs, dim=1), F.softmax(teacher_soft/T, dim=1))*params.multiplier\n\n    KD_loss = (1. - alpha)*loss_CE + alpha*loss_soft_regu\n\n    return KD_loss\n\n\ndef loss_label_smoothing(outputs, labels):\n    """"""\n    loss function for label smoothing regularization\n    """"""\n    alpha = 0.1\n    N = outputs.size(0)  # batch_size\n    C = outputs.size(1)  # number of classes\n    smoothed_labels = torch.full(size=(N, C), fill_value= alpha / (C - 1)).cuda()\n    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=1-alpha)\n\n    log_prob = torch.nn.functional.log_softmax(outputs, dim=1)\n    loss = -torch.sum(log_prob * smoothed_labels) / N\n\n    return loss\n'"
train_kd.py,2,"b'import os\nimport time\nimport math\nimport utils\nfrom tqdm import tqdm\nimport logging\nfrom torch.autograd import Variable\nfrom evaluate import evaluate, evaluate_kd\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import StepLR, MultiStepLR\n\n# KD train and evaluate\ndef train_and_evaluate_kd(model, teacher_model, train_dataloader, val_dataloader, optimizer,\n                       loss_fn_kd, warmup_scheduler, params, args, restore_file=None):\n    """"""\n    KD Train the model and evaluate every epoch.\n    """"""\n    # reload weights from restore_file if specified\n    if restore_file is not None:\n        restore_path = os.path.join(args.model_dir, args.restore_file + \'.pth.tar\')\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        utils.load_checkpoint(restore_path, model, optimizer)\n\n    # tensorboard setting\n    log_dir = args.model_dir + \'/tensorboard/\'\n    writer = SummaryWriter(log_dir=log_dir)\n\n    best_val_acc = 0.0\n    teacher_model.eval()\n    teacher_acc = evaluate_kd(teacher_model, val_dataloader, params)\n    print("">>>>>>>>>The teacher accuracy: {}>>>>>>>>>"".format(teacher_acc[\'accuracy\']))\n\n    scheduler = MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n    for epoch in range(params.num_epochs):\n\n        if epoch > 0:   # 0 is the warm up epoch\n            scheduler.step()\n        logging.info(""Epoch {}/{}, lr:{}"".format(epoch + 1, params.num_epochs, optimizer.param_groups[0][\'lr\']))\n\n        # KD Train\n        train_acc, train_loss = train_kd(model, teacher_model, optimizer, loss_fn_kd, train_dataloader, warmup_scheduler, params, args, epoch)\n        # Evaluate\n        val_metrics = evaluate_kd(model, val_dataloader, params)\n\n        val_acc = val_metrics[\'accuracy\']\n        is_best = val_acc>=best_val_acc\n\n        # Save weights\n        utils.save_checkpoint({\'epoch\': epoch + 1,\n                               \'state_dict\': model.state_dict(),\n                               \'optim_dict\' : optimizer.state_dict()},\n                               is_best=is_best,\n                               checkpoint=args.model_dir)\n\n        # If best_eval, best_save_path\n        if is_best:\n            logging.info(""- Found new best accuracy"")\n            best_val_acc = val_acc\n\n            # Save best val metrics in a json file in the model directory\n            file_name = ""eval_best_result.json""\n            best_json_path = os.path.join(args.model_dir, file_name)\n            utils.save_dict_to_json(val_metrics, best_json_path)\n\n        # Save latest val metrics in a json file in the model directory\n        last_json_path = os.path.join(args.model_dir, ""eval_last_result.json"")\n        utils.save_dict_to_json(val_metrics, last_json_path)\n\n        # Tensorboard\n        writer.add_scalar(\'Train_accuracy\', train_acc, epoch)\n        writer.add_scalar(\'Train_loss\', train_loss, epoch)\n        writer.add_scalar(\'Test_accuracy\', val_metrics[\'accuracy\'], epoch)\n        writer.add_scalar(\'Test_loss\', val_metrics[\'loss\'], epoch)\n        # export scalar data to JSON for external processing\n    writer.close()\n\n\n# Defining train_kd functions\ndef train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, warmup_scheduler, params, args, epoch, flag=None):\n    """"""\n    KD Train the model on `num_steps` batches\n    """"""\n    # set model to training mode\n    model.train()\n    teacher_model.eval()\n    loss_avg = utils.RunningAverage()\n    losses = utils.AverageMeter()\n    total = 0\n    correct = 0\n    # Use tqdm for progress bar\n    with tqdm(total=len(dataloader)) as t:\n        for i, (train_batch, labels_batch) in enumerate(dataloader):\n            if epoch<=0:\n                warmup_scheduler.step()\n\n            train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n            # convert to torch Variables\n            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n\n            # compute model output, fetch teacher output, and compute KD loss\n            output_batch = model(train_batch)\n\n            # get one batch output from teacher model\n            output_teacher_batch = teacher_model(train_batch).cuda()\n            output_teacher_batch = Variable(output_teacher_batch, requires_grad=False)\n\n            loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n\n            # clear previous gradients, compute gradients of all variables wrt loss\n            optimizer.zero_grad()\n            loss.backward()\n\n            # performs updates using calculated gradients\n            optimizer.step()\n\n            _, predicted = output_batch.max(1)\n            total += labels_batch.size(0)\n            correct += predicted.eq(labels_batch).sum().item()\n            # update the average loss\n            loss_avg.update(loss.data)\n            losses.update(loss.item(), train_batch.size(0))\n\n            t.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()), lr=\'{:05.6f}\'.format(optimizer.param_groups[0][\'lr\']))\n            t.update()\n\n    acc = 100.*correct/total\n    logging.info(""- Train accuracy: {acc:.4f}, training loss: {loss:.4f}"".format(acc = acc, loss = losses.avg))\n    return acc, losses.avg\n\n\n# normal training\ndef train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,\n                       loss_fn, params, model_dir, warmup_scheduler, args, restore_file=None):\n    """"""\n    Train the model and evaluate every epoch.\n    """"""\n    # reload weights from restore_file if specified\n    if restore_file is not None:\n        restore_path = os.path.join(args.model_dir, args.restore_file + \'.pth.tar\')\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        utils.load_checkpoint(restore_path, model, optimizer)\n\n    # dir setting, tensorboard events will save in the dirctory\n    log_dir = args.model_dir + \'/base_train/\'\n    if args.regularization:\n        log_dir = args.model_dir + \'/Tf-KD_regularization/\'\n        model_dir = log_dir\n    elif args.label_smoothing:\n        log_dir = args.model_dir + \'/label_smoothing/\'\n        model_dir = log_dir\n    writer = SummaryWriter(log_dir=log_dir)\n\n    best_val_acc = 0.0\n\n    # learning rate schedulers\n    scheduler = MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n\n    for epoch in range(params.num_epochs):\n        if epoch > 0:   # 1 is the warm up epoch\n            scheduler.step(epoch)\n\n        # Run one epoch\n        logging.info(""Epoch {}/{}, lr:{}"".format(epoch + 1, params.num_epochs, optimizer.param_groups[0][\'lr\']))\n\n        # compute number of batches in one epoch (one full pass over the training set)\n        train_acc, train_loss = train(model, optimizer, loss_fn, train_dataloader, params, epoch, warmup_scheduler, args)\n\n        # Evaluate for one epoch on validation set\n        val_metrics = evaluate(model, loss_fn, val_dataloader, params, args)\n\n        val_acc = val_metrics[\'accuracy\']\n        is_best = val_acc>=best_val_acc\n\n        # Save weights\n        utils.save_checkpoint({\'epoch\': epoch + 1,\n                               \'state_dict\': model.state_dict(),\n                               \'optim_dict\' : optimizer.state_dict()},\n                                is_best=is_best,\n                                checkpoint=model_dir)\n        # If best_eval, best_save_path\n        if is_best:\n            logging.info(""- Found new best accuracy"")\n            best_val_acc = val_acc\n\n            # Save best val metrics in a json file in the model directory\n            best_json_path = os.path.join(model_dir, ""eval_best_results.json"")\n            utils.save_dict_to_json(val_metrics, best_json_path)\n\n        # Save latest val metrics in a json file in the model directory\n        last_json_path = os.path.join(model_dir, ""eval_last_results.json"")\n        utils.save_dict_to_json(val_metrics, last_json_path)\n\n        # Tensorboard\n        writer.add_scalar(\'Train_accuracy\', train_acc, epoch)\n        writer.add_scalar(\'Train_loss\', train_loss, epoch)\n        writer.add_scalar(\'Test_accuracy\', val_metrics[\'accuracy\'], epoch)\n        writer.add_scalar(\'Test_loss\', val_metrics[\'loss\'], epoch)\n    writer.close()\n\n\n# normal training function\ndef train(model, optimizer, loss_fn, dataloader, params, epoch, warmup_scheduler, args):\n    """"""\n    Noraml training, without KD\n    """"""\n\n    # set model to training mode\n    model.train()\n    loss_avg = utils.RunningAverage()\n    losses = utils.AverageMeter()\n    total = 0\n    correct = 0\n\n    # Use tqdm for progress bar\n    with tqdm(total=len(dataloader)) as t:\n        for i, (train_batch, labels_batch) in enumerate(dataloader):\n\n            train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n            if epoch<=0:\n                warmup_scheduler.step()\n            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n\n            optimizer.zero_grad()\n            output_batch = model(train_batch)\n            if args.regularization:\n                loss = loss_fn(output_batch, labels_batch, params)\n            else:\n                loss = loss_fn(output_batch, labels_batch)\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = output_batch.max(1)\n            total += labels_batch.size(0)\n            correct += predicted.eq(labels_batch).sum().item()\n\n            # update the average loss\n            loss_avg.update(loss.data)\n            losses.update(loss.data, train_batch.size(0))\n\n            t.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()), lr=\'{:05.6f}\'.format(optimizer.param_groups[0][\'lr\']))\n            t.update()\n\n    acc = 100. * correct / total\n    logging.info(""- Train accuracy: {acc: .4f}, training loss: {loss: .4f}"".format(acc=acc, loss=losses.avg))\n    return acc, losses.avg\n\n\n\n\n\n\n\n\n\n'"
utils.py,7,"b'""""""\nTensorboard logger code referenced from:\nhttps://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/\nOther helper functions:\nhttps://github.com/cs230-stanford/cs230-stanford.github.io\n""""""\n\nimport json\nimport logging\nimport os\nimport shutil\nimport torch\nfrom collections import OrderedDict\nfrom torch.optim.lr_scheduler import _LRScheduler\n#import tensorflow as tf\nimport numpy as np\nimport scipy.misc \ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def save(self, json_path):\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n            \n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']""""""\n        return self.__dict__\n\n\nclass RunningAverage():\n    """"""A simple class that maintains the running average of a quantity\n    \n    Example:\n    ```\n    loss_avg = RunningAverage()\n    loss_avg.update(2)\n    loss_avg.update(4)\n    loss_avg() = 3\n    ```\n    """"""\n    def __init__(self):\n        self.steps = 0\n        self.total = 0\n    \n    def update(self, val):\n        self.total += val\n        self.steps += 1\n    \n    def __call__(self):\n        return self.total/float(self.steps)\n\nclass AverageMeter(object):\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val*n\n        self.count += n\n        self.avg = self.sum/self.count\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\'lr\']\n\ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef save_checkpoint(state, is_best, checkpoint, epoch_checkpoint = False):\n    """"""Saves model and training parameters at checkpoint + \'last.pth.tar\'. If is_best==True, also saves\n    checkpoint + \'best.pth.tar\'\n\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such as epoch, optimizer state_dict\n        is_best: (bool) True if it is the best model seen till now\n        checkpoint: (string) folder where parameters are to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'last.pth.tar\')\n    if not os.path.exists(checkpoint):\n        print(""Checkpoint Directory does not exist! Making directory {}"".format(checkpoint))\n        os.mkdir(checkpoint)\n    else:\n        print(""Checkpoint Directory exists! "")\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'best.pth.tar\'))\n    if epoch_checkpoint == True:\n        epoch_file = str(state[\'epoch\']-1) + \'.pth.tar\'\n        shutil.copyfile(filepath, os.path.join(checkpoint, epoch_file))\n\n\n\n\n\ndef load_checkpoint(checkpoint, model, optimizer=None):\n    """"""Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n    optimizer assuming it is present in checkpoint.\n\n    Args:\n        checkpoint: (string) filename which needs to be loaded\n        model: (torch.nn.Module) model for which the parameters are loaded\n        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n    """"""\n    if not os.path.exists(checkpoint):\n        raise(""File doesn\'t exist {}"".format(checkpoint))\n    if torch.cuda.is_available():\n        checkpoint = torch.load(checkpoint)\n    else:\n        # this helps avoid errors when loading single-GPU-trained weights onto CPU-model\n        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\'optim_dict\'])\n\n    return checkpoint\n\n\'\'\'\nclass Board_Logger(object):\n    """"""Tensorboard log utility""""""\n    \n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        """"""Log a list of images.""""""\n\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n\n    def histo_summary(self, tag, values, step, bins=1000):\n        """"""Log a histogram of the tensor of values.""""""\n\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush\n\'\'\'\n\nclass WarmUpLR(_LRScheduler):\n    """"""warmup_training learning rate scheduler\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        total_iters: totoal_iters of warmup phase\n    """"""\n\n    def __init__(self, optimizer, total_iters, last_epoch=-1):\n        self.total_iters = total_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        """"""we will use the first m batches, and set the learning\n        rate to base_lr * m / total_iters\n        """"""\n        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]'"
ImageNet_train/main.py,32,"b'import argparse\nimport os\nimport random\nimport shutil\nimport time\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.multiprocessing as mp\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom loss_function import loss_soft_regularization, loss_label_smoothing, loss_fn_kd\n\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'-a\', \'--arch\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'-at\', \'--arch_teacher\', metavar=\'ARCH\', default=\'densenet121\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: densenet121)\')\nparser.add_argument(\'-j\', \'--workers\', default=16, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=512, type=int,\n                    metavar=\'N\',\n                    help=\'mini-batch size (default: 256), this is the total \'\n                         \'batch size of all GPUs on the current node when \'\n                         \'using Data Parallel or Distributed Data Parallel\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\', dest=\'lr\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--wd\', \'--weight-decay\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\',\n                    dest=\'weight_decay\')\nparser.add_argument(\'-p\', \'--print-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--world-size\', default=-1, type=int,\n                    help=\'number of nodes for distributed training\')\nparser.add_argument(\'--rank\', default=-1, type=int,\n                    help=\'node rank for distributed training\')\nparser.add_argument(\'--dist-url\', default=\'tcp://224.66.41.62:23456\', type=str,\n                    help=\'url used to set up distributed training\')\nparser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                    help=\'distributed backend\')\nparser.add_argument(\'--seed\', default=None, type=int,\n                    help=\'seed for initializing training. \')\nparser.add_argument(\'--gpu\', default=None, type=int,\n                    help=\'GPU id to use.\')\nparser.add_argument(\'--multiprocessing-distributed\', action=\'store_true\',\n                    help=\'Use multi-processing distributed training to launch \'\n                         \'N processes per node, which has N GPUs. This is the \'\n                         \'fastest way to use PyTorch for either single node or \'\n                         \'multi node data parallel training\')\nparser.add_argument(\'--alpha\', default=0.1, type=float, help=""alpha to balance KD loss and cross entropy loss"")\nparser.add_argument(\'--T\', default=20, type=float, help=""Temperature"")\n#parser.add_argument(\'--selfKD\', default=1, type=int, help=""parameter for self training or normal training"")\n#parser.add_argument(\'--regularization\', action=\'store_true\', help=\'parameter for regularization\')\n#parser.add_argument(\'--smoothing\', action=\'store_true\', help=\'parameter for label smoothing\')\n#parser.add_argument(\'--selfKD\', action=\'store_true\', help=""parameter for self training or normal training"")\n\nparser.add_argument(\'--regularization\', default=0, type=int, help=\'parameter for regularization\')\nparser.add_argument(\'--smoothing\', default=0, type=int, help=\'parameter for label smoothing\')\nparser.add_argument(\'--KD\', default=0, type=int, help=""parameter for self training or normal training"")\n\n\n\nbest_acc1 = 0\nargs, unparsed = parser.parse_known_args()\n\n\ndef main():\n    #args = parser.parse_args()\n    #print(args)\n    print_parameters(args)\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        cudnn.deterministic = True\n        warnings.warn(\'You have chosen to seed training. \'\n                      \'This will turn on the CUDNN deterministic setting, \'\n                      \'which can slow down your training considerably! \'\n                      \'You may see unexpected behavior when restarting \'\n                      \'from checkpoints.\')\n\n    if args.gpu is not None:\n        warnings.warn(\'You have chosen a specific GPU. This will completely \'\n                      \'disable data parallelism.\')\n\n    if args.dist_url == ""env://"" and args.world_size == -1:\n        args.world_size = int(os.environ[""WORLD_SIZE""])\n\n    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n\n    # tensorboard setting\n    if args.KD == 1:\n        print("">>>>>>>>>>>>>>>>>KD training, self-training or normal KD !>>>>>>>>>>>>>>>>"")\n        log_dir = \'logs/\' + str(args.arch) + \'/tensorboard_Teacher_\' + str(args.arch_teacher) + \'_T_\' + str(args.T) + \'_a_\' + str(args.alpha) + \'/\'\n    elif args.regularization == 1:\n        print("">>>>>>>>>>>>>Regularization, Teacher-free knowledge distillation!>>>>>>>>>>>>"")\n        log_dir = \'logs/\' + str(args.arch) + \'/tensorboard_regularization/\'\n    elif args.smoothing == 1:\n        print("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Label Smoothing!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"")\n        log_dir = \'logs/\' + str(args.arch) + \'/tensorboard_LS/\'\n    else:\n        print("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Normal Training>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"")\n        log_dir = \'logs/\' + str(args.arch) + \'/tensorboard_normal_train/\'\n\n    warnings.filterwarnings(""ignore"")\n    writer = SummaryWriter(log_dir=log_dir)\n    print(""Tensorboard dir: {}"".format(log_dir))\n\n    ngpus_per_node = torch.cuda.device_count()\n    if args.multiprocessing_distributed:\n        # Since we have ngpus_per_node processes per node, the total world_size\n        # needs to be adjusted accordingly\n        args.world_size = ngpus_per_node * args.world_size\n        # Use torch.multiprocessing.spawn to launch distributed processes: the\n        # main_worker process function\n        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args), writer=writer, log_dir=log_dir)\n    else:\n        # Simply call main_worker function\n        main_worker(args.gpu, ngpus_per_node, args, writer, log_dir)\n\n\ndef main_worker(gpu, ngpus_per_node, args, writer, log_dir):\n    global best_acc1\n    args.gpu = gpu\n\n    if args.gpu is not None:\n        print(""Use GPU: {} for training"".format(args.gpu))\n\n    if args.distributed:\n        if args.dist_url == ""env://"" and args.rank == -1:\n            args.rank = int(os.environ[""RANK""])\n        if args.multiprocessing_distributed:\n            # For multiprocessing distributed training, rank needs to be the\n            # global rank among all the processes\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=args.world_size, rank=args.rank)\n\n\n    # create model\n    print(""=> creating model \'{}\'"".format(args.arch))\n    model = models.__dict__[args.arch]()\n    print(""=> creating teacher model \'{}\'"".format(args.arch))\n    teacher_model = models.__dict__[args.arch_teacher](pretrained=True)\n    # Use the following lines if you load a pre-trained model by yourself\n    #teacher_file = \'pretrained_model/\' + \'teacher_model_\'+ str(args.arch)+\'.pth\'\n    #teacher_model = torch.load(teacher_file)\n\n    if args.distributed:\n        # For multiprocessing distributed, DistributedDataParallel constructor\n        # should always set the single device scope, otherwise,\n        # DistributedDataParallel will use all available devices.\n        if args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            model.cuda(args.gpu)\n            teacher_model.cuda(args.gpu)\n            # When using a single GPU per process and per\n            # DistributedDataParallel, we need to divide the batch size\n            # ourselves based on the total number of GPUs we have\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n            teacher_model = torch.nn.parallel.DistributedDataParallel(teacher_model, device_ids=[args.gpu])\n        else:\n            model.cuda()\n            teacher_model.cuda()\n            # DistributedDataParallel will divide and allocate batch_size to all\n            # available GPUs if device_ids are not set\n            model = torch.nn.parallel.DistributedDataParallel(model)\n            teacher_model = torch.nn.parallel.DistributedDataParallel(teacher_model)\n\n    elif args.gpu is not None:\n        torch.cuda.set_device(args.gpu)\n        model = model.cuda(args.gpu)\n        teacher_model = teacher_model.cuda(args.gpu)\n    else:\n        # DataParallel will divide and allocate batch_size to all available GPUs\n        if args.arch.startswith(\'alexnet\') or args.arch.startswith(\'vgg\'):\n            model.features = torch.nn.DataParallel(model.features)\n            model.cuda()\n            teacher_model.features = torch.nn.DataParallel(teacher_model.features)\n            teacher_model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n            teacher_model = torch.nn.DataParallel(teacher_model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr*(args.batch_size/256),\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc1 = checkpoint[\'best_acc1\']\n            if args.gpu is not None:\n                # best_acc1 may be from a checkpoint from a different GPU\n                best_acc1 = best_acc1.to(args.gpu)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_dataset = datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    else:\n        train_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion, args)\n        return\n\n    # learning rate scheduler, decay 1/10 in 30, 60 and 80 epoch\n    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80], gamma=0.1)\n\n    # Test the accuracy of teacher\n    acc1, acc5, test_loss = validate(val_loader, teacher_model, criterion, args)\n    print("">>>>>>>>>>>>>>>The teacher accuracy, top1:{}, top5:{}>>>>>>>>>>>>"".format(acc1, acc5))\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n\n        # learning rate update\n        scheduler.step(epoch)\n\n        # train for one epoch\n        train_acc1, train_acc5, train_loss_CE = train(train_loader, model, teacher_model, criterion, optimizer, epoch, args)\n\n        # evaluate on validation set\n        acc1, acc5, test_loss = validate(val_loader, model, criterion, args)\n\n        # remember best acc@1 and save checkpoint\n        is_best = acc1 > best_acc1\n        best_acc1 = max(acc1, best_acc1)\n\n        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n                and args.rank % ngpus_per_node == 0):\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_acc1\': best_acc1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, dir = log_dir)\n\n        # Tensorboard\n        writer.add_scalar(\'Train_acc_top1\', train_acc1, epoch)\n        writer.add_scalar(\'Train_acc_top5\', train_acc5, epoch)\n        writer.add_scalar(\'Train_loss_CE\', train_loss_CE, epoch)\n        writer.add_scalar(\'Test_acc_top1\', acc1, epoch)\n        writer.add_scalar(\'Test_acc_top5\', acc5, epoch)\n        writer.add_scalar(\'Test_loss_CE\', test_loss, epoch)\n            # export scalar data to JSON for external processing\n    writer.close()\n\n\ndef train(train_loader, model, teacher_model, criterion, optimizer, epoch, args):\n    batch_time = AverageMeter(\'Time\', \':6.3f\')\n    data_time = AverageMeter(\'Data\', \':6.3f\')\n    losses = AverageMeter(\'Loss\', \':.4e\')\n    losses_CE = AverageMeter(\'Loss_CE\', \':.4e\')\n    top1 = AverageMeter(\'Acc@1\', \':6.2f\')\n    top5 = AverageMeter(\'Acc@5\', \':6.2f\')\n    print(\'learning rate: {}\'.format(optimizer.param_groups[0][\'lr\']))\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses, top1, top5],\n        prefix=""Epoch: [{}]"".format(epoch))\n\n    # switch to train mode\n    model.train()\n    teacher_model.eval()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if args.gpu is not None:\n            images = images.cuda(args.gpu, non_blocking=True)\n        target = target.cuda(args.gpu, non_blocking=True)\n\n        # compute output\n        output = model(images)\n        # teacher_output = teacher_model(images)\n\n        if args.KD == 1:\n            teacher_output = teacher_model(images)\n            loss = loss_fn_kd(output, target, teacher_output, args.alpha, args.T)\n        elif args.regularization == 1:\n            loss = loss_soft_regularization(output, target)\n        elif args.smoothing == 1:\n            loss = loss_label_smoothing(output, target)\n        else:\n            loss = criterion(output, target)\n\n        loss_CE = criterion(output, target)\n\n        # measure accuracy and record loss\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), images.size(0))\n        losses_CE.update(loss_CE.item(), images.size(0))\n        top1.update(acc1[0], images.size(0))\n        top5.update(acc5[0], images.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            progress.display(i)\n    return top1.avg, top5.avg, losses_CE.avg\n\n\ndef validate(val_loader, model, criterion, args):\n    batch_time = AverageMeter(\'Time\', \':6.3f\')\n    losses = AverageMeter(\'Loss\', \':.4e\')\n    top1 = AverageMeter(\'Acc@1\', \':6.2f\')\n    top5 = AverageMeter(\'Acc@5\', \':6.2f\')\n    progress = ProgressMeter(\n        len(val_loader),\n        [batch_time, losses, top1, top5],\n        prefix=\'Test: \')\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (images, target) in enumerate(val_loader):\n            if args.gpu is not None:\n                images = images.cuda(args.gpu, non_blocking=True)\n            target = target.cuda(args.gpu, non_blocking=True)\n\n            # compute output\n            output = model(images)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), images.size(0))\n            top1.update(acc1[0], images.size(0))\n            top5.update(acc5[0], images.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                progress.display(i)\n\n        # TODO: this should also be done with the ProgressMeter\n        print(\' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\'\n              .format(top1=top1, top5=top5))\n\n    return top1.avg, top5.avg, losses.avg\n\n\ndef save_checkpoint(state, is_best, dir, filename=\'checkpoint.pth.tar\'):\n    dir_name = dir + filename\n    torch.save(state, dir_name)\n    if is_best:\n        shutil.copyfile(dir_name, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self, name, fmt=\':f\'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = \'{name} {val\' + self.fmt + \'} ({avg\' + self.fmt + \'})\'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=""""):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print(\'\\t\'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \'{:\' + str(num_digits) + \'d}\'\n        return \'[\' + fmt + \'/\' + fmt.format(num_batches) + \']\'\n\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef print_parameters(argv):\n    for arg in vars(argv):\n        print(arg, getattr(argv, arg))\n\nif __name__ == \'__main__\':\n    main()'"
model/alexnet.py,1,"b'\'\'\'AlexNet for CIFAR10. FC layers are removed. Paddings are adjusted.\nWithout BN, the start learning rate should be 0.01\n(c) YANG, Wei\n\'\'\'\nimport torch.nn as nn\n\n\n__all__ = [\'alexnet\']\n\n\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=100):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\ndef alexnet(**kwargs):\n    r""""""AlexNet model architecture from the\n    `""One weird trick..."" <https://arxiv.org/abs/1404.5997>`_ paper.\n    """"""\n    model = AlexNet(**kwargs)\n    return model'"
model/densenet.py,2,"b'""""""\ndense net in pytorch\n[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\n    Densely Connected Convolutional Networks\n    https://arxiv.org/abs/1608.06993v5\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\n\n#""""""Bottleneck layers. Although each layer only produces k\n#output feature-maps, it typically has many more inputs. It\n#has been noted in [37, 11] that a 1\xc3\x971 convolution can be in-\n#troduced as bottleneck layer before each 3\xc3\x973 convolution\n#to reduce the number of input feature-maps, and thus to\n#improve computational efficiency.""""""\nclass Bottleneck(nn.Module):\n    def __init__(self, in_channels, growth_rate):\n        super().__init__()\n        #""""""In  our experiments, we let each 1\xc3\x971 convolution\n        #produce 4k feature-maps.""""""\n        inner_channel = 4 * growth_rate\n\n        #""""""We find this design especially effective for DenseNet and\n        #we refer to our network with such a bottleneck layer, i.e.,\n        #to the BN-ReLU-Conv(1\xc3\x971)-BN-ReLU-Conv(3\xc3\x973) version of H ` ,\n        #as DenseNet-B.""""""\n        self.bottle_neck = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n            nn.BatchNorm2d(inner_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n        )\n\n    def forward(self, x):\n        return torch.cat([x, self.bottle_neck(x)], 1)\n\n#""""""We refer to layers between blocks as transition\n#layers, which do convolution and pooling.""""""\nclass Transition(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #""""""The transition layers used in our experiments\n        #consist of a batch normalization layer and an 1\xc3\x971\n        #convolutional layer followed by a 2\xc3\x972 average pooling\n        #layer"""""".\n        self.down_sample = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.AvgPool2d(2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.down_sample(x)\n\n#DesneNet-BC\n#B stands for bottleneck layer(BN-RELU-CONV(1x1)-BN-RELU-CONV(3x3))\n#C stands for compression factor(0<=theta<=1)\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=100):\n        super().__init__()\n        self.growth_rate = growth_rate\n\n        #""""""Before entering the first dense block, a convolution\n        #with 16 (or twice the growth rate for DenseNet-BC)\n        #output channels is performed on the input images.""""""\n        inner_channels = 2 * growth_rate\n\n        #For convolutional layers with kernel size 3\xc3\x973, each\n        #side of the inputs is zero-padded by one pixel to keep\n        #the feature-map size fixed.\n        self.conv1 = nn.Conv2d(3, inner_channels, kernel_size=3, padding=1, bias=False)\n\n        self.features = nn.Sequential()\n\n        for index in range(len(nblocks) - 1):\n            self.features.add_module(""dense_block_layer_{}"".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n            inner_channels += growth_rate * nblocks[index]\n\n            #""""""If a dense block contains m feature-maps, we let the\n            #following transition layer generate \xce\xb8m output feature-\n            #maps, where 0 < \xce\xb8 \xe2\x89\xa4 1 is referred to as the compression\n            #fac-tor.\n            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n            self.features.add_module(""transition_layer_{}"".format(index), Transition(inner_channels, out_channels))\n            inner_channels = out_channels\n\n        self.features.add_module(""dense_block{}"".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n        self.features.add_module(\'bn\', nn.BatchNorm2d(inner_channels))\n        self.features.add_module(\'relu\', nn.ReLU(inplace=True))\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.linear = nn.Linear(inner_channels, num_class)\n\n    def forward(self, x):\n        output = self.conv1(x)\n        output = self.features(output)\n        output = self.avgpool(output)\n        output = output.view(output.size()[0], -1)\n        output = self.linear(output)\n        return output\n\n    def _make_dense_layers(self, block, in_channels, nblocks):\n        dense_block = nn.Sequential()\n        for index in range(nblocks):\n            dense_block.add_module(\'bottle_neck_layer_{}\'.format(index), block(in_channels, self.growth_rate))\n            in_channels += self.growth_rate\n        return dense_block\n\ndef densenet121(**kwargs):\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, **kwargs)\n\ndef densenet169(**kwargs):\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32,**kwargs)\n\ndef densenet201(**kwargs):\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32, **kwargs)\n\ndef densenet161(**kwargs):\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48, **kwargs)\n'"
model/googlenet.py,2,"b'""""""google net in pytorch\n[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    Going Deeper with Convolutions\n    https://arxiv.org/abs/1409.4842v1\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass Inception(nn.Module):\n    def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n        super().__init__()\n\n        # 1x1conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(input_channels, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(inplace=True)\n        )\n\n        # 1x1conv -> 3x3conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n            nn.BatchNorm2d(n3x3_reduce),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(inplace=True)\n        )\n\n        # 1x1conv -> 5x5conv branch\n        # we use 2 3x3 conv filters stacked instead\n        # of 1 5x5 filters to obtain the same receptive\n        # field with fewer parameters\n        self.b3 = nn.Sequential(\n            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n            nn.BatchNorm2d(n5x5_reduce),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5, n5x5),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(inplace=True)\n        )\n\n        # 3x3pooling -> 1x1conv\n        # same conv\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n            nn.BatchNorm2d(pool_proj),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n\n\nclass GoogleNet(nn.Module):\n\n    def __init__(self, num_class=100):\n        super().__init__()\n        self.prelayer = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n\n        # although we only use 1 conv layer as prelayer,\n        # we still use name a3, b3.......\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        # """"""In general, an Inception network is a network consisting of\n        # modules of the above type stacked upon each other, with occasional\n        # max-pooling layers with stride 2 to halve the resolution of the\n        # grid""""""\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        # input feature size: 8*8*1024\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout2d(p=0.4)\n        self.linear = nn.Linear(1024, num_class)\n\n    def forward(self, x):\n        output = self.prelayer(x)\n        output = self.a3(output)\n        output = self.b3(output)\n\n        output = self.maxpool(output)\n\n        output = self.a4(output)\n        output = self.b4(output)\n        output = self.c4(output)\n        output = self.d4(output)\n        output = self.e4(output)\n\n        output = self.maxpool(output)\n\n        output = self.a5(output)\n        output = self.b5(output)\n\n        # """"""It was found that a move from fully connected layers to\n        # average pooling improved the top-1 accuracy by about 0.6%,\n        # however the use of dropout remained essential even after\n        # removing the fully connected layers.""""""\n        output = self.avgpool(output)\n        output = self.dropout(output)\n        output = output.view(output.size()[0], -1)\n        output = self.linear(output)\n\n        return output\n\n\ndef googlenet(**kwargs):\n    return GoogleNet(**kwargs)'"
model/mobilenetv2.py,2,"b'""""""mobilenetv2 in pytorch\n[1] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\n    MobileNetV2: Inverted Residuals and Linear Bottlenecks\n    https://arxiv.org/abs/1801.04381\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LinearBottleNeck(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n        super().__init__()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * t, 1),\n            nn.BatchNorm2d(in_channels * t),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n            nn.BatchNorm2d(in_channels * t),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * t, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n\n        residual = self.residual(x)\n\n        if self.stride == 1 and self.in_channels == self.out_channels:\n            residual += x\n\n        return residual\n\nclass MobileNetV2(nn.Module):\n\n    def __init__(self, class_num=100):\n        super().__init__()\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 32, 1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU6(inplace=True)\n        )\n\n        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(320, 1280, 1),\n            nn.BatchNorm2d(1280),\n            nn.ReLU6(inplace=True)\n        )\n\n        self.conv2 = nn.Conv2d(1280, class_num, 1)\n\n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.stage6(x)\n        x = self.stage7(x)\n        x = self.conv1(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n\n    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n\n        layers = []\n        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n\n        while repeat - 1:\n            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n            repeat -= 1\n\n        return nn.Sequential(*layers)\n\ndef mobilenetv2(**kwargs):\n    return MobileNetV2(**kwargs)'"
model/net.py,4,"b'""""""\n   Baseline CNN, losss function and metrics\n   Also customizes knowledge distillation (KD) loss function here\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    """"""\n    This is the standard way to define your own network in PyTorch. You typically choose the components\n    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n    such as F.relu, F.sigmoid, F.softmax, F.max_pool2d. Be careful to ensure your dimensions are correct after each\n    step. You are encouraged to have a look at the network in pytorch/nlp/model/net.py to get a better sense of how\n    you can go about defining your own network.\n    The documentation for all the various components available o you is here: http://pytorch.org/docs/master/nn.html\n    """"""\n\n    def __init__(self, params):\n        """"""\n        We define an convolutional network that predicts the sign from an image. The components\n        required are:\n        Args:\n            params: (Params) contains num_channels\n        """"""\n        super(Net, self).__init__()\n        self.num_channels = params.num_channels\n\n        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,\n        # stride, padding). We also include batch normalisation layers that help stabilise training.\n        # For more details on how to use these layers, check out the documentation.\n        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(self.num_channels)\n        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(self.num_channels*2)\n        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(self.num_channels*4)\n\n        # 2 fully connected layers to transform the output of the convolution layers to the final output\n        self.fc1 = nn.Linear(4*4*self.num_channels*4, self.num_channels*4)\n        self.fcbn1 = nn.BatchNorm1d(self.num_channels*4)\n        self.fc2 = nn.Linear(self.num_channels*4, params.num_class)\n        self.dropout_rate = params.dropout_rate\n\n    def forward(self, s):\n        """"""\n        This function defines how we use the components of our network to operate on an input batch.\n        Args:\n            s: (Variable) contains a batch of images, of dimension batch_size x 3 x 32 x 32 .\n        Returns:\n            out: (Variable) dimension batch_size x 6 with the log probabilities for the labels of each image.\n        Note: the dimensions after each step are provided\n        """"""\n        #                                                  -> batch_size x 3 x 32 x 32\n        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3\n        s = self.bn1(self.conv1(s))                         # batch_size x num_channels x 32 x 32\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 16 x 16\n        s = self.bn2(self.conv2(s))                         # batch_size x num_channels*2 x 16 x 16\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 8 x 8\n        s = self.bn3(self.conv3(s))                         # batch_size x num_channels*4 x 8 x 8\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 4 x 4\n\n        # flatten the output for each image\n        s = s.view(-1, 4*4*self.num_channels*4)             # batch_size x 4*4*num_channels*4\n\n        # apply 2 fully connected layers with dropout\n        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))),\n            p=self.dropout_rate, training=self.training)    # batch_size x self.num_channels*4\n        s = self.fc2(s)                                     # batch_size x 10\n\n        return s\n\n\n\n\n'"
model/resnet.py,7,"b'\'\'\'ResNet in PyTorch.\n\nFor Pre-activation ResNet, see \'preact_resnet.py\'.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=100):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n        self.avg_pool = nn.AvgPool2d((4,4), stride=(4,4))\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)  # shape: [batch_size, 64, 32, 32]\n        feature4 = self.avg_pool(out).view(out.size(0), -1)  # self.avg_pool(out): [batch_size, 64, 8, 8]\n        # feature4: [batch_size, 4096]\n        out = self.layer2(out) # shape: [batch_size, 128, 16, 16]\n        feature3 = self.avg_pool(out).view(out.size(0), -1)\n        # feature3: [batch_size, 2048]\n        out = self.layer3(out) # shape: [batch_size, 256, 8, 8]\n        feature2 = F.avg_pool2d(out, out.size(-1))  # [batch_size, channel_num]\n        feature2 = torch.squeeze(feature2)\n        # feature2: [batch_size, 1024]\n        out = self.layer4(out)\n        #print(out.shape)\n        feature1 = F.avg_pool2d(out, out.size(-1))  # average pooling to [batch_size, channel_num]\n        feature1 = torch.squeeze(feature1)\n        #print(feature1.shape)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        #feature1 = out    # [batch_size, 512]\n        out = self.linear(out)\n\n        return out #, feature1, feature2 #, feature3, feature4\n\ndef ResNet18(**kwargs):\n    return ResNet(block=BasicBlock, num_blocks=[2,2,2,2], **kwargs)\n\ndef ResNet34(**kwargs):\n    return ResNet(BasicBlock, [3,4,6,3], **kwargs)\n\ndef ResNet50(**kwargs):\n    return ResNet(Bottleneck, [3,4,6,3], **kwargs)\n\ndef ResNet101(**kwargs):\n    return ResNet(Bottleneck, [3,4,23,3], **kwargs)\n\ndef ResNet152(**kwargs):\n    return ResNet(block=Bottleneck, num_blocks=[3,8,36,3], **kwargs)\n\n\n# def test():\n#     net = ResNet18()\n#     y = net(Variable(torch.randn(1,3,32,32)))\n#     print(y.size())\n\n# test()\n\ndef loss_fn(outputs, labels):\n    """"""\n    Compute the cross entropy loss given outputs and labels.\n\n    Returns:\n        loss (Variable): cross entropy loss for all images in the batch\n\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    """"""\n    return nn.CrossEntropyLoss()(outputs, labels)\n\n\ndef accuracy(outputs, labels):\n    """"""\n    Compute the accuracy, given the outputs and labels for all images.\n\n    Returns: (float) accuracy in [0,1]\n    """"""\n    outputs = np.argmax(outputs, axis=1)\n    return np.sum(outputs==labels)/float(labels.size)\n\n\n# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\nmetrics = {\n    \'accuracy\': accuracy,\n    # could add more metrics such as accuracy for each token type\n}'"
model/resnext.py,4,"b'from __future__ import division\n"""""" \nCreates a ResNeXt Model as defined in:\nXie, S., Girshick, R., Dollar, P., Tu, Z., & He, K. (2016). \nAggregated residual transformations for deep neural networks. \narXiv preprint arXiv:1611.05431.\nimport from https://github.com/prlz77/ResNeXt.pytorch/blob/master/models/model.py\n""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport numpy as np\n\n# __all__ = [\'resnext\']\n\nclass ResNeXtBottleneck(nn.Module):\n    """"""\n    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n    """"""\n    def __init__(self, in_channels, out_channels, stride, cardinality, widen_factor):\n        """""" Constructor\n        Args:\n            in_channels: input channel dimensionality\n            out_channels: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            cardinality: num of convolution groups.\n            widen_factor: factor to reduce the input dimensionality before convolution.\n        """"""\n        super(ResNeXtBottleneck, self).__init__()\n        D = cardinality * out_channels // widen_factor\n        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D)\n        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn = nn.BatchNorm2d(D)\n        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\'shortcut_conv\', nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False))\n            self.shortcut.add_module(\'shortcut_bn\', nn.BatchNorm2d(out_channels))\n\n    def forward(self, x):\n        bottleneck = self.conv_reduce.forward(x)\n        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)\n        bottleneck = self.conv_conv.forward(bottleneck)\n        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)\n        bottleneck = self.conv_expand.forward(bottleneck)\n        bottleneck = self.bn_expand.forward(bottleneck)\n        residual = self.shortcut.forward(x)\n        return F.relu(residual + bottleneck, inplace=True)\n\n\nclass CifarResNeXt(nn.Module):\n    """"""\n    ResNext optimized for the Cifar dataset, as specified in\n    https://arxiv.org/pdf/1611.05431.pdf\n    """"""\n    def __init__(self, cardinality, depth, num_classes, widen_factor=4, dropRate=0):\n        """""" Constructor\n        Args:\n            cardinality: number of convolution groups.\n            depth: number of layers.\n            num_classes: number of classes\n            widen_factor: factor to adjust the channel dimensionality\n        """"""\n        super(CifarResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.depth = depth\n        self.block_depth = (self.depth - 2) // 9\n        self.widen_factor = widen_factor\n        self.num_classes = num_classes\n        self.output_size = 64\n        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]\n\n        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn_1 = nn.BatchNorm2d(64)\n        self.stage_1 = self.block(\'stage_1\', self.stages[0], self.stages[1], 1)\n        self.stage_2 = self.block(\'stage_2\', self.stages[1], self.stages[2], 2)\n        self.stage_3 = self.block(\'stage_3\', self.stages[2], self.stages[3], 2)\n        self.classifier = nn.Linear(1024, num_classes)\n        init.kaiming_normal(self.classifier.weight)\n\n        for key in self.state_dict():\n            if key.split(\'.\')[-1] == \'weight\':\n                if \'conv\' in key:\n                    init.kaiming_normal(self.state_dict()[key], mode=\'fan_out\')\n                if \'bn\' in key:\n                    self.state_dict()[key][...] = 1\n            elif key.split(\'.\')[-1] == \'bias\':\n                self.state_dict()[key][...] = 0\n\n    def block(self, name, in_channels, out_channels, pool_stride=2):\n        """""" Stack n bottleneck modules where n is inferred from the depth of the network.\n        Args:\n            name: string name of the current block.\n            in_channels: number of input channels\n            out_channels: number of output channels\n            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n        Returns: a Module consisting of n sequential bottlenecks.\n        """"""\n        block = nn.Sequential()\n        for bottleneck in range(self.block_depth):\n            name_ = \'%s_bottleneck_%d\' % (name, bottleneck)\n            if bottleneck == 0:\n                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,\n                                                          self.widen_factor))\n            else:\n                block.add_module(name_,\n                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.widen_factor))\n        return block\n\n    def forward(self, x):\n        x = self.conv_1_3x3.forward(x)\n        x = F.relu(self.bn_1.forward(x), inplace=True)\n        x = self.stage_1.forward(x)\n        x = self.stage_2.forward(x)\n        x = self.stage_3.forward(x)\n        x = F.avg_pool2d(x, 8, 1)\n        x = x.view(-1, 1024)\n        return self.classifier(x)\n\n\ndef loss_fn(outputs, labels):\n    """"""\n    Compute the cross entropy loss given outputs and labels.\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    """"""\n    return nn.CrossEntropyLoss()(outputs, labels)\n\n\ndef accuracy(outputs, labels):\n    """"""\n    Compute the accuracy, given the outputs and labels for all images.\n    Returns: (float) accuracy in [0,1]\n    """"""\n    outputs = np.argmax(outputs, axis=1)\n    return np.sum(outputs==labels)/float(labels.size)\n\n\n# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\nmetrics = {\n    \'accuracy\': accuracy,\n    # could add more metrics such as accuracy for each token type\n}'"
model/shufflenetv2.py,4,"b'""""""shufflenetv2 in pytorch\n[1] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun\n    ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    https://arxiv.org/abs/1807.11164\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef channel_split(x, split):\n    """"""split a tensor into two pieces along channel dimension\n    Args:\n        x: input tensor\n        split:(int) channel size for each pieces\n    """"""\n    assert x.size(1) == split * 2\n    return torch.split(x, split, dim=1)\n\n\ndef channel_shuffle(x, groups):\n    """"""channel shuffle operation\n    Args:\n        x: input tensor\n        groups: input branch number\n    """"""\n\n    batch_size, channels, height, width = x.size()\n    channels_per_group = int(channels / groups)\n\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    x = x.transpose(1, 2).contiguous()\n    x = x.view(batch_size, -1, height, width)\n\n    return x\n\n\nclass ShuffleUnit(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if stride != 1 or in_channels != out_channels:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n\n            in_channels = int(in_channels / 2)\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    def forward(self, x):\n\n        if self.stride == 1 and self.out_channels == self.in_channels:\n            shortcut, residual = channel_split(x, int(self.in_channels / 2))\n        else:\n            shortcut = x\n            residual = x\n\n        shortcut = self.shortcut(shortcut)\n        residual = self.residual(residual)\n        x = torch.cat([shortcut, residual], dim=1)\n        x = channel_shuffle(x, 2)\n\n        return x\n\n\nclass ShuffleNetV2(nn.Module):\n\n    def __init__(self, ratio=1, class_num=100):\n        super().__init__()\n        if ratio == 0.5:\n            out_channels = [48, 96, 192, 1024]\n        elif ratio == 1:\n            out_channels = [116, 232, 464, 1024]\n        elif ratio == 1.5:\n            out_channels = [176, 352, 704, 1024]\n        elif ratio == 2:\n            out_channels = [244, 488, 976, 2048]\n        else:\n            ValueError(\'unsupported ratio number\')\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 24, 3, padding=1),\n            nn.BatchNorm2d(24)\n        )\n\n        self.stage2 = self._make_stage(24, out_channels[0], 3)\n        self.stage3 = self._make_stage(out_channels[0], out_channels[1], 7)\n        self.stage4 = self._make_stage(out_channels[1], out_channels[2], 3)\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(out_channels[2], out_channels[3], 1),\n            nn.BatchNorm2d(out_channels[3]),\n            nn.ReLU(inplace=True)\n        )\n\n        self.fc = nn.Linear(out_channels[3], class_num)\n\n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.conv5(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_stage(self, in_channels, out_channels, repeat):\n        layers = []\n        layers.append(ShuffleUnit(in_channels, out_channels, 2))\n\n        while repeat:\n            layers.append(ShuffleUnit(out_channels, out_channels, 1))\n            repeat -= 1\n\n        return nn.Sequential(*layers)\n\n\ndef shufflenetv2(**kwargs):\n    return ShuffleNetV2(**kwargs)'"
model/utils.py,2,b'try:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url'
model/wrn.py,4,"b'import numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# __all__ = [\'wrn\']\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                               padding=0, bias=False) or None\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.layer(x)\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n        assert (depth - 4) % 6 == 0, \'depth should be 6n+4\'\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n\n\ndef loss_fn(outputs, labels):\n    """"""\n    Compute the cross entropy loss given outputs and labels.\n\n    Args:\n        outputs: (Variable) dimension batch_size x 6 - output of the model\n        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n\n    Returns:\n        loss (Variable): cross entropy loss for all images in the batch\n\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    """"""\n    return nn.CrossEntropyLoss()(outputs, labels)\n\n\ndef accuracy(outputs, labels):\n    """"""\n    Compute the accuracy, given the outputs and labels for all images.\n\n    Args:\n        outputs: (np.ndarray) dimension batch_size x 6 - log softmax output of the model\n        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n\n    Returns: (float) accuracy in [0,1]\n    """"""\n    outputs = np.argmax(outputs, axis=1)\n    return np.sum(outputs==labels)/float(labels.size)\n\n\n# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\nmetrics = {\n    \'accuracy\': accuracy,\n    # could add more metrics such as accuracy for each token type\n}'"
