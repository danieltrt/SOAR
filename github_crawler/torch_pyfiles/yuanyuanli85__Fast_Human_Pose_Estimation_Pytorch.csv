file_path,api_count,code
example/mpii.py,16,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nfrom pose import Bar\nfrom pose.utils.logger import Logger, savefig\nfrom pose.utils.evaluation import accuracy, AverageMeter, final_preds\nfrom pose.utils.misc import save_checkpoint, save_pred, adjust_learning_rate\nfrom pose.utils.osutils import mkdir_p, isfile, isdir, join\nfrom pose.utils.imutils import batch_with_heatmap\nfrom pose.utils.transforms import fliplr, flip_back\nimport pose.models as models\nimport pose.datasets as datasets\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\nidx = [1,2,3,4,5,6,11,12,15,16]\n\nbest_acc = 0\n\n\ndef main(args):\n    global best_acc\n\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # create model\n    print(""==> creating model \'{}\', stacks={}, blocks={}"".format(args.arch, args.stacks, args.blocks))\n    model = models.__dict__[args.arch](num_stacks=args.stacks, num_blocks=args.blocks, num_classes=args.num_classes, mobile=args.mobile)\n\n    model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = torch.nn.MSELoss(size_average=True).cuda()\n\n    optimizer = torch.optim.RMSprop(model.parameters(), \n                                lr=args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    title = \'mpii-\' + args.arch\n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc = checkpoint[\'best_acc\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:        \n        logger = Logger(join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\', \'Val Loss\', \'Train Acc\', \'Val Acc\'])\n\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    # Data loading code\n    train_loader = torch.utils.data.DataLoader(\n        datasets.Mpii(\'data/mpii/mpii_annotations.json\', \'data/mpii/images\',\n                      sigma=args.sigma, label_type=args.label_type,\n                      inp_res=args.in_res, out_res=args.in_res//4),\n        batch_size=args.train_batch, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        datasets.Mpii(\'data/mpii/mpii_annotations.json\', \'data/mpii/images\',\n                      sigma=args.sigma, label_type=args.label_type, train=False,\n                      inp_res=args.in_res, out_res=args.in_res // 4),\n        batch_size=args.test_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\') \n        loss, acc, predictions = validate(val_loader, model, criterion, args.num_classes, args.in_res//4, args.debug, args.flip)\n        save_pred(predictions, checkpoint=args.checkpoint)\n        return\n\n    lr = args.lr\n    for epoch in range(args.start_epoch, args.epochs):\n        lr = adjust_learning_rate(optimizer, epoch, lr, args.schedule, args.gamma)\n        print(\'\\nEpoch: %d | LR: %.8f\' % (epoch + 1, lr))\n\n        # decay sigma\n        if args.sigma_decay > 0:\n            train_loader.dataset.sigma *=  args.sigma_decay\n            val_loader.dataset.sigma *=  args.sigma_decay\n\n        # train for one epoch\n        train_loss, train_acc = train(train_loader, model, criterion, optimizer, args.debug, args.flip)\n\n        # evaluate on validation set\n        valid_loss, valid_acc, predictions = validate(val_loader, model, criterion, args.num_classes,\n                                                      args.in_res//4, args.debug, args.flip)\n\n        # append logger file\n        logger.append([epoch + 1, lr, train_loss, valid_loss, train_acc, valid_acc])\n\n        # remember best acc and save checkpoint\n        is_best = valid_acc > best_acc\n        best_acc = max(valid_acc, best_acc)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_acc\': best_acc,\n            \'optimizer\' : optimizer.state_dict(),\n        }, predictions, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n    logger.plot([\'Train Acc\', \'Val Acc\'])\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n\n\ndef train(train_loader, model, criterion, optimizer, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acces = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n\n    gt_win, pred_win = None, None\n    bar = Bar(\'Processing\', max=len(train_loader))\n    for i, (inputs, target, meta) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input_var = torch.autograd.Variable(inputs.cuda())\n        target_var = torch.autograd.Variable(target.cuda(async=True))\n\n        # compute output\n        output = model(input_var)\n        score_map = output[-1].data.cpu()\n\n        loss = criterion(output[0], target_var)\n        for j in range(1, len(output)):\n            loss += criterion(output[j], target_var)\n        acc = accuracy(score_map, target, idx)\n\n        if debug: # visualize groundtruth and predictions\n            gt_batch_img = batch_with_heatmap(inputs, target)\n            pred_batch_img = batch_with_heatmap(inputs, score_map)\n            if not gt_win or not pred_win:\n                ax1 = plt.subplot(121)\n                ax1.title.set_text(\'Groundtruth\')\n                gt_win = plt.imshow(gt_batch_img)\n                ax2 = plt.subplot(122)\n                ax2.title.set_text(\'Prediction\')\n                pred_win = plt.imshow(pred_batch_img)\n            else:\n                gt_win.set_data(gt_batch_img)\n                pred_win.set_data(pred_batch_img)\n            plt.pause(.05)\n            plt.draw()\n\n        # measure accuracy and record loss\n        losses.update(loss.item(), inputs.size(0))\n        acces.update(acc[0], inputs.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Acc: {acc: .4f}\'.format(\n                    batch=i + 1,\n                    size=len(train_loader),\n                    data=data_time.val,\n                    bt=batch_time.val,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    acc=acces.avg\n                    )\n        bar.next()\n\n    bar.finish()\n    return losses.avg, acces.avg\n\n\ndef validate(val_loader, model, criterion, num_classes, out_res, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acces = AverageMeter()\n\n    # predictions\n    predictions = torch.Tensor(val_loader.dataset.__len__(), num_classes, 2)\n\n    # switch to evaluate mode\n    model.eval()\n\n    gt_win, pred_win = None, None\n    end = time.time()\n    bar = Bar(\'Processing\', max=len(val_loader))\n    for i, (inputs, target, meta) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n\n        input_var = torch.autograd.Variable(inputs.cuda(), volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        score_map = output[-1].data.cpu()\n        if flip:\n            flip_input_var = torch.autograd.Variable(\n                    torch.from_numpy(fliplr(inputs.clone().numpy())).float().cuda(), \n                    volatile=True\n                )\n            flip_output_var = model(flip_input_var)\n            flip_output = flip_back(flip_output_var[-1].data.cpu())\n            score_map += flip_output\n\n\n\n        loss = 0\n        for o in output:\n            loss += criterion(o, target_var)\n        acc = accuracy(score_map, target.cpu(), idx)\n\n        # generate predictions\n        preds = final_preds(score_map, meta[\'center\'], meta[\'scale\'], [out_res, out_res])\n        for n in range(score_map.size(0)):\n            predictions[meta[\'index\'][n], :, :] = preds[n, :, :]\n\n\n        if debug:\n            gt_batch_img = batch_with_heatmap(inputs, target)\n            pred_batch_img = batch_with_heatmap(inputs, score_map)\n            if not gt_win or not pred_win:\n                plt.subplot(121)\n                gt_win = plt.imshow(gt_batch_img)\n                plt.subplot(122)\n                pred_win = plt.imshow(pred_batch_img)\n            else:\n                gt_win.set_data(gt_batch_img)\n                pred_win.set_data(pred_batch_img)\n            plt.pause(.05)\n            plt.draw()\n\n        # measure accuracy and record loss\n        losses.update(loss.item(), inputs.size(0))\n        acces.update(acc[0], inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Acc: {acc: .4f}\'.format(\n                    batch=i + 1,\n                    size=len(val_loader),\n                    data=data_time.val,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    acc=acces.avg\n                    )\n        bar.next()\n\n    bar.finish()\n    return losses.avg, acces.avg, predictions\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                            \' | \'.join(model_names) +\n                            \' (default: resnet18)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'--features\', default=256, type=int, metavar=\'N\',\n                        help=\'Number of features in the hourglass\')\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    parser.add_argument(\'--num-classes\', default=16, type=int, metavar=\'N\',\n                        help=\'Number of keypoints\')\n    parser.add_argument(\'--mobile\', default=False, type=bool, metavar=\'N\',\n                        help=\'use depthwise convolution in bottneck-block\')\n    # Training strategy\n    parser.add_argument(\'-j\', \'--workers\', default=1, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--train-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'train batchsize\')\n    parser.add_argument(\'--test-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'test batchsize\')\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=2.5e-4, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=0, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 0)\')\n    parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\n    parser.add_argument(\'--gamma\', type=float, default=0.1,\n                        help=\'LR is multiplied by gamma on schedule.\')\n    # Data processing\n    parser.add_argument(\'-f\', \'--flip\', dest=\'flip\', action=\'store_true\',\n                        help=\'flip the input during validation\')\n    parser.add_argument(\'--sigma\', type=float, default=1,\n                        help=\'Groundtruth Gaussian sigma.\')\n    parser.add_argument(\'--sigma-decay\', type=float, default=0,\n                        help=\'Sigma decay rate for each epoch.\')\n    parser.add_argument(\'--label-type\', metavar=\'LABELTYPE\', default=\'Gaussian\',\n                        choices=[\'Gaussian\', \'Cauchy\'],\n                        help=\'Labelmap dist type: (default=Gaussian)\')\n    parser.add_argument(\'--in_res\', default=256, type=int,\n                        choices=[256, 192],\n                        help=\'input resolution for network\')\n    # Miscs\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'-d\', \'--debug\', dest=\'debug\', action=\'store_true\',\n                        help=\'show intermediate results\')\n\n    main(parser.parse_args())'"
example/mpii_kd.py,21,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nfrom pose import Bar\nfrom pose.utils.logger import Logger, savefig\nfrom pose.utils.evaluation import accuracy, AverageMeter, final_preds\nfrom pose.utils.misc import save_checkpoint, save_pred, adjust_learning_rate\nfrom pose.utils.osutils import mkdir_p, isfile, isdir, join\nfrom pose.utils.imutils import batch_with_heatmap\nfrom pose.utils.transforms import fliplr, flip_back\nimport pose.models as models\nimport pose.datasets as datasets\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\nidx = [1,2,3,4,5,6,11,12,15,16]\n\nbest_acc = 0\n\ndef load_teacher_network(arch, stacks, blocks, t_checkpoint):\n    tmodel = models.__dict__[arch](num_stacks=stacks, num_blocks=blocks, num_classes=16, mobile=False)\n    tmodel = torch.nn.DataParallel(tmodel).cuda()\n\n    checkpoint = torch.load(t_checkpoint)\n    tmodel.load_state_dict(checkpoint[\'state_dict\'])\n    tmodel.eval()\n    return tmodel\n\n\ndef main(args):\n    global best_acc\n\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # load teacher network\n    print(""==> creating teacher model \'{}\', stacks={}, blocks={}"".format(args.arch, args.teacher_stack, args.blocks))\n    tmodel = load_teacher_network(args.arch, args.teacher_stack, args.blocks, args.teacher_checkpoint)\n\n    # create model\n    print(""==> creating student model \'{}\', stacks={}, blocks={}"".format(args.arch, args.stacks, args.blocks))\n    model = models.__dict__[args.arch](num_stacks=args.stacks, num_blocks=args.blocks, num_classes=args.num_classes, mobile=args.mobile)\n\n    model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = torch.nn.MSELoss(size_average=True).cuda()\n\n    optimizer = torch.optim.RMSprop(model.parameters(), \n                                lr=args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    title = \'mpii-\' + args.arch\n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc = checkpoint[\'best_acc\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:        \n        logger = Logger(join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\', \'Val Loss\', \'Train Acc\', \'Val Acc\'])\n\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    # Data loading code\n    train_loader = torch.utils.data.DataLoader(\n        datasets.Mpii(\'data/mpii/mpii_annotations.json\', \'data/mpii/images\',\n                      sigma=args.sigma, label_type=args.label_type,\n                      unlabeled_folder=args.unlabeled_data,\n                      inp_res=args.in_res, out_res=args.in_res//4),\n        batch_size=args.train_batch, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        datasets.Mpii(\'data/mpii/mpii_annotations.json\', \'data/mpii/images\',\n                      sigma=args.sigma, label_type=args.label_type, train=False,\n                      inp_res=args.in_res, out_res=args.in_res // 4),\n        batch_size=args.test_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if os.path.exists(args.unlabeled_data):\n        print(""==> loading training data from mpii {}, unlabeled data {}, val data {}"".format(\n                train_loader.dataset.__len__(), len(os.listdir(args.unlabeled_data)), val_loader.dataset.__len__()))\n    else:\n        print(""==> loading training data from mpii {}, unlabeled data {}, val data {}"".format(\n                train_loader.dataset.__len__(), 0, val_loader.dataset.__len__()))\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\') \n        loss, acc, predictions = validate(val_loader, model, criterion, args.num_classes, args.in_res//4, args.debug, args.flip)\n        save_pred(predictions, checkpoint=args.checkpoint)\n        return\n\n    lr = args.lr\n    for epoch in range(args.start_epoch, args.epochs):\n        lr = adjust_learning_rate(optimizer, epoch, lr, args.schedule, args.gamma)\n        print(\'\\nEpoch: %d | LR: %.8f\' % (epoch + 1, lr))\n\n        # decay sigma\n        if args.sigma_decay > 0:\n            train_loader.dataset.sigma *=  args.sigma_decay\n            val_loader.dataset.sigma *=  args.sigma_decay\n\n        # train for one epoch\n        train_loss, train_acc = train(train_loader, model, tmodel, criterion, optimizer, args.kdloss_alpha, args.debug, args.flip)\n\n        # evaluate on validation set\n        valid_loss, valid_acc, predictions = validate(val_loader, model, criterion, args.num_classes,\n                                                      args.in_res//4, args.debug, args.flip)\n\n        # append logger file\n        logger.append([epoch + 1, lr, train_loss, valid_loss, train_acc, valid_acc])\n\n        # remember best acc and save checkpoint\n        is_best = valid_acc > best_acc\n        best_acc = max(valid_acc, best_acc)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_acc\': best_acc,\n            \'optimizer\' : optimizer.state_dict(),\n        }, predictions, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n    logger.plot([\'Train Acc\', \'Val Acc\'])\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n\n\n\ndef loss_with_mask(criterion, predict, target, mask):\n    predict_x = predict.permute(1, 2, 3, 0)\n    predict_x = predict_x * mask\n    predict_x = predict_x.permute(3, 0, 1, 2)\n\n    loss = criterion(predict_x, target)\n    return loss\n\n\n\ndef train(train_loader, model, tmodel, criterion, optimizer, kdloss_alpha, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    kdlosses = AverageMeter()\n    unkdlosses = AverageMeter()\n    tslosses = AverageMeter()\n    gtlosses = AverageMeter()\n    acces = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n\n    gt_win, pred_win = None, None\n    bar = Bar(\'Processing\', max=len(train_loader))\n    for i, (inputs, target, meta) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input_var = torch.autograd.Variable(inputs.cuda())\n        target_var = torch.autograd.Variable(target.cuda(async=True))\n\n        # compute output\n        output = model(input_var)\n        score_map = output[-1].data.cpu()\n\n        # compute teacher network output\n        toutput = tmodel(input_var)\n        toutput = toutput[-1].detach()\n\n        # lmse : student vs ground truth\n        # gtmask will filter out the samples without ground truth\n        gtloss = torch.tensor(0.0).cuda()\n        kdloss = torch.tensor(0.0).cuda()\n        kdloss_unlabeled = torch.tensor(0.0).cuda()\n        unkdloss_alpha = 1.0\n        gtmask = meta[\'gtmask\']\n\n        train_batch = score_map.shape[0]\n\n        for j in range(0, len(output)):\n            _output = output[j]\n            for i in range(gtmask.shape[0]):\n                if gtmask[i] < 0.1:\n                    # unlabeled data, gtmask=0.0, kdloss only\n                    # need to dividen train_batch to keep number equal\n                    kdloss_unlabeled += criterion(_output[i,:,:,:], toutput[i, :,:,:])/train_batch\n                else:\n                    # labeled data: kdloss + gtloss\n                    gtloss += criterion(_output[i,:,:,:], target_var[i, :,:,:])/train_batch\n                    kdloss += criterion(_output[i,:,:,:], toutput[i,:,:,:])/train_batch\n\n        loss_labeled = kdloss_alpha * (kdloss) + (1 - kdloss_alpha)*gtloss\n        total_loss   = loss_labeled + unkdloss_alpha * kdloss_unlabeled\n\n        acc = accuracy(score_map, target, idx)\n\n        if debug: # visualize groundtruth and predictions\n            gt_batch_img = batch_with_heatmap(inputs, target)\n            pred_batch_img = batch_with_heatmap(inputs, score_map)\n            teacher_batch_img = batch_with_heatmap(inputs, toutput)\n            if not gt_win or not pred_win or not pred_teacher:\n                ax1 = plt.subplot(131)\n                ax1.title.set_text(\'Groundtruth\')\n                gt_win = plt.imshow(gt_batch_img)\n                ax2 = plt.subplot(132)\n                ax2.title.set_text(\'Prediction\')\n                pred_win = plt.imshow(pred_batch_img)\n                ax2 = plt.subplot(133)\n                ax2.title.set_text(\'teacher\')\n                pred_teacher = plt.imshow(teacher_batch_img)\n            else:\n                gt_win.set_data(gt_batch_img)\n                pred_win.set_data(pred_batch_img)\n                pred_teacher.set_data(teacher_batch_img)\n            plt.pause(.05)\n            plt.draw()\n\n        # measure accuracy and record loss\n        gtlosses.update(gtloss.item(), inputs.size(0))\n        kdlosses.update(kdloss.item(), inputs.size(0))\n        unkdlosses.update(kdloss_unlabeled.item(), inputs.size(0))\n        losses.update(total_loss.item(), inputs.size(0))\n        acces.update(acc[0], inputs.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} \' \\\n                      \'| Loss: {loss:.6f} | KdLoss:{kdloss:.6f}| unKdLoss:{unkdloss:.6f}| GtLoss:{gtloss:.6f} | Acc: {acc: .4f}\'.format(\n                    batch=i + 1,\n                    size=len(train_loader),\n                    data=data_time.val,\n                    bt=batch_time.val,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    kdloss=kdlosses.avg,\n                    unkdloss=unkdlosses.avg,\n                    tsloss=tslosses.avg,\n                    gtloss=gtlosses.avg,\n                    acc=acces.avg\n                    )\n        bar.next()\n\n    bar.finish()\n    return losses.avg, acces.avg\n\n\ndef validate(val_loader, model, criterion, num_classes, out_res, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acces = AverageMeter()\n\n    # predictions\n    predictions = torch.Tensor(val_loader.dataset.__len__(), num_classes, 2)\n\n    # switch to evaluate mode\n    model.eval()\n\n    gt_win, pred_win = None, None\n    end = time.time()\n    bar = Bar(\'Processing\', max=len(val_loader))\n    for i, (inputs, target, meta) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n\n        input_var = torch.autograd.Variable(inputs.cuda(), volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        score_map = output[-1].data.cpu()\n        if flip:\n            flip_input_var = torch.autograd.Variable(\n                    torch.from_numpy(fliplr(inputs.clone().numpy())).float().cuda(), \n                    volatile=True\n                )\n            flip_output_var = model(flip_input_var)\n            flip_output = flip_back(flip_output_var[-1].data.cpu())\n            score_map += flip_output\n\n        loss = 0\n        for o in output:\n            loss += criterion(o, target_var)\n        acc = accuracy(score_map, target.cpu(), idx)\n\n        # generate predictions\n        preds = final_preds(score_map, meta[\'center\'], meta[\'scale\'], [out_res, out_res])\n        for n in range(score_map.size(0)):\n            predictions[meta[\'index\'][n], :, :] = preds[n, :, :]\n\n\n        if debug:\n            gt_batch_img = batch_with_heatmap(inputs, target)\n            pred_batch_img = batch_with_heatmap(inputs, score_map)\n            if not gt_win or not pred_win:\n                plt.subplot(121)\n                gt_win = plt.imshow(gt_batch_img)\n                plt.subplot(122)\n                pred_win = plt.imshow(pred_batch_img)\n            else:\n                gt_win.set_data(gt_batch_img)\n                pred_win.set_data(pred_batch_img)\n            plt.pause(.05)\n            plt.draw()\n\n        # measure accuracy and record loss\n        losses.update(loss.item(), inputs.size(0))\n        acces.update(acc[0], inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Acc: {acc: .4f}\'.format(\n                    batch=i + 1,\n                    size=len(val_loader),\n                    data=data_time.val,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    acc=acces.avg\n                    )\n        bar.next()\n\n    bar.finish()\n    return losses.avg, acces.avg, predictions\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                            \' | \'.join(model_names) +\n                            \' (default: resnet18)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'--features\', default=256, type=int, metavar=\'N\',\n                        help=\'Number of features in the hourglass\')\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    parser.add_argument(\'--num-classes\', default=16, type=int, metavar=\'N\',\n                        help=\'Number of keypoints\')\n    parser.add_argument(\'--mobile\', default=False, type=bool, metavar=\'N\',\n                        help=\'use depthwise convolution in bottneck-block\')\n    # Training strategy\n    parser.add_argument(\'-j\', \'--workers\', default=1, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--train-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'train batchsize\')\n    parser.add_argument(\'--test-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'test batchsize\')\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=2.5e-4, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=0, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 0)\')\n    parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\n    parser.add_argument(\'--gamma\', type=float, default=0.1,\n                        help=\'LR is multiplied by gamma on schedule.\')\n    # Data processing\n    parser.add_argument(\'-f\', \'--flip\', dest=\'flip\', action=\'store_true\',\n                        help=\'flip the input during validation\')\n    parser.add_argument(\'--sigma\', type=float, default=1,\n                        help=\'Groundtruth Gaussian sigma.\')\n    parser.add_argument(\'--sigma-decay\', type=float, default=0,\n                        help=\'Sigma decay rate for each epoch.\')\n    parser.add_argument(\'--label-type\', metavar=\'LABELTYPE\', default=\'Gaussian\',\n                        choices=[\'Gaussian\', \'Cauchy\'],\n                        help=\'Labelmap dist type: (default=Gaussian)\')\n    parser.add_argument(\'--in_res\', default=256, type=int,\n                        choices=[256, 192],\n                        help=\'input resolution for network\')\n    parser.add_argument(\'--teacher_checkpoint\', required=True, type=str,\n                        help=\'teacher network\')\n    parser.add_argument(\'--teacher_stack\', default=8, type=int,\n                        help=\'teacher network stack\')\n    parser.add_argument(\'--kdloss_alpha\', default=0.5, type=float,\n                        help=\'weight of kdloss\')\n    parser.add_argument(\'--unlabeled_data\', default=\'\', type=str,\n                        help=\'unlabeled data address\')\n\n    # Miscs\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'-d\', \'--debug\', dest=\'debug\', action=\'store_true\',\n                        help=\'show intermediate results\')\n\n    main(parser.parse_args())\n'"
pose/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom . import datasets\nfrom . import models\nfrom . import utils\n\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar\n\n__version__ = \'0.1.0\''"
tools/eval_PCKh.py,0,"b'from scipy.io import loadmat\nfrom numpy import transpose\nimport skimage.io as sio\nimport numpy as np\nimport os\nimport argparse\n\ndef main(predfile):\n\n    detection = loadmat(\'data/mpii/detections.mat\')\n    det_idxs = detection[\'RELEASE_img_index\']\n    debug = 0\n    threshold = 0.5\n    SC_BIAS = 0.6\n\n    pa = [2, 3, 7, 7, 4, 5, 8, 9, 10, 0, 12, 13, 8, 8, 14, 15]\n\n    dict = loadmat(\'data/mpii/detections_our_format.mat\')\n    dataset_joints = dict[\'dataset_joints\']\n    jnt_missing = dict[\'jnt_missing\']\n    pos_pred_src = dict[\'pos_pred_src\']\n    pos_gt_src = dict[\'pos_gt_src\']\n    headboxes_src = dict[\'headboxes_src\']\n\n    #predictions\n    preds = loadmat(predfile)[\'preds\']\n    pos_pred_src = transpose(preds, [1, 2, 0])\n\n    head = np.where(dataset_joints == \'head\')[1][0]\n    lsho = np.where(dataset_joints == \'lsho\')[1][0]\n    lelb = np.where(dataset_joints == \'lelb\')[1][0]\n    lwri = np.where(dataset_joints == \'lwri\')[1][0]\n    lhip = np.where(dataset_joints == \'lhip\')[1][0]\n    lkne = np.where(dataset_joints == \'lkne\')[1][0]\n    lank = np.where(dataset_joints == \'lank\')[1][0]\n\n    rsho = np.where(dataset_joints == \'rsho\')[1][0]\n    relb = np.where(dataset_joints == \'relb\')[1][0]\n    rwri = np.where(dataset_joints == \'rwri\')[1][0]\n    rkne = np.where(dataset_joints == \'rkne\')[1][0]\n    rank = np.where(dataset_joints == \'rank\')[1][0]\n    rhip = np.where(dataset_joints == \'rhip\')[1][0]\n\n    jnt_visible = 1 - jnt_missing\n    uv_error = pos_pred_src - pos_gt_src\n    uv_err = np.linalg.norm(uv_error, axis=1)\n    headsizes = headboxes_src[1, :, :] - headboxes_src[0, :, :]\n    headsizes = np.linalg.norm(headsizes, axis=0)\n    headsizes *= SC_BIAS\n    scale = np.multiply(headsizes, np.ones((len(uv_err), 1)))\n    scaled_uv_err = np.divide(uv_err, scale)\n    scaled_uv_err = np.multiply(scaled_uv_err, jnt_visible)\n    jnt_count = np.sum(jnt_visible, axis=1)\n    less_than_threshold = np.multiply((scaled_uv_err < threshold), jnt_visible)\n    PCKh = np.divide(100. * np.sum(less_than_threshold, axis=1), jnt_count)\n\n\n    # save\n    rng = np.arange(0, 0.5, 0.01)\n    pckAll = np.zeros((len(rng), 16))\n\n    for r in range(len(rng)):\n        threshold = rng[r]\n        less_than_threshold = np.multiply(scaled_uv_err < threshold, jnt_visible)\n        pckAll[r, :] = np.divide(100.*np.sum(less_than_threshold, axis=1), jnt_count)\n\n    name = predfile.split(os.sep)[-1]\n    PCKh = np.ma.array(PCKh, mask=False)\n    PCKh.mask[6:8] = True\n    print(""Model,  Head,   Shoulder, Elbow,  Wrist,   Hip ,     Knee  , Ankle ,  Mean"")\n    print(\'{:s}   {:.2f}  {:.2f}     {:.2f}  {:.2f}   {:.2f}   {:.2f}   {:.2f}   {:.2f}\'.format(\'hg\', PCKh[head], 0.5 * (PCKh[lsho] + PCKh[rsho])\\\n            , 0.5 * (PCKh[lelb] + PCKh[relb]),0.5 * (PCKh[lwri] + PCKh[rwri]), 0.5 * (PCKh[lhip] + PCKh[rhip]), 0.5 * (PCKh[lkne] + PCKh[rkne]) \\\n            , 0.5 * (PCKh[lank] + PCKh[rank]), np.mean(PCKh)))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Evaluation\')\n    parser.add_argument(\'--matfile\', type=str, required=True, help=\'mat file of prediction\')\n    args = parser.parse_args()\n\n    main(args.matfile)'"
tools/mpii_demo.py,4,"b'\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nfrom pose.utils.osutils import mkdir_p, isfile, isdir, join\nimport pose.models as models\nfrom scipy.ndimage import gaussian_filter, maximum_filter\nimport cv2\nimport numpy as np\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(models.__dict__[name]))\n\ndef load_image(imgfile, w, h ):\n    image = cv2.imread(imgfile)\n    image = cv2.resize(image, (w, h))\n    image = image[:, :, ::-1]  # BGR -> RGB\n    image = image / 255.0\n    image = image - np.array([[[0.4404, 0.4440, 0.4327]]])  # Extract mean RGB\n    image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n    image = image[np.newaxis, :, :, :]\n    return image\n\ndef load_model(arch, stacks, blocks, num_classes, mobile, checkpoint_resume):\n    # create model\n    model = models.__dict__[arch](num_stacks=stacks, num_blocks=blocks, num_classes=num_classes, mobile=mobile)\n\n    # optionally resume from a checkpoint\n    if isfile(checkpoint_resume):\n        print(""=> loading checkpoint \'{}\'"".format(checkpoint_resume))\n        checkpoint =  torch.load(checkpoint_resume, map_location=lambda storage, loc: storage)\n        # create new OrderedDict that does not contain `module.`\n        from collections import OrderedDict\n        new_state_dict = OrderedDict()\n        for k, v in checkpoint[\'state_dict\'].items():\n            name = k[7:]  # remove `module.`\n            new_state_dict[name] = v\n        # load params\n        model.load_state_dict(new_state_dict)\n        print(""=> loaded checkpoint \'{}\' (epoch {})"".format(checkpoint_resume, checkpoint[\'epoch\']))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(checkpoint_resume))\n\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n    model.eval()\n    return model\n\ndef inference(model, image, device):\n    input_tensor = torch.from_numpy(image).float().to(device)\n    model = model.to(device)\n    output = model(input_tensor)\n    output = output[-1]\n    output = output.data.cpu()\n    kps = post_process_heatmap(output[0,:,:,:])\n    return kps\n\n\ndef post_process_heatmap(heatMap, kpConfidenceTh=0.2):\n    kplst = list()\n    for i in range(heatMap.shape[0]):\n        _map = heatMap[i, :, :]\n        _map = gaussian_filter(_map, sigma=1)\n        _nmsPeaks = non_max_supression(_map, windowSize=3, threshold=1e-6)\n\n        y, x = np.where(_nmsPeaks == _nmsPeaks.max())\n        if len(x) > 0 and len(y) > 0:\n            kplst.append((int(x[0]), int(y[0]), _nmsPeaks[y[0], x[0]]))\n        else:\n            kplst.append((0, 0, 0))\n\n    kp = np.array(kplst)\n    return kp\n\n\ndef non_max_supression(plain, windowSize=3, threshold=1e-6):\n    # clear value less than threshold\n    under_th_indices = plain < threshold\n    plain[under_th_indices] = 0\n    return plain * (plain == maximum_filter(plain, footprint=np.ones((windowSize, windowSize))))\n\ndef render_kps(cvmat, kps, scale_x, scale_y):\n    for _kp in kps:\n        _x, _y, _conf = _kp\n        if _conf > 0.2:\n            cv2.circle(cvmat, center=(int(_x*4*scale_x), int(_y*4*scale_y)), color=(0,0,255), radius=5)\n\n    return cvmat\n\n\ndef main(args):\n    # load checkpoint\n    model = load_model(args.arch, args.stacks, args.blocks, args.num_classes, args.mobile, args.checkpoint)\n    in_res_h , in_res_w = args.in_res, args.in_res\n\n    # load image from file and do preprocess\n    image = load_image(args.image, in_res_w, in_res_h)\n\n    # do inference\n    kps = inference(model, image, args.device)\n\n    # render the detected keypoints\n    cvmat = cv2.imread(args.image)\n    scale_x = cvmat.shape[1]*1.0/in_res_w\n    scale_y = cvmat.shape[0]*1.0/in_res_h\n    render_kps(cvmat, kps, scale_x, scale_y)\n\n    cv2.imshow(\'x\', cvmat)\n    cv2.waitKey(0)\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                             \' | \'.join(model_names) +\n                             \' (default: resnet18)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    parser.add_argument(\'--num-classes\', default=16, type=int, metavar=\'N\',\n                        help=\'Number of keypoints\')\n    parser.add_argument(\'--mobile\', default=False, type=bool, metavar=\'N\',\n                        help=\'use depthwise convolution in bottneck-block\')\n    parser.add_argument(\'--checkpoint\', required=True, type=str, metavar=\'N\',\n                        help=\'pre-trained model checkpoint\')\n    parser.add_argument(\'--in_res\', required=True, type=int, metavar=\'N\',\n                        help=\'input shape 128 or 256\')\n    parser.add_argument(\'--image\', default=\'data/sample.jpg\', type=str, metavar=\'N\',\n                        help=\'input image\')\n    parser.add_argument(\'--device\', default=\'cuda\', type=str, metavar=\'N\',\n                        help=\'device\')\n    main(parser.parse_args())\n'"
tools/mpii_export_to_onxx.py,5,"b'from __future__ import print_function, absolute_import\n\nimport argparse\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport pose.models as models\nimport os\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(models.__dict__[name]))\n\ndef main(args):\n\n\n    # create model\n    print(""==> creating model \'{}\', stacks={}, blocks={}"".format(args.arch, args.stacks, args.blocks))\n    model = models.__dict__[args.arch](num_stacks=args.stacks, num_blocks=args.blocks, num_classes=args.num_classes,\n                                       mobile=args.mobile)\n    model.eval()\n\n    # optionally resume from a checkpoint\n    title = \'mpii-\' + args.arch\n    if args.checkpoint:\n        if os.path.isfile(args.checkpoint):\n            print(""=> loading checkpoint \'{}\'"".format(args.checkpoint))\n            checkpoint = torch.load(args.checkpoint, map_location=lambda storage, loc: storage)\n            args.start_epoch = checkpoint[\'epoch\']\n\n            # create new OrderedDict that does not contain `module.`\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[\'state_dict\'].items():\n                name = k[7:]  # remove `module.`\n                new_state_dict[name] = v\n            # load params\n            model.load_state_dict(new_state_dict)\n\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.checkpoint, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.checkpoint))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.checkpoint))\n\n    dummy_input = torch.randn(1, 3, args.in_res, args.in_res)\n    torch.onnx.export(model, dummy_input, args.out_onnx)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                             \' | \'.join(model_names) +\n                             \' (default: resnet18)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    parser.add_argument(\'--num-classes\', default=16, type=int, metavar=\'N\',\n                        help=\'Number of keypoints\')\n    parser.add_argument(\'--mobile\', default=False, type=bool, metavar=\'N\',\n                        help=\'use depthwise convolution in bottneck-block\')\n    parser.add_argument(\'--out_onnx\', required=True, type=str, metavar=\'N\',\n                        help=\'exported onnx file\')\n    parser.add_argument(\'--checkpoint\', required=True, type=str, metavar=\'N\',\n                        help=\'pre-trained model checkpoint\')\n    parser.add_argument(\'--in_res\', required=True, type=int, metavar=\'N\',\n                        help=\'input shape 128 or 256\')\n    main(parser.parse_args())\n'"
pose/datasets/__init__.py,0,"b""from .mpii import Mpii\nfrom .mscoco import Mscoco\nfrom .lsp import LSP\n\n__all__ = ('Mpii', 'Mscoco', 'LSP')"""
pose/datasets/lsp.py,12,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass LSP(data.Dataset):\n    """"""\n    LSP extended dataset (11,000 train, 1000 test)\n    Original datasets contain 14 keypoints. We interpolate mid-hip and mid-shoulder and change the indices to match\n    the MPII dataset (16 keypoints).\n\n    Wei Yang (bearpaw@GitHub)\n    2017-09-28\n    """"""\n    def __init__(self, jsonfile, img_folder, inp_res=256, out_res=64, train=True, sigma=1,\n                 scale_factor=0.25, rot_factor=30, label_type=\'Gaussian\'):\n        self.img_folder = img_folder    # root image folders\n        self.is_train = train           # training set or test set\n        self.inp_res = inp_res\n        self.out_res = out_res\n        self.sigma = sigma\n        self.scale_factor = scale_factor\n        self.rot_factor = rot_factor\n        self.label_type = label_type\n\n        # create train/val split\n        with open(jsonfile) as anno_file:   \n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val[\'isValidation\'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = \'./data/lsp/mean.pth.tar\'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            for index in self.train:\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                \'mean\': mean,\n                \'std\': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print(\'    Mean: %.4f, %.4f, %.4f\' % (meanstd[\'mean\'][0], meanstd[\'mean\'][1], meanstd[\'mean\'][2]))\n            print(\'    Std:  %.4f, %.4f, %.4f\' % (meanstd[\'std\'][0], meanstd[\'std\'][1], meanstd[\'std\'][2]))\n            \n        return meanstd[\'mean\'], meanstd[\'std\']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n        pts = torch.Tensor(a[\'joint_self\'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a[\'objpos\']) - 1\n        c = torch.Tensor(a[\'objpos\'])\n        s = a[\'scale_provided\']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            # c[1] = c[1] + 15 * s\n            s = s * 1.4375\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        # if self.is_train:\n        #     s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n        #     r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n        #\n        #     # # Flip\n        #     # if random.random() <= 0.5:\n        #     #     img = torch.from_numpy(fliplr(img.numpy())).float()\n        #     #     pts = shufflelr(pts, width=img.size(2), dataset=\'mpii\')\n        #     #     c[0] = img.size(2) - c[0]\n        #\n        #     # Color\n        #     img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        #     img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        #     img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        for i in range(nparts):\n            # if tpts[i, 2] > 0: # This is evil!!\n            if tpts[i, 0] > 0:\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i] = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n\n        # Meta info\n        meta = {\'index\' : index, \'center\' : c, \'scale\' : s, \n        \'pts\' : pts, \'tpts\' : tpts}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)'"
pose/datasets/mpii.py,17,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass Mpii(data.Dataset):\n    def __init__(self, jsonfile, img_folder, inp_res=256, out_res=64, train=True, sigma=1,\n                 scale_factor=0.25, rot_factor=30, label_type=\'Gaussian\', unlabeled_folder=\'\'):\n        self.img_folder = img_folder    # root image folders\n        self.is_train = train           # training set or test set\n        self.inp_res = inp_res\n        self.out_res = out_res\n        self.sigma = sigma\n        self.scale_factor = scale_factor\n        self.rot_factor = rot_factor\n        self.label_type = label_type\n        self.unlabeled_folder = unlabeled_folder\n\n        # create train/val split\n        with open(jsonfile) as anno_file:   \n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val[\'isValidation\'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n\n        if self.is_train and os.path.exists(self.unlabeled_folder):\n            self._load_unlabeled_anno()\n        self.mean, self.std = self._compute_mean()\n\n    def _load_unlabeled_anno(self):\n        def create_anno_unlabeled(imgfile):\n            anno = dict()\n            anno[\'img_paths\'] = imgfile\n            anno[\'dataset\'] = \'unlabeled\'\n            return anno\n\n        assert (self.train), \'only supported for training dataset\'\n        train_size = len(self.anno)\n        for i, mfile in enumerate(os.listdir(self.unlabeled_folder)):\n            self.anno.append(create_anno_unlabeled(mfile))\n            self.train.append(i+train_size)\n\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = \'./data/mpii/mean.pth.tar\'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            for index in self.train:\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                \'mean\': mean,\n                \'std\': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print(\'    Mean: %.4f, %.4f, %.4f\' % (meanstd[\'mean\'][0], meanstd[\'mean\'][1], meanstd[\'mean\'][2]))\n            print(\'    Std:  %.4f, %.4f, %.4f\' % (meanstd[\'std\'][0], meanstd[\'std\'][1], meanstd[\'std\'][2]))\n            \n        return meanstd[\'mean\'], meanstd[\'std\']\n\n    def __getitem__(self, index):\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        if a[\'dataset\'] == \'MPI\':\n            return self.__getitem_mpi__(index)\n        else:\n            return self.__getitem_unlabeled__(index)\n\n\n    def __getitem_unlabeled__(self, index):\n\n        assert self.is_train, ""unlabled data only used in train mode""\n\n        a = self.anno[self.train[index]]\n\n        img_path = os.path.join(self.unlabeled_folder, a[\'img_paths\'])\n        img = load_image(img_path)  # CxHxW\n\n        # Flip\n        if random.random() <= 0.5:\n            img = torch.from_numpy(fliplr(img.numpy())).float()\n\n        # Color\n        img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = im_to_torch(scipy.misc.imresize(img, [self.inp_res, self.inp_res]))\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # target\n        target = torch.zeros(16, self.out_res, self.out_res)\n\n        # meta\n        pts = torch.Tensor(16,3)\n        tpts = pts.clone()\n        meta = {\'index\' : index, \'gtmask\': torch.tensor(0.0), \'center\' : torch.Tensor(2), \'scale\' : torch.tensor(1.0), \'pts\' : pts, \'tpts\' : tpts}\n\n        return inp, target, meta\n\n    def __getitem_mpi__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n        pts = torch.Tensor(a[\'joint_self\'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a[\'objpos\']) - 1\n        c = torch.Tensor(a[\'objpos\'])\n        s = a[\'scale_provided\']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            c[1] = c[1] + 15 * s\n            s = s * 1.25\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        if self.is_train:\n            s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n            r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n\n            # Flip\n            if random.random() <= 0.5:\n                img = torch.from_numpy(fliplr(img.numpy())).float()\n                pts = shufflelr(pts, width=img.size(2), dataset=\'mpii\')\n                c[0] = img.size(2) - c[0]\n\n            # Color\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        for i in range(nparts):\n            # if tpts[i, 2] > 0: # This is evil!!\n            if tpts[i, 1] > 0:\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i] = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n\n        # Meta info\n        meta = {\'index\' : index, \'gtmask\': torch.tensor(1.0), \'center\' : c, \'scale\' : s, \'pts\' : pts, \'tpts\' : tpts}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)\n'"
pose/datasets/mscoco.py,12,"b""from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass Mscoco(data.Dataset):\n    def __init__(self, jsonfile, img_folder, inp_res=256, out_res=64, train=True, sigma=1,\n                 scale_factor=0.25, rot_factor=30, label_type='Gaussian'):\n        self.img_folder = img_folder    # root image folders\n        self.is_train = train           # training set or test set\n        self.inp_res = inp_res\n        self.out_res = out_res\n        self.sigma = sigma\n        self.scale_factor = scale_factor\n        self.rot_factor = rot_factor\n        self.label_type = label_type\n\n        # create train/val split\n        with open(jsonfile) as anno_file:   \n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val['isValidation'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = './data/mscoco/mean.pth.tar'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            print('==> compute mean')\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            cnt = 0\n            for index in self.train:\n                cnt += 1\n                print( '{} | {}'.format(cnt, len(self.train)))\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a['img_paths'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                'mean': mean,\n                'std': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print('    Mean: %.4f, %.4f, %.4f' % (meanstd['mean'][0], meanstd['mean'][1], meanstd['mean'][2]))\n            print('    Std:  %.4f, %.4f, %.4f' % (meanstd['std'][0], meanstd['std'][1], meanstd['std'][2]))\n            \n        return meanstd['mean'], meanstd['std']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a['img_paths'])\n        pts = torch.Tensor(a['joint_self'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a['objpos']) - 1\n        c = torch.Tensor(a['objpos'])\n        s = a['scale_provided']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            c[1] = c[1] + 15 * s\n            s = s * 1.25\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        if self.is_train:\n            s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n            r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n\n            # Flip\n            if random.random() <= 0.5:\n                img = torch.from_numpy(fliplr(img.numpy())).float()\n                pts = shufflelr(pts, width=img.size(2), dataset='mpii')\n                c[0] = img.size(2) - c[0]\n\n            # Color\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        for i in range(nparts):\n            if tpts[i, 2] > 0: # COCO visible: 0-no label, 1-label + invisible, 2-label + visible\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i] = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n\n        # Meta info\n        meta = {'index' : index, 'center' : c, 'scale' : s, \n        'pts' : pts, 'tpts' : tpts}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)"""
pose/models/__init__.py,0,b'from .hourglass import *\nfrom .preresnet import *'
pose/models/hourglass.py,2,"b""'''\nHourglass network inserted in the pre-activated Resnet \nUse lr=0.01 for current version\n(c) YANG, Wei \n'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# from .preresnet import BasicBlock, Bottleneck\n\n\n__all__ = ['HourglassNet', 'hg']\n\nclass Bottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, mobile=False):\n        super(Bottleneck, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        if mobile:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                   padding=1, bias=True, groups=planes)\n        else:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                   padding=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Hourglass(nn.Module):\n    def __init__(self, block, num_blocks, planes, depth, mobile):\n        super(Hourglass, self).__init__()\n        self.mobile = mobile\n        self.depth = depth\n        self.block = block\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n\n    def _make_residual(self, block, num_blocks, planes):\n        layers = []\n        for i in range(0, num_blocks):\n            layers.append(block(planes*block.expansion, planes, mobile=self.mobile))\n        return nn.Sequential(*layers)\n\n    def _make_hour_glass(self, block, num_blocks, planes, depth):\n        hg = []\n        for i in range(depth):\n            res = []\n            for j in range(3):\n                res.append(self._make_residual(block, num_blocks, planes))\n            if i == 0:\n                res.append(self._make_residual(block, num_blocks, planes))\n            hg.append(nn.ModuleList(res))\n        return nn.ModuleList(hg)\n\n    def _hour_glass_forward(self, n, x):\n        up1 = self.hg[n-1][0](x)\n        low1 = F.max_pool2d(x, 2, stride=2)\n        low1 = self.hg[n-1][1](low1)\n\n        if n > 1:\n            low2 = self._hour_glass_forward(n-1, low1)\n        else:\n            low2 = self.hg[n-1][3](low1)\n        low3 = self.hg[n-1][2](low2)\n        up2 = self.upsample(low3)\n        out = up1 + up2\n        return out\n\n    def forward(self, x):\n        return self._hour_glass_forward(self.depth, x)\n\n\nclass HourglassNet(nn.Module):\n    '''Hourglass model from Newell et al ECCV 2016'''\n    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16, mobile=False):\n        super(HourglassNet, self).__init__()\n\n        self.mobile = mobile\n        self.inplanes = 64\n        self.num_feats = 128\n        self.num_stacks = num_stacks\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=True)\n        self.bn1 = nn.BatchNorm2d(self.inplanes) \n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_residual(block, self.inplanes, 1)\n        self.layer2 = self._make_residual(block, self.inplanes, 1)\n        self.layer3 = self._make_residual(block, self.num_feats, 1)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n\n        # build hourglass modules\n        ch = self.num_feats*block.expansion\n        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\n        for i in range(num_stacks):\n            hg.append(Hourglass(block, num_blocks, self.num_feats, 4, self.mobile))\n            res.append(self._make_residual(block, self.num_feats, num_blocks))\n            fc.append(self._make_fc(ch, ch))\n            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\n            if i < num_stacks-1:\n                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\n        self.hg = nn.ModuleList(hg)\n        self.res = nn.ModuleList(res)\n        self.fc = nn.ModuleList(fc)\n        self.score = nn.ModuleList(score)\n        self.fc_ = nn.ModuleList(fc_) \n        self.score_ = nn.ModuleList(score_)\n\n    def _make_residual(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.mobile))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, mobile=self.mobile))\n\n        return nn.Sequential(*layers)\n\n    def _make_fc(self, inplanes, outplanes):\n        bn = nn.BatchNorm2d(inplanes)\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n        return nn.Sequential(\n                conv,\n                bn,\n                self.relu,\n            )\n\n    def forward(self, x):\n        out = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x) \n\n        x = self.layer1(x)  \n        x = self.maxpool(x)\n        x = self.layer2(x)  \n        x = self.layer3(x)  \n\n        for i in range(self.num_stacks):\n            y = self.hg[i](x)\n            y = self.res[i](y)\n            y = self.fc[i](y)\n            score = self.score[i](y)\n            out.append(score)\n            if i < self.num_stacks-1:\n                fc_ = self.fc_[i](y)\n                score_ = self.score_[i](score)\n                x = x + fc_ + score_\n\n        return out\n\n\ndef hg(**kwargs):\n    model = HourglassNet(Bottleneck, num_stacks=kwargs['num_stacks'], num_blocks=kwargs['num_blocks'],\n                         num_classes=kwargs['num_classes'], mobile=kwargs['mobile'])\n    return model\n"""
pose/models/preresnet.py,2,"b'\'\'\'Pre-activated Resnet for cifar dataset. \nPorted form https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua\n(c) YANG, Wei \n\'\'\'\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'PreResNet\', \'preresnet20\', \'preresnet32\', \'preresnet44\', \'preresnet56\',\n           \'preresnet110\', \'preresnet1202\']\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 16\n        super(PreResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.layer1 = self._make_layer(block, 16, layers[0])\n        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.bn1 = nn.BatchNorm2d(64*block.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Conv2d(64*block.expansion, 64*block.expansion, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64*block.expansion)\n        self.fc2 = nn.Conv2d(64*block.expansion, num_classes, kernel_size=1)\n        # self.avgpool = nn.AvgPool2d(8)\n        # self.fc = nn.Linear(64*block.expansion, num_classes)\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                # nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.fc1(self.relu(self.bn1(x)))\n        x = self.fc2(self.relu(self.bn2(x)))\n        # x = self.sigmoid(x)\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n\n        return [x]\n\n\ndef preresnet20(**kwargs):\n    """"""Constructs a PreResNet-20 model.\n    """"""\n    model = PreResNet(BasicBlock, [3, 3, 3], **kwargs)\n    return model\n\n\ndef preresnet32(**kwargs):\n    """"""Constructs a PreResNet-32 model.\n    """"""\n    model = PreResNet(BasicBlock, [5, 5, 5], **kwargs)\n    return model\n\n\ndef preresnet44(**kwargs):\n    """"""Constructs a PreResNet-44 model.\n    """"""\n    model = PreResNet(Bottleneck, [7, 7, 7], **kwargs)\n    return model\n\n\ndef preresnet56(**kwargs):\n    """"""Constructs a PreResNet-56 model.\n    """"""\n    model = PreResNet(Bottleneck, [9, 9, 9], **kwargs)\n    return model\n\n\ndef preresnet110(**kwargs):\n    """"""Constructs a PreResNet-110 model.\n    """"""\n    model = PreResNet(Bottleneck, [18, 18, 18], **kwargs)\n    return model\n\ndef preresnet1202(**kwargs):\n    """"""Constructs a PreResNet-1202 model.\n    """"""\n    model = PreResNet(Bottleneck, [200, 200, 200], **kwargs)\n    return model'"
pose/utils/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .evaluation import *\nfrom .imutils import *\nfrom .logger import *\nfrom .misc import *\nfrom .osutils import *\nfrom .transforms import *\n'
pose/utils/evaluation.py,8,"b'from __future__ import absolute_import\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\nfrom .misc import *\nfrom .transforms import transform, transform_preds\n\n__all__ = [\'accuracy\', \'AverageMeter\']\n\ndef get_preds(scores):\n    \'\'\' get predictions from score maps in torch Tensor\n        return type: torch.LongTensor\n    \'\'\'\n    assert scores.dim() == 4, \'Score maps should be 4-dim\'\n    maxval, idx = torch.max(scores.view(scores.size(0), scores.size(1), -1), 2)\n\n    maxval = maxval.view(scores.size(0), scores.size(1), 1)\n    idx = idx.view(scores.size(0), scores.size(1), 1) + 1\n\n    preds = idx.repeat(1, 1, 2).float()\n\n    preds[:,:,0] = (preds[:,:,0] - 1) % scores.size(3) + 1\n    preds[:,:,1] = torch.floor((preds[:,:,1] - 1) / scores.size(3)) + 1\n\n    pred_mask = maxval.gt(0).repeat(1, 1, 2).float()\n    preds *= pred_mask\n    return preds\n\ndef calc_dists(preds, target, normalize):\n    preds = preds.float()\n    target = target.float()\n    dists = torch.zeros(preds.size(1), preds.size(0))\n    for n in range(preds.size(0)):\n        for c in range(preds.size(1)):\n            if target[n,c,0] > 1 and target[n, c, 1] > 1:\n                dists[c, n] = torch.dist(preds[n,c,:], target[n,c,:])/normalize[n]\n            else:\n                dists[c, n] = -1\n    return dists\n\ndef dist_acc(dists, thr=0.5):\n    \'\'\' Return percentage below threshold while ignoring values with a -1 \'\'\'\n    if dists.ne(-1).sum() > 0:\n        return float(dists.le(thr).eq(dists.ne(-1)).sum()) / float(dists.ne(-1).sum())\n    else:\n        return -1\n\ndef accuracy(output, target, idxs, thr=0.5):\n    \'\'\' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n        First value to be returned is average accuracy across \'idxs\', followed by individual accuracies\n    \'\'\'\n    preds   = get_preds(output)\n    gts     = get_preds(target)\n    norm    = torch.ones(preds.size(0))*output.size(3)/10\n    dists   = calc_dists(preds, gts, norm)\n\n    acc = torch.zeros(len(idxs)+1)\n    avg_acc = 0\n    cnt = 0\n\n    for i in range(len(idxs)):\n        acc[i+1] = dist_acc(dists[idxs[i]-1])\n        if acc[i+1] >= 0: \n            avg_acc = avg_acc + acc[i+1]\n            cnt += 1\n            \n    if cnt != 0:  \n        acc[0] = avg_acc / cnt\n    return acc\n\ndef final_preds(output, center, scale, res):\n    coords = get_preds(output) # float type\n\n    # pose-processing\n    for n in range(coords.size(0)):\n        for p in range(coords.size(1)):\n            hm = output[n][p]\n            px = int(math.floor(coords[n][p][0]))\n            py = int(math.floor(coords[n][p][1]))\n            if px > 1 and px < res[0] and py > 1 and py < res[1]:\n                diff = torch.Tensor([hm[py - 1][px] - hm[py - 1][px - 2], hm[py][px - 1]-hm[py - 2][px - 1]])\n                coords[n][p] += diff.sign() * .25\n    coords += 0.5\n    preds = coords.clone()\n\n    # Transform back\n    for i in range(coords.size(0)):\n        preds[i] = transform_preds(coords[i], center[i], scale[i], res)\n\n    if preds.dim() < 3:\n        preds = preds.view(1, preds.size())\n\n    return preds\n\n    \nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
pose/utils/imutils.py,3,"b'from __future__ import absolute_import\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport scipy.misc\n\nfrom .misc import *\n\ndef im_to_numpy(img):\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0)) # H*W*C\n    return img\n\ndef im_to_torch(img):\n    img = np.transpose(img, (2, 0, 1)) # C*H*W\n    img = to_torch(img).float()\n    if img.max() > 1:\n        img /= 255\n    return img\n\ndef load_image(img_path):\n    # H x W x C => C x H x W\n    return im_to_torch(scipy.misc.imread(img_path, mode=\'RGB\'))\n\ndef resize(img, owidth, oheight):\n    img = im_to_numpy(img)\n    print(\'%f %f\' % (img.min(), img.max()))\n    img = scipy.misc.imresize(\n            img,\n            (oheight, owidth)\n        )\n    img = im_to_torch(img)\n    print(\'%f %f\' % (img.min(), img.max()))\n    return img\n\n# =============================================================================\n# Helpful functions generating groundtruth labelmap \n# =============================================================================\n\ndef gaussian(shape=(7,7),sigma=1):\n    """"""\n    2D gaussian mask - should give the same result as MATLAB\'s\n    fspecial(\'gaussian\',[shape],[sigma])\n    """"""\n    m,n = [(ss-1.)/2. for ss in shape]\n    y,x = np.ogrid[-m:m+1,-n:n+1]\n    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n    return to_torch(h).float()\n\ndef draw_labelmap(img, pt, sigma, type=\'Gaussian\'):\n    # Draw a 2D gaussian \n    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n    img = to_numpy(img)\n\n    # Check that any part of the gaussian is in-bounds\n    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n            br[0] < 0 or br[1] < 0):\n        # If not, just return the image as is\n        return to_torch(img)\n\n    # Generate gaussian\n    size = 6 * sigma + 1\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    x0 = y0 = size // 2\n    # The gaussian is not normalized, we want the center value to equal 1\n    if type == \'Gaussian\':\n        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n    elif type == \'Cauchy\':\n        g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)\n\n\n    # Usable gaussian range\n    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Image range\n    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n\n    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n    return to_torch(img)\n\n# =============================================================================\n# Helpful display functions\n# =============================================================================\n\ndef gauss(x, a, b, c, d=0):\n    return a * np.exp(-(x - b)**2 / (2 * c**2)) + d\n\ndef color_heatmap(x):\n    x = to_numpy(x)\n    color = np.zeros((x.shape[0],x.shape[1],3))\n    color[:,:,0] = gauss(x, .5, .6, .2) + gauss(x, 1, .8, .3)\n    color[:,:,1] = gauss(x, 1, .5, .3)\n    color[:,:,2] = gauss(x, 1, .2, .3)\n    color[color > 1] = 1\n    color = (color * 255).astype(np.uint8)\n    return color\n\ndef imshow(img):\n    npimg = im_to_numpy(img*255).astype(np.uint8)\n    plt.imshow(npimg)\n    plt.axis(\'off\')\n\ndef show_joints(img, pts):\n    imshow(img)\n    \n    for i in range(pts.size(0)):\n        if pts[i, 2] > 0:\n            plt.plot(pts[i, 0], pts[i, 1], \'yo\')\n    plt.axis(\'off\')\n\ndef show_sample(inputs, target):\n    num_sample = inputs.size(0)\n    num_joints = target.size(1)\n    height = target.size(2)\n    width = target.size(3)\n\n    for n in range(num_sample):\n        inp = resize(inputs[n], width, height)\n        out = inp\n        for p in range(num_joints):\n            tgt = inp*0.5 + color_heatmap(target[n,p,:,:])*0.5\n            out = torch.cat((out, tgt), 2)\n        \n        imshow(out)\n        plt.show()\n\ndef sample_with_heatmap(inp, out, num_rows=2, parts_to_show=None):\n    inp = to_numpy(inp * 255)\n    out = to_numpy(out)\n\n    img = np.zeros((inp.shape[1], inp.shape[2], inp.shape[0]))\n    for i in range(3):\n        img[:, :, i] = inp[i, :, :]\n\n    if parts_to_show is None:\n        parts_to_show = np.arange(out.shape[0])\n\n    # Generate a single image to display input/output pair\n    num_cols = int(np.ceil(float(len(parts_to_show)) / num_rows))\n    size = img.shape[0] // num_rows\n\n    full_img = np.zeros((img.shape[0], size * (num_cols + num_rows), 3), np.uint8)\n    full_img[:img.shape[0], :img.shape[1]] = img\n\n    inp_small = scipy.misc.imresize(img, [size, size])\n\n    # Set up heatmap display for each part\n    for i, part in enumerate(parts_to_show):\n        part_idx = part\n        out_resized = scipy.misc.imresize(out[part_idx], [size, size])\n        out_resized = out_resized.astype(float)/255\n        out_img = inp_small.copy() * .3\n        color_hm = color_heatmap(out_resized)\n        out_img += color_hm * .7\n\n        col_offset = (i % num_cols + num_rows) * size\n        row_offset = (i // num_cols) * size\n        full_img[row_offset:row_offset + size, col_offset:col_offset + size] = out_img\n\n    return full_img\n\ndef batch_with_heatmap(inputs, outputs, mean=torch.Tensor([0.5, 0.5, 0.5]), num_rows=2, parts_to_show=None):\n    batch_img = []\n    for n in range(min(inputs.size(0), 4)):\n        inp = inputs[n] + mean.view(3, 1, 1).expand_as(inputs[n])\n        batch_img.append(\n            sample_with_heatmap(inp.clamp(0, 1), outputs[n], num_rows=num_rows, parts_to_show=parts_to_show)\n        )\n    return np.concatenate(batch_img)\n'"
pose/utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\n\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
pose/utils/misc.py,4,"b'from __future__ import absolute_import\n\nimport os\nimport shutil\nimport torch \nimport math\nimport numpy as np\nimport scipy.io\nimport matplotlib.pyplot as plt\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef save_checkpoint(state, preds, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\', snapshot=None):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    scipy.io.savemat(os.path.join(checkpoint, \'preds.mat\'), mdict={\'preds\' : preds})\n\n    if snapshot and state.epoch % snapshot == 0:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'checkpoint_{}.pth.tar\'.format(state.epoch)))\n\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n        scipy.io.savemat(os.path.join(checkpoint, \'preds_best.mat\'), mdict={\'preds\' : preds})\n\n\ndef save_pred(preds, checkpoint=\'checkpoint\', filename=\'preds_valid.mat\'):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    scipy.io.savemat(filepath, mdict={\'preds\' : preds})\n\n\ndef adjust_learning_rate(optimizer, epoch, lr, schedule, gamma):\n    """"""Sets the learning rate to the initial LR decayed by schedule""""""\n    if epoch in schedule:\n        lr *= gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n    return lr'"
pose/utils/osutils.py,0,"b'from __future__ import absolute_import\n\nimport os\nimport errno\n\ndef mkdir_p(dir_path):\n    try:\n        os.makedirs(dir_path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\ndef isfile(fname):\n    return os.path.isfile(fname) \n\ndef isdir(dirname):\n    return os.path.isdir(dirname)\n\ndef join(path, *paths):\n    return os.path.join(path, *paths)\n'"
pose/utils/transforms.py,3,"b'from __future__ import absolute_import\n\nimport os\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom .misc import *\nfrom .imutils import *\n\n\ndef color_normalize(x, mean, std):\n    if x.size(0) == 1:\n        x = x.repeat(3, 1, 1)\n\n    for t, m, s in zip(x, mean, std):\n        t.sub_(m)\n    return x\n\n\ndef flip_back(flip_output, dataset=\'mpii\'):\n    """"""\n    flip output map\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # flip output horizontally\n    flip_output = fliplr(flip_output.numpy())\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = np.copy(flip_output[:, pair[0], :, :])\n        flip_output[:, pair[0], :, :] = flip_output[:, pair[1], :, :]\n        flip_output[:, pair[1], :, :] = tmp\n\n    return torch.from_numpy(flip_output).float()\n\n\ndef shufflelr(x, width, dataset=\'mpii\'):\n    """"""\n    flip coords\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # Flip horizontal\n    x[:, 0] = width - x[:, 0]\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = x[pair[0], :].clone()\n        x[pair[0], :] = x[pair[1], :]\n        x[pair[1], :] = tmp\n\n    return x\n\n\ndef fliplr(x):\n    if x.ndim == 3:\n        x = np.transpose(np.fliplr(np.transpose(x, (0, 2, 1))), (0, 2, 1))\n    elif x.ndim == 4:\n        for i in range(x.shape[0]):\n            x[i] = np.transpose(np.fliplr(np.transpose(x[i], (0, 2, 1))), (0, 2, 1))\n    return x.astype(float)\n\n\ndef get_transform(center, scale, res, rot=0):\n    """"""\n    General image processing functions\n    """"""\n    # Generate transformation matrix\n    h = 200 * scale\n    t = np.zeros((3, 3))\n    t[0, 0] = float(res[1]) / h\n    t[1, 1] = float(res[0]) / h\n    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n    t[2, 2] = 1\n    if not rot == 0:\n        rot = -rot # To match direction of rotation from cropping\n        rot_mat = np.zeros((3,3))\n        rot_rad = rot * np.pi / 180\n        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n        rot_mat[0,:2] = [cs, -sn]\n        rot_mat[1,:2] = [sn, cs]\n        rot_mat[2,2] = 1\n        # Need to rotate around center\n        t_mat = np.eye(3)\n        t_mat[0,2] = -res[1]/2\n        t_mat[1,2] = -res[0]/2\n        t_inv = t_mat.copy()\n        t_inv[:2,2] *= -1\n        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n    return t\n\n\ndef transform(pt, center, scale, res, invert=0, rot=0):\n    # Transform pixel location to different reference\n    t = get_transform(center, scale, res, rot=rot)\n    if invert:\n        t = np.linalg.inv(t)\n    new_pt = np.array([pt[0] - 1, pt[1] - 1, 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2].astype(int) + 1\n\n\ndef transform_preds(coords, center, scale, res):\n    # size = coords.size()\n    # coords = coords.view(-1, coords.size(-1))\n    # print(coords.size())\n    for p in range(coords.size(0)):\n        coords[p, 0:2] = to_torch(transform(coords[p, 0:2], center, scale, res, 1, 0))\n    return coords\n\n\ndef crop(img, center, scale, res, rot=0):\n    img = im_to_numpy(img)\n\n    # Preprocessing for efficient cropping\n    ht, wd = img.shape[0], img.shape[1]\n    sf = scale * 200.0 / res[0]\n    if sf < 2:\n        sf = 1\n    else:\n        new_size = int(np.math.floor(max(ht, wd) / sf))\n        new_ht = int(np.math.floor(ht / sf))\n        new_wd = int(np.math.floor(wd / sf))\n        if new_size < 2:\n            return torch.zeros(res[0], res[1], img.shape[2]) \\\n                        if len(img.shape) > 2 else torch.zeros(res[0], res[1])\n        else:\n            img = scipy.misc.imresize(img, [new_ht, new_wd])\n            center = center * 1.0 / sf\n            scale = scale / sf\n\n    # Upper left point\n    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n    # Bottom right point\n    br = np.array(transform(res, center, scale, res, invert=1))\n\n    # Padding so that when rotated proper amount of context is included\n    pad = int(np.linalg.norm(br - ul) / 2 - float(br[1] - ul[1]) / 2)\n    if not rot == 0:\n        ul -= pad\n        br += pad\n\n    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n    if len(img.shape) > 2:\n        new_shape += [img.shape[2]]\n    new_img = np.zeros(new_shape)\n\n    # Range to fill new array\n    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n    # Range to sample from original image\n    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n    old_y = max(0, ul[1]), min(len(img), br[1])\n    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n\n    if not rot == 0:\n        # Remove padding\n        new_img = scipy.misc.imrotate(new_img, rot)\n        new_img = new_img[pad:-pad, pad:-pad]\n\n    new_img = im_to_torch(scipy.misc.imresize(new_img, res))\n    return new_img\n'"
