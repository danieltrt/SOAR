file_path,api_count,code
config.py,0,"b""MAX_LENGTH = 15\nteacher_forcing_ratio = 1.0\nsave_dir = './save'\n"""
evaluate.py,10,"b'import torch\nimport random\nfrom train import indexesFromSentence\nfrom load import SOS_token, EOS_token\nfrom load import MAX_LENGTH, loadPrepareData, Voc\nfrom model import *\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if USE_CUDA else ""cpu"")\n\nclass Sentence:\n    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n        if(len(sentence_idxes) != len(sentence_scores)):\n            raise ValueError(""length of indexes and scores should be the same"")\n        self.decoder_hidden = decoder_hidden\n        self.last_idx = last_idx\n        self.sentence_idxes =  sentence_idxes\n        self.sentence_scores = sentence_scores\n\n    def avgScore(self):\n        if len(self.sentence_scores) == 0:\n            raise ValueError(""Calculate average score of sentence, but got no word"")\n        # return mean of sentence_score\n        return sum(self.sentence_scores) / len(self.sentence_scores)\n\n    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n        topv = torch.log(topv)\n        terminates, sentences = [], []\n        for i in range(beam_size):\n            if topi[0][i] == EOS_token:\n                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + [\'<EOS>\'],\n                                   self.avgScore())) # tuple(word_list, score_float\n                continue\n            idxes = self.sentence_idxes[:] # pass by value\n            scores = self.sentence_scores[:] # pass by value\n            idxes.append(topi[0][i])\n            scores.append(topv[0][i])\n            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n        return terminates, sentences\n\n    def toWordScore(self, voc):\n        words = []\n        for i in range(len(self.sentence_idxes)):\n            if self.sentence_idxes[i] == EOS_token:\n                words.append(\'<EOS>\')\n            else:\n                words.append(voc.index2word[self.sentence_idxes[i].item()])\n        if self.sentence_idxes[-1] != EOS_token:\n            words.append(\'<EOS>\')\n        return (words, self.avgScore())\n\ndef beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n    prev_top_sentences.append(Sentence(decoder_hidden))\n    for i in range(max_length):\n        for sentence in prev_top_sentences:\n            decoder_input = torch.LongTensor([[sentence.last_idx]])\n            decoder_input = decoder_input.to(device)\n\n            decoder_hidden = sentence.decoder_hidden\n            decoder_output, decoder_hidden, _ = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            topv, topi = decoder_output.topk(beam_size)\n            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n            terminal_sentences.extend(term)\n            next_top_sentences.extend(top)\n\n        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n        prev_top_sentences = next_top_sentences[:beam_size]\n        next_top_sentences = []\n\n    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n\n    n = min(len(terminal_sentences), 15)\n    return terminal_sentences[:n]\n\ndef decode(decoder, decoder_hidden, encoder_outputs, voc, max_length=MAX_LENGTH):\n\n    decoder_input = torch.LongTensor([[SOS_token]])\n    decoder_input = decoder_input.to(device)\n\n    decoded_words = []\n    decoder_attentions = torch.zeros(max_length, max_length) #TODO: or (MAX_LEN+1, MAX_LEN+1)\n\n    for di in range(max_length):\n        decoder_output, decoder_hidden, decoder_attn = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n        )\n        _, topi = decoder_output.topk(3)\n        ni = topi[0][0]\n        if ni == EOS_token:\n            decoded_words.append(\'<EOS>\')\n            break\n        else:\n            decoded_words.append(voc.index2word[ni.item()])\n\n        decoder_input = torch.LongTensor([[ni]])\n        decoder_input = decoder_input.to(device)\n\n    return decoded_words, decoder_attentions[:di + 1]\n\n\ndef evaluate(encoder, decoder, voc, sentence, beam_size, max_length=MAX_LENGTH):\n    indexes_batch = [indexesFromSentence(voc, sentence)] #[1, seq_len]\n    lengths = [len(indexes) for indexes in indexes_batch]\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n    input_batch = input_batch.to(device)\n\n    encoder_outputs, encoder_hidden = encoder(input_batch, lengths, None)\n\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    if beam_size == 1:\n        return decode(decoder, decoder_hidden, encoder_outputs, voc)\n    else:\n        return beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size)\n\n\ndef evaluateRandomly(encoder, decoder, voc, pairs, reverse, beam_size, n=10):\n    for _ in range(n):\n        pair = random.choice(pairs)\n        print(""============================================================="")\n        if reverse:\n            print(\'>\', "" "".join(reversed(pair[0].split())))\n        else:\n            print(\'>\', pair[0])\n        if beam_size == 1:\n            output_words, _ = evaluate(encoder, decoder, voc, pair[0], beam_size)\n            output_sentence = \' \'.join(output_words)\n            print(\'<\', output_sentence)\n        else:\n            output_words_list = evaluate(encoder, decoder, voc, pair[0], beam_size)\n            for output_words, score in output_words_list:\n                output_sentence = \' \'.join(output_words)\n                print(""{:.3f} < {}"".format(score, output_sentence))\n\ndef evaluateInput(encoder, decoder, voc, beam_size):\n    pair = \'\'\n    while(1):\n        try:\n            pair = input(\'> \')\n            if pair == \'q\': break\n            if beam_size == 1:\n                output_words, _ = evaluate(encoder, decoder, voc, pair, beam_size)\n                output_sentence = \' \'.join(output_words)\n                print(\'<\', output_sentence)\n            else:\n                output_words_list = evaluate(encoder, decoder, voc, pair, beam_size)\n                for output_words, score in output_words_list:\n                    output_sentence = \' \'.join(output_words)\n                    print(""{:.3f} < {}"".format(score, output_sentence))\n        except KeyError:\n            print(""Incorrect spelling."")\n\n\ndef runTest(n_layers, hidden_size, reverse, modelFile, beam_size, inp, corpus):\n    torch.set_grad_enabled(False)\n\n    voc, pairs = loadPrepareData(corpus)\n    embedding = nn.Embedding(voc.n_words, hidden_size)\n    encoder = EncoderRNN(voc.n_words, hidden_size, embedding, n_layers)\n    attn_model = \'dot\'\n    decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.n_words, n_layers)\n\n    checkpoint = torch.load(modelFile)\n    encoder.load_state_dict(checkpoint[\'en\'])\n    decoder.load_state_dict(checkpoint[\'de\'])\n\n    # train mode set to false, effect only on dropout, batchNorm\n    encoder.train(False);\n    decoder.train(False);\n\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n\n    if inp:\n        evaluateInput(encoder, decoder, voc, beam_size)\n    else:\n        evaluateRandomly(encoder, decoder, voc, pairs, reverse, beam_size, 20)\n'"
load.py,4,"b'import torch\nimport re\nimport os\nimport unicodedata\n\nfrom config import MAX_LENGTH, save_dir\n\nSOS_token = 0\nEOS_token = 1\nPAD_token = 2\n\nclass Voc:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: ""SOS"", 1: ""EOS"", 2:""PAD""}\n        self.n_words = 3  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(\' \'):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\n# Turn a Unicode string to plain ASCII, thanks to\n# http://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return \'\'.join(\n        c for c in unicodedata.normalize(\'NFD\', s)\n        if unicodedata.category(c) != \'Mn\'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r""([.!?])"", r"" \\1"", s)\n    s = re.sub(r""[^a-zA-Z.!?]+"", r"" "", s)\n    s = re.sub(r""\\s+"", r"" "", s).strip()\n    return s\n\ndef readVocs(corpus, corpus_name):\n    print(""Reading lines..."")\n\n    # combine every two lines into pairs and normalize\n    with open(corpus) as f:\n        content = f.readlines()\n    # import gzip\n    # content = gzip.open(corpus, \'rt\')\n    lines = [x.strip() for x in content]\n    it = iter(lines)\n    # pairs = [[normalizeString(x), normalizeString(next(it))] for x in it]\n    pairs = [[x, next(it)] for x in it]\n\n    voc = Voc(corpus_name)\n    return voc, pairs\n\ndef filterPair(p):\n    # input sequences need to preserve the last word for EOS_token\n    return len(p[0].split(\' \')) < MAX_LENGTH and \\\n        len(p[1].split(\' \')) < MAX_LENGTH\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\ndef prepareData(corpus, corpus_name):\n    voc, pairs = readVocs(corpus, corpus_name)\n    print(""Read {!s} sentence pairs"".format(len(pairs)))\n    pairs = filterPairs(pairs)\n    print(""Trimmed to {!s} sentence pairs"".format(len(pairs)))\n    print(""Counting words..."")\n    for pair in pairs:\n        voc.addSentence(pair[0])\n        voc.addSentence(pair[1])\n    print(""Counted words:"", voc.n_words)\n    directory = os.path.join(save_dir, \'training_data\', corpus_name)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    torch.save(voc, os.path.join(directory, \'{!s}.tar\'.format(\'voc\')))\n    torch.save(pairs, os.path.join(directory, \'{!s}.tar\'.format(\'pairs\')))\n    return voc, pairs\n\ndef loadPrepareData(corpus):\n    corpus_name = corpus.split(\'/\')[-1].split(\'.\')[0]\n    try:\n        print(""Start loading training data ..."")\n        voc = torch.load(os.path.join(save_dir, \'training_data\', corpus_name, \'voc.tar\'))\n        pairs = torch.load(os.path.join(save_dir, \'training_data\', corpus_name, \'pairs.tar\'))\n    except FileNotFoundError:\n        print(""Saved data not found, start preparing trianing data ..."")\n        voc, pairs = prepareData(corpus, corpus_name)\n    return voc, pairs\n'"
main.py,0,"b""import argparse\nfrom train import trainIters\nfrom evaluate import runTest\n\ndef parse():\n    parser = argparse.ArgumentParser(description='Attention Seq2Seq Chatbot')\n    parser.add_argument('-tr', '--train', help='Train the model with corpus')\n    parser.add_argument('-te', '--test', help='Test the saved model')\n    parser.add_argument('-l', '--load', help='Load the model and train')\n    parser.add_argument('-c', '--corpus', help='Test the saved model with vocabulary of the corpus')\n    parser.add_argument('-r', '--reverse', action='store_true', help='Reverse the input sequence')\n    parser.add_argument('-f', '--filter', action='store_true', help='Filter to small training data set')\n    parser.add_argument('-i', '--input', action='store_true', help='Test the model by input the sentence')\n    parser.add_argument('-it', '--iteration', type=int, default=10000, help='Train the model with it iterations')\n    parser.add_argument('-p', '--print', type=int, default=100, help='Print every p iterations')\n    parser.add_argument('-b', '--batch_size', type=int, default=64, help='Batch size')\n    parser.add_argument('-la', '--layer', type=int, default=1, help='Number of layers in encoder and decoder')\n    parser.add_argument('-hi', '--hidden', type=int, default=256, help='Hidden size in encoder and decoder')\n    parser.add_argument('-be', '--beam', type=int, default=1, help='Hidden size in encoder and decoder')\n    parser.add_argument('-s', '--save', type=int, default=500, help='Save every s iterations')\n    parser.add_argument('-lr', '--learning_rate', type=float, default=0.01, help='Learning rate')\n    parser.add_argument('-d', '--dropout', type=float, default=0.1, help='Dropout probability for rnn and dropout layers')\n\n    args = parser.parse_args()\n    return args\n\ndef parseFilename(filename, test=False):\n    filename = filename.split('/')\n    dataType = filename[-1][:-4] # remove '.tar'\n    parse = dataType.split('_')\n    reverse = 'reverse' in parse\n    layers, hidden = filename[-2].split('_')\n    n_layers = int(layers.split('-')[0])\n    hidden_size = int(hidden)\n    return n_layers, hidden_size, reverse\n\ndef run(args):\n    reverse, fil, n_iteration, print_every, save_every, learning_rate, \\\n        n_layers, hidden_size, batch_size, beam_size, inp, dropout = \\\n        args.reverse, args.filter, args.iteration, args.print, args.save, args.learning_rate, \\\n        args.layer, args.hidden, args.batch_size, args.beam, args.input, args.dropout\n    if args.train and not args.load:\n        trainIters(args.train, reverse, n_iteration, learning_rate, batch_size,\n                    n_layers, hidden_size, print_every, save_every, dropout)\n    elif args.load:\n        n_layers, hidden_size, reverse = parseFilename(args.load)\n        trainIters(args.train, reverse, n_iteration, learning_rate, batch_size,\n                    n_layers, hidden_size, print_every, save_every, dropout, loadFilename=args.load)\n    elif args.test:\n        n_layers, hidden_size, reverse = parseFilename(args.test, True)\n        runTest(n_layers, hidden_size, reverse, args.test, beam_size, inp, args.corpus)\n\n\nif __name__ == '__main__':\n    args = parse()\n    run(args)\n"""
model.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if USE_CUDA else ""cpu"")\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        outputs, hidden = self.gru(packed, hidden) # output: (seq_len, batch, hidden*n_dir)\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs (1, batch, hidden)\n        return outputs, hidden\n\nclass Attn(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n\n        self.method = method\n        self.hidden_size = hidden_size\n\n        if self.method == \'general\':\n            self.attn = nn.Linear(self.hidden_size, hidden_size)\n\n        elif self.method == \'concat\':\n            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden [1, 64, 512], encoder_outputs [14, 64, 512]\n        max_len = encoder_outputs.size(0)\n        batch_size = encoder_outputs.size(1)\n\n        # Create variable to store attention energies\n        attn_energies = torch.zeros(batch_size, max_len) # B x S\n        attn_energies = attn_energies.to(device)\n\n        # For each batch of encoder outputs\n        for b in range(batch_size):\n            # Calculate energy for each encoder output\n            for i in range(max_len):\n                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n\n        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n\n    def score(self, hidden, encoder_output):\n        # hidden [1, 512], encoder_output [1, 512]\n        if self.method == \'dot\':\n            energy = hidden.squeeze(0).dot(encoder_output.squeeze(0))\n            return energy\n\n        elif self.method == \'general\':\n            energy = self.attn(encoder_output)\n            energy = hidden.squeeze(0).dot(energy.squeeze(0))\n            return energy\n\n        elif self.method == \'concat\':\n            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n            energy = self.v.squeeze(0).dot(energy.squeeze(0))\n            return energy\n\nclass LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        # Choose attention model\n        if attn_model != \'none\':\n            self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_seq, last_hidden, encoder_outputs):\n        # Note: we run this one step at a time\n\n        # Get the embedding of the current input word (last output word)\n        embedded = self.embedding(input_seq)\n        embedded = self.embedding_dropout(embedded) #[1, 64, 512]\n        if(embedded.size(0) != 1):\n            raise ValueError(\'Decoder input sequence length should be 1\')\n\n        # Get current hidden state from input word and last hidden state\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n\n        # Calculate attention from current RNN state and all encoder outputs;\n        # apply to encoder outputs to get weighted average\n        attn_weights = self.attn(rnn_output, encoder_outputs) #[64, 1, 14]\n        # encoder_outputs [14, 64, 512]\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) #[64, 1, 512]\n\n        # Attentional vector using the RNN hidden state and context vector\n        # concatenated together (Luong eq. 5)\n        rnn_output = rnn_output.squeeze(0) #[64, 512]\n        context = context.squeeze(1) #[64, 512]\n        concat_input = torch.cat((rnn_output, context), 1) #[64, 1024]\n        concat_output = torch.tanh(self.concat(concat_input)) #[64, 512]\n\n        # Finally predict next token (Luong eq. 6, without softmax)\n        output = self.out(concat_output) #[64, output_size]\n\n        # Return final output, hidden state, and attention weights (for visualization)\n        return output, hidden, attn_weights\n'"
train.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\n\nimport itertools\nimport random\nimport math\nimport os\nfrom tqdm import tqdm\nfrom load import loadPrepareData\nfrom load import SOS_token, EOS_token, PAD_token\nfrom model import EncoderRNN, LuongAttnDecoderRNN\nfrom config import MAX_LENGTH, teacher_forcing_ratio, save_dir\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if USE_CUDA else ""cpu"")\n\ncudnn.benchmark = True\n#############################################\n# generate file name for saving parameters\n#############################################\ndef filename(reverse, obj):\n\tfilename = \'\'\n\tif reverse:\n\t\tfilename += \'reverse_\'\n\tfilename += obj\n\treturn filename\n\n\n#############################################\n# Prepare Training Data\n#############################################\ndef indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(\' \')] + [EOS_token]\n\n# batch_first: true -> false, i.e. shape: seq_len * batch\ndef zeroPadding(l, fillvalue=PAD_token):\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n\ndef binaryMatrix(l, value=PAD_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\n# convert to index, add EOS\n# return input pack_padded_sequence\ndef inputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    lengths = [len(indexes) for indexes in indexes_batch]\n    padList = zeroPadding(indexes_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n\n# convert to index, add EOS, zero padding\n# return output variable, mask, max length of the sentences in batch\ndef outputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    max_target_len = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.ByteTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar, mask, max_target_len\n\n# pair_batch is a list of (input, output) with length batch_size\n# sort list of (input, output) pairs by input length, reverse input\n# return input, lengths for pack_padded_sequence, output_variable, mask\ndef batch2TrainData(voc, pair_batch, reverse):\n    if reverse:\n        pair_batch = [pair[::-1] for pair in pair_batch]\n    pair_batch.sort(key=lambda x: len(x[0].split("" "")), reverse=True)\n    input_batch, output_batch = [], []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp, lengths = inputVar(input_batch, voc)\n    output, mask, max_target_len = outputVar(output_batch, voc)\n    return inp, lengths, output, mask, max_target_len\n\n#############################################\n# Training\n#############################################\n\ndef train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n          encoder_optimizer, decoder_optimizer, batch_size, max_length=MAX_LENGTH):\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_variable = input_variable.to(device)\n    target_variable = target_variable.to(device)\n    mask = mask.to(device)\n\n    loss = 0\n    print_losses = []\n    n_totals = 0\n\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths, None)\n\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Run through decoder one time step at a time\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden, _ = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            decoder_input = target_variable[t].view(1, -1) # Next input is current target\n            loss += F.cross_entropy(decoder_output, target_variable[t], ignore_index=EOS_token)\n    else:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden, decoder_attn = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            _, topi = decoder_output.topk(1) # [64, 1]\n\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            loss += F.cross_entropy(decoder_output, target_variable[t], ignore_index=EOS_token)\n\n    loss.backward()\n\n    clip = 50.0\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / max_target_len \n\n\ndef trainIters(corpus, reverse, n_iteration, learning_rate, batch_size, n_layers, hidden_size,\n                print_every, save_every, dropout, loadFilename=None, attn_model=\'dot\', decoder_learning_ratio=5.0):\n\n    voc, pairs = loadPrepareData(corpus)\n\n    # training data\n    corpus_name = os.path.split(corpus)[-1].split(\'.\')[0]\n    training_batches = None\n    try:\n        training_batches = torch.load(os.path.join(save_dir, \'training_data\', corpus_name,\n                                                   \'{}_{}_{}.tar\'.format(n_iteration, \\\n                                                                         filename(reverse, \'training_batches\'), \\\n                                                                         batch_size)))\n    except FileNotFoundError:\n        print(\'Training pairs not found, generating ...\')\n        training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)], reverse)\n                          for _ in range(n_iteration)]\n        torch.save(training_batches, os.path.join(save_dir, \'training_data\', corpus_name,\n                                                  \'{}_{}_{}.tar\'.format(n_iteration, \\\n                                                                        filename(reverse, \'training_batches\'), \\\n                                                                        batch_size)))\n    # model\n    checkpoint = None\n    print(\'Building encoder and decoder ...\')\n    embedding = nn.Embedding(voc.n_words, hidden_size)\n    encoder = EncoderRNN(voc.n_words, hidden_size, embedding, n_layers, dropout)\n    attn_model = \'dot\'\n    decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.n_words, n_layers, dropout)\n    if loadFilename:\n        checkpoint = torch.load(loadFilename)\n        encoder.load_state_dict(checkpoint[\'en\'])\n        decoder.load_state_dict(checkpoint[\'de\'])\n    # use cuda\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n\n    # optimizer\n    print(\'Building optimizers ...\')\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n    if loadFilename:\n        encoder_optimizer.load_state_dict(checkpoint[\'en_opt\'])\n        decoder_optimizer.load_state_dict(checkpoint[\'de_opt\'])\n\n    # initialize\n    print(\'Initializing ...\')\n    start_iteration = 1\n    perplexity = []\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint[\'iteration\'] + 1\n        perplexity = checkpoint[\'plt\']\n\n    for iteration in tqdm(range(start_iteration, n_iteration + 1)):\n        training_batch = training_batches[iteration - 1]\n        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size)\n        print_loss += loss\n        perplexity.append(loss)\n\n        if iteration % print_every == 0:\n            print_loss_avg = math.exp(print_loss / print_every)\n            print(\'%d %d%% %.4f\' % (iteration, iteration / n_iteration * 100, print_loss_avg))\n            print_loss = 0\n\n        if (iteration % save_every == 0):\n            directory = os.path.join(save_dir, \'model\', corpus_name, \'{}-{}_{}\'.format(n_layers, n_layers, hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                \'iteration\': iteration,\n                \'en\': encoder.state_dict(),\n                \'de\': decoder.state_dict(),\n                \'en_opt\': encoder_optimizer.state_dict(),\n                \'de_opt\': decoder_optimizer.state_dict(),\n                \'loss\': loss,\n                \'plt\': perplexity\n            }, os.path.join(directory, \'{}_{}.tar\'.format(iteration, filename(reverse, \'backup_bidir_model\'))))\n'"
