file_path,api_count,code
sotoxic/__init__.py,0,b''
tools/bagging.py,0,"b'import sys\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom scipy.special import expit,logit\n\nensembeled = sys.argv[1:]\nprint(""Going ensemble on"",)\nsubs = []\nfor e in ensembeled:\n\tprint(e)\n\tsubs.append(pd.read_csv(e))\n\nclasses = [\'toxic\', \'severe_toxic\', \'obscene\', \'threat\', \'insult\', \'identity_hate\']\nfor sub in subs[1:]:\n\tfor c in classes:\n\t    subs[0][c] += sub[c]\nfor c in classes:\n\tsubs[0][c] /= len(subs)\n\nsubs[0].to_csv(\'Bagging.csv\', index=False)'"
tools/check_corr.py,0,"b'import pandas as pd\nimport sys\n\ndf1 = pd.read_csv(sys.argv[1])\ndf2 = pd.read_csv(sys.argv[2])\nres = 0\nfor col in df1.columns.values[1:]: # skip id\n    cur = df1[col].corr(df2[col])\n    corr = (df1[col].rank() / len(df1)).corr(df2[col].rank() / len(df2))\n    print(col, corr)\n    res += corr\nprint(""Avg Rank"", res / 6)\n'"
sotoxic/config/__init__.py,0,b''
sotoxic/config/dataset_config.py,0,"b""# for dataset's path\nTRAIN_PATH = 'Dataset/train.csv'\nTEST_PATH = 'Dataset/test.csv'\nSAMPLE_SUBMISSION_PATH = 'Dataset/sample_submission.csv'\n\n# for feature's path\nCLEAN_WORDS_PATH = 'features/cleanwords.txt'\nFASTTEXT_PATH = 'features/crawl-300d-2M.vec'\nGLOVE_PATH = 'features/glove.840B.300d.txt'\n\n"""
sotoxic/config/model_config.py,0,"b'MODEL_CHECKPOINT_FOLDER = ""checkpoints/""\nTEMPORARY_CHECKPOINTS_PATH = \'temporary_checkpoints/\'\nMAX_SENTENCE_LENGTH = 350\n\nuse_cuda = True\n\n'"
sotoxic/data_helper/__init__.py,0,b''
sotoxic/data_helper/data_loader.py,0,"b'import pandas as pd\nimport numpy as np\n\n\nclass DataLoader(object):\n\n    def __init__(self):\n        pass\n\n    def load_dataset(self, dataset_path):\n        \'\'\'return a pandas processed csv\'\'\'\n        return pd.read_csv(dataset_path)\n\n    def load_clean_words(self, clean_words_path):\n        \'\'\'return a dict whose key is typo, value is correct word\'\'\'\n        clean_word_dict = {}\n        with open(clean_words_path, \'r\', encoding=\'utf-8\') as cl:\n            for line in cl:\n                line = line.strip(\'\\n\')\n                typo, correct = line.split(\',\')\n                clean_word_dict[typo] = correct\n        return clean_word_dict\n\n    def load_embedding(self, embedding_path, keras_like=True):\n        \'\'\'return a dict whose key is word, value is pretrained word embedding\'\'\'\n        if keras_like:\n            embeddings_index = {}\n            f = open(embedding_path, \'r\', encoding=\'utf-8\')\n            for line in f:\n                values = line.split()\n                try:\n                    word = values[0]\n                    coefs = np.asarray(values[1:], dtype=\'float32\')\n                    embeddings_index[word] = coefs\n                except:\n                    print(""Err on "", values[:2])\n            f.close()\n            print(\'Total %s word vectors.\' % len(embeddings_index))\n            return embeddings_index'"
sotoxic/data_helper/data_transformer.py,0,"b'import numpy as np\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sotoxic.config import dataset_config, model_config\nfrom sotoxic.data_helper import data_loader\n\n\nclass DataTransformer(object):\n\n    def __init__(self, max_num_words, max_sequence_length, char_level):\n        self.data_loader = data_loader.DataLoader()\n        self.clean_word_dict = self.data_loader.load_clean_words(dataset_config.CLEAN_WORDS_PATH)\n        self.train_df = self.data_loader.load_dataset(dataset_config.TRAIN_PATH)\n        self.test_df = self.data_loader.load_dataset(dataset_config.TEST_PATH)\n\n        self.max_num_words = max_num_words\n        self.max_sequence_length = max_sequence_length\n        self.char_level = char_level\n        self.tokenizer = None\n\n    def prepare_data(self):\n        list_sentences_train = self.train_df[""comment_text""].fillna(""no comment"").values\n        list_classes = [""toxic"", ""severe_toxic"", ""obscene"", ""threat"", ""insult"", ""identity_hate""]\n        list_sentences_test = self.test_df[""comment_text""].fillna(""no comment"").values\n\n        print(""Doing preprocessing..."")\n\n        self.comments = [self.clean_text(text) for text in list_sentences_train]\n        self.test_comments = [self.clean_text(text) for text in list_sentences_test]\n\n        self.build_tokenizer(self.comments + self.test_comments)\n        train_sequences = self.tokenizer.texts_to_sequences(self.comments)\n        training_labels = self.train_df[list_classes].values\n        test_sequences = self.tokenizer.texts_to_sequences(self.test_comments)\n\n        print(""Preprocessed."")\n\n        return train_sequences, training_labels, test_sequences\n\n    def clean_text(self, text, clean_wiki_tokens=True, drop_image=True):\n        replace_numbers = re.compile(r\'\\d+\', re.IGNORECASE)\n        special_character_removal = re.compile(r\'[^a-z\\d ]\', re.IGNORECASE)\n        text = text.lower()\n        text = re.sub(r""https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)"", """", text)\n        text = re.sub(r""(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}"", """", text)\n    \n        if clean_wiki_tokens:\n            # Drop dataset stopwords\n            text = re.sub(r""\\.jpg"", "" "", text)\n            text = re.sub(r""\\.png"", "" "", text)\n            text = re.sub(r""\\.gif"", "" "", text)\n            text = re.sub(r""\\.bmp"", "" "", text)\n            text = re.sub(r""\\.pdf"", "" "", text)\n            text = re.sub(r""image:"", "" "", text)\n            text = re.sub(r""#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})"", "" "",text)\n            \n            text = re.sub(r""\\{\\|[^\\}]*\\|\\}"", "" "", text) # clean the css part\n            \n            text = re.sub(r""\\[?\\[user:.*\\]"", "" "", text)\n            text = re.sub(r""\\[?\\[user:.*\\|"", "" "", text)\n\n            text = re.sub(r""\\[?\\[wikipedia:.*\\]"", "" "", text)\n            text = re.sub(r""\\[?\\[wikipedia:.*\\|"", "" "", text)\n            text = re.sub(r""\\[?\\[special:.*\\]"", "" "", text)\n            text = re.sub(r""\\[?\\[special:.*\\|"", "" "", text)\n            text = re.sub(r""\\[?\\[category:.*\\]"", "" "", text)\n            text = re.sub(r""\\[?\\[category:.*\\|"", "" "", text)\n\n            #text = re.sub(r""{{[a-zA-Z0-9]*}}"", "" "", text)\n            #text = re.sub(r\'\\""{2,}\', "" "", text)\n            #text = re.sub(r\'={2,}\', "" "", text)\n            #text = re.sub(r\':{2,}\', "" "", text)\n            #text = re.sub(r\'\\{{2,}\', "" "", text)\n            #text = re.sub(r\'\\}{2,}\', "" "", text)\n            text = re.sub(r""wp:"", "" "", text)\n            text = re.sub(r""file:"", "" "", text)\n\n        for typo, correct in self.clean_word_dict.items():\n            text = re.sub(typo, "" "" + correct + "" "", text)\n            \n        text = re.sub(r""\xc2\xb4"", ""\'"", text)\n        text = re.sub(r""\xe2\x80\x94"", "" "", text)\n        text = re.sub(r""\xe2\x80\x99"", ""\'"", text)\n        text = re.sub(r""\xe2\x80\x98"", ""\'"", text)\n        text = re.sub(r""what\'s"", ""what is "", text)\n        text = re.sub(r""\\\'s"", "" "", text)\n        text = re.sub(r""\\\'ve"", "" have "", text)\n        text = re.sub(r""can\'t"", ""cannot "", text)\n        text = re.sub(r""n\'t"", "" not "", text)\n        text = re.sub(r""i\'m"", ""i am "", text)\n        text = re.sub(r""i\xe2\x80\x99m"", ""i am"", text)\n        text = re.sub(r""\\\'re"", "" are "", text)\n        text = re.sub(r""\\\'d"", "" would "", text)\n        text = re.sub(r""\\\'ll"", "" will "", text)\n        text = re.sub(r"","", "" "", text)\n        text = re.sub(r""\xe2\x80\x93"", "" "", text)\n        text = re.sub(r""\xe2\x88\x92"", "" "", text)\n        text = re.sub(r""\\."", "" "", text)\n        text = re.sub(r""!"", "" ! "", text)\n        text = re.sub(r""\\/"", "" "", text)\n        text = re.sub(r""_"", "" "", text)\n        text = re.sub(r""\\?"", "" ? "", text)\n        text = re.sub(r""\\^"", "" ^ "", text)\n        text = re.sub(r""\\+"", "" + "", text)\n        text = re.sub(r""\\-"", "" - "", text)\n        text = re.sub(r""\\="", "" = "", text)\n        text = re.sub(r""#"", "" # "", text)\n        text = re.sub(r""\'"", "" "", text)\n        text = re.sub(r""<3"", "" love "", text)\n        text = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", text)\n        text = re.sub(r"":"", "" : "", text)\n        text = re.sub(r"" e g "", "" eg "", text)\n        text = re.sub(r"" b g "", "" bg "", text)\n        text = re.sub(r"" u s "", "" american "", text)\n        text = re.sub(r""\\0s"", ""0"", text)\n        text = re.sub(r"" 9 11 "", ""911"", text)\n        text = re.sub(r""e - mail"", ""email"", text)\n        text = re.sub(r""j k"", ""jk"", text)\n        text = re.sub(r""\\s{2,}"", "" "", text)\n\n        from collections import defaultdict\n        self.word_count_dict = defaultdict(int)\n        text = text.split()\n        for t in text:\n            self.word_count_dict[t] += 1\n        text = "" "".join(text)\n\n        return (text)\n\n    def build_embedding_matrix(self, embeddings_index):\n        nb_words = min(self.max_num_words, len(embeddings_index))\n        embedding_matrix = np.zeros((nb_words, 300))\n        word_index = self.tokenizer.word_index\n        null_words = open(\'null-word.txt\', \'w\', encoding=\'utf-8\')\n\n        for word, i in word_index.items():\n\n            if i >= self.max_num_words:\n                null_words.write(word + \', \' + str(self.word_count_dict[word]) + \'\\n\')\n                continue\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n            else:\n                null_words.write(word + \', \' + str(self.word_count_dict[word]) + \'\\n\')\n        print(\'Null word embeddings: %d\' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n        return embedding_matrix\n\n    def build_tokenizer(self, comments):\n        self.tokenizer = Tokenizer(num_words=self.max_num_words, char_level=self.char_level)\n        self.tokenizer.fit_on_texts(comments)\n'"
sotoxic/models/__init__.py,0,b''
sotoxic/models/torch_base.py,3,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport importlib\n\nfrom sotoxic.utils import generators\nimportlib.reload(generators)  # for debugging on jupyter\n\nfrom keras.models import Model\n\n\nclass BaseModel(nn.Module):\n\n    def __init__(self):\n        super(BaseModel, self).__init__()\n        print(""Choose the torch base model."")\n        self.manager = ModelManager()\n\n    def save(self, path):\n        self.manager.save_model(self, path)\n\n    def load(self, path):\n        self.manager.load_model(self, path)\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def predict(self, x, batch_size=256, verbose=0):\n        self.eval_mode()\n        predictions = []\n        for batch_x in generators.test_batches_generator(x, batch_size):\n            preds_var = self.forward(batch_x)\n            preds_logits = nn.Sigmoid()(preds_var)\n            predictions.append(preds_logits.data.cpu().numpy())\n        predictions = np.concatenate(predictions, axis=0)\n        return predictions\n\n    def set_dropout(self, p):\n        pass\n\n    def train_mode(self, p):\n        self.set_dropout(p)\n        self.train()\n\n    def eval_mode(self):\n        self.eval()\n\n\nclass ModelManager(object):\n\n    def __init__(self, path=None):\n        self.path = path\n\n    def save_model(self, model, path=None):\n        path = self.path if path is None else path\n        torch.save(model.state_dict(), path)\n        print(""Model has been saved as %s.\\n"" % path)\n\n    def load_model(self, model, path=None):\n        path = self.path if path is None else path\n        model.load_state_dict(torch.load(path))\n        model.eval()\n        print(""A pre-trained model at %s has been loaded.\\n"" % path)'"
sotoxic/train/__init__.py,0,b''
sotoxic/train/trainer.py,4,"b'import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport importlib\n\nfrom sklearn.metrics import roc_auc_score\nfrom sotoxic.utils.score import log_loss\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sotoxic.config import model_config\nfrom sotoxic import utils\nimportlib.reload(utils)\n\n\nclass ModelTrainer(object):\n\n    def __init__(self, model_stamp, epoch_num, learning_rate=1e-3,\n                 shuffle_inputs=False, verbose_round=40, early_stopping_round=8):\n        self.models = []\n        self.model_stamp = model_stamp\n        self.val_loss = -1\n        self.auc = -1\n        self.epoch_num = epoch_num\n        self.learning_rate = learning_rate\n        self.eps = 1e-10\n        self.verbose_round = verbose_round\n        self.early_stopping_round = early_stopping_round\n        self.shuffle_inputs = shuffle_inputs\n\n    def train_folds(self, X, y, fold_count, batch_size, get_model_func, skip_fold=0):\n        fold_size = len(X) // fold_count\n        models = []\n        fold_predictions = []\n        score = 0\n        total_auc = 0\n\n        for fold_id in range(0, fold_count):\n            fold_start = fold_size * fold_id\n            fold_end = fold_start + fold_size\n\n            if fold_id == fold_count - 1:\n                fold_end = len(X)\n\n            train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n            train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n\n            val_x = X[fold_start:fold_end]\n            val_y = y[fold_start:fold_end]\n\n            if fold_id < skip_fold:\n                model = get_model_func()\n                model.load(model_config.MODEL_CHECKPOINT_FOLDER + self.model_stamp + str(fold_id) + "".pt"")\n                model = model.eval()\n                model = model.cuda()\n                fold_prediction = model.predict(val_x)\n                auc = roc_auc_score(val_y, fold_prediction)\n                bst_val_score = log_loss(y=val_y, y_pred=fold_prediction)\n            else:\n                model, bst_val_score, auc, fold_prediction = self._train_model_by_logloss(\n                    get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n            score += bst_val_score\n            total_auc += auc\n            models.append(model)\n            fold_predictions.append(fold_prediction)\n\n        self.models = models\n        self.val_loss = score / fold_count\n        self.auc = total_auc / fold_count\n        return models, self.val_loss, self.auc, fold_predictions\n\n    def keep_train_folds(self, X, y, fold_count, batch_size, old_models):\n        fold_size = len(X) // fold_count\n        models = []\n        fold_predictions = []\n        score = 0\n        total_auc = 0\n\n        for fold_id in range(0, fold_count):\n            fold_start = fold_size * fold_id\n            fold_end = fold_start + fold_size\n\n            if fold_id == fold_count - 1:\n                fold_end = len(X)\n\n            train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n            train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n\n            val_x = X[fold_start:fold_end]\n            val_y = y[fold_start:fold_end]\n\n            model, bst_val_score, auc, fold_prediction = self._train_model_by_logloss(\n                old_models[fold_id], batch_size, train_x, train_y, val_x, val_y, fold_id)\n            score += bst_val_score\n            total_auc += auc\n            models.append(model)\n            fold_predictions.append(fold_prediction)\n\n        self.models = models\n        self.val_loss = score / fold_count\n        self.auc = total_auc / fold_count\n        return models, self.val_loss, self.auc, fold_predictions\n\n    def _train_model_by_auc(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        # --- Deprecated. ---\n        # return a list which holds [models, val_loss, auc, prediction]\n        raise NotImplementedError\n\n    def _train_model_by_logloss(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        # return a list which holds [models, val_loss, auc, prediction]\n        raise NotImplementedError\n\n    def evaluate(self, test_data, dataframe, submit_path_prefix):\n        \'\'\'\n        print(""Predicting results..."")\n        test_predicts_list = []\n        for fold_id, model in enumerate(self.models):\n            test_predicts = model.predict(test_data, batch_size=512, verbose=1)\n            test_predicts_list.append(test_predicts)\n            np.save(""predict_path/"", test_predicts)\n\n        test_predicts = np.zeros(test_predicts_list[0].shape)\n        for fold_predict in test_predicts_list:\n            test_predicts += fold_predict\n        test_predicts /= len(test_predicts_list)\n\n        ids = dataframe[""id""].values\n        ids = ids.reshape((len(ids), 1))\n        CLASSES = [""toxic"", ""severe_toxic"", ""obscene"", ""threat"", ""insult"", ""identity_hate""]\n        test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n        test_predicts[""id""] = ids\n        test_predicts = test_predicts[[""id""] + CLASSES]\n        submit_path = submit_path_prefix + ""-L{:4f}-A{:4f}.csv"".format(self.val_loss, self.total_auc)\n        test_predicts.to_csv(submit_path, index=False)\n        \'\'\'\n\nclass KerasModelTrainer(ModelTrainer):\n\n    def __init__(self, *args, **kwargs):\n        super(KerasModelTrainer, self).__init__(*args, **kwargs)\n        pass\n\n    def _train_model_by_auc(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        pass\n\n    def _train_model_by_logloss(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=6)\n        bst_model_path = self.model_stamp + str(fold_id) + \'.h5\'\n        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n        hist = model.fit(train_x, train_y,\n                         validation_data=(val_x, val_y),\n                         epochs=self.epoch_num, batch_size=batch_size, shuffle=True,\n                         callbacks=[early_stopping, model_checkpoint])\n        bst_val_score = min(hist.history[\'val_loss\'])\n        predictions = model.predict(val_x)\n        auc = roc_auc_score(val_y, predictions)\n        print(""AUC Score"", auc)\n        return model, bst_val_score, auc, predictions\n\n\nclass PyTorchModelTrainer(ModelTrainer):\n\n    def __init__(self, *args, **kwargs):\n        super(PyTorchModelTrainer, self).__init__(*args, **kwargs)\n        self.criterion = torch.nn.BCELoss(size_average=True)\n\n    def adjust_learning_rate(self):\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.93\n\n    def _train_model_by_auc(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        pass\n\n    def _train_model_by_logloss(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n        print(""Training on fold"", fold_id)\n\n        if model_config.use_cuda:\n            model = model.cuda()\n        best_auc = -1\n        best_logloss = -1\n        best_epoch = 0\n        current_epoch = 1\n        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=self.learning_rate)\n\n        for epoch in range(self.epoch_num):\n            epoch_logloss = 0\n            for batch_id, (inputs_var, targets_var) in enumerate(\n                    utils.generators.mini_batches_generator(train_x, train_y, batch_size, row_shuffle=self.shuffle_inputs)):\n                loss = self._train_batch(model=model, inputs_var=inputs_var, targets_var=targets_var)\n\n                # logging, TODO: add tensorboard for visualization\n                epoch_logloss += loss\n                if batch_id % self.verbose_round == 0:\n                    print(""Epoch:{} Batch:{} Log-loss{}"".format(epoch + 1, batch_id, loss))\n\n            # validation\n            print(""Epoch average log loss:{}"".format(epoch_logloss / batch_id))\n            val_pred = model.predict(val_x)\n\n            current_logloss = log_loss(val_y, val_pred)\n            current_epoch += 1\n            if best_logloss > current_logloss or best_logloss == -1:\n                best_logloss = current_logloss\n                model.save(model_config.TEMPORARY_CHECKPOINTS_PATH + self.model_stamp + ""-TEMP.pt"")\n                best_auc = roc_auc_score(val_y, val_pred)\n                best_epoch = current_epoch\n            else:\n                if current_epoch - best_epoch == self.early_stopping_round:\n                    break\n            print(""In Epoch{}, val_loss:{}, best_val_loss:{}, best_auc:{}"".format(epoch + 1, current_logloss, best_logloss, best_auc))\n            self.adjust_learning_rate()\n\n        model.load(model_config.TEMPORARY_CHECKPOINTS_PATH + self.model_stamp + ""-TEMP.pt"")\n        best_val_pred = model.predict(val_x)\n        model.save(model_config.MODEL_CHECKPOINT_FOLDER + self.model_stamp + str(fold_id) + "".pt"")\n        return model, best_logloss, best_auc, best_val_pred\n\n    def _train_batch(self, model, inputs_var, targets_var):\n        self.optimizer.zero_grad()\n        model.train()\n        preds_var = model.forward(inputs_var)\n        # training\n        loss = F.binary_cross_entropy_with_logits(preds_var, targets_var)\n        # loss = self.criterion(preds_var, targets_var)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 3)\n        self.optimizer.step()\n        return loss.data.cpu().numpy()[0]'"
sotoxic/utils/__init__.py,0,b''
sotoxic/utils/generators.py,6,"b'import torch\nimport numpy as np\nimport importlib\n\nfrom torch.autograd import Variable\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sotoxic.config import model_config\nfrom sotoxic.models.pytorch import dropout\nimportlib.reload(dropout)\n\n\ndef mini_batches_generator(inputs, targets, batch_size, row_shuffle=False):\n    inputs_data_size = len(inputs)\n    targets_data_size = len(targets)\n    assert inputs_data_size == targets_data_size, ""The length of inputs({}) and targets({}) must be consistent"".format(\n        inputs_data_size, targets_data_size)\n\n    if row_shuffle:\n        for input_seqs in inputs:\n            np.random.shuffle(input_seqs)\n\n    shuffled_input, shuffled_target = shuffle(inputs, targets)\n    mini_batches = [\n        (shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n        for k in range(0, inputs_data_size, batch_size)\n    ]\n    dp = dropout.EmbeddingDropout(p=0.2)\n\n    for batch_xs, batch_ys in mini_batches:\n        lengths = [len(s) for s in batch_xs]\n        max_length = min(model_config.MAX_SENTENCE_LENGTH, max(lengths))\n        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding=\'post\', truncating=\'pre\')\n\n        lengths_var = Variable(torch.Tensor(lengths), requires_grad=False)\n        inputs_tensor = torch.from_numpy(batch_tensors).long()\n        inputs_dropped_tensor = dp.forward(inputs_tensor)\n        inputs_var = Variable(inputs_dropped_tensor)\n        targets_var = Variable(torch.from_numpy(batch_ys).float())\n\n        if model_config.use_cuda:\n            inputs_var = inputs_var.cuda()\n            targets_var = targets_var.cuda()\n            lengths_var = lengths_var.cuda()\n        yield (inputs_var, lengths_var), targets_var\n\n\ndef test_batches_generator(inputs, batch_size):\n    inputs_data_size = len(inputs)\n    mini_batches = [inputs[k: k + batch_size] for k in range(0, inputs_data_size, batch_size)]\n\n    for batch_xs in mini_batches:\n        lengths = [len(s) for s in batch_xs]\n        max_length = min(model_config.MAX_SENTENCE_LENGTH, max(lengths))\n        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding=\'post\', truncating=\'pre\')\n\n        lengths_var = Variable(torch.Tensor(lengths))\n        inputs_var = Variable(torch.from_numpy(batch_tensors).long())\n\n        if model_config.use_cuda:\n            inputs_var = inputs_var.cuda()\n            lengths_var = lengths_var.cuda()\n\n        yield (inputs_var, lengths_var)\n'"
sotoxic/utils/score.py,0,"b'from sklearn import metrics\n\n\ndef roc_auc_score(y, y_pred):\n    return metrics.roc_auc_score(y, y_pred)\n\n\ndef log_loss(y, y_pred):\n    total_loss = 0\n    for j in range(6):\n        loss = metrics.log_loss(y[:, j], y_pred[:, j])\n        total_loss += loss\n    total_loss /= 6.\n\n    return total_loss'"
sotoxic/utils/submit.py,0,b''
sotoxic/models/keras/__init__.py,0,b''
sotoxic/models/keras/model_zoo.py,0,"b'from __future__ import absolute_import, division\n\nimport tensorflow as tf\nfrom keras.layers import Dense, Input, Embedding, Lambda, Dropout, Activation, SpatialDropout1D, Reshape, GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.engine import InputSpec, Layer\nfrom keras import backend as K\n\n\nclass AttentionWeightedAverage(Layer):\n    """"""\n    Computes a weighted average of the different channels across timesteps.\n    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n    """"""\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get(\'uniform\')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name=\'{}_W\'.format(self.name),\n                                 initializer=self.init)\n        self.trainable_weights = [self.W]\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # computes a probability distribution over the timesteps\n        # uses \'max trick\' for numerical stability\n        # reshape is done to avoid issue with Tensorflow\n        # and 1-dimensional weights\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None\n\n\nclass KMaxPooling(Layer):\n    """"""\n    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n    TensorFlow backend.\n    """"""\n\n    def __init__(self, k=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_spec = InputSpec(ndim=3)\n        self.k = k\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], (input_shape[2] * self.k))\n\n    def call(self, inputs):\n        # swap last two dimensions since top_k will be applied along the last dimension\n        shifted_input = tf.transpose(inputs, [0, 2, 1])\n\n        # extract top_k, returns two tensors [values, indices]\n        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n\n        # return flattened output\n        return Flatten()(top_k)\n\n\ndef get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    embedding_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)\n\n    filter_nums = 300  # 500->375, 400->373, 300->\n\n    comment_input = Input(shape=(max_sequence_length,), dtype=\'int32\')\n    embedded_sequences = embedding_layer(comment_input)\n    embedded_sequences = SpatialDropout1D(0.25)(embedded_sequences)\n\n    conv_0 = Conv1D(filter_nums, 1, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_1 = Conv1D(filter_nums, 2, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_2 = Conv1D(filter_nums, 3, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_3 = Conv1D(filter_nums, 4, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n\n    attn_0 = AttentionWeightedAverage()(conv_0)\n    avg_0 = GlobalAveragePooling1D()(conv_0)\n    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n\n    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n    attn_1 = AttentionWeightedAverage()(conv_1)\n    avg_1 = GlobalAveragePooling1D()(conv_1)\n\n    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n    attn_2 = AttentionWeightedAverage()(conv_2)\n    avg_2 = GlobalAveragePooling1D()(conv_2)\n\n    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n    attn_3 = AttentionWeightedAverage()(conv_3)\n    avg_3 = GlobalAveragePooling1D()(conv_3)\n\n    v0_col = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode=\'concat\', concat_axis=1)\n    v1_col = merge([attn_0, attn_1, attn_2, attn_3], mode=\'concat\', concat_axis=1)\n    v2_col = merge([avg_1, avg_2, avg_0, avg_3], mode=\'concat\', concat_axis=1)\n    merged_tensor = merge([v0_col, v1_col, v2_col], mode=\'concat\', concat_axis=1)\n    output = Dropout(0.7)(merged_tensor)\n    output = Dense(units=144)(output)\n    output = Activation(\'relu\')(output)\n    output = Dense(units=out_size, activation=\'sigmoid\')(output)\n\n    model = Model(inputs=comment_input, outputs=output)\n    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n    model.compile(loss=\'binary_crossentropy\', optimizer=adam_optimizer, metrics=[\'accuracy\'])\n    model.summary()\n    return model\n\n\ndef get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    embedding_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)\n\n    filter_nums = 180\n    drop = 0.6\n\n    comment_input = Input(shape=(max_sequence_length,), dtype=\'int32\')\n    embedded_sequences = embedding_layer(comment_input)\n    embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n\n    conv_0 = Conv1D(filter_nums, 1, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_1 = Conv1D(filter_nums, 2, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_2 = Conv1D(filter_nums, 3, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n    conv_3 = Conv1D(filter_nums, 4, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(embedded_sequences)\n\n    # conv_0 = Conv1D(filter_nums / 2, 1, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(conv_0)\n    # conv_1 = Conv1D(filter_nums / 2, 2, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(conv_1)\n    # conv_2 = Conv1D(filter_nums / 2, 3, strides=2, kernel_initializer=""normal"", padding=""valid"", activation=""relu"")(conv_2)\n\n    maxpool_0 = KMaxPooling(k=3)(conv_0)\n    maxpool_1 = KMaxPooling(k=3)(conv_1)\n    maxpool_2 = KMaxPooling(k=3)(conv_2)\n    maxpool_3 = KMaxPooling(k=3)(conv_3)\n\n    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode=\'concat\', concat_axis=1)\n    output = Dropout(drop)(merged_tensor)\n    output = Dense(units=144, activation=\'relu\')(output)\n    output = Dense(units=out_size, activation=\'sigmoid\')(output)\n\n    model = Model(inputs=comment_input, outputs=output)\n    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-7)\n    model.compile(loss=\'binary_crossentropy\', optimizer=adam_optimizer, metrics=[\'accuracy\'])\n    model.summary()\n    return model\n\n\ndef get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    recurrent_units = 64\n    filter1_nums = 128\n\n    input_layer = Input(shape=(max_sequence_length,))\n    embedding_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)(input_layer)\n\n    embedding_layer = SpatialDropout1D(0.2)(embedding_layer)\n    rnn_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n\n    #conv_1 = Conv1D(filter1_nums, 1, kernel_initializer=""uniform"", padding=""valid"", activation=""relu"", strides=1)(rnn_1)\n    #maxpool = GlobalMaxPooling1D()(conv_1)\n    #attn = AttentionWeightedAverage()(conv_1)\n    #average = GlobalAveragePooling1D()(conv_1)\n\n    conv_2 = Conv1D(filter1_nums, 2, kernel_initializer=""normal"", padding=""valid"", activation=""relu"", strides=1)(rnn_1)\n    maxpool = GlobalMaxPooling1D()(conv_2)\n    attn = AttentionWeightedAverage()(conv_2)\n    average = GlobalAveragePooling1D()(conv_2)\n\n    concatenated = concatenate([maxpool, attn, average], axis=1)\n    x = Dropout(0.5)(concatenated)\n    x = Dense(120, activation=""relu"")(x)\n    output_layer = Dense(out_size, activation=""sigmoid"")(x)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n    adam_optimizer = optimizers.Adam(lr=1e-3, clipvalue=5, decay=1e-5)\n    model.compile(loss=\'binary_crossentropy\', optimizer=adam_optimizer, metrics=[\'accuracy\'])\n    model.summary()\n    return model\n\n\ndef get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    recurrent_units = 60\n    input_layer = Input(shape=(max_sequence_length,))\n    embedding_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)(input_layer)\n    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n\n    rnn_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n    rnn_2 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_1)\n    x = concatenate([rnn_1, rnn_2], axis=2)\n\n    last = Lambda(lambda t: t[:, -1], name=\'last\')(x)\n    maxpool = GlobalMaxPooling1D()(x)\n    attn = AttentionWeightedAverage()(x)\n    average = GlobalAveragePooling1D()(x)\n\n    all_views = concatenate([last, maxpool, average, attn], axis=1)\n    x = Dropout(0.5)(all_views)\n    x = Dense(144, activation=""relu"")(x)\n    output_layer = Dense(out_size, activation=""sigmoid"")(x)\n    model = Model(inputs=input_layer, outputs=output_layer)\n    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n    model.compile(loss=\'binary_crossentropy\', optimizer=adam_optimizer, metrics=[\'accuracy\'])\n    model.summary()\n    return model\n\n\ndef get_dropout_bi_gru(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    recurrent_units = 64\n    input_layer = Input(shape=(max_sequence_length,))\n    embedding_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)(input_layer)\n    embedding_layer = SpatialDropout1D(0.15)(embedding_layer)\n    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n    x = Dropout(0.35)(x)\n    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(x)\n\n    last = Lambda(lambda t: t[:, -1])(x)\n    maxpool = GlobalMaxPooling1D()(x)\n    average = GlobalAveragePooling1D()(x)\n    concatenated = concatenate([last, maxpool, average], axis=1)\n\n    x = Dropout(0.5)(concatenated)\n    x = Dense(72, activation=""relu"")(x)\n    output_layer = Dense(out_size, activation=""sigmoid"")(x)\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(loss=\'binary_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\ndef get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    recurrent_units = 56\n    input_layer = Input(shape=(max_sequence_length,), name=\'Onehot\')\n    input_layer_2 = Input(shape=(max_sequence_length,), name=\'POS\')\n\n    word_layer = Embedding(nb_words,\n                                embedding_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)(input_layer)\n    pos_layer = Embedding(50, 36,\n                         input_length=max_sequence_length,\n                         trainable=True)(input_layer_2)\n    embedding_layer = concatenate([word_layer, pos_layer], axis=2)\n    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n\n    r1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n    r1 = SpatialDropout1D(0.3)(r1)\n    r2 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(r1)\n\n    last = Lambda(lambda t: t[:, -1], name=\'last\')(r2)\n    maxpool = GlobalMaxPooling1D()(r2)\n    attn = AttentionWeightedAverage()(r2)\n    average = GlobalAveragePooling1D()(r2)\n\n    concatenated = concatenate([last, maxpool, attn, average], axis=1)\n    x = Dropout(0.5)(concatenated)\n    x = Dense(128, activation=""relu"")(x)\n    x = Dropout(0.25)(x)\n    output_layer = Dense(out_size, activation=""sigmoid"")(x)\n\n    model = Model(inputs=[input_layer, input_layer_2], outputs=output_layer)\n    adam_optimizer = optimizers.Adam(lr=1e-3, clipvalue=5, decay=1e-7)\n    model.compile(loss=\'binary_crossentropy\',\n                  optimizer=adam_optimizer,\n                  metrics=[\'accuracy\'])\n    return model\n'"
sotoxic/models/pytorch/__init__.py,0,b''
sotoxic/models/pytorch/attention.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass DotAttention(nn.Module):\n\n    def __init__(self, hidden_size):\n        super(DotAttention, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.attn_vector = nn.Parameter(\n            torch.Tensor(1, hidden_size), requires_grad=True)\n\n        init.xavier_uniform(self.attn_vector.data)\n\n    def get_mask(self):\n        pass\n\n    def forward(self, inputs, lengths=None):\n        batch_size, max_len = inputs.size()[:2]\n        \'\'\'\n        print(""INPUTS"", inputs.size())\n        print(""ATTN"", self.attn_vector  # (1, hidden_size)\n                            .unsqueeze(0)  # (1, hidden_size, 1)\n                            .transpose(2, 1)\n                            .repeat(batch_size, 1, 1).size())\'\'\'\n        # apply attention layer\n        weights = torch.bmm(inputs,\n                            self.attn_vector  # (1, hidden_size)\n                            .unsqueeze(0)  # (1, 1, hidden_size)\n                            .transpose(2, 1) # (1, hidden_size, 1)\n                            .repeat(batch_size, 1, 1)) # (batch_size, hidden_size, 1))\n\n        attn_energies = F.softmax(F.relu(weights.squeeze()))\n\n        # create mask based on the sentence lengths\n        #idxes = torch.arange(0, max_len, out=torch.LongTensor(max_len)).unsqueeze(0).cuda()  # some day, you\'ll be able to directly do this on cuda\n        #mask = Variable((idxes < lengths.data.unsqueeze(1)).float())\n\n        # apply mask and renormalize attention scores (weights)\n        #masked = attn_weights * mask\n        _sums = attn_energies.sum(-1).unsqueeze(1).expand_as(attn_energies)  # sums per row\n        attn_weights = attn_energies / _sums\n\n        # apply attention weights\n        weighted = torch.mul(inputs, attn_weights.unsqueeze(-1).expand_as(inputs))\n\n        # get the final fixed vector representations of the sentences\n        representations = weighted.sum(1).squeeze()\n\n        return representations, attn_weights'"
sotoxic/models/pytorch/bgru.py,4,"b""import importlib\nfrom sotoxic.models.pytorch import gru\nfrom sotoxic.models.pytorch import attention\nimportlib.reload(gru)\nimportlib.reload(attention)\n\nimport torch\nfrom collections import OrderedDict\nfrom torch import nn\n\nimport importlib\n\nfrom sotoxic.models import torch_base\nfrom sotoxic.models.pytorch import gru\nfrom sotoxic.models.pytorch import attention\nimportlib.reload(gru)\nimportlib.reload(attention)\n\n\nclass GRUClassifier(torch_base.BaseModel):\n\n    def __init__(self, input_size, hidden_size, embedding):\n        super(GRUClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n                          batch_first=True, num_layers=2, dropout=0.35, bidirectional=True)\n        self.attn = attention.DotAttention(hidden_size=2*hidden_size)\n\n        self.classifier = nn.Sequential(\n            OrderedDict([\n                ('gru_dropout', nn.Dropout(0.5)),\n                ('h1', nn.Linear(self.hidden_size * 6, 108)),\n                ('relu1', nn.ReLU()),\n                ('out', nn.Linear(108, 6)),\n            ]))\n\n    def set_dropout(self, ratio1):\n        pass\n\n    def forward(self, _input, hidden=None, lengths=None):\n        _input, lengths = _input\n        embedded = self.embedding(_input)\n\n        out, _ = self.gru(embedded)\n        last = out[:, -1, :]\n        attn, _ = self.attn.forward(out)\n        max, _ = torch.max(out, dim=1)\n        concatenated = torch.cat([last, max, attn], dim=1)\n        result = self.classifier(concatenated)\n        return result\n\n\nclass BayesianGRUClassifier(torch_base.BaseModel):\n\n    def __init__(self, input_size, hidden_size, embedding):\n        super(BayesianGRUClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        self.embedding = embedding\n        #self.embedding_dropout = nn.Dropout(0.15)\n\n        self.gru_1 = gru.BiBayesianGRU(input_size=input_size, hidden_size=hidden_size, dropout=0.3)\n        self.gru_2 = gru.BiBayesianGRU(input_size=2 * hidden_size, hidden_size=hidden_size, dropout=0.3)\n        #self.attn = attention.Attention(attention_size=2 * hidden_size)\n\n        self.classifier = nn.Sequential(\n            OrderedDict([\n                ('gru_dropout', nn.Dropout(0.5)),\n                ('h1', nn.Linear(self.hidden_size * 4, 72)),\n                ('relu1', nn.ReLU()),\n                ('out', nn.Linear(72, 6)),\n            ]))\n\n    def set_dropout(self, ratio1):\n        self.gru_1.set_dropout(ratio1)\n        self.gru_2.set_dropout(ratio1)\n\n    def forward(self, _input, hidden=None, lengths=None):\n        _input, lengths = _input\n        embedded = self.embedding(_input)\n\n        out1, _ = self.gru_1.forward(embedded, lengths=lengths)\n        out2, _ = self.gru_2.forward(out1, lengths=lengths)\n\n        last = out2[:, -1, :]\n        #attn, _ = self.attn(out2, lengths)\n        max, _ = torch.max(out2, dim=1)\n\n        concatenated = torch.cat([last, max], dim=1)\n\n        result = self.classifier(concatenated)\n        return result\n"""
sotoxic/models/pytorch/dropout.py,10,"b'import torch\nimport numpy as np\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom itertools import repeat\n\nclass EmbeddingDropout():\n\n    def __init__(self, p=0.5):\n        super(EmbeddingDropout, self).__init__()\n        if p < 0 or p > 1:\n            raise ValueError(""dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p))\n        self.p = p\n        self.training = True\n\n    def forward(self, input):\n        # input must be tensor\n        if self.p > 0 and self.training:\n            dim = input.dim()\n            if dim == 1:\n                input = input.view(1, -1)\n            batch_size = input.size(0)\n            for i in range(batch_size):\n                x = np.unique(input[i].numpy())\n                x = np.nonzero(x)[0]\n                if len(x) == 0:\n                    return input\n                x = torch.from_numpy(x)\n                noise = x.new().resize_as_(x)\n                noise.bernoulli_(self.p)\n                x = x.mul(noise)\n                for value in x:\n                    if value > 0:\n                        mask = input[i].eq(value)\n                        input[i].masked_fill_(mask, 0)\n            if dim == 1:\n                input = input.view(-1)\n\n        return input\n\n\nclass SequentialDropout(nn.Module):\n\n    def __init__(self, p=0.5):\n        super(SequentialDropout, self).__init__()\n        if p < 0 or p > 1:\n            raise ValueError(""dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p))\n        self.p = p\n        self.restart = True\n\n    def _make_noise(self, input):\n        return Variable(input.data.new().resize_as_(input.data))\n\n    def forward(self, input):\n        if self.p > 0 and self.training:\n            if self.restart:\n                self.noise = self._make_noise(input)\n                self.noise.data.bernoulli_(1 - self.p).div_(1 - self.p)\n                if self.p == 1:\n                    self.noise.data.fill_(0)\n                self.noise = self.noise.expand_as(input)\n                self.restart = False\n            return input.mul(self.noise)\n\n        return input\n\n    def end_of_sequence(self):\n        self.restart = True\n\n    def backward(self, grad_output):\n        self.end_of_sequence()\n        if self.p > 0 and self.training:\n            return grad_output.mul(self.noise)\n        else:\n            return grad_output\n\n    def __repr__(self):\n        return type(self).__name__ + \'({:.4f})\'.format(self.p)\n\nif __name__ == \'__main__\':\n\n    dp = SequentialDropout(p=0.5)\n    input = Variable(torch.ones(1,10), volatile=True)\n\n    dist_total = torch.zeros(1)\n    output_last = dp(input)\n    for i in range(50):\n        output_new = dp(input)\n        dist_total += torch.dist(output_new, output_last).data\n        output_last = output_new\n    \n    if not torch.equal(dist_total, torch.zeros(1)):\n        print(\'Error\')\n        print(dist_total)\n\n    dp.end_of_sequence()\n\n    dist_total = torch.zeros(1)\n    for i in range(50):\n        dist_total += torch.dist(output_last, dp(input)).data\n        dp.end_of_sequence()\n\n    if torch.equal(dist_total, torch.zeros(1)):\n        print(\'Error\')\n\n    ####\n\n    dp = EmbeddingDropout(p=0.15)\n    input = torch.Tensor([[1,2,3,0,0],[5,3,2,2,0]]).long()\n    print(input)\n    print(dp.forward(input))'"
sotoxic/models/pytorch/gru.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom sotoxic.models.pytorch.dropout import SequentialDropout\n\n\nclass AbstractGRUCell(nn.Module):\n\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False):\n        super(AbstractGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias_ih = bias_ih\n        self.bias_hh = bias_hh\n\n        # Modules\n        self.weight_ir = nn.Linear(input_size, hidden_size, bias=bias_ih)\n        self.weight_ii = nn.Linear(input_size, hidden_size, bias=bias_ih)\n        self.weight_in = nn.Linear(input_size, hidden_size, bias=bias_ih)\n        self.weight_hr = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n        self.weight_hi = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n        self.weight_hn = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n\n    def forward(self, x, hx=None):\n        raise NotImplementedError\n\n\nclass GRUCell(AbstractGRUCell):\n\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False):\n        super(GRUCell, self).__init__(input_size, hidden_size,\n                                      bias_ih, bias_hh)\n\n    def forward(self, x, hx=None):\n        if hx is None:\n            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n        r = F.sigmoid(self.weight_ir(x) + self.weight_hr(hx))\n        i = F.sigmoid(self.weight_ii(x) + self.weight_hi(hx))\n        n = F.tanh(self.weight_in(x) + r * self.weight_hn(hx))\n        hx = (1 - i) * n + i * hx\n        return hx\n\n\nclass BayesianGRUCell(AbstractGRUCell):\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False,\n                       dropout=0.25):\n        super(BayesianGRUCell, self).__init__(input_size, hidden_size,\n                                          bias_ih, bias_hh)\n        self.set_dropout(dropout)\n\n    def set_dropout(self, dropout):\n        self.dropout = dropout\n        self.drop_ir = SequentialDropout(p=dropout)\n        self.drop_ii = SequentialDropout(p=dropout)\n        self.drop_in = SequentialDropout(p=dropout)\n        self.drop_hr = SequentialDropout(p=dropout)\n        self.drop_hi = SequentialDropout(p=dropout)\n        self.drop_hn = SequentialDropout(p=dropout)\n\n    def end_of_sequence(self):\n        self.drop_ir.end_of_sequence()\n        self.drop_ii.end_of_sequence()\n        self.drop_in.end_of_sequence()\n        self.drop_hr.end_of_sequence()\n        self.drop_hi.end_of_sequence()\n        self.drop_hn.end_of_sequence()\n\n    def forward(self, x, hx=None):\n        if hx is None:\n            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n        x_ir = self.drop_ir(x)\n        x_ii = self.drop_ii(x)\n        x_in = self.drop_in(x)\n        x_hr = self.drop_hr(hx)\n        x_hi = self.drop_hi(hx)\n        x_hn = self.drop_hn(hx)\n        r = F.sigmoid(self.weight_ir(x_ir) + self.weight_hr(x_hr))\n        i = F.sigmoid(self.weight_ii(x_ii) + self.weight_hi(x_hi))\n        n = F.tanh(self.weight_in(x_in) + r * self.weight_hn(x_hn))\n        hx = (1 - i) * n + i * hx\n        return hx\n\n\nclass AbstractGRU(nn.Module):\n\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False):\n        super(AbstractGRU, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias_ih = bias_ih\n        self.bias_hh = bias_hh\n        self._load_gru_cell()\n\n    def _load_gru_cell(self):\n        raise NotImplementedError\n\n    def forward(self, x, hx=None, max_length=None):\n        batch_size = x.size(0)\n        seq_length = x.size(1)\n        if max_length is None:\n            max_length = seq_length\n        output = []\n        for i in range(max_length):\n            hx = self.gru_cell(x[:,i,:], hx=hx)\n            output.append(hx.view(batch_size, 1, self.hidden_size))\n        output = torch.cat(output, 1)\n        return output, hx\n\n\nclass GRU(AbstractGRU):\n\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False):\n        super(GRU, self).__init__(input_size, hidden_size,\n                                          bias_ih, bias_hh)\n\n    def _load_gru_cell(self):\n        self.gru_cell = GRUCell(self.input_size, self.hidden_size,\n                                self.bias_ih, self.bias_hh)\n\n\nclass BiBayesianGRU(AbstractGRU):\n\n    def __init__(self, input_size, hidden_size,\n                       bias_ih=True, bias_hh=False,\n                       dropout=0.25):\n        self.dropout = dropout\n        self.hidden_size = hidden_size\n        super(BiBayesianGRU, self).__init__(input_size, hidden_size,\n                                          bias_ih, bias_hh)\n\n    def _load_gru_cell(self):\n        self.gru_cell = BayesianGRUCell(self.input_size, self.hidden_size,\n                                        self.bias_ih, self.bias_hh,\n                                        dropout=self.dropout)\n\n    def set_dropout(self, dropout):\n        self.dropout = dropout\n        self.gru_cell.set_dropout(dropout)\n\n    def init_hidden(self, batch_size):\n        return Variable(torch.zeros(1, batch_size, self.hidden_size)).cuda()\n\n    def forward(self, x, hx=None, max_length=None, lengths=None):\n        batch_size = x.size(0)\n        seq_length = x.size(1)\n        if max_length is None:\n            max_length = seq_length\n        lefts = []\n        rights = []\n\n        \'\'\'\n        max len = 8\n        A C E P P P P P l=3\n        B D F G P P P P l=4\n        \'\'\'\n\n        # left part\n        lhx = self.init_hidden(batch_size)\n        for time in range(max_length):\n            new_hx = self.gru_cell(x[:, time, :], hx=lhx)\n            #print(""X"", x[:, time, :])\n            #print(""H"", new_hx)\n            mask = (time < lengths).float().unsqueeze(1).expand_as(new_hx)\n            #print(""M"", mask)\n            lhx = new_hx * mask + lhx * (1 - mask) # for mask the padding dynamically\n            lefts.append(lhx.view(batch_size, 1, self.hidden_size))\n        self.gru_cell.end_of_sequence()\n        lefts = torch.cat(lefts, 1)\n\n\n        # right part\n        rhx = self.init_hidden(batch_size)\n        for time in range(max_length - 1, -1, -1):\n            new_hx = self.gru_cell(x[:, time, :], hx=rhx)\n            mask = (time < lengths).float().unsqueeze(1).expand_as(new_hx)\n            rhx = new_hx * mask + rhx * (1 - mask)\n            rights.append(rhx.view(batch_size, 1, self.hidden_size))\n        self.gru_cell.end_of_sequence()\n        rights = torch.cat(rights, 1)\n\n\n        output = torch.cat((lefts, rights), dim=2)\n        return output, lhx\n\n'"
sotoxic/models/pytorch/rhn.py,7,"b""import torch\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom collections import OrderedDict\nfrom sotoxic.models.pytorch import dropout as dr\nfrom sotoxic.models.pytorch import attention\nimport importlib\n\nfrom sotoxic.models import torch_base\nimportlib.reload(attention)\n\n\nclass RecurrentHighwayClassifier(torch_base.BaseModel):\n\n    def __init__(self, input_size, hidden_size, recurrence_length, embedding, recurrent_dropout=0.3):\n        super(RecurrentHighwayClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.L = recurrence_length\n        self.recurrent_dropout = recurrent_dropout\n        self.highways = nn.ModuleList()\n        self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=True, recurrent_dropout=recurrent_dropout))\n\n        for _ in range(self.L - 1):\n            self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=False, recurrent_dropout=recurrent_dropout))\n\n        self.embedding = embedding\n\n        self.classifier = nn.Sequential(\n            OrderedDict([\n                ('h1_dropout', nn.Dropout(0.5)),\n                ('h1', nn.Linear(self.hidden_size * 4, 74)),\n                ('relu1', nn.ReLU()),\n                ('out', nn.Linear(74, 6)),\n            ]))\n\n    def init_state(self, batch_size):\n        hidden = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n        return hidden\n\n    def train_mode(self, prob):\n        self.set_dropout(prob)\n        self.train()\n\n    def set_dropout(self, p):\n        for rhn_cell in self.highways:\n            rhn_cell.set_dropout(p)\n\n    def eval_mode(self):\n        self.eval()\n\n    def forward(self, _input, hidden=None, lengths=None):\n        _input, lengths = _input\n        batch_size = _input.size(0)\n        max_time = _input.size(1)\n\n        if hidden is None:\n            hidden = self.init_state(batch_size)\n        embed_batch = self.embedding(_input)\n\n        lefts = []\n        rights = []\n\n        for time in range(max_time):\n            for tick in range(self.L):\n                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n                hidden = next_hidden * mask + hidden * (1 - mask)  # for mask the padding dynamically\n            lefts.append(hidden.unsqueeze(1))\n        lefts = torch.cat(lefts, 1)\n\n        for rhn_cell in self.highways:\n            rhn_cell.end_of_sequence()\n\n        for time in range(max_time):\n            for tick in range(self.L):\n                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n                hidden = next_hidden * mask + hidden * (1 - mask)  # for mask the padding dynamically\n            rights.append(hidden.unsqueeze(1))\n        rights = torch.cat(rights, 1)\n\n        for rhn_cell in self.highways:\n            rhn_cell.end_of_sequence()\n\n        outputs = torch.cat((lefts, rights), dim=2)\n\n        last = outputs[:, -1, :]\n        max, _ = torch.max(outputs, dim=1)\n\n        concatenated = torch.cat([last, max], dim=1)\n        result = self.classifier(concatenated)\n        return result\n\n\nclass RHNCell(nn.Module):\n\n    def __init__(self, input_size, hidden_size, is_first_layer, recurrent_dropout):\n        super(RHNCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.is_first_layer = is_first_layer\n\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        self.set_dropout(recurrent_dropout)\n\n        # input weight matrices\n        if self.is_first_layer:\n            self.W_H = nn.Linear(input_size, hidden_size)\n            self.W_C = nn.Linear(input_size, hidden_size)\n\n        # hidden weight matrices\n        self.R_H = nn.Linear(hidden_size, hidden_size, bias=True)\n        self.R_C = nn.Linear(hidden_size, hidden_size, bias=True)\n\n    def set_dropout(self, dropout):\n        self.dropout = dropout\n        self.drop_ir = dr.SequentialDropout(p=dropout)\n        self.drop_ii = dr.SequentialDropout(p=dropout)\n        self.drop_hr = dr.SequentialDropout(p=dropout)\n        self.drop_hi = dr.SequentialDropout(p=dropout)\n\n    def end_of_sequence(self):\n        self.drop_ir.end_of_sequence()\n        self.drop_ii.end_of_sequence()\n        self.drop_hr.end_of_sequence()\n        self.drop_hi.end_of_sequence()\n\n    def forward(self, _input, prev_hidden):\n        c_i = self.drop_hr(prev_hidden)\n        h_i = self.drop_hi(prev_hidden)\n\n        if self.is_first_layer:\n            x_i = self.drop_ii(_input)\n            x_r = self.drop_ir(_input)\n            hl = self.tanh(self.W_H(x_i) + self.R_H(h_i))\n            tl = self.sigmoid(self.W_C(x_r) + self.R_C(c_i))\n        else:\n            hl = self.tanh(self.R_H(h_i))\n            tl = self.sigmoid(self.R_C(c_i))\n\n        h = (hl * tl) + (prev_hidden * (1 - tl))\n        return h\n"""
sotoxic/models/tensorflow/__init__.py,0,b''
