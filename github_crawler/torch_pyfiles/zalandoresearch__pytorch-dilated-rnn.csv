file_path,api_count,code
__init__.py,0,b''
classify.py,7,"b'import torch\nimport torchvision\nimport torch.autograd as autograd\nimport torch.utils.data as Data\nimport torch.optim as optim\nimport torch.nn as nn\nimport drnn\n\n\nuse_cuda = torch.cuda.is_available()\n\n\nclass Classifier(nn.Module):\n\n    def __init__(self, n_inputs, n_hidden, n_layers, n_classes, cell_type=""GRU""):\n        super(Classifier, self).__init__()\n\n        self.drnn = drnn.DRNN(n_inputs, n_hidden, n_layers, dropout=0, cell_type=cell_type)\n        self.linear = nn.Linear(n_hidden, n_classes)\n\n    def forward(self, inputs):\n        layer_outputs, _ = self.drnn(inputs)\n        pred = self.linear(layer_outputs[-1])\n\n        return pred\n\n\nif __name__ == \'__main__\':\n\n    pixel_wise = False\n    data_dir = \'.MNIST_data\'\n    n_classes = 10\n\n    cell_type = ""GRU""\n    n_hidden = 20\n    if pixel_wise:\n        n_layers = 9\n    else:\n        n_layers = 4\n\n    batch_size = 128\n    learning_rate = 1.0e-3\n    training_iters = 30000\n    display_step = 25\n\n    train_data = torchvision.datasets.MNIST(root=data_dir,\n                                            train=True,\n                                            transform=torchvision.transforms.ToTensor(),\n                                            download=True)\n\n\n    test_data = torchvision.datasets.MNIST(root=data_dir,\n                                           train = False)\n\n    test_x = test_data.type(torch.FloatTensor)[:2000] / 255.0\n    if pixel_wise:\n        test_x = test_x.view(test_x.size(0), 784).unsqueeze(2).transpose(1, 0)\n    else:\n        test_x = test_x.transpose(1, 0)\n\n    test_y = test_data.test_labels[:2000]\n\n    if use_cuda:\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n\n    train_loader = Data.DataLoader(train_data, batch_size, shuffle=False, num_workers=1)\n\n    print(""==> Building a dRNN with %s cells"" %cell_type)\n\n    if pixel_wise:\n        model = Classifier(1, n_hidden, n_layers, n_classes, cell_type=cell_type)\n    else:\n        model = Classifier(28, n_hidden, n_layers, n_classes, cell_type=cell_type)\n\n    if use_cuda:\n        model.cuda()\n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    for iter in range(training_iters):\n        for step, (batch_x, batch_y) in enumerate(train_loader):\n            batch_x = batch_x.view(-1, 28, 28)\n\n            if pixel_wise:\n                batch_x = batch_x.view(batch_size, 784).unsqueeze(2).transpose(1, 0)\n            else:\n                batch_x = batch_x.transpose(1, 0)\n\n            if use_cuda:\n                batch_x = batch_x.cuda()\n                batch_y = batch_y.cuda()\n\n            optimizer.zero_grad()\n\n            pred = model.forward(batch_x)\n\n            loss = criterion(pred, batch_y)\n\n            loss.backward()\n            optimizer.step()\n\n            if (step + 1) % display_step == 0:\n                print(""Iter "" + str(iter + 1) + "", Step "" + str(step+1) +"", Average Loss: "" + ""{:.6f}"".format(loss.data[0]))\n\n        test_output = model.forward(test_x)\n        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n        accuracy = sum(pred_y == test_y) / float(test_y.size(0))\n\n        print(""========> Validation Accuracy: {:.6f}"".format(accuracy))\n\n    print(""end"")\n\n\n'"
drnn.py,8,"b'import torch\nimport torch.nn as nn\n\n\nuse_cuda = torch.cuda.is_available()\n\n\nclass DRNN(nn.Module):\n\n    def __init__(self, n_input, n_hidden, n_layers, dropout=0, cell_type=\'GRU\', batch_first=False):\n        super(DRNN, self).__init__()\n\n        self.dilations = [2 ** i for i in range(n_layers)]\n        self.cell_type = cell_type\n        self.batch_first = batch_first\n\n        layers = []\n        if self.cell_type == ""GRU"":\n            cell = nn.GRU\n        elif self.cell_type == ""RNN"":\n            cell = nn.RNN\n        elif self.cell_type == ""LSTM"":\n            cell = nn.LSTM\n        else:\n            raise NotImplementedError\n\n        for i in range(n_layers):\n            if i == 0:\n                c = cell(n_input, n_hidden, dropout=dropout)\n            else:\n                c = cell(n_hidden, n_hidden, dropout=dropout)\n            layers.append(c)\n        self.cells = nn.Sequential(*layers)\n\n    def forward(self, inputs, hidden=None):\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        outputs = []\n        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n            if hidden is None:\n                inputs, _ = self.drnn_layer(cell, inputs, dilation)\n            else:\n                inputs, hidden[i] = self.drnn_layer(cell, inputs, dilation, hidden[i])\n\n            outputs.append(inputs[-dilation:])\n\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        return inputs, outputs\n\n    def drnn_layer(self, cell, inputs, rate, hidden=None):\n        n_steps = len(inputs)\n        batch_size = inputs[0].size(0)\n        hidden_size = cell.hidden_size\n\n        inputs, _ = self._pad_inputs(inputs, n_steps, rate)\n        dilated_inputs = self._prepare_inputs(inputs, rate)\n\n        if hidden is None:\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n        else:\n            hidden = self._prepare_inputs(hidden, rate)\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size, hidden=hidden)\n\n        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n\n        return outputs, hidden\n\n    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n        if hidden is None:\n            if self.cell_type == \'LSTM\':\n                c, m = self.init_hidden(batch_size * rate, hidden_size)\n                hidden = (c.unsqueeze(0), m.unsqueeze(0))\n            else:\n                hidden = self.init_hidden(batch_size * rate, hidden_size).unsqueeze(0)\n\n        dilated_outputs, hidden = cell(dilated_inputs, hidden)\n\n        return dilated_outputs, hidden\n\n    def _unpad_outputs(self, splitted_outputs, n_steps):\n        return splitted_outputs[:n_steps]\n\n    def _split_outputs(self, dilated_outputs, rate):\n        batchsize = dilated_outputs.size(1) // rate\n\n        blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n\n        interleaved = torch.stack((blocks)).transpose(1, 0).contiguous()\n        interleaved = interleaved.view(dilated_outputs.size(0) * rate,\n                                       batchsize,\n                                       dilated_outputs.size(2))\n        return interleaved\n\n    def _pad_inputs(self, inputs, n_steps, rate):\n        is_even = (n_steps % rate) == 0\n\n        if not is_even:\n            dilated_steps = n_steps // rate + 1\n\n            zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n                                 inputs.size(1),\n                                 inputs.size(2))\n            if use_cuda:\n                zeros_ = zeros_.cuda()\n\n            inputs = torch.cat((inputs, zeros_))\n        else:\n            dilated_steps = n_steps // rate\n\n        return inputs, dilated_steps\n\n    def _prepare_inputs(self, inputs, rate):\n        dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n        return dilated_inputs\n\n    def init_hidden(self, batch_size, hidden_dim):\n        hidden = torch.zeros(batch_size, hidden_dim)\n        if use_cuda:\n            hidden = hidden.cuda()\n        if self.cell_type == ""LSTM"":\n            memory = torch.zeros(batch_size, hidden_dim)\n            if use_cuda:\n                memory = memory.cuda()\n            return (hidden, memory)\n        else:\n            return hidden\n'"
lm.py,10,"b'import argparse\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport os\nimport time\nimport drnn\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'./coco/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'DRNN\',\n                    help=\'type of recurrent net\')\nparser.add_argument(\'--emsize\', type=int, default=100,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=64,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=4,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=1,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=40,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=100, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.2,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--tied\', action=\'store_true\',\n                    help=\'tie the word embedding and softmax weights\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nparser.add_argument(\'--save\', type=str,  default=\'./model.pt\',\n                    help=\'path to save the final model\')\n\n\nargs = parser.parse_args()\n\n\nuse_cuda = torch.cuda.is_available()\nif use_cuda:\n    print(""="" * 2 + ""> USING CUDA!!!"")\n\n\nclass RNNModel(nn.Module):\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if rnn_type in [\'LSTM\', \'GRU\']:\n            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n        elif rnn_type == \'DRNN\':\n            self.rnn = drnn.DRNN(ninp, nhid, nlayers, 0, \'GRU\')\n\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n            self.decoder.weight = self.encoder.weight\n        try:\n            self.init_weights()\n        except AttributeError:\n            pass\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == \'LSTM\':\n            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n        else:\n            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'))\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'))\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'))\n\n    def tokenize(self, path):\n        assert os.path.exists(path)\n\n        with open(path, \'r\') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        with open(path, \'r\') as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for i, line in enumerate(f):\n                words = line.split() + [\'<eos>\']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids\n\n\ndef batchify(data, bsz, cuda):\n    nbatch = data.size(0) // bsz\n    data = data.narrow(0, 0, nbatch * bsz)\n    data = data.view(bsz, -1).t().contiguous()\n    if cuda:\n        data = data.cuda()\n    return data\n\n\ndef get_batch(source, i, bptt, evaluation=False):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = Variable(source[i:i+seq_len], volatile=evaluation)\n    target = Variable(source[i+1:i+1+seq_len].view(-1))\n    return data, target\n\n\ndef repackage_hidden(h):\n    if type(h) == Variable:\n        return Variable(h.data)\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\ndef evaluate(data_source):\n    lm.eval()\n    total_loss = 0\n    ntokens = len(corpus.dictionary)\n\n    if args.model == \'DRNN\':\n        hidden = [Variable(torch.zeros(2 ** i, eval_batch_size, args.nhid))\n                  for i in range(args.nlayers)]\n        if use_cuda:\n            hidden = [x.cuda() for x in hidden]\n\n    else:\n        hidden = lm.init_hidden(eval_batch_size)\n\n    iter_ = range(0, data_source.size(0) - 1, args.bptt)\n    for i in iter_[:-1]:\n        dat_, targets = get_batch(data_source, i, args.bptt, evaluation=True)\n        output, hidden = lm(dat_, hidden)\n        output_flat = output.view(-1, ntokens)\n        total_loss += len(dat_) * criterion(output_flat, targets).data\n        hidden = repackage_hidden(hidden)\n    return total_loss[0] / len(data_source)\n\n\ndef train(lm, train_data):\n    lm.train()\n\n    total_loss = 0\n    start_time = time.time()\n\n    if args.model == \'DRNN\':\n        hidden = [Variable(torch.zeros(2 ** i, args.batch_size, args.nhid))\n                  for i in range(args.nlayers)]\n        if use_cuda:\n            hidden = [x.cuda() for x in hidden]\n\n    else:\n        hidden = lm.init_hidden(args.batch_size)\n\n    iter_ = range(0, train_data.size(0) - 1, args.bptt)\n    for batch, i in enumerate(iter_[:-1]):\n\n        dat_, targets = get_batch(train_data, i, args.bptt)\n\n        hidden = repackage_hidden(hidden)\n        lm.zero_grad()\n\n        output, hidden = lm(dat_, hidden)\n        loss = criterion(output.view(-1, len(corpus.dictionary)), targets)\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm(lm.parameters(), args.clip)\n        for p in lm.parameters():\n            p.data.add_(-lr, p.grad.data)\n\n        total_loss += loss.data\n\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss[0] / args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | \'\n                    \'loss {:5.2f} | ppl {:8.2f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, lr,\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\n\n\nif __name__ == \'__main__\':\n\n    torch.manual_seed(args.seed)\n\n    print(""getting data..."")\n    corpus = Corpus(args.data)\n\n    eval_batch_size = 10\n\n    print(""batching..."")\n\n    stops = [i for i in range(len(corpus.train))\n             if corpus.train[i] == corpus.dictionary.word2idx[""<eos>""]]\n\n    train_data = batchify(corpus.train, args.batch_size, use_cuda)\n    valid_data = batchify(corpus.valid, eval_batch_size, use_cuda)\n    test_data = batchify(corpus.test, eval_batch_size, use_cuda)\n\n    print(""getting model..."")\n\n    ntokens = len(corpus.dictionary)\n    lm = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)\n\n    if use_cuda:\n        lm.cuda()\n\n    criterion = nn.CrossEntropyLoss()\n\n    lr = args.lr\n    best_val_loss = None\n\n    print(""training..."")\n    try:\n        for epoch in range(1, args.epochs + 1):\n            epoch_start_time = time.time()\n            train(lm, train_data)\n\n            val_loss = evaluate(valid_data)\n\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                  \'valid ppl {:8.2f}\'.format(epoch, (time.time() - epoch_start_time),\n                                             val_loss, math.exp(val_loss)))\n            print(\'-\' * 89)\n\n            if not best_val_loss or val_loss < best_val_loss:\n                with open(args.save, \'wb\') as f:\n                    torch.save(lm, f)\n                best_val_loss = val_loss\n            else:\n                lr /= 4.0\n\n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early\')\n\n    with open(args.save, \'rb\') as f:\n        lm = torch.load(f)\n\n    test_loss = evaluate(test_data)\n\n    print(\'=\' * 89)\n    print(\'| End of training | test loss {:5.2f} | test ppl {:8.2f}\'.format(\n        test_loss, math.exp(test_loss)))\n    print(\'=\' * 89)\n'"
tests.py,5,"b""import unittest\nimport drnn\nimport torch\n\n\nclass TestForward(unittest.TestCase):\n    def test(self):\n        model = drnn.DRNN(10, 10, 4, 0, 'GRU')\n\n        x = torch.randn(23, 3, 10)\n        out = model(x)[0]\n\n        self.assertTrue(out.size(0) == 23)\n        self.assertTrue(out.size(1) == 3)\n        self.assertTrue(out.size(2) == 10)\n\n\nclass TestReshape(unittest.TestCase):\n    def test(self):\n        model = drnn.DRNN(10, 10, 4, 0, 'GRU')\n\n        x = torch.randn(24, 3, 10)\n\n        split_x = model._prepare_inputs(x, 2)\n\n        second_block = x[1::2]\n        check = split_x[:, x.size(1):, :]\n\n        self.assertTrue((second_block == check).all())\n\n        unsplit_x = model._split_outputs(split_x, 2)\n\n        self.assertTrue((x == unsplit_x).all())\n\n\nclass TestHidden(unittest.TestCase):\n    def test(self):\n        model = drnn.DRNN(10, 10, 4, 0, 'GRU')\n\n        x = torch.randn(23, 3, 10)\n\n        hidden = model(x)[1]\n\n        self.assertEqual(len(hidden), 4)\n\n        for hid in hidden:\n            print(hid.size())\n\n\nclass TestPassHidden(unittest.TestCase):\n    def test(self):\n        model = drnn.DRNN(10, 10, 4, 0, 'GRU')\n\n        hidden = []\n        for i in range(4):\n            hidden.append(torch.randn(2 ** i, 3, 10))\n\n        x = torch.randn(24, 3, 10)\n        hidden = model(x, hidden)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
char_rnn/char_rnn_test.py,5,"b'import argparse\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom drnn import DRNN\nfrom char_rnn.utils import *\nfrom char_rnn.model import DRNN_Char\n\n\nimport warnings\nwarnings.filterwarnings(""ignore"")   # Suppress the RunTimeWarning on unicode\n\n\nparser = argparse.ArgumentParser(description=\'Sequence Modeling - Character Level Language Model\')\nparser.add_argument(\'--batch_size\', type=int, default=32, metavar=\'N\',\n                    help=\'batch size (default: 32)\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA (default: True)\')\nparser.add_argument(\'--dropout\', type=float, default=0.1,\n                    help=\'dropout applied to layers (default: 0.1)\')\nparser.add_argument(\'--emb_dropout\', type=float, default=0.1,\n                    help=\'dropout applied to the embedded layer (0 = no dropout) (default: 0.1)\')\nparser.add_argument(\'--clip\', type=float, default=0.15,\n                    help=\'gradient clip, -1 means no clip (default: 0.15)\')\nparser.add_argument(\'--epochs\', type=int, default=100,\n                    help=\'upper epoch limit (default: 100)\')\nparser.add_argument(\'--levels\', type=int, default=3,\n                    help=\'# of levels (default: 3)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'report interval (default: 100\')\nparser.add_argument(\'--lr\', type=float, default=4,\n                    help=\'initial learning rate (default: 4)\')\nparser.add_argument(\'--emsize\', type=int, default=100,\n                    help=\'dimension of character embeddings (default: 100)\')\nparser.add_argument(\'--optim\', type=str, default=\'SGD\',\n                    help=\'optimizer to use (default: SGD)\')\nparser.add_argument(\'--nhid\', type=int, default=450,\n                    help=\'number of hidden units per layer (default: 450)\')\nparser.add_argument(\'--validseqlen\', type=int, default=320,\n                    help=\'valid sequence length (default: 320)\')\nparser.add_argument(\'--seq_len\', type=int, default=400,\n                    help=\'total sequence length, including effective history (default: 400)\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed (default: 1111)\')\nparser.add_argument(\'--dataset\', type=str, default=\'ptb\',\n                    help=\'dataset to use (default: ptb)\')\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\n\nprint(args)\nfile, file_len, valfile, valfile_len, testfile, testfile_len, corpus = data_generator(args)\n\nn_characters = len(corpus.dict)\ntrain_data = batchify(char_tensor(corpus, file), args.batch_size, args)\nval_data = batchify(char_tensor(corpus, valfile), 1, args)\ntest_data = batchify(char_tensor(corpus, testfile), 1, args)\nprint(""Corpus size: "", n_characters)\n\n\nmodel = DRNN_Char(input_size=args.emsize,\n                  output_size=n_characters, \n                  hidden_size=args.nhid, \n                  num_layers=args.levels,\n                  dropout=args.dropout, \n                  emb_dropout=args.emb_dropout)\n\n\nif args.cuda:\n    model.cuda()\n\n\ncriterion = nn.CrossEntropyLoss()\nlr = args.lr\noptimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n\n\ndef evaluate(source):\n    model.eval()\n    total_loss = 0\n    count = 0\n    source_len = source.size(1)\n    for batch, i in enumerate(range(0, source_len - 1, args.validseqlen)):\n        if i + args.seq_len - args.validseqlen >= source_len:\n            continue\n        inp, target = get_batch(source, i, args)\n        output = model(inp)\n        eff_history = args.seq_len - args.validseqlen\n        final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n        final_target = target[:, eff_history:].contiguous().view(-1)\n        loss = criterion(final_output, final_target)\n\n        total_loss += loss.data * final_output.size(0)\n        count += final_output.size(0)\n\n    val_loss = total_loss[0] / count * 1.0\n    return val_loss\n\n\ndef train(epoch):\n    model.train()\n    total_loss = 0\n    start_time = time.time()\n    losses = []\n    source = train_data\n    source_len = source.size(1)\n    for batch_idx, i in enumerate(range(0, source_len - 1, args.validseqlen)):\n        if i + args.seq_len - args.validseqlen >= source_len:\n            continue\n        inp, target = get_batch(source, i, args)\n        optimizer.zero_grad()\n        output = model(inp)\n        eff_history = args.seq_len - args.validseqlen\n        final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n        final_target = target[:, eff_history:].contiguous().view(-1)\n        loss = criterion(final_output, final_target)\n        loss.backward()\n\n        if args.clip > 0:\n            torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n        optimizer.step()\n        total_loss += loss.data\n\n        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n            cur_loss = total_loss[0] / args.log_interval\n            losses.append(cur_loss)\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | \'\n                  \'loss {:5.3f} | bpc {:5.3f}\'.format(\n                epoch, batch_idx, int((source_len-0.5) / args.validseqlen), lr,\n                              elapsed * 1000 / args.log_interval, cur_loss, cur_loss / math.log(2)))\n            total_loss = 0\n            start_time = time.time()\n\n        # if batch % (200 * args.log_interval) == 0 and batch > 0:\n        #     vloss = evaluate(val_data)\n        #     print(\'-\' * 89)\n        #     print(\'| In epoch {:3d} | valid loss {:5.3f} | \'\n        #           \'valid bpc {:8.3f}\'.format(epoch, vloss, vloss / math.log(2)))\n        #     model.train()\n\n    return sum(losses) * 1.0 / len(losses)\n\n\ndef main():\n    global lr\n    try:\n        print(""Training for %d epochs..."" % args.epochs)\n        all_losses = []\n        best_vloss = 1e7\n        for epoch in range(1, args.epochs + 1):\n            loss = train(epoch)\n\n            vloss = evaluate(val_data)\n            print(\'-\' * 89)\n            print(\'| End of epoch {:3d} | valid loss {:5.3f} | valid bpc {:8.3f}\'.format(\n                epoch, vloss, vloss / math.log(2)))\n\n            test_loss = evaluate(test_data)\n            print(\'=\' * 89)\n            print(\'| End of epoch {:3d} | test loss {:5.3f} | test bpc {:8.3f}\'.format(\n                epoch, test_loss, test_loss / math.log(2)))\n            print(\'=\' * 89)\n\n            if epoch > 5 and vloss > max(all_losses[-3:]):\n                lr = lr / 10.\n                for param_group in optimizer.param_groups:\n                    param_group[\'lr\'] = lr\n            all_losses.append(vloss)\n\n            if vloss < best_vloss:\n                print(""Saving..."")\n                save(model)\n                best_vloss = vloss\n\n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(""Saving before quit..."")\n        save(model)\n\n    # Run on test data.\n    test_loss = evaluate(test_data)\n    print(\'=\' * 89)\n    print(\'| End of training | test loss {:5.3f} | test bpc {:8.3f}\'.format(\n        test_loss, test_loss / math.log(2)))\n    print(\'=\' * 89)\n\n# train_by_random_chunk()\nif __name__ == ""__main__"":\n    main()\n'"
char_rnn/model.py,0,"b""from torch import nn\n\nfrom drnn import DRNN\n\n\nclass DRNN_Char(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers,\n                 output_size, dropout=0.2, emb_dropout=0.2):\n        super(DRNN_Char, self).__init__()\n        self.encoder = nn.Embedding(output_size, input_size)\n        self.drnn = DRNN(cell_type='QRNN',\n                         dropout=dropout,\n                         n_hidden=hidden_size,\n                         n_input=input_size,\n                         n_layers=num_layers,\n                         batch_first=True)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        self.drop = nn.Dropout(emb_dropout)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, x):\n        # input has dimension (N, L_in), and emb has dimension (N, L_in, C_in)\n        emb = self.drop(self.encoder(x))\n        y, _ = self.drnn(emb)\n        o = self.decoder(y)\n        return o.contiguous()"""
char_rnn/utils.py,3,"b'from collections import Counter\nimport observations\nimport os\nimport pickle\n\nimport torch\n\n\ncuda = torch.cuda.is_available()\n\n\ndef data_generator(args):\n    file, testfile, valfile = getattr(observations, args.dataset)(\'data/\')\n    file_len = len(file)\n    valfile_len = len(valfile)\n    testfile_len = len(testfile)\n    corpus = Corpus(file + "" "" + valfile + "" "" + testfile)\n\n    #############################################################\n    # Use the following if you want to pickle the loaded data\n    #\n    # pickle_name = ""{0}.corpus"".format(args.dataset)\n    # if os.path.exists(pickle_name):\n    #     corpus = pickle.load(open(pickle_name, \'rb\'))\n    # else:\n    #     corpus = Corpus(file + "" "" + valfile + "" "" + testfile)\n    #     pickle.dump(corpus, open(pickle_name, \'wb\'))\n    #############################################################\n\n    return file, file_len, valfile, valfile_len, testfile, testfile_len, corpus\n\n\ndef read_file(filename):\n    file = open(filename).read()\n    return file, len(file)\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.char2idx = {}\n        self.idx2char = []\n        self.counter = Counter()\n\n    def add_word(self, char):\n        self.counter[char] += 1\n\n    def prep_dict(self):\n        for char in self.counter:\n            if char not in self.char2idx:\n                self.idx2char.append(char)\n                self.char2idx[char] = len(self.idx2char) - 1\n\n    def __len__(self):\n        return len(self.idx2char)\n\n\nclass Corpus(object):\n    def __init__(self, string):\n        self.dict = Dictionary()\n        for c in string:\n            self.dict.add_word(c)\n        self.dict.prep_dict()\n\n\ndef char_tensor(corpus, string):\n    tensor = torch.zeros(len(string)).long()\n    for i in range(len(string)):\n        tensor[i] = corpus.dict.char2idx[string[i]]\n    return tensor.cuda() if cuda else tensor\n\n\ndef batchify(data, batch_size, args):\n    """"""The output should have size [L x batch_size], where L could be a long sequence length""""""\n    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n    nbatch = data.size(0) // batch_size\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * batch_size)\n    # Evenly divide the data across the batch_size batches.\n    data = data.view(batch_size, -1)\n    if args.cuda:\n        data = data.cuda()\n    return data\n\n\ndef get_batch(source, start_index, args):\n    seq_len = min(args.seq_len, source.size(1) - 1 - start_index)\n    end_index = start_index + seq_len\n    inp = source[:, start_index:end_index].contiguous()\n    target = source[:, start_index+1:end_index+1].contiguous()  # The successors of the inp.\n    return inp, target\n\n\ndef save(model):\n    save_filename = \'model.pt\'\n    torch.save(model, save_filename)\n    print(\'Saved as %s\' % save_filename)\n\n\n'"
copy_memory/copymem_test.py,5,"b'import time\nimport argparse\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\n\nfrom drnn import DRNN\nfrom copy_memory.utils import data_generator\nfrom copy_memory.model import DRNN_Copy\n\n\nparser = argparse.ArgumentParser(description=\'Sequence Modeling - Copying Memory Task\')\nparser.add_argument(\'--batch_size\', type=int, default=32, metavar=\'N\',\n                    help=\'batch size (default: 32)\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA (default: True)\')\nparser.add_argument(\'--dropout\', type=float, default=0.0,\n                    help=\'dropout applied to layers (default: 0.0)\')\nparser.add_argument(\'--clip\', type=float, default=1.0,\n                    help=\'gradient clip, -1 means no clip (default: 1.0)\')\nparser.add_argument(\'--epochs\', type=int, default=50,\n                    help=\'upper epoch limit (default: 50)\')\nparser.add_argument(\'--iters\', type=int, default=100,\n                    help=\'number of iters per epoch (default: 100)\')\nparser.add_argument(\'--levels\', type=int, default=8,\n                    help=\'# of levels (default: 8)\')\nparser.add_argument(\'--blank_len\', type=int, default=1000, metavar=\'N\',\n                    help=\'The size of the blank (i.e. T) (default: 1000)\')\nparser.add_argument(\'--seq_len\', type=int, default=10,\n                    help=\'initial history size (default: 10)\')\nparser.add_argument(\'--log-interval\', type=int, default=50, metavar=\'N\',\n                    help=\'report interval (default: 50\')\nparser.add_argument(\'--lr\', type=float, default=5e-4,\n                    help=\'initial learning rate (default: 5e-4)\')\nparser.add_argument(\'--optim\', type=str, default=\'RMSprop\',\n                    help=\'optimizer to use (default: RMSprop)\')\nparser.add_argument(\'--nhid\', type=int, default=10,\n                    help=\'number of hidden units per layer (default: 10)\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed (default: 1111)\')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\n\nbatch_size = args.batch_size\nseq_len = args.seq_len\nepochs = args.epochs\niters = args.iters\nT = args.blank_len\nn_steps = T + (2 * seq_len)\nn_classes = 10  # Digits 0 - 9\nn_train = 10000\nn_test = 1000\ndropout = args.dropout\ninput_size = 1\nhidden_size = args.nhid\nnum_layers = args.levels\n\n\nprint(args)\nprint(""Preparing data..."")\ntrain_x, train_y = data_generator(T, seq_len, n_train)\ntest_x, test_y = data_generator(T, seq_len, n_test)\n\n\nmodel = DRNN_Copy(input_size=input_size,\n                  hidden_size=hidden_size,\n                  num_layers=num_layers,\n                  dropout=dropout,\n                  output_size=n_classes)\n\n\nif torch.cuda.is_available():\n    model.cuda()\n    train_x = train_x.cuda()\n    train_y = train_y.cuda()\n    test_x = test_x.cuda()\n    test_y = test_y.cuda()\n\ncriterion = nn.CrossEntropyLoss()\nlr = args.lr\noptimizer = optim.RMSprop(model.parameters(), lr=lr)\n\n\ndef evaluate():\n    model.eval()\n    out =  model(test_x.unsqueeze(2).contiguous())\n    loss = criterion(out.view(-1, n_classes), test_y.view(-1))\n    pred = out.view(-1, n_classes).data.max(1, keepdim=True)[1]\n    correct = pred.eq(test_y.data.view_as(pred)).cpu().sum()\n    counter = out.view(-1, n_classes).size(0)\n    print(\'\\nTest set: Average loss: {:.8f}  |  Accuracy: {:.4f}\\n\'.format(\n        loss.data[0], 100. * correct / counter))\n    return loss.data[0]\n\n\ndef train(ep):\n    global batch_size, seq_len, iters, epochs\n    model.train()\n    total_loss = 0\n    start_time = time.time()\n    correct = 0\n    counter = 0\n    for batch_idx, batch in enumerate(range(0, n_train, batch_size)):\n        start_ind = batch\n        end_ind = start_ind + batch_size\n\n        x = train_x[start_ind:end_ind] # (batch, steps)\n        y = train_y[start_ind:end_ind] # (batch, steps)\n\n        #import pdb\n        #pdb.set_trace()\n\n        optimizer.zero_grad()\n        out = model(x.unsqueeze(2).contiguous()) # out: (batch, steps, output_size)\n\n        loss = criterion(out.view(-1, n_classes), y.view(-1))\n        pred = out.view(-1, n_classes).data.max(1, keepdim=True)[1]\n        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n        counter += out.view(-1, n_classes).size(0)\n        if args.clip > 0:\n            torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n\n        if batch_idx > 0 and batch_idx % args.log_interval == 0:\n            avg_loss = total_loss / args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | \'\n                  \'loss {:5.8f} | accuracy {:5.4f}\'.format(\n                ep, batch_idx, n_train // batch_size+1, args.lr, elapsed * 1000 / args.log_interval,\n                avg_loss.data[0], 100. * correct / counter))\n            start_time = time.time()\n            total_loss = 0\n            correct = 0\n            counter = 0\n\n\nfor ep in range(1, epochs + 1):\n    train(ep)\n    evaluate()\n'"
copy_memory/model.py,0,"b""from torch import nn\n\nfrom drnn import DRNN\n\n\nclass DRNN_Copy(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n        super(DRNN_Copy, self).__init__()\n        self.drnn = DRNN(cell_type='GRU', dropout=dropout, n_hidden=hidden_size,\n                         n_input=input_size, n_layers=num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.linear.weight.data.normal_(0,0.01)\n\n    def forward(self, x): # x: (batch, steps, input_size)\n        y1, _ = self.drnn(x) # y1: (batch, steps, hidden_size)\n        #import pdb\n        #pdb.set_trace()\n        return self.linear(y1) # (batch, steps, output_size)"""
copy_memory/utils.py,7,"b'import numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\ndef data_generator(T, mem_length, b_size):\n    """"""\n    Generate data for the copying memory task\n\n    :param T: The total blank time length\n    :param mem_length: The length of the memory to be recalled\n    :param b_size: The batch size\n    :return: Input and target data tensor\n    """"""\n    seq = torch.from_numpy(np.random.randint(1, 9, size=(b_size, mem_length))).float()\n    zeros = torch.zeros((b_size, T))\n    marker = 9 * torch.ones((b_size, mem_length + 1))\n    placeholders = torch.zeros((b_size, mem_length))\n\n    x = torch.cat((seq, zeros[:, :-1], marker), 1)\n    y = torch.cat((placeholders, zeros, seq), 1).long()\n\n    x, y = Variable(x), Variable(y)\n    return x, y'"
