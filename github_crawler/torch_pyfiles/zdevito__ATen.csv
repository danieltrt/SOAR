file_path,api_count,code
aten/src/ATen/code_template.py,0,"b'import re\n\n# match $identifier or ${identifier} and replace with value in env\n# If this identifier is at the beginning of whitespace on a line\n# and its value is a list then it is treated as\n# block subsitution by indenting to that depth and putting each element\n# of the list on its own line\n# if the identifier is on a line starting with non-whitespace and a list\n# then it is comma separated ${,foo} will insert a comma before the list\n# if this list is not empty and ${foo,} will insert one after.\n\n\nclass CodeTemplate(object):\n    # Python 2.7.5 has a bug where the leading (^[^\\n\\S]*)? does not work,\n    # workaround via appending another [^\\n\\S]? inside\n\n    substitution_str = r\'(^[^\\n\\S]*[^\\n\\S]?)?\\$([^\\d\\W]\\w*|\\{,?[^\\d\\W]\\w*\\,?})\'\n\n    # older versions of Python have a bug where \\w* does not work,\n    # so we need to replace with the non-shortened version [a-zA-Z0-9_]*\n    # https://bugs.python.org/issue18647\n\n    substitution_str = substitution_str.replace(r\'\\w\', r\'[a-zA-Z0-9_]\')\n\n    subtitution = re.compile(substitution_str, re.MULTILINE)\n\n    @staticmethod\n    def from_file(filename):\n        with open(filename, \'r\') as f:\n            return CodeTemplate(f.read(), filename)\n\n    def __init__(self, pattern, filename=""""):\n        self.pattern = pattern\n        self.filename = filename\n\n    def substitute(self, env=None, **kwargs):\n        if env is None:\n            env = {}\n\n        def lookup(v):\n            return kwargs[v] if v in kwargs else env[v]\n\n        def indent_lines(indent, v):\n            return """".join([indent + l + ""\\n"" for e in v for l in str(e).splitlines()]).rstrip()\n\n        def replace(match):\n            indent = match.group(1)\n            key = match.group(2)\n            comma_before = \'\'\n            comma_after = \'\'\n            if key[0] == ""{"":\n                key = key[1:-1]\n                if key[0] == "","":\n                    comma_before = \', \'\n                    key = key[1:]\n                if key[-1] == \',\':\n                    comma_after = \', \'\n                    key = key[:-1]\n            v = lookup(key)\n            if indent is not None:\n                if not isinstance(v, list):\n                    v = [v]\n                return indent_lines(indent, v)\n            elif isinstance(v, list):\n                middle = \', \'.join([str(x) for x in v])\n                if len(v) == 0:\n                    return middle\n                return comma_before + middle + comma_after\n            else:\n                return str(v)\n        return self.subtitution.sub(replace, self.pattern)\n\n\nif __name__ == ""__main__"":\n    c = CodeTemplate(""""""\\\n    int foo($args) {\n\n        $bar\n            $bar\n        $a+$b\n    }\n    int commatest(int a${,stuff})\n    int notest(int a${,empty,})\n    """""")\n    print(c.substitute(args=[""hi"", 8], bar=[""what"", 7],\n                       a=3, b=4, stuff=[""things..."", ""others""], empty=[]))\n'"
aten/src/ATen/common_with_cwrap.py,0,"b'# this code should be common among cwrap and ATen preprocessing\n# for now, I have put it in one place but right now is copied out of cwrap\n\n\ndef parse_arguments(args):\n    new_args = []\n    for arg in args:\n        # Simple arg declaration of form ""<type> <name>""\n        if isinstance(arg, str):\n            t, _, name = arg.partition(\' \')\n            new_args.append({\'type\': t, \'name\': name})\n        elif isinstance(arg, dict):\n            if \'arg\' in arg:\n                arg[\'type\'], _, arg[\'name\'] = arg[\'arg\'].partition(\' \')\n                del arg[\'arg\']\n            new_args.append(arg)\n        else:\n            raise AssertionError()\n    return new_args\n\n\ndef set_declaration_defaults(declaration):\n    if \'schema_string\' not in declaration:\n        declaration[\'schema_string\'] = \'\'\n    if \'matches_jit_signature\' not in declaration:\n        declaration[\'matches_jit_signature\'] = False\n    declaration.setdefault(\'arguments\', [])\n    declaration.setdefault(\'return\', \'void\')\n    if \'cname\' not in declaration:\n        declaration[\'cname\'] = declaration[\'name\']\n    if \'backends\' not in declaration:\n        declaration[\'backends\'] = [\'CPU\', \'CUDA\']\n    if \'api_name\' not in declaration:\n        declaration[\'api_name\'] = declaration[\'name\']\n    # Simulate multiple dispatch, even if it\'s not necessary\n    if \'options\' not in declaration:\n        declaration[\'options\'] = [{\'arguments\': declaration[\'arguments\']}]\n        del declaration[\'arguments\']\n    # Parse arguments (some of them can be strings)\n    for option in declaration[\'options\']:\n        option[\'arguments\'] = parse_arguments(option[\'arguments\'])\n    # Propagate defaults from declaration to options\n    for option in declaration[\'options\']:\n        for k, v in declaration.items():\n            # TODO(zach): why does cwrap not propagate \'name\'? I need it\n            # propagaged for ATen\n            if k != \'options\':\n                option.setdefault(k, v)\n\n# TODO(zach): added option to remove keyword handling for C++ which cannot\n# support it.\n\n\ndef filter_unique_options(options, allow_kwarg, type_to_signature, remove_self):\n    def exclude_arg(arg):\n        return arg[\'type\'] == \'CONSTANT\'\n\n    def exclude_arg_with_self_check(arg):\n        return exclude_arg(arg) or (remove_self and arg[\'name\'] == \'self\')\n\n    def signature(option, kwarg_only_count):\n        if kwarg_only_count == 0:\n            kwarg_only_count = None\n        else:\n            kwarg_only_count = -kwarg_only_count\n        arg_signature = \'#\'.join(\n            type_to_signature.get(arg[\'type\'], arg[\'type\'])\n            for arg in option[\'arguments\'][:kwarg_only_count]\n            if not exclude_arg_with_self_check(arg))\n        if kwarg_only_count is None:\n            return arg_signature\n        kwarg_only_signature = \'#\'.join(\n            arg[\'name\'] + \'#\' + arg[\'type\']\n            for arg in option[\'arguments\'][kwarg_only_count:]\n            if not exclude_arg(arg))\n        return arg_signature + ""#-#"" + kwarg_only_signature\n    seen_signatures = set()\n    unique = []\n    for option in options:\n        # if only check num_kwarg_only == 0 if allow_kwarg == False\n        limit = len(option[\'arguments\']) if allow_kwarg else 0\n        for num_kwarg_only in range(0, limit + 1):\n            sig = signature(option, num_kwarg_only)\n            if sig not in seen_signatures:\n                if num_kwarg_only > 0:\n                    for arg in option[\'arguments\'][-num_kwarg_only:]:\n                        arg[\'kwarg_only\'] = True\n                unique.append(option)\n                seen_signatures.add(sig)\n                break\n    return unique\n\n\ndef sort_by_number_of_args(declaration, reverse=True):\n    def num_args(option):\n        return len(option[\'arguments\'])\n    declaration[\'options\'].sort(key=num_args, reverse=reverse)\n\n\nclass Function(object):\n\n    def __init__(self, name):\n        self.name = name\n        self.arguments = []\n\n    def add_argument(self, arg):\n        assert isinstance(arg, Argument)\n        self.arguments.append(arg)\n\n    def __repr__(self):\n        return self.name + \'(\' + \', \'.join(map(lambda a: a.__repr__(), self.arguments)) + \')\'\n\n\nclass Argument(object):\n\n    def __init__(self, _type, name, is_optional):\n        self.type = _type\n        self.name = name\n        self.is_optional = is_optional\n\n    def __repr__(self):\n        return self.type + \' \' + self.name\n\n\ndef parse_header(path):\n    with open(path, \'r\') as f:\n        lines = f.read().split(\'\\n\')\n\n    # Remove empty lines and prebackend directives\n    lines = filter(lambda l: l and not l.startswith(\'#\'), lines)\n    # Remove line comments\n    lines = map(lambda l: l.partition(\'//\'), lines)\n    # Select line and comment part\n    lines = map(lambda l: (l[0].strip(), l[2].strip()), lines)\n    # Remove trailing special signs\n    lines = map(lambda l: (l[0].rstrip(\');\').rstrip(\',\'), l[1]), lines)\n    # Split arguments\n    lines = map(lambda l: (l[0].split(\',\'), l[1]), lines)\n    # Flatten lines\n    new_lines = []\n    for l, c in lines:\n        for split in l:\n            new_lines.append((split, c))\n    lines = new_lines\n    del new_lines\n    # Remove unnecessary whitespace\n    lines = map(lambda l: (l[0].strip(), l[1]), lines)\n    # Remove empty lines\n    lines = filter(lambda l: l[0], lines)\n    generic_functions = []\n    for l, c in lines:\n        if l.startswith(\'TH_API void THNN_\'):\n            fn_name = l[len(\'TH_API void THNN_\'):]\n            if fn_name[0] == \'(\' and fn_name[-2] == \')\':\n                fn_name = fn_name[1:-2]\n            else:\n                fn_name = fn_name[:-1]\n            generic_functions.append(Function(fn_name))\n        elif l.startswith(\'THC_API void THNN_\'):\n            fn_name = l[len(\'THC_API void THNN_\'):]\n            if fn_name[0] == \'(\' and fn_name[-2] == \')\':\n                fn_name = fn_name[1:-2]\n            else:\n                fn_name = fn_name[:-1]\n            generic_functions.append(Function(fn_name))\n        elif l:\n            t, name = l.split()\n            if \'*\' in name:\n                t = t + \'*\'\n                name = name[1:]\n            generic_functions[-1].add_argument(\n                Argument(t, name, \'[OPTIONAL]\' in c))\n    return generic_functions\n'"
aten/src/ATen/cwrap_parser.py,0,"b""import yaml\ntry:\n    # use faster C loader if available\n    from yaml import CLoader as Loader\nexcept ImportError:\n    from yaml import Loader\n\n# follows similar logic to cwrap, ignores !inc, and just looks for [[]]\n\n\ndef parse(filename):\n    with open(filename, 'r') as file:\n        declaration_lines = []\n        declarations = []\n        in_declaration = False\n        for line in file.readlines():\n            line = line.rstrip()\n            if line == '[[':\n                declaration_lines = []\n                in_declaration = True\n            elif line == ']]':\n                in_declaration = False\n                declaration = yaml.load('\\n'.join(declaration_lines), Loader=Loader)\n                declarations.append(declaration)\n            elif in_declaration:\n                declaration_lines.append(line)\n        return declarations\n"""
aten/src/ATen/env.py,0,"b""import os\n\n# This file copied from tools/setup_helpers/env.py\n# PLEASE DO NOT ADD ANYTHING TO THIS FILE, the BUILD_NAMEDTENSOR flag is temporary.\ndef check_env_flag(name, default=''):\n    return os.getenv(name, default).upper() in ['ON', '1', 'YES', 'TRUE', 'Y']\n\n\ndef check_negative_env_flag(name, default=''):\n    return os.getenv(name, default).upper() in ['OFF', '0', 'NO', 'FALSE', 'N']\n\nBUILD_NAMEDTENSOR = True\n"""
aten/src/ATen/function_wrapper.py,0,"b'# HEY! Trying to understand what this file does?  Read\n# ""what has to be done to add a Operation ..."" first!\n\nimport re\nfrom code_template import CodeTemplate\n\ntry:\n    import typing  # noqa: F401\nexcept ImportError:\n    raise RuntimeError(\n        \'Missing build dependency: Unable to import the `typing` module. \'\n        \'Please install it via `conda install typing` or `pip install typing`\')\n\n# flake8 doesn\'t take into account usages in type annotations.\nfrom typing import Union, Set  # noqa: F401\nfrom typing import Any, Dict, List, Optional, Tuple, NamedTuple\n\ntry:\n    from mypy_extensions import TypedDict\nexcept ImportError:\n    # Avoid the dependency on the mypy_extensions package.\n    # It is required, however, for type checking.\n    def TypedDict(name, attrs, total=True):  # type: ignore\n        return Dict[Any, Any]\n\nimport sys\nif sys.version_info[0] == 3:\n    string_type = str\nelse:\n    string_type = basestring\n\nfrom env import BUILD_NAMEDTENSOR\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# what has to be done to add a Operation ...\n#\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# TH functions are generated into at::legacy::cpu and at::legacy::cuda,\n# where they can be called directly by a native function, they can be wrapped\n# by a native function that handles dispatch\n\n# Handle broadcasting for TH functions that need it\nLEGACY_TH_DECLARATION_BROADCAST = CodeTemplate(""""""\\\n${return_type} ${api_name}(${type_method_formals});\n"""""")\nLEGACY_TH_DEFINITION_BROADCAST = CodeTemplate(""""""\\\n${return_type} ${api_name}(${type_method_formals}) {\n#ifdef BUILD_NAMEDTENSOR\n    ${named_guard_declaration}\n#endif\n    ${device_guard_declaration}\n    Tensor ${broadcast_returns};\n    std::tie(${broadcast_returns}) = ${broadcast_function}(${broadcast_actuals}, ""${api_name}"");\n    return ${method_prefix_derived}${api_name}(${broadcast_modified_actuals});\n}\n"""""")\n\nLEGACY_TH_DECLARATION = CodeTemplate(""""""\\\n${return_type} ${method_prefix_derived}${api_name}(${type_method_formals});\n"""""")\nLEGACY_TH_DEFINITION = CodeTemplate(""""""\\\n${return_type} ${method_prefix_derived}${api_name}(${type_method_formals}) {\n#ifdef BUILD_NAMEDTENSOR\n    ${named_guard_declaration}\n#endif\n    ${device_guard_declaration}\n    ${type_definition_body}\n}\n"""""")\nLEGACY_TH_DEFINITION_SWITCH_STATEMENT = CodeTemplate(""""""\\\n${dispatch_scalar_type_declaration}\nswitch (dispatch_scalar_type) {\n    ${cases}\n    default:\n        AT_ERROR(""${api_name} not supported on ${Type} for "", dispatch_scalar_type);\n}\n"""""")\nLEGACY_TH_DEFINITION_CASE = CodeTemplate(""""""\\\ncase ScalarType::${ScalarName}: {\n    ${case_body}\n    break;\n}\n"""""")\n\n# Native functions are generated and registered on the dispatcher. We register the\n# function on Backend::Undefined if it does not have backend dependent dispatch.\n# In this case, it will be called for all backends, but can be overwritten on a\n# per backend basis.\nNATIVE_DISPATCH_DECLARATION = CodeTemplate(""""""\\\n${return_type} ${api_name}(${type_method_formals});\n"""""")\n\nNATIVE_DISPATCH_DEFINITION_DEFAULT = CodeTemplate(""""""\\\n${return_type} ${api_name}(${type_method_formals}) {\n#ifdef BUILD_NAMEDTENSOR\n    ${named_guard_declaration}\n#endif\n    ${device_guard_declaration}\n    ${return_call} at::native::${native_type_method_dispatch}(${native_actuals});\n}\n"""""")\n\nNATIVE_DISPATCH_DEFINITION_BACKEND = CodeTemplate(""""""\\\n${return_type} ${api_name}(${type_method_formals}) {\n#ifdef BUILD_NAMEDTENSOR\n    ${named_guard_declaration}\n#endif\n    ${device_guard_declaration}\n    ${return_call} at::native::${native_type_method_dispatch}(${native_actuals});\n}\n"""""")\n\nDEFAULT_UNBOXEDONLY_FUNCTION_REGISTRATION = CodeTemplate(""""""\\\n.op(torch::RegisterOperators::options()\n  .schema(""${schema_string}"")\n  .impl_unboxedOnlyCatchAllKernel<${return_type} (${formals_types}), &TypeDefault::${api_name}>()\n  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))\n"""""")\nBACKEND_UNBOXEDONLY_FUNCTION_REGISTRATION = CodeTemplate(""""""\\\n.op(torch::RegisterOperators::options()\n  .schema(""${schema_string}"")\n  .impl_unboxedOnlyKernel<${return_type} (${formals_types}), &${Type}::${api_name}>(TensorTypeId::${Backend}TensorId)\n  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))\n"""""")\nDEFAULT_FUNCTION_REGISTRATION = CodeTemplate(""""""\\\n.op(torch::RegisterOperators::options()\n  .schema(""${schema_string}"")\n  .catchAllKernel<${return_type} (${formals_types})>(&TypeDefault::${api_name})\n  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))\n"""""")\nBACKEND_FUNCTION_REGISTRATION = CodeTemplate(""""""\\\n.op(torch::RegisterOperators::options()\n  .schema(""${schema_string}"")\n  .kernel<${return_type} (${formals_types})>(TensorTypeId::${Backend}TensorId, &${Type}::${api_name})\n  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))\n"""""")\n\n# add non-virtual declaration to TensorBody.h\nTENSOR_METHOD_DECLARATION = CodeTemplate(""""""\\\n${return_type} ${api_name}(${method_formals_with_defaults}) const;\n"""""")\n# add non-virtual declaration to Tensor.cpp\nC10_UNBOXEDONLY_TENSOR_METHOD_DEFINITION = CodeTemplate(""""""\\\ninline ${return_type} Tensor::${api_name}(${method_formals}) const {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_method_body}\n#else\n    static c10::OperatorHandle op = c10::Dispatcher::singleton().findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxedOnly<${formals_types_with_return}>(\n        op${method_actuals_with_comma_prefix});\n#endif\n}\n"""""")\nC10_TENSOR_METHOD_DEFINITION = CodeTemplate(""""""\\\ninline ${return_type} Tensor::${api_name}(${method_formals}) const {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_method_body}\n#else\n    static c10::OperatorHandle op = c10::Dispatcher::singleton().findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxed<${formals_types_with_return}>(\n        op${method_actuals_with_comma_prefix});\n#endif\n}\n"""""")\n# add a method declaration in Functions.h\nFUNCTION_DECLARATION = CodeTemplate(""""""\\\nstatic inline ${return_type} ${api_name}(${formals_with_defaults});\n"""""")\n# add a method declaration in Functions.h\nDEPRECATED_FUNCTION_DECLARATION = CodeTemplate(""""""\\\nC10_DEPRECATED static inline ${return_type} ${api_name}(${formals_with_defaults});\n"""""")\n# add method definition in Functions.h\nC10_UNBOXEDONLY_FUNCTION_DEFINITION = CodeTemplate(""""""\\\nstatic inline ${return_type} ${api_name}(${formals}) {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_function_body}\n#else\n    static c10::OperatorHandle op = c10::Dispatcher::singleton()\n        .findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxedOnly<${formals_types_with_return}>(\n        op${native_actuals_with_comma_prefix});\n#endif\n}\n"""""")\nC10_FUNCTION_DEFINITION = CodeTemplate(""""""\\\nstatic inline ${return_type} ${api_name}(${formals}) {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_function_body}\n#else\n    static c10::OperatorHandle op = c10::Dispatcher::singleton()\n        .findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxed<${formals_types_with_return}>(\n        op${native_actuals_with_comma_prefix});\n#endif\n}\n"""""")\n\n# In order to rely on the linker to strip unused ops, it requires us to dispatch statically\n# in Functions.h and TensorMethods.h.\n#\n# NB: The default body also needs to apply a variable guard, as in some\n# situations what we think is a default body actually does have an\n# explicit derivative, and thereby would have gotten unwrapped by\n# the time you get to the implementation.\nSTATIC_DISPATCH_FUNCTION_DEFAULT_BODY = CodeTemplate(""""""\\\nat::AutoNonVariableTypeMode _var_guard(true);\n${return_call} TypeDefault::${native_type_method_dispatch}(${native_arguments});\n"""""")\nSTATIC_DISPATCH_FUNCTION_SWITCH_BODY = CodeTemplate(""""""\\\nat::AutoNonVariableTypeMode _var_guard(true);\nswitch(tensorTypeIdToBackend(c10::impl::dispatchTypeId(${type_set}))) {\n    ${static_dispatch_function_switches}\n    default:\n        AT_ERROR(""${api_name} not implemented for "", at::toString(${type_set}));\n}\n"""""")\nSTATIC_DISPATCH_FUNCTION_SWITCH_STATEMENT = CodeTemplate(""""""\\\ncase Backend::${backend}:\n    ${return_call} ${backend}Type::${api_name}(${native_arguments});\n    break;\n"""""")\n\n# add a native declaration for a native function\nNATIVE_DECLARATION = CodeTemplate(""""""\\\nCAFFE2_API ${return_type} ${native_type_method_dispatch}(${formals_with_defaults});\n"""""")\n\n# special method definition for factory functions in Functions.h that initializes backends\nC10_UNBOXEDONLY_FACTORY_DEFINITION = CodeTemplate(""""""\\\nstatic inline ${return_type} ${api_name}(${formals}) {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_function_body}\n#else\n    globalLegacyTypeDispatch().initForTensorTypeSet(${inferred_type_set});\n    static c10::OperatorHandle op = c10::Dispatcher::singleton()\n        .findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxedOnly<${formals_types_with_return}>(\n        op${native_actuals_with_comma_prefix});\n#endif\n}\n"""""")\nC10_FACTORY_DEFINITION = CodeTemplate(""""""\\\nstatic inline ${return_type} ${api_name}(${formals}) {\n#ifdef USE_STATIC_DISPATCH\n    ${static_dispatch_function_body}\n#else\n    globalLegacyTypeDispatch().initForTensorTypeSet(${inferred_type_set});\n    static c10::OperatorHandle op = c10::Dispatcher::singleton()\n        .findSchema({""aten::${operator_name}"", ""${overload_name}""}).value();\n    return c10::Dispatcher::singleton().callUnboxed<${formals_types_with_return}>(\n        op${native_actuals_with_comma_prefix});\n#endif\n}\n"""""")\n\nZERO_DIM_CHECK = CodeTemplate(""""""\\\nif (${check_name}.dim() == 0) {\n    return ${api_name}(${zero_dim_actuals});\n}"""""")\n\nSPARSE_CHECK = CodeTemplate(""""""\\\nif(${check_name}.is_sparse()) {\n    return static_cast<const TypeExtendedInterface*>(this)->${api_name}(${sparse_actuals});\n}"""""")\n\nCONDITIONAL_INITIALIZER = CodeTemplate(""""""\\\nif (${name}.defined()) {\n    ${initializer}\n}"""""")\n\nCALL_TEMPLATE = CodeTemplate(""${cname}(${actuals})"")\n\nOPERATOR_NAME = CodeTemplate(""""""\\\n    {""aten::${operator_name}"", ""${overload_name}""},\n"""""")\n\nNAMEDTENSOR_CHECK = CodeTemplate(""""""\\\n#ifdef BUILD_NAMEDTENSOR\n${code}\n#endif"""""")\n\n# scalar_name, c_type, accreal, is_floating_type\nscalar_types = [\n    (\'Bool\', \'bool\', \'BoolAccrealNotDefined\', False),\n    (\'Byte\', \'uint8_t\', \'Long\', False),\n    (\'Char\', \'int8_t\', \'Long\', False),\n    (\'Double\', \'double\', \'Double\', True),\n    (\'Float\', \'float\', \'Double\', True),\n    (\'Int\', \'int\', \'Long\', False),\n    (\'Long\', \'int64_t\', \'Long\', False),\n    (\'Short\', \'int16_t\', \'Long\', False),\n    (\'Half\', \'Half\', \'Double\', True),\n    (\'BFloat16\', \'BFloat16\', \'BFloat16AccrealNotDefined\', True),\n]\n\nstatic_dispatch_backends = [\'CPU\', \'QuantizedCPU\', \'SparseCPU\']\n\n\nclass NYIError(Exception):\n    """"""Indicates we don\'t support this declaration yet""""""\n\n    __slots__ = [\'reason\']\n\n    def __init__(self, reason):\n        self.reason = reason\n\n\nTYPE_FORMAL_GENERIC = {\n    \'THTensor*\': \'Tensor &\',\n    \'THByteTensor*\': \'Tensor &\',\n    \'THIndexTensor*\': \'Tensor &\',\n    \'THBoolTensor*\': \'Tensor &\',\n    \'THIntegerTensor*\': \'Tensor &\',\n    \'THDenseTensor*\': \'Tensor &\',\n    \'THDenseIndexTensor*\': \'Tensor &\',\n    \'THStorage*\': \'Storage\',\n    \'THGenerator*\': \'Generator *\',\n    \'IntArrayRefSize\': \'IntArrayRef\',\n    \'accreal\': \'Scalar\',\n    \'real\': \'Scalar\',\n    \'long\': \'int64_t\',\n}\n\nDYNAMIC_TYPE = {\n    \'THTensor*\': \'Tensor\',\n    \'THByteTensor*\': \'ByteTensor\',\n    \'THBoolTensor*\': \'BoolTensor\',\n    \'THIndexTensor*\': \'IndexTensor\',\n    \'THIntegerTensor*\': \'IntegerTensor\',\n    \'THDenseTensor*\': \'Tensor\',\n    \'THDenseIndexTensor*\': \'IndexTensor\',\n    \'THStorage*\': \'Storage\',\n    \'THGenerator*\': \'Generator*\',\n    \'IntArrayRefSize\': \'IntArrayRef\',\n    \'accreal\': \'accreal\',\n    \'real\': \'real\',\n    \'long\': \'int64_t\',\n}\n\nNATIVE_DYNAMIC_TYPE = {\n    \'Tensor &\': \'Tensor\',\n    \'const Tensor &\': \'Tensor\',\n}\n\nTYPE_RETURN = {\n    \'THTensor*\': \'Tensor\',\n    \'THIndexTensor*\': \'Tensor\',\n    \'THByteTensor*\': \'Tensor\',\n    \'THBoolTensor*\': \'Tensor\',\n    \'THIntegerTensor*\': \'Tensor\',\n    \'THDenseTensor*\': \'Tensor\',\n    \'THDenseIndexTensor*\': \'Tensor\',\n    \'real\': \'Tensor\',\n    \'accreal\': \'Tensor\',\n    \'long\': \'int64_t\',\n}\n\nCHECKED_CAST = {\n    \'THTensor*\':\n        CodeTemplate(\n            \'checked_dense_tensor_unwrap(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, ""${api_name}"", ${null_okay}, \'\n            \'DeviceType::${DeviceType}, ScalarType::${ScalarName})\'),\n    \'THByteTensor*\':\n        CodeTemplate(\n            \'checked_dense_tensor_unwrap(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, ""${api_name}"", ${null_okay}, \'\n            \'DeviceType::${DeviceType}, ScalarType::Byte)\'),\n    \'THBoolTensor*\':\n        CodeTemplate(\n            \'checked_dense_tensor_unwrap(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, ""${api_name}"", ${null_okay}, \'\n            \'DeviceType::${DeviceType}, ScalarType::Bool)\'),\n    \'THIndexTensor*\':\n        CodeTemplate(\n            \'checked_dense_tensor_unwrap(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, ""${api_name}"", ${null_okay}, \'\n            \'DeviceType::${DeviceType}, ScalarType::Long)\'),\n    \'THIntegerTensor*\':\n        CodeTemplate(\n            \'checked_dense_tensor_unwrap(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, ""${api_name}"", ${null_okay}, \'\n            \'DeviceType::${DeviceType}, ScalarType::Int)\'),\n    \'THStorage*\':\n        CodeTemplate(\n            \'checked_storage(\'\n            \'${arg_name}, ""${arg_name}"", ${arg_pos}, \'\n            # We\'re punning here (Backend and DeviceType constructors coincide)\n            # but DeviceType is the correct way to classify storages\n            \'DeviceType::${Backend}, at::scalarTypeToTypeMeta(ScalarType::${ScalarName}))\'),\n    # This is a cast done via direct-construction\n    \'IntArrayRefStride\': CodeTemplate(\'at::IntArrayRef ${result_name} = get_intlist_stride_th(${arg_name});\'),\n    \'real\': CodeTemplate(\'${arg_name}.to${ScalarName}()\'),\n    \'accreal\': CodeTemplate(\'${arg_name}.to${AccScalarName}()\'),\n    \'TensorList\': CodeTemplate(\n            \'checked_tensor_list_unwrap(${arg_name},""${arg_name}"",${arg_pos}, \'\n            \'Backend::${Backend}, ScalarType::${ScalarName})\'),\n    \'IntArrayRef\': CodeTemplate(\'check_intlist<${size}>(${arg_name}, ""${arg_name}"", ${arg_pos})\')\n}\n\nCHECKED_USE = {\n    \'THTensor*\': \'{}_\',\n    \'THIndexTensor*\': \'{}_\',\n    \'THByteTensor*\': \'{}_\',\n    \'THBoolTensor*\': \'{}_\',\n    \'THIntegerTensor*\': \'{}_\',\n    \'THDenseTensor*\': \'{}_\',\n    \'THDenseIndexTensor*\': \'{}_\',\n    \'THStorage*\': \'{}_.unsafeGetStorageImpl()\',\n    \'TensorList\': ""{0}_.data(), {0}_.size()"",\n}\n\nCHECKED_USE_NULLABLE = CodeTemplate(\'${arg_name}_ ? ${usage} : NULL\')\n\nALLOC_NOARGS_WRAP = {\n    \'THTensor*\': \'c10::make_intrusive<TensorImpl, UndefinedTensorImpl>\'\n                 \'(c10::Storage(caffe2::TypeMeta::Make<${ScalarType}>(), 0, allocator(), true),\'\n                 \'TensorTypeId::${Backend}TensorId).release()\',\n    \'THByteTensor*\': \'c10::make_intrusive<TensorImpl, UndefinedTensorImpl>\'\n                     \'(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),\'\n                     \'TensorTypeId::${Backend}TensorId).release()\',\n    \'THBoolTensor*\': \'c10::make_intrusive<TensorImpl, UndefinedTensorImpl>\'\n                     \'(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),\'\n                     \'TensorTypeId::${Backend}TensorId).release()\',\n    \'THIndexTensor*\': \'c10::make_intrusive<TensorImpl, UndefinedTensorImpl>\'\n                     \'(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),\'\n                     \'TensorTypeId::${Backend}TensorId).release()\',\n    \'THIntegerTensor*\': \'c10::make_intrusive<TensorImpl, UndefinedTensorImpl>\'\n                        \'(c10::Storage(scalarTypeToTypeMeta(ScalarType::Int), 0, allocator(), true),\'\n                        \'TensorTypeId::${Backend}TensorId).release()\',\n}\n\nALLOC_WRAP = {\n    \'THTensor*\': \'${arguments}\',\n    \'THByteTensor*\': \'${arguments}\',\n    \'THBoolTensor*\': \'${arguments}\',\n    \'THIndexTensor*\': \'${arguments}\',\n    \'THIntegerTensor*\': \'${arguments}\',\n    \'THDenseTensor*\': \'${arguments}\',\n    \'THDenseIndexTensor*\': \'${arguments}\',\n}\n\n# Replacements for constants when calling into TH\nCONSTANT_REPLACEMENTS = [\n    (\'AS_REAL\', \'${ScalarType}\'),\n]\n\n# Replacements for constants in header file function definitions\nHEADER_CONSTANT_REPLACEMENTS = [\n    (r\'AS_REAL\\((.*)\\)\', r\'\\1\'),\n]\n\n\nclass nested_dict(object):\n    def __init__(self, base, parent):\n        self.base, self.parent = base, parent\n\n    def __getitem__(self, x):\n        r = self.base.get(x)\n        if r is not None:\n            return r\n        return self.parent[x]\n\n\nEnvironment = TypedDict(\'Environment\', {\n    \'state\': str,\n    \'ScalarType\': str,\n    \'ScalarName\': str,\n    \'THTensor\': str,\n    \'THType\': str,\n    \'Backend\': str,\n    \'DeviceType\': str,\n    \'AccScalarName\': str,\n})\n\nTopEnvironment = TypedDict(\'TopEnvironment\', {\n    \'type_registrations\': List[str],\n    \'type_headers\': List[str],\n    \'function_registrations\': List[str],\n    \'list_of_aten_ops\': List[str],\n    \'type_method_declarations\': List[str],\n    \'type_method_definitions\': List[str],\n    \'tensor_method_declarations\': List[str],\n    \'tensor_method_definitions\': List[str],\n    \'function_declarations\': List[str],\n    \'function_definitions\': List[str],\n    \'type_ids\': List[str],\n    \'native_function_declarations\': List[str],\n})\n\n# A Declarations.cwrap formal argument\n# type can contain THTensor* types\nTHFormal = TypedDict(\'THFormal\', {\n    \'name\': str,\n    \'type\': str,\n    \'dynamic_type\': str,\n    \'kwarg_only\': bool,\n    \'is_nullable\': bool,\n    \'default\': str,\n    \'output\': bool,\n    \'size\': int,\n    \'allocate\': bool,\n    \'mask\': bool,\n    \'wrap_dim\': str,\n    # Broadcast is originally a str but gets unwrapped to a List or Dict in-place\n    \'broadcast\': Any,\n    \'resize\': str,\n    \'cpu_zero\': bool,\n    \'zero\': bool,\n}, total=False)\n\n# Generic ATen formal or native_functions.yaml formal argument.\n# type can contain Tensor& reference types.\nAtFormal = TypedDict(\'AtFormal\', {\n    \'name\': str,\n    \'type\': str,\n    \'dynamic_type\': str,\n    \'kwarg_only\': bool,\n    \'is_nullable\': bool,\n    \'default\': str,\n    \'output\': bool,\n    \'size\': int,\n}, total=False)\n\n# Note [field_name versus name]\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# What is the difference between ""field_name"" and ""name""?\n#\n# Return values of ATen operators always have a name: if it is not\n# explicitly assigned a name inside native_functions.yaml like func:\n# myop() -> (Tensor indices, Tensor value), then the codegen will\n# automatically assign it a name like result0, or name might be\n# specified inside Declarations.cwrap.  We don\'t want these assigned\n# names to become part of the public API when we return a namedtuple for\n# any such multiple-return function.\n#\n# Thus field_name is like name, but it is defined only when there is a\n# name specified in native_functions.yaml. If field_name is defined,\n# then the codegen would generate code to return namedtuple. Otherwise,\n# it would just return tuple.\n\nReturnType = TypedDict(\'ReturnType\', {\n    \'name\': str,\n    # See Note [field_name versus name]\n    \'field_name\': str,\n    \'type\': str,\n    \'dynamic_type\': str,\n}, total=False)\n\nReturnDecl = TypedDict(\'ReturnDecl\', {\n    \'kind\': str,\n    \'type\': str,\n    \'arguments\': List[int],\n}, total=False)\n\n# Represents a buffer in nn.yaml\nNNBuffer = TypedDict(\'NNBuffer\', {\n    \'name\': str,\n})\n\nFunctionOption = TypedDict(\'FunctionOption\', {\n    \'actuals\': List[str],\n    \'api_name\': str,\n    \'arguments\': List[THFormal],\n    \'aten_custom_call\': str,\n    \'backend_types\': Dict[str, List[str]],\n    \'backends\': List[str],\n    \'broadcast_actuals\': List[str],\n    \'broadcast_function\': str,\n    \'broadcast_modified_actuals\': List[str],\n    \'broadcast_returns\': List[str],\n    \'buffers\': List[NNBuffer],\n    # cimpls is really a List[FunctionOption]\n    \'cimpls\': List[Any],\n    \'cname\': str,\n    # explicitly specify whether the function is a factory function or other special category\n    \'category_override\': str,\n    \'condition\': str,\n    \'device_guard\': bool,\n    \'device_guard_declaration\': str,\n    \'dispatch_scalar_type_declaration\': str,\n    \'use_c10_dispatcher\': str,\n    \'with_gil\': bool,\n    \'cpu_half\': bool,\n    \'cpu_bfloat16\': bool,\n    \'cuda_bfloat16\': bool,\n    \'deprecated\': bool,\n    \'cpu_bool\': bool,\n    \'cuda_bool\': bool,\n    # See Note [field_name versus name]\n    \'field_name\': str,\n    \'formals_list\': List[AtFormal],\n    \'formals_with_defaults\': List[str],\n    \'formals\': List[str],\n    \'formals_types\': List[str],\n    \'formals_types_with_return\': List[str],\n    \'inferred_type_set\': str,\n    \'inplace\': bool,\n    \'matches_jit_signature\': bool,\n    # This controls whether or not we generate the interface in Type or\n    # TypeExtendedInterface\n    \'extended_method\': bool,\n    \'method_actuals\': List[str],\n    \'method_actuals_with_comma_prefix\': str,\n    \'method_formals_with_defaults\': List[str],\n    \'method_formals\': List[str],\n    \'method_prefix_derived\': str,\n    \'named_guard_declaration\': str,\n    \'mode\': str,\n    \'python_module\': str,\n    \'name\': str,\n    \'operator_name\': str,\n    \'overload_name\': str,\n    \'native_actuals\': List[str],\n    \'native_actuals_with_comma_prefix\': str,\n    \'native_type_method_dispatch\': str,\n    # options should be List[FunctionOption]\n    \'options\': Any,\n    \'schema_string\': str,\n    \'requires_tensor\': bool,\n    \'return_call\': str,\n    \'return_type\': str,\n    \'return\': ReturnDecl,\n    \'returns\': List[ReturnType],\n    \'scalar_check\': str,\n    \'sparse\': bool,\n    \'type_definition_body\': List[str],\n    \'type_method_actuals\': List[str],\n    \'type_method_definition_dispatch\': str,\n    \'type_method_formals\': List[str],\n    \'variants\': str,\n    \'when_spares_dispatch\': str,\n    \'when_sparse_dispatch\': str,\n    \'with_gil\': bool,\n    \'zero_dim_dispatch_when_scalar\': str,\n})\n\nOutputDeclaration = NamedTuple(\'OutputDeclaration\', [\n    (\'name\', str),\n    (\'operator_name\', str),\n    (\'overload_name\', str),\n    (\'use_c10_dispatcher\', str),\n    (\'category_override\', str),\n    (\'matches_jit_signature\', bool),\n    (\'schema_string\', str),\n    (\'method_prefix_derived\', str),\n    (\'arguments\', List[AtFormal]),\n    (\'method_of\', List[str]),\n    (\'mode\', str),\n    (\'python_module\', str),\n    (\'buffers\', Optional[List[str]]),\n    (\'returns\', List[ReturnType]),\n    (\'inplace\', bool),\n    (\'is_factory_method\', bool),\n    (\'abstract\', bool),\n    (\'requires_tensor\', bool),\n    (\'device_guard\', bool),\n    (\'with_gil\', bool),\n    (\'deprecated\', bool),\n])\n\nFunctionCode = NamedTuple(\'FunctionCode\', [\n    (\'definition\', str),\n    (\'declaration\', str),\n])\n\n\ndef device_guard(option, dispatch_options, dispatch_tensor):\n    # For factory methods the `DeviceGuard` is already in the template.\n    if option.get(\'device_guard\', True):\n        if dispatch_options:\n            return \'const DeviceGuard device_guard({}.device());\'.format(dispatch_options[\'name\'])\n        if dispatch_tensor:\n            return \'const OptionalDeviceGuard device_guard(device_of({}));\'.format(dispatch_tensor)\n    return \'// DeviceGuard omitted\'\n\n\ndef named_guard(option, tensors, tensorlists):\n    if option.get(\'supports_named_tensor\', False) or (len(tensors) + len(tensorlists) == 0):\n        return \'\'\n    # Override: supports_named_tensor = False for _th_ functions. This is because:\n    # There is always some at:: function that calls the _th_ function.\n    if option[\'name\'].startswith(\'_th_\'):\n        return \'\'\n    named_conditions = []\n    for tensor in tensors:\n        named_conditions.append(\'{}.has_names()\'.format(tensor))\n    for tensorlist in tensorlists:\n        named_conditions.append(\'at::has_names({})\'.format(tensorlist))\n    return (""""""\\\nif ({named_conditions}) {{\n    AT_ERROR(\n        ""{op} is not yet supported with named tensors. Please drop names via ""\n        ""`tensor = tensor.rename(None)`, call the op with an unnamed tensor, ""\n        ""and set names on the result of the operation."");\n}}"""""".format(named_conditions=\' || \'.join(named_conditions), op=option[\'name\']))\n\n\ndef dispatch_scalar_type(option, dispatch_options, dispatch_tensor):\n    if dispatch_options:\n        return \'auto dispatch_scalar_type = typeMetaToScalarType({}.dtype());\'.format(dispatch_options[\'name\'])\n    if dispatch_tensor:\n        return \'auto dispatch_scalar_type = infer_scalar_type({});\'.format(dispatch_tensor)\n    return \'// dispatch_scalar_type omitted\'\n\n\ndef is_real_argument_to_wrapper(argument):\n    # type: (THFormal) -> bool\n    return not argument.get(\'output\', False) and\\\n        argument[\'type\'] != \'CONSTANT\' and\\\n        argument[\'type\'] != \'argument\'\n\n\ndef is_mutable_formal_argument(argument, option):\n    # type: (THFormal, FunctionOption) -> bool\n    return argument.get(\'output\') or option[\'inplace\'] and argument[\'name\'] == \'self\'\n\n\ndef check_methods_do_not_start_with_underscore(name, is_method):\n    if name in {\'_values\', \'_indices\', \'_nnz\', \'_dimI\', \'_dimV\', \'_coalesced_\',\n                \'_version\'}:\n        return\n    if is_method and name.startswith(\'_\') and not name.startswith(\'__\') and not name.startswith(\'_th_\'):\n        message = ""Function \'{}\' starts with a single underscore and is "".format(name)\n        message += ""configured to have a method on Tensor. Functions that start with ""\n        message += "" a single underscore should only be functions in the at:: ""\n        message += ""namespace and not methods on Tensor!""\n        raise RuntimeError(message)\n\n\ndef to_return_type(arg, option):\n    # type: (THFormal, FunctionOption) -> ReturnType\n    t = arg[\'type\']\n    rt = TYPE_RETURN.get(t, t)\n    if rt == \'Tensor\' and not arg.get(\'allocate\'):\n        rt = rt + \' &\'\n        if not is_mutable_formal_argument(arg, option):\n            rt = \'const \' + rt\n    return {\n        \'name\': arg[\'name\'],\n        \'type\': rt,\n        \'dynamic_type\': DYNAMIC_TYPE.get(arg[\'type\'], arg[\'type\']),\n    }\n\n\ndef create_generic(top_env, declarations):\n    # type: (TopEnvironment, List[FunctionOption]) -> List[OutputDeclaration]\n    # translates defaults from cwrap types to C++ values\n    def translate_default(argument, type_str, default):\n        # type: (THFormal, str, Any) -> Any\n        if default is None:\n            # cause the default constructor for the object to run\n            return \'{}\'\n        for pattern, replacement in HEADER_CONSTANT_REPLACEMENTS:\n            default = re.sub(pattern, replacement, str(default))\n        if type_str in {\'Scalar\', \'int64_t\', \'double\'}:\n            try:\n                return int(default)\n            except Exception:\n                try:\n                    return float(default)\n                except Exception:\n                    return default\n        elif type_str == \'bool\':\n            assert default.lower() in [\'true\', \'false\']\n            return default.lower() == \'true\'\n        else:\n            return default\n\n    # change from THTensor* to Tensor & so we get how it will appear\n    # in the aten argument list...\n    def translate_formal(argument, option):\n        # type: (THFormal, FunctionOption) -> AtFormal\n        type_str = TYPE_FORMAL_GENERIC.get(argument[\'type\'], argument[\'type\'])\n        if type_str == \'Tensor &\' and not is_mutable_formal_argument(argument, option):\n            type_str = \'const \' + type_str\n        translated = {\n            \'name\': argument[\'name\'],\n            \'type\': type_str,\n            \'dynamic_type\': DYNAMIC_TYPE.get(argument[\'type\'], argument[\'type\']),\n        }  # type: AtFormal\n        if \'kwarg_only\' in argument:\n            translated[\'kwarg_only\'] = argument[\'kwarg_only\']\n        if \'default\' in argument:\n            default = translate_default(argument, type_str, argument[\'default\'])\n            translated[\'default\'] = default\n        if argument.get(\'output\'):\n            translated[\'output\'] = True\n        if argument.get(\'size\'):\n            translated[\'size\'] = argument[\'size\']\n        if argument.get(\'is_nullable\') is not None:\n            translated[\'is_nullable\'] = argument[\'is_nullable\']\n        return translated\n\n    def get_formals(option, include_constants=False):\n        # type: (FunctionOption, bool) -> List[AtFormal]\n        seen = set()  # type: Set[str]\n        pos_args = []  # type: List[THFormal]\n        kwd_args = []  # type: List[THFormal]\n\n        def insert(argument):\n            # type: (THFormal) -> None\n            if argument[\'name\'] not in seen:\n                seen.add(argument[\'name\'])\n                if argument.get(\'kwarg_only\', False):\n                    kwd_args.append(argument)\n                else:\n                    pos_args.append(argument)\n\n        def has_output_mask(argument):\n            # type: (THFormal) -> bool\n            return argument.get(\'allocate\', False) and argument.get(\'mask\', False)\n\n        for argument in option[\'arguments\']:\n            if argument.get(\'output\') and not argument.get(\'allocate\', False):\n                insert(argument)\n        for argument in option[\'arguments\']:\n            if include_constants and argument[\'type\'] == \'CONSTANT\':\n                insert(argument)\n            elif is_real_argument_to_wrapper(argument):\n                insert(argument)\n        if any(has_output_mask(arg) for arg in option[\'arguments\']):\n            mask_size = sum(has_output_mask(arg) for arg in option[\'arguments\'])\n            insert({\n                \'name\': \'output_mask\',\n                # NB: Lack of space in comma works around parsing\n                # problem in gen_variable_type.py\n                \'type\': \'std::array<bool,{}>\'.format(mask_size),\n                \'default\': \'{{\' + \', \'.join([\'true\'] * mask_size) + \'}}\',\n            })\n\n        result = pos_args + kwd_args\n        return [translate_formal(argument, option) for argument in result]\n\n    def get_return_types(option):\n        # type: (FunctionOption) -> List[ReturnType]\n        ret = option[\'return\']\n        if ret[\'kind\'] == \'arguments\':\n            argument_indices = ret[\'arguments\']\n            if len(argument_indices) == 1:\n                the_arg = option[\'arguments\'][argument_indices[0]]\n                return [to_return_type(the_arg, option)]\n            else:\n                return [to_return_type(option[\'arguments\'][idx], option)\n                        for idx in argument_indices]\n        elif ret[\'kind\'] == \'type\':\n            return [{\n                \'type\': TYPE_RETURN.get(ret[\'type\'], ret[\'type\']),\n                \'dynamic_type\': DYNAMIC_TYPE.get(ret[\'type\'], ret[\'type\']),\n            }]\n        else:\n            raise Exception(""format_return_type"")\n\n    def format_return_type(return_types):\n        # type: (List[ReturnType]) -> str\n        if len(return_types) == 0:\n            return ""void""\n        elif len(return_types) == 1:\n            return return_types[0][\'type\']\n        return ""std::tuple<{}>"".format(\',\'.join(r[\'type\'] for r in return_types))\n\n    def is_any_tensor_type(formal):\n        return (formal[\'dynamic_type\'] == \'Tensor\' or formal[\'dynamic_type\'] == \'ByteTensor\'\n                or formal[\'dynamic_type\'] == \'IndexTensor\' or formal[\'dynamic_type\'] == \'BoolTensor\')\n\n    def find_tensors(formals):\n        # type: (List[AtFormal]) -> List[str]\n        return [formal[\'name\'] for formal in formals if is_any_tensor_type(formal)]\n\n    def find_tensorlists(formals):\n        # type: (List[AtFormal]) -> List[str]\n        return [formal[\'name\'] for formal in formals if formal[\'dynamic_type\'] == \'TensorList\']\n\n    def find_dispatch_tensor(formals):\n        # type: (List[AtFormal]) -> Optional[str]\n        # Determine legacy TH-style single dispatch tensor.\n        #\n        # Also used to determine what tensor should be used to provide a default\n        # DeviceGuard.  Unlike dispatch, we don\'t guard on ALL tensor arguments\n        # (because this is not actually a thing you can do.)  Guarding on the\n        # first argument is best effort to help people avoid doing this\n        # themselves.\n\n        for formal in formals:\n            if formal[\'name\'] == \'self\' and is_any_tensor_type(formal) and not formal.get(\'is_nullable\', False):\n                return formal[\'name\']\n        # otherwise dispatch to the first Tensor or TensorList\n        for formal in formals:\n            if \'TensorList\' == formal[\'dynamic_type\'] or is_any_tensor_type(formal) and \\\n               not formal.get(\'is_nullable\', False):\n                return formal[\'name\']\n\n        return None\n\n    def find_multidispatch_tensors(formals):\n        # type: (List[AtFormal]) -> List[str]\n        # Compute the list of all tensor arguments which should be considered\n        # for multiple dispatch.  Note that this doesn\'t completely replace\n        # find_dispatch_tensor because we use the ""dispatch tensor"" to determine\n        # device guards.  TensorOptions is included as part of this calculation.\n        #\n        # The interaction of multiple dispatch with TensorOptions\n        # is quite interesting.  In particular, suppose I have:\n        #\n        #   cuda_tensor.new_like(1, device=\'cpu\')\n        #\n        # Multiple dispatch will attempt a dispatch to CUDA, even though\n        # the end tensor that should be produced here is a CPU one.  The\n        # upshot is that if you have an operator with mixed TensorOptions\n        # and Tensor arguments, you MUST only ever register it generically.\n        r = []\n        for formal in formals:\n            if formal[\'dynamic_type\'] in [\'TensorOptions\', \'TensorList\'] or is_any_tensor_type(formal):\n                r.append(formal[\'name\'])\n        return r\n\n    def format_formal(f):\n        # type: (AtFormal) -> str\n        return \'{} {}\'.format(f[\'type\'], f[\'name\'])\n\n    def formal_with_default(f):\n        # type: (AtFormal) -> str\n        s = format_formal(f)\n        v = f.get(\'default\')\n        if v is None:\n            return s\n        if isinstance(v, bool):\n            v = str(v).lower()\n        return \'{}={}\'.format(s, v)\n\n    def get_broadcast_argument(option):\n        # type: (FunctionOption) -> Optional[THFormal]\n        for argument in option[\'arguments\']:\n            if argument.get(\'broadcast\'):\n                return argument\n        return None\n\n    def get_broadcast_actuals(broadcast_arg, broadcast_inplace, broadcast_dims):\n        # type: (THFormal, bool, bool) -> List[str]\n        # Note: broadcast_dims can change type...\n        # return the actuals that will be passed to the broadcast function.\n        # 1) in the common case, this is the broadcasted argument (e.g. ""self"") followed by the tensors\n        #    that it is broadcasted against (comma-separated) (e.g. ""self, tensor1, tensor2"").\n        # 2) in the broadcast_dims case, this is the broadcasted argument (e.g. ""self"") followed by the sizes\n        #    it is broadcasted to (as an initializer list), so e.g. the specification\n        #    ""mat1.dim0,mat2.dim1"" gets transformed to ""self, {mat1.size(0),mat2.size(1)}""\n        if not broadcast_dims:\n            broadcast_actuals = [broadcast_arg[\'name\']] + broadcast_arg[\'broadcast\'].split()[0].split("","")\n        else:\n            broadcast_dims_spec = broadcast_arg[\'broadcast\'].split()[1].split(\':\')[1].split(\',\')\n            # generate size call for each dimension\n            broadcast_dims = ([x.split(\'.\')[0] + \'.size(\' + x.split(\'.\')[1].replace(\'dim\', \'\') + \')\'  # type: ignore\n                              for x in broadcast_dims_spec])\n            broadcast_dims_init_list = \'{\' + \',\'.join(broadcast_dims) + \'}\'  # type: ignore\n            broadcast_actuals = [broadcast_arg[\'name\'], broadcast_dims_init_list]\n\n        return broadcast_actuals\n\n    def process_legacy_th_option(option):\n        # type: (FunctionOption) -> None\n        # Mutably populate option with derived values computed from values\n        # passed in to option.\n        option[\'inplace\'] = re.search(\n            \'(^__i|[^_]_$)\', option[\'api_name\']) is not None\n\n        # print(yaml.dump(option))\n        formals = get_formals(option)\n        option[\'formals_list\'] = formals\n        option[\'formals\'] = [format_formal(f) for f in formals]\n        option[\'formals_with_defaults\'] = [formal_with_default(f) for f in formals]\n        option[\'returns\'] = get_return_types(option)\n        option[\'return_type\'] = format_return_type(option[\'returns\'])\n        option[\'return_call\'] = \'return \' if option[\'return_type\'] != \'void\' else \'\'\n        option[\'actuals\'] = [f[\'name\'] for f in formals]\n\n        option[\'method_formals\'] = [format_formal(f) for f in formals\n                                    if f[\'name\'] != \'self\']\n        option[\'method_formals_with_defaults\'] = (\n            [formal_with_default(f) for f in formals if f[\'name\'] != \'self\'])\n        # *this is \'const Tensor&\' since all Tensor methods are const and must\n        # be const_casted to be accepted as native function\'s non-const argument\n        option[\'method_actuals\'] = [\n            f[\'name\'] if f[\'name\'] != \'self\' else \'const_cast<Tensor&>(*this)\' for f in formals]\n\n        # There are no cases where these differ, but they do in native_functions\n        option[\'type_method_formals\'] = option[\'formals\']\n        option[\'type_method_actuals\'] = option[\'actuals\']\n\n        assert \'method\' not in option[\'variants\'], \'TH functions cannot be methods\'\n        is_function = \'function\' in option[\'variants\']\n        # NB: TH functions don\'t support multiple dispatch\n        dispatch_tensor = find_dispatch_tensor(formals)\n        is_namespace_function = is_function and dispatch_tensor is not None\n\n        broadcast_arg = get_broadcast_argument(option)\n        # ""s_"" for ""same size"".\n        option[\'method_prefix_derived\'] = \'\' if broadcast_arg is None else \'s_\'\n        if option[\'mode\'] == \'TH\':\n            option[\'device_guard\'] = False\n        option[\'device_guard_declaration\'] = device_guard(option, False, dispatch_tensor)\n        option[\'named_guard_declaration\'] = named_guard(option, find_tensors(formals),\n                                                        find_tensorlists(formals))\n        option[\'dispatch_scalar_type_declaration\'] = dispatch_scalar_type(option, False, dispatch_tensor)\n\n        assert option[\'extended_method\'], \'Expected legacy operator to be an extended method\'\n\n        if broadcast_arg is not None:\n            broadcast_inplace = \'inplace\' in broadcast_arg[\'broadcast\']\n            broadcast_dims = \'dims:\' in broadcast_arg[\'broadcast\']\n            option[\'broadcast_actuals\'] = get_broadcast_actuals(broadcast_arg, broadcast_inplace, broadcast_dims)\n            if not broadcast_dims:\n                option[\'broadcast_returns\'] = ([""b_"" + x for x in option[\'broadcast_actuals\']\n                                               if x != broadcast_arg[\'name\'] or not broadcast_inplace])\n            else:\n                option[\'broadcast_returns\'] = [""b_"" + broadcast_arg[\'name\']]\n\n            option[\'broadcast_function\'] = \'expand_\' + (\'inplace\' if broadcast_inplace\n                                                        else \'size\' if broadcast_dims else \'outplace\')\n            option[\'broadcast_modified_actuals\'] = [\'b_\' + y if \'b_\' + y in option[\'broadcast_returns\'] else y\n                                                    for y in option[\'actuals\']]\n\n    def native_get_formals(option, include_constants=False):\n        # type: (FunctionOption, bool) -> List[AtFormal]\n        seen = set()  # type: Set[str]\n        pos_args = []\n        kwd_args = []\n\n        def insert(argument):\n            # type: (AtFormal) -> None\n            if argument[\'name\'] not in seen:\n                seen.add(argument[\'name\'])\n                if argument.get(\'kwarg_only\', False):\n                    kwd_args.append(argument)\n                else:\n                    pos_args.append(argument)\n\n        for argument in option[\'arguments\']:\n            insert(argument)\n\n        # not clear we need dynamic_type translation as we can specify the correct type\n        # directly in native functions\n        def add_dynamic_type(argument, option):\n            # type: (AtFormal, FunctionOption) -> AtFormal\n            argument[\'dynamic_type\'] = NATIVE_DYNAMIC_TYPE.get(argument[\'type\'], argument[\'type\'])\n            return argument\n\n        result = pos_args + kwd_args\n        result = [add_dynamic_type(argument, option) for argument in result]\n\n        # ensure we get reference-type formals when appropriate\n        def native_translate_formals(argument, option):\n            # type: (AtFormal, FunctionOption) -> AtFormal\n            def translate_map(const):\n                # type: (bool) -> Dict[str, str]\n                return {\n                    \'Tensor\': \'const Tensor &\' if const else \'Tensor &\',\n                    \'Type\': \'const Type &\' if const else \'Type &\',\n                    \'TensorOptions\': \'const TensorOptions &\' if const else \'TensorOptions &\',\n                    \'TensorList\': \'TensorList\',\n                }\n\n            if argument.get(\'is_nullable\') and argument[\'type\'] not in translate_map(False).keys():\n                argument[\'type\'] = ""c10::optional<{}>"".format(argument[\'type\'])\n\n            if (option[\'inplace\'] and argument[\'name\'] == \'self\') or argument.get(\'output\', False):\n                argument[\'type\'] = translate_map(False).get(argument[\'type\'], argument[\'type\'])\n            else:\n                argument[\'type\'] = translate_map(True).get(argument[\'type\'], argument[\'type\'])\n\n            return argument\n\n        result = [native_translate_formals(argument, option) for argument in result]\n        return result\n\n    # this can return multiple return types in a list, e.g. [\'Tensor\', \'Tensor\']\n    def native_get_return_types(option):\n        # type: (FunctionOption) -> List[ReturnType]\n        ret = option[\'return\']\n\n        return_types = []  # List[ReturnType]\n        for t_raw in ret:\n            # See Note [field_name versus name]\n            field_name = None\n            if isinstance(t_raw, string_type):\n                t = t_raw\n                name = None\n            else:\n                t = t_raw[\'type\']\n                name = t_raw[\'name\']\n                if \'field_name\' in t_raw:\n                    field_name = t_raw[\'field_name\']\n\n            # can\'t actually return a TensorList (since it\'s a reference object)\n            actual_return_type = {\'TensorList\': \'std::vector<Tensor>\'}.get(t, t)\n\n            if actual_return_type == \'Tensor\' and (option[\'inplace\'] or option[\'api_name\'].endswith(\'_out\')):\n                # follow normal ATen convention of returning Tensor & for inplace functions.\n                actual_return_type = \'Tensor &\'\n\n            rtype = {\n                \'type\': actual_return_type,\n                \'dynamic_type\': NATIVE_DYNAMIC_TYPE.get(t, t),\n            }  # type: ReturnType\n            if name is not None:\n                rtype[\'name\'] = name\n            if field_name is not None:\n                rtype[\'field_name\'] = field_name\n            return_types.append(rtype)\n\n        return return_types\n\n    def process_native(option):\n        # type: (FunctionOption) -> Optional[OutputDeclaration]\n        assert option[\'python_module\'] == \'\' or option[\'python_module\'] == \'nn\', \\\n            ""Found python_module of {} for decl {}, but only \\\'\\\' string or \\\'nn\\\' are supported"".format(\n                option[\'python_module\'], option[\'name\'])\n        formals = native_get_formals(option)\n        option[\'formals_list\'] = formals\n        option[\'formals\'] = [format_formal(f) for f in formals]\n        option[\'formals_with_defaults\'] = [formal_with_default(f) for f in formals]\n        option[\'returns\'] = native_get_return_types(option)\n        option[\'return_type\'] = format_return_type(option[\'returns\'])\n        option[\'return_call\'] = \'return \' if option[\'return_type\'] != \'void\' else \'\'\n        option[\'actuals\'] = [f[\'name\'] for f in formals]\n\n        option[\'formals_types\'] = [f[\'type\'] for f in option[\'formals_list\']]\n        option[\'native_actuals\'] = [f[\'name\'] for f in option[\'formals_list\']]\n        if len(option[\'native_actuals\']) == 0:\n            option[\'native_actuals_with_comma_prefix\'] = \'\'\n        else:\n            option[\'native_actuals_with_comma_prefix\'] = \', \' + \', \'.join(option[\'native_actuals\'])\n\n        option[\'formals_types_with_return\'] = [option[\'return_type\']]\n        if len(option[\'formals_types\']) > 0:\n            option[\'formals_types_with_return\'].extend(option[\'formals_types\'])\n\n        option[\'method_formals\'] = [format_formal(f) for f in formals\n                                    if f[\'name\'] != \'self\']\n        option[\'method_formals_with_defaults\'] = (\n            [formal_with_default(f) for f in formals if f[\'name\'] != \'self\'])\n        # *this is \'const Tensor&\' since all Tensor methods are const and must\n        # be const_casted to be accepted as native function\'s non-const argument\n        option[\'method_actuals\'] = [\n            f[\'name\'] if f[\'name\'] != \'self\' else \'const_cast<Tensor&>(*this)\' for f in formals]\n        if len(option[\'method_actuals\']) == 0:\n            option[\'method_actuals_with_comma_prefix\'] = \'\'\n        else:\n            option[\'method_actuals_with_comma_prefix\'] = \', \' + \', \'.join(option[\'method_actuals\'])\n\n        def find_formal(formal_name, formals):\n            for formal in formals:\n                if formal_name == formal[\'dynamic_type\']:\n                    return formal\n            return None\n\n        def has_named_tensor_formals(formals):\n            return any([\'Dimname\' in formal[\'dynamic_type\'] for formal in formals])\n\n        def gen_tensor_method(option, multidispatch_tensors):\n            # type: (Any, List[str]) -> FunctionCode\n            def swizzle_self(t):  # blegh\n                if t == \'self\':\n                    return \'*this\'\n                else:\n                    return t\n            option[\'inferred_type_set\'] = \'c10::detail::multi_dispatch_tensor_type_set({})\'.format(\n                \', \'.join(swizzle_self(t) for t in multidispatch_tensors)\n            )\n\n            if isinstance(type_method_dispatch, dict):\n                static_dispatch_function_switches = []\n                # NB: As this code is currently written, there will NEVER be\n                # a backend generated for variable dispatch.  There is nothing\n                # stopping us from actually implementing this, however, if you\n                # really wanted variable on mobile, there\'s nothing stopping\n                # you from implementing this (however, you would have an\n                # annoying phase problem, since code generation for variable\n                # happens in tools/ which happens later than here.)\n                #\n                # If you pass in a variable to the dispatch, and variable is\n                # enabled, this switch will fail.  This is intentional: you\n                # probably need to disable variable globally in the mobile\n                # calling code.\n                for backend in static_dispatch_backends:\n                    if backend in type_method_dispatch:\n                        static_dispatch_function_switches.append(STATIC_DISPATCH_FUNCTION_SWITCH_STATEMENT.substitute(\n                            option,\n                            backend=backend,\n                            backend_function=type_method_dispatch[backend],\n                            native_arguments=option[\'method_actuals\']))\n                static_dispatch_method_body = STATIC_DISPATCH_FUNCTION_SWITCH_BODY.substitute(\n                    option,\n                    type_set=\'type_set()\',\n                    static_dispatch_function_switches=static_dispatch_function_switches)\n            else:\n                static_dispatch_method_body = STATIC_DISPATCH_FUNCTION_DEFAULT_BODY.substitute(\n                    option, native_arguments=option[\'method_actuals\'])\n\n            if option[\'use_c10_dispatcher\'] == \'unboxed_only\':\n                method_definition = C10_UNBOXEDONLY_TENSOR_METHOD_DEFINITION\n            else:\n                assert option[\'use_c10_dispatcher\'] == \'full\'\n                method_definition = C10_TENSOR_METHOD_DEFINITION\n            return FunctionCode(\n                declaration=TENSOR_METHOD_DECLARATION.substitute(\n                    option, static_dispatch_method_body=static_dispatch_method_body),\n                definition=method_definition.substitute(\n                    option, static_dispatch_method_body=static_dispatch_method_body))\n\n        def gen_namespace_function(option, multidispatch_tensors):\n            # type: (Any, List[str]) -> FunctionCode\n            option[\'inferred_type_set\'] = (\n                \'c10::detail::multi_dispatch_tensor_type_set({})\'.format(\', \'.join(multidispatch_tensors)))\n            declaration = DEPRECATED_FUNCTION_DECLARATION if option[\'deprecated\'] else FUNCTION_DECLARATION\n            fn_declaration = declaration.substitute(option)\n\n            if isinstance(type_method_dispatch, dict):\n                static_dispatch_function_switches = []\n                for backend in static_dispatch_backends:\n                    if backend in type_method_dispatch:\n                        static_dispatch_function_switches.append(STATIC_DISPATCH_FUNCTION_SWITCH_STATEMENT.substitute(\n                            option,\n                            backend=backend,\n                            backend_function=type_method_dispatch[backend],\n                            native_arguments=option[\'native_actuals\']))\n                static_dispatch_function_body = STATIC_DISPATCH_FUNCTION_SWITCH_BODY.substitute(\n                    option,\n                    type_set=option[\'inferred_type_set\'],\n                    static_dispatch_function_switches=static_dispatch_function_switches)\n            else:\n                static_dispatch_function_body = STATIC_DISPATCH_FUNCTION_DEFAULT_BODY.substitute(\n                    option, native_arguments=option[\'native_actuals\'])\n\n            if is_factory_method:\n                if option[\'use_c10_dispatcher\'] == \'unboxed_only\':\n                    fn_definition = C10_UNBOXEDONLY_FACTORY_DEFINITION.substitute(\n                        option, static_dispatch_function_body=static_dispatch_function_body)\n                else:\n                    assert option[\'use_c10_dispatcher\'] == \'full\'\n                    fn_definition = C10_FACTORY_DEFINITION.substitute(\n                        option, static_dispatch_function_body=static_dispatch_function_body)\n            else:\n                if option[\'use_c10_dispatcher\'] == \'unboxed_only\':\n                    fn_definition = C10_UNBOXEDONLY_FUNCTION_DEFINITION.substitute(\n                        option, static_dispatch_function_body=static_dispatch_function_body)\n                else:\n                    assert option[\'use_c10_dispatcher\'] == \'full\'\n                    fn_definition = C10_FUNCTION_DEFINITION.substitute(\n                        option, static_dispatch_function_body=static_dispatch_function_body)\n            return FunctionCode(definition=fn_definition, declaration=fn_declaration)\n\n        # Emit #ifdef BUILD_NAMEDTENSOR macros for any code generated here\n        # that is sent to top_env.\n        is_named_tensor_only = (has_named_tensor_formals(formals) or\n                                option[\'api_name\'] == \'align_tensors\' or\n                                option[\'api_name\'] == \'align_as\')\n\n        def check_namedtensor_enabled(code):\n            if is_named_tensor_only:\n                return NAMEDTENSOR_CHECK.substitute(code=code)\n            return code\n\n        def add_namedtensor_enabled_macro(code):\n            # type: (FunctionCode) -> FunctionCode\n            return FunctionCode(\n                definition=NAMEDTENSOR_CHECK.substitute(code=code.definition),\n                declaration=NAMEDTENSOR_CHECK.substitute(code=code.declaration))\n\n        assert find_formal(\'Type\', formals) is None, \\\n            ""Found Type argument in {}({}). Use TensorOptions instead."".format(\n                option[\'name\'], "", "".join(option[\'method_formals_with_defaults\']))\n\n        type_method_dispatch = option[\'type_method_definition_dispatch\']\n\n        multidispatch_tensors = find_multidispatch_tensors(formals)\n\n        option[\'type_method_formals\'] = [format_formal(f) for f in formals]\n        option[\'type_method_actuals\'] = [f[\'name\'] for f in formals]\n        option[\'native_actuals\'] = [f[\'name\'] for f in formals]\n        if len(option[\'native_actuals\']) == 0:\n            option[\'native_actuals_with_comma_prefix\'] = \'\'\n        else:\n            option[\'native_actuals_with_comma_prefix\'] = \', \' + \', \'.join(option[\'native_actuals\'])\n\n        is_method = \'method\' in option[\'variants\']\n        is_namespace_function = \'function\' in option[\'variants\']\n        # For method-only entries, the first argument should be self\n        if is_method and not is_namespace_function:\n            assert formals[0][\'name\'] == \'self\'\n        is_factory_method = find_formal(\'TensorOptions\', formals) and \'method\' not in option[\'variants\']\n\n        check_methods_do_not_start_with_underscore(option[\'name\'], is_method)\n\n        option[\'method_prefix_derived\'] = \'\'\n        # NB: Device guard and scalar type generated code is still based on the\n        # first argument.  Scalar type test will be removed once TH is removed.\n        # If you need more complex device guard behavior, you should disable\n        # device guard and then manually add the guards you need.\n        dispatch_options = find_formal(\'TensorOptions\', formals)\n        guard_tensor = None if dispatch_options else find_dispatch_tensor(formals)\n        option[\'device_guard_declaration\'] = device_guard(option, dispatch_options, guard_tensor)\n        option[\'named_guard_declaration\'] = named_guard(option, find_tensors(formals),\n                                                        find_tensorlists(formals))\n        option[\'dispatch_scalar_type_declaration\'] = dispatch_scalar_type(option, dispatch_options, guard_tensor)\n\n        broadcast_arg = get_broadcast_argument(option)\n        if broadcast_arg is not None:\n            raise Exception(""broadcasting is not yet supported for native functions, ""\n                            ""but specified for function {}"", option[\'name\'])\n\n        top_env[\'list_of_aten_ops\'].append(\n            check_namedtensor_enabled(OPERATOR_NAME.substitute(option))\n        )\n        option[\'native_type_method_dispatch\'] = type_method_dispatch\n\n        # Note [Abstract ATen methods]\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # An abstract ATen method is one whose dispatch differs between\n        # types.  These are implemented in derived types (with a\n        # standard (throwing) definition in Type).  A concrete ATen\n        # method is one which has the same dispatch for all types;\n        # we just implement it in the base Type.  This is exposed\n        # in Declarations.yaml via a field named \'abstract\'.\n        abstract = False\n        if isinstance(type_method_dispatch, dict):\n            abstract = True\n        else:\n            top_env[\'type_method_declarations\'].append(\n                check_namedtensor_enabled(NATIVE_DISPATCH_DECLARATION.substitute(option)))\n            top_env[\'type_method_definitions\'].append(\n                check_namedtensor_enabled(NATIVE_DISPATCH_DEFINITION_DEFAULT.substitute(option)))\n            if option[\'use_c10_dispatcher\'] == \'full\':\n                top_env[\'function_registrations\'].append(\n                    check_namedtensor_enabled(DEFAULT_FUNCTION_REGISTRATION.substitute(option)))\n            else:\n                assert option[\'use_c10_dispatcher\'] == \'unboxed_only\'\n                top_env[\'function_registrations\'].append(\n                    check_namedtensor_enabled(DEFAULT_UNBOXEDONLY_FUNCTION_REGISTRATION.substitute(option)))\n\n        # generate the at::native function declarations (i.e. what the user will implement)\n        if isinstance(type_method_dispatch, dict):\n            generated_native_functions = []  # type: List[str]\n            for key in sorted(type_method_dispatch.keys()):\n                value = type_method_dispatch[key]\n                # skip functions in different namespace, e.g. legacy::cpu\n                if ""::"" in value:\n                    continue\n                if value not in generated_native_functions:\n                    option[\'native_type_method_dispatch\'] = value\n                    top_env[\'native_function_declarations\'].append(\n                        check_namedtensor_enabled(NATIVE_DECLARATION.substitute(option)))\n                    generated_native_functions.append(value)\n        else:\n            top_env[\'native_function_declarations\'].append(\n                check_namedtensor_enabled(NATIVE_DECLARATION.substitute(option)))\n\n        method_of = [\'Type\']\n        if is_method:\n            code = gen_tensor_method(option, multidispatch_tensors)\n            if is_named_tensor_only:\n                code = add_namedtensor_enabled_macro(code)\n            top_env[\'tensor_method_declarations\'].append(code.declaration)\n            top_env[\'tensor_method_definitions\'].append(code.definition)\n            method_of.append(\'Tensor\')\n\n        if is_namespace_function:\n            code = gen_namespace_function(option, multidispatch_tensors)\n            if is_named_tensor_only:\n                code = add_namedtensor_enabled_macro(code)\n            top_env[\'function_definitions\'].append(code.definition)\n            top_env[\'function_declarations\'].append(code.declaration)\n            method_of.append(\'namespace\')\n\n        if not BUILD_NAMEDTENSOR and is_named_tensor_only:\n            return None\n        return OutputDeclaration(\n            name=option[\'api_name\'],\n            operator_name=option[\'operator_name\'],\n            overload_name=option[\'overload_name\'],\n            use_c10_dispatcher=option[\'use_c10_dispatcher\'],\n            category_override=option[\'category_override\'],\n            matches_jit_signature=option[""matches_jit_signature""],\n            schema_string=option[""schema_string""],\n            method_prefix_derived=option[\'method_prefix_derived\'],\n            arguments=formals,\n            method_of=method_of,\n            mode=option[\'mode\'],\n            python_module=option[\'python_module\'],\n            buffers=None,\n            returns=option[\'returns\'],\n            inplace=option[\'inplace\'],\n            is_factory_method=is_factory_method,\n            # See Note [Abstract ATen methods]\n            abstract=abstract,\n            requires_tensor=option.get(\'requires_tensor\', False),\n            device_guard=option.get(\'device_guard\', True),\n            with_gil=option.get(\'with_gil\', False),\n            deprecated=option[\'deprecated\'],\n        )\n\n    output_declarations = []  # type: List[OutputDeclaration]\n    for declaration in declarations:\n        output_options = []  # type: List[OutputDeclaration]\n        for option in declaration[\'options\']:\n            option[""matches_jit_signature""] = declaration[""matches_jit_signature""]\n            option[""schema_string""] = declaration[""schema_string""]\n            try:\n                if option[\'mode\'] != \'native\':\n                    # Mutably populate option with values\n                    process_legacy_th_option(option)\n                else:\n                    output_option = process_native(option)\n                    if output_option:\n                        output_options.append(output_option)\n            except NYIError:\n                option[\'skip\'] = True\n        output_declarations.extend(output_options)\n\n    return output_declarations\n\n\ndef create_derived(backend_type_env, declarations):\n    # type: (Environment, List[FunctionOption]) -> Tuple[List[str], List[str], List[str], List[str], List[str]]\n    type_object_declarations = []  # type: List[str]\n    type_object_definitions = []  # type: List[str]\n    function_registrations = []  # type: List[str]\n    legacy_th_declarations = []  # type: List[str]\n    legacy_th_definitions = []  # type: List[str]\n    is_cuda = \'CUDA\' in backend_type_env[\'Backend\']\n\n    def requires_checked_cast(argument):\n        # type: (THFormal) -> bool\n        if argument[\'type\'] == \'IntArrayRef\':\n            return \'size\' in argument\n        return argument[\'type\'] in CHECKED_CAST\n\n    def nullable_argument(argument):\n        # type: (THFormal) -> bool\n        return argument.get(\'is_nullable\', False)\n\n    def get_argument(env, argument, option):\n        # type: (Environment, THFormal, FunctionOption) -> str\n        if requires_checked_cast(argument):\n            checked_use = CHECKED_USE.get(\n                argument[\'type\'], \'{}_\').format(argument[\'name\'])\n            if nullable_argument(argument):\n                checked_use = CHECKED_USE_NULLABLE.substitute(\n                    env={}, arg_name=argument[\'name\'], usage=checked_use)\n            return checked_use\n        elif argument[\'type\'] == \'CONSTANT\':\n            v = str(argument.get(\'default\', argument[\'name\']))\n            for pattern, replacement in CONSTANT_REPLACEMENTS:\n                v = re.sub(pattern, replacement, v)\n            return CodeTemplate(v).substitute(env)\n        # e.g. argument 0, i.e. repeat the 0th argument in this position...\n        elif argument[\'type\'] == \'argument\':\n            index = int(argument[\'name\'])\n            return get_argument(env, option[\'arguments\'][index], option)\n        else:\n            return argument[\'name\']\n\n    def drop_argument(argument, option):\n        # type: (THFormal, FunctionOption) -> bool\n        # Devices are handled in the body of the function.\n        if argument[\'name\'] == \'device\':\n            return True\n        return False\n\n    def get_arguments(env, arguments, option):\n        # type: (Environment, List[THFormal], FunctionOption) -> List[str]\n        return [get_argument(env, argument, option)\n                for argument in arguments if not drop_argument(argument, option)]\n\n    def is_actual_return_long(env, ret):\n        # type: (Environment, ReturnDecl) -> bool\n        if ret[\'type\'] == \'long\':\n            return True\n        if ret[\'type\'] == \'real\':\n            return env[\'ScalarName\'] == \'Long\'\n        if ret[\'type\'] == \'accreal\':\n            return env[\'AccScalarName\'] == \'Long\'\n        return False\n\n    def handle_zero_dim(env, option):\n        # type: (Environment, FunctionOption) -> List[str]\n        zero_dim_dispatch = option.get(\'zero_dim_dispatch_when_scalar\', \'\')\n        if not zero_dim_dispatch:\n            return []\n        broadcasts_arg = zero_dim_dispatch in option.get(\'broadcast_actuals\', \'\')\n        # if the argument broadcasts, then this would only affect cases where all broadcasted\n        # tensors were zero-dim, which is inconsistent with the scalar handling.\n        if broadcasts_arg:\n            return []\n        zero_dim_actuals = [arg[\'name\']\n                            if arg[\'name\'] != zero_dim_dispatch else ""{}.item()"".format(arg[\'name\'])\n                            for arg in option[\'formals_list\']]\n        return [ZERO_DIM_CHECK.substitute(env, check_name=zero_dim_dispatch, zero_dim_actuals=zero_dim_actuals)]\n\n    def allocate_arg(env, arg, output_count):\n        # type: (Environment, THFormal, int) -> List[str]\n        name = arg[\'name\']\n        allocation = CodeTemplate(ALLOC_NOARGS_WRAP[arg[\'type\']]).substitute(env)\n        tensor_arg = \'{}_\'.format(name)\n        if arg.get(\'mask\', False):\n            allocation = \'output_mask[{}] ? {} : nullptr\'.format(output_count, allocation)\n            tensor_arg = (\'{}_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*){}_\'\n                          .format(name, name))\n        intrusive_ptr_type = \'c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>\'\n        return [\n            \'auto {}_ = {};\'.format(name, allocation),\n            \'auto {} = Tensor({}::reclaim({}));\'.format(name, intrusive_ptr_type, tensor_arg),\n        ]\n\n    def resize_arg(arg):\n        # type: (THFormal) -> str\n        resize = arg[\'resize\']\n        if isinstance(resize, str):\n            return ""{}.resize_({}.sizes());"".format(arg[\'name\'], resize)\n        else:\n            resize_scalar = arg.get(\'resize_scalar\', False)\n            if resize_scalar:\n                dims = [\'{}.dim() == 0 ? 1 : {}.size({})\'.format(name, name, dim) for name, dim in resize]\n            else:\n                dims = [\'{}.size({})\'.format(name, dim) for name, dim in resize]\n            return ""{}.resize_({{ {} }});"".format(arg[\'name\'], \',\'.join(dims))\n\n    def handle_call(env, option, cimpl):\n        # type: (Environment, FunctionOption, FunctionOption) -> str\n        is_nn = option[\'mode\'] == \'NN\'\n        actuals = get_arguments(env, cimpl[\'arguments\'], option)\n        if is_cuda or is_nn:\n            actuals = [\'globalContext().getTHCState()\'] + actuals\n\n        cname = cimpl[\'cname\']\n        if option.get(\'sparse\', False):\n            if is_cuda:\n                cname = \'THCS\' + env[\'ScalarName\'] + ""Tensor_"" + cname\n            else:\n                cname = env[\'THTensor\'].replace(\'TH\', \'THS\') + \'_\' + cname\n        elif is_nn:\n            cname = \'THNN_{}\'.format(env[\'THType\']) + cname\n        else:\n            cname = env[\'THTensor\'] + \'_\' + cname\n\n        call = CALL_TEMPLATE.substitute(actuals=actuals, cname=cname)\n        if cimpl.get(\'condition\') is not None:\n            call = \'if ({}) {}\'.format(cimpl[\'condition\'], call)\n        return call\n\n    def emit_body(env, option, scalar_type_cases):\n        # type: (Environment, FunctionOption, List[str]) -> List[str]\n        body = []  # type: List[str]\n        body += handle_zero_dim(env, option)\n\n        cases = []\n        for scalar_name, c_type, accreal, _ in scalar_types:\n            if scalar_name in scalar_type_cases:\n                case_body = []\n                # arguments are potentially duplicated because of one argument\n                # referencing another\n                seen_names = set()  # type: Set[str]\n                seen_tensorlists = set()  # type: Set[str]\n                count = 0\n                output_count = 0\n\n                case_env = {\n                    \'Backend\': env[\'Backend\'],\n                    \'DeviceType\': env[\'DeviceType\'],\n                    \'state\': env[\'state\'],\n                    \'ScalarType\': c_type,\n                    \'ScalarName\': scalar_name,\n                    \'AccScalarName\': accreal,\n                    \'THType\': scalar_name,\n                    \'THTensor\': \'TH{}Tensor\'.format(scalar_name)\n                }  # type: Environment\n                if case_env[\'Backend\'] == \'CUDA\':\n                    sname = \'\' if scalar_name == ""Float"" else scalar_name\n                    case_env[\'THType\'] = \'Cuda{}\'.format(sname)\n                    case_env[\'THTensor\'] = \'THCuda{}Tensor\'.format(sname)\n\n                # scalar_check is the heuristic conditions when a result may be a scalar_check\n                # if there is a IntArrayRefSize argument, then its dimensions are used to determine scalar.\n                # otherwise, it is true if all the input tensors are scalars,\n                scalar_check_is_from_size = False\n                scalar_check_is_from_option = False\n                scalar_check = None\n                scalar_check_opt = option.get(\'scalar_check\')\n                if scalar_check_opt is not None:\n                    if isinstance(scalar_check_opt, bool):\n                        scalar_check = str(scalar_check_opt).lower()\n                    else:\n                        scalar_check = scalar_check_opt\n                    scalar_check_is_from_option = True\n\n                for arg in option[\'arguments\']:\n                    if is_real_argument_to_wrapper(arg):\n                        count += 1\n                    if arg[\'type\'] == \'IntArrayRefSize\' and not scalar_check_is_from_option:\n                        scalar_check_is_from_size = True\n                        scalar_check = \'{}.size() == 0\'.format(arg[\'name\'])\n                    if arg[\'type\'] == \'TensorList\':\n                        seen_tensorlists.add(arg[\'name\'])\n\n                    wrap_dim_target = arg.get(\'wrap_dim\', None)\n                    if wrap_dim_target is not None:\n                        # for Tensors, ""name_"" is the TensorImpl, but for TensorLists, it is an\n                        # std::vector of TH*s.  Since TH*s have different dimension rules, we used\n                        # ""name"" instead, but keep ""name_"" for tensor to avoid an extra function call.\n                        if wrap_dim_target not in seen_tensorlists:\n                            wrap_dim_target = wrap_dim_target + ""_""\n                        case_body.append(""{} = maybe_wrap_dim({}, {});"".format(\n                            arg[\'name\'], arg[\'name\'], wrap_dim_target))\n\n                    # only generated checked casts the first time we see it\n                    if arg[\'name\'] not in seen_names and requires_checked_cast(arg):\n                        seen_names.add(arg[\'name\'])\n\n                        # make a new allocation of TensorImpl, then wrap a Tensor around it.\n                        if arg.get(\'allocate\', False):\n                            case_body += allocate_arg(case_env, arg, output_count)\n                            output_count += 1\n                        # extract the TensorImpl from an existing tensor (or Storage, etc.)\n                        else:\n                            # special case where we allow undefined Tensors, and thus\n                            # the checked cast succeeds even if the Tensor is not\n                            # defined\n                            null_okay = \'true\' if nullable_argument(arg) else \'false\'\n\n                            check_cast = CHECKED_CAST[arg[\'type\']].substitute(\n                                case_env, arg_name=arg[\'name\'], arg_pos=count,\n                                api_name=option[\'api_name\'], null_okay=null_okay,\n                                size=arg.get(\'size\'))\n                            case_body.append(""auto {}_ = {};"".format(\n                                arg[\'name\'], check_cast))\n                        if drop_argument(arg, option):\n                            case_body.append(\n                                ""(void) {}_; //silence unused warning"".format(arg[\'name\']))\n\n                        initializers = []\n\n                        # resize tensors for special ops that require it\n                        if \'resize\' in arg:\n                            initializers.append(resize_arg(arg))\n\n                        # also special handling where we zero some outputs.\n                        if arg.get(\'zero\', False) or (arg.get(\'cpu_zero\', False) and not is_cuda):\n                            initializers.append(""{}.zero_();"".format(arg[\'name\']))\n\n                        # only initialize non-null arguments\n                        if nullable_argument(arg) and len(initializers) > 0:\n                            case_body.append(CONDITIONAL_INITIALIZER.substitute({\n                                \'name\': arg[\'name\'],\n                                \'initializer\': initializers\n                            }))\n                        else:\n                            case_body += initializers\n\n                        # for out-of-place: dim() == 0 for all input tensors is and\'d to form\n                        # the test for whether the output is also a scalar\n                        # for in-place: dim() == 0 shouldn\'t change as a result of the operation\n                        if (not arg.get(\'output\') and \'Tensor\' in arg[\'type\'] and\n                                \'TensorList\' not in arg[\'type\'] and\n                                \'THS\' not in arg[\'type\'] and\n                                not scalar_check_is_from_size and\n                                not scalar_check_is_from_option and\n                                not option[\'inplace\']):\n                            check = \'{}->dim() == 0\'.format(arg[\'name\'] + \'_\')\n                            if nullable_argument(arg):\n                                check = \'(!{} || {})\'.format(arg[\'name\'] + \'_\', check)\n                            scalar_check = (check if scalar_check is None\n                                            else scalar_check + \' && \' + check)\n\n                # cimpls, if it exists, contains the underlying C function names and\n                # arguments. Otherwise use option\n                cimpls = option.get(\'cimpls\', [option])\n                calls = [handle_call(case_env, option, cimpl) for cimpl in cimpls]\n\n                ret = option[\'return\']\n\n                if ret[\'kind\'] == \'arguments\':\n                    if \'aten_custom_call\' in option:\n                        # all aten_custom_call bodies handle settings on their own.\n                        scalar_check = None\n                        case_body.append(CodeTemplate(\n                            option[\'aten_custom_call\']).substitute(case_env))\n                    else:\n                        case_body.extend([call + \';\' for call in calls])\n                    arguments_indices = ret[\'arguments\']\n                    arguments = [option[\'arguments\'][argi]\n                                 for argi in arguments_indices]\n                    if scalar_check is not None and scalar_check != \'false\':\n                        if not isinstance(scalar_check, dict):\n                            if len(arguments) > 1:\n                                case_body.append(""bool maybe_scalar = {};"".format(scalar_check))\n                                scalar_check = \'maybe_scalar\'\n                        for arg in arguments:\n                            scalar_check_arg = (scalar_check if not isinstance(scalar_check, dict)\n                                                else scalar_check.get(arg[\'name\']))  # type: ignore\n                            # maybe_zero_dim(false) is a no-op\n                            if scalar_check_arg is not None and scalar_check_arg != \'false\':\n                                stmt = ""{}_->maybe_zero_dim({});"".format(arg[\'name\'], scalar_check_arg)\n                                if nullable_argument(arg):\n                                    stmt = ""if ({}_) {}"".format(arg[\'name\'], stmt)\n                                case_body.append(stmt)\n                    if len(arguments_indices) == 1:\n                        arg = arguments[0]\n                        case_body.append(""return {};"".format(arg[\'name\']))\n                    else:\n                        types = [to_return_type(arg, option)[\'type\']\n                                 for arg in arguments]\n                        # TODO: check for move semantics...\n                        names = [arg[\'name\'] for arg in arguments]\n                        case_body.append(CodeTemplate(""return std::tuple<${types}>(${names});"").substitute(\n                            types=types, names=names))\n                elif ret[\'kind\'] == \'type\':\n                    assert len(calls) == 1\n                    call = calls[0]\n                    if \'aten_custom_call\' in option:\n                        # all aten_custom_call bodies handle settings on their own.\n                        scalar_check = None\n                        case_body.append(CodeTemplate(\n                            option[\'aten_custom_call\']).substitute(case_env))\n\n                    if ret[\'type\'] in ALLOC_WRAP.keys():\n                        maybe_scalar = ""->maybe_zero_dim({})"".format(scalar_check) \\\n                                       if scalar_check is not None \\\n                                       else """"\n                        wrapped_tensor = CodeTemplate(ALLOC_WRAP[ret[\'type\']]).substitute(\n                            case_env, arguments=[call])\n                        return_tensor = (\n                            ""return Tensor("" +\n                            ""c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim("" +\n                            ""(${wrapped_tensor})${maybe_scalar}));"")\n                        case_body.append(CodeTemplate(return_tensor).substitute(\n                            case_env, wrapped_tensor=wrapped_tensor, maybe_scalar=maybe_scalar))\n                    # return the same underlying Tensor type for both real and accreal; this ensures\n                    # e.g. x.sum(0) and x.sum() return the same type. We explicitly cast to the\n                    # ScalarType before constructing the scalar_tensor to avoid overflow checking.\n                    elif ret[\'type\'] == \'accreal\' or ret[\'type\'] == \'real\':\n                        return_scalar = (\'return at::scalar_tensor(convert<${ScalarType}>(${call}), \'\n                                         \'options(ScalarType::${ScalarName}));\')\n                        case_body.append(CodeTemplate(return_scalar).substitute(case_env, call=call))\n                    else:\n                        # we using int64_t for long in the API, so correct it here...\n                        if is_actual_return_long(case_env, ret):\n                            call = ""static_cast<int64_t>({})"".format(call)\n                        case_body.append(""return {};"".format(call))\n                else:\n                    raise Exception(""NYI - return handling"")\n\n                cases.append(LEGACY_TH_DEFINITION_CASE.substitute(case_env, case_body=case_body))\n        body.append(LEGACY_TH_DEFINITION_SWITCH_STATEMENT.substitute(env, cases=cases))\n        return body\n\n    def process_legacy_th_option(option):\n        # type: (FunctionOption) -> None\n        backend = backend_type_env[\'Backend\']\n        if backend in option[\'backend_types\']:\n            env = nested_dict(option, backend_type_env)\n            body = emit_body(env, option, option[\'backend_types\'][backend])  # type: ignore\n            option[\'type_definition_body\'] = body\n            if option.get(\'broadcast_actuals\', None):\n                legacy_th_declarations.append(\n                    LEGACY_TH_DECLARATION_BROADCAST.substitute(env))\n                legacy_th_definitions.append(\n                    LEGACY_TH_DEFINITION_BROADCAST.substitute(env))\n            legacy_th_declarations.append(\n                LEGACY_TH_DECLARATION.substitute(env))\n            legacy_th_definitions.append(\n                LEGACY_TH_DEFINITION.substitute(env))\n\n    def process_native(option):\n        # type: (FunctionOption) -> None\n        dispatch = option[\'type_method_definition_dispatch\']\n        env = nested_dict(option, backend_type_env)\n\n        if isinstance(dispatch, dict):\n            backend = backend_type_env[\'Backend\']\n            if backend in option[\'backend_types\']:\n                native_dispatch = dispatch.get(backend)\n                if native_dispatch:\n                    type_object_declarations.append(\n                        NATIVE_DISPATCH_DECLARATION.substitute(env))\n                    option[\'native_type_method_dispatch\'] = native_dispatch\n                    type_object_definitions.append(\n                        NATIVE_DISPATCH_DEFINITION_BACKEND.substitute(env))\n                    if option[\'use_c10_dispatcher\'] == \'full\':\n                        function_registrations.append(\n                            BACKEND_FUNCTION_REGISTRATION.substitute(env))\n                    else:\n                        assert option[\'use_c10_dispatcher\'] == \'unboxed_only\'\n                        function_registrations.append(\n                            BACKEND_UNBOXEDONLY_FUNCTION_REGISTRATION.substitute(env))\n\n    for declaration in declarations:\n        for option in declaration[\'options\']:\n            if not option.get(\'skip\', False):\n                try:\n                    if option[\'mode\'] == \'NN\' and option.get(\'cimpls\') is None:\n                        continue\n                    if option[\'mode\'] != \'native\':\n                        process_legacy_th_option(option)\n                    else:\n                        process_native(option)\n                except NYIError:\n                    pass\n    return (type_object_declarations, type_object_definitions, function_registrations,\n            legacy_th_declarations, legacy_th_definitions)\n'"
aten/src/ATen/gen.py,0,"b'\nimport argparse\nimport os\n\nimport yaml\nfrom collections import OrderedDict\n\nimport sys\nfrom os import path\nsys.path.append(path.dirname(path.abspath(__file__)))\n\nimport cwrap_parser\nimport nn_parse\nimport native_parse\nimport preprocess_declarations\nimport function_wrapper\n\nfrom code_template import CodeTemplate\nfrom env import BUILD_NAMEDTENSOR\n\n\n# This file is the top-level entry point for code generation in ATen.\n# It takes an arbitrary number of arguments specifying metadata files to\n# process (.cwrap, .yaml and .h) and outputs a number generated header\n# and cpp files in ATen/ (see invocations of \'write\' for each file that\n# is written.) It is invoked from cmake; look for the \'cwrap_files\'\n# variable for an up-to-date list of files which are passed.\n\nparser = argparse.ArgumentParser(description=\'Generate ATen source files\')\nparser.add_argument(\'files\', help=\'cwrap files\', nargs=\'+\')\n\nparser.add_argument(\n    \'-s\',\n    \'--source-path\',\n    help=\'path to source directory for ATen\',\n    default=\'.\')\nparser.add_argument(\n    \'-o\',\n    \'--output-dependencies\',\n    help=\'output a list of dependencies into the given file and exit\')\nparser.add_argument(\n    \'-d\', \'--install_dir\', help=\'output directory\', default=\'ATen\')\nparser.add_argument(\n    \'--rocm\',\n    action=\'store_true\',\n    help=\'reinterpret CUDA as ROCm/HIP and adjust filepaths accordingly\')\noptions = parser.parse_args()\n# NB: It is mandatory to NOT use os.path.join here, as the install directory\n# will eventually be ingested by cmake, which does not respect Windows style\n# path slashes.  If you switch this to use os.path.join, you\'ll get an error\n# like:\n#\n#   Syntax error in cmake code when parsing string\n#\n#     C:/Jenkins/workspace/pytorch-builds/pytorch-win-ws2016-cuda9-cudnn7-py3-build/build/aten/src/ATen\\core/TensorMethods.h\n#\n#   Invalid character escape \'\\c\'.\ncore_install_dir = options.install_dir + \'/core\' if options.install_dir is not None else None\nif options.install_dir is not None and not os.path.exists(options.install_dir):\n    os.makedirs(options.install_dir)\nif core_install_dir is not None and not os.path.exists(core_install_dir):\n    os.makedirs(core_install_dir)\n\n\nclass FileManager(object):\n    def __init__(self, install_dir=None):\n        self.install_dir = install_dir if install_dir else options.install_dir\n        self.filenames = set()\n        self.outputs_written = False\n        self.undeclared_files = []\n\n    def will_write(self, filename):\n        filename = \'{}/{}\'.format(self.install_dir, filename)\n        if self.outputs_written:\n            raise Exception(""\'will_write\' can only be called before "" +\n                            ""the call to write_outputs, refactor so outputs are registered "" +\n                            ""before running the generators"")\n        self.filenames.add(filename)\n\n    def _write_if_changed(self, filename, contents):\n        try:\n            with open(filename, \'r\') as f:\n                old_contents = f.read()\n        except IOError:\n            old_contents = None\n        if contents != old_contents:\n            with open(filename, \'w\') as f:\n                f.write(contents)\n\n    def write_outputs(self, filename):\n        """"""Write a file containing the list of all outputs which are\n        generated by this script.""""""\n        self._write_if_changed(\n            filename,\n            \'\'.join(name + "";"" for name in sorted(self.filenames)))\n        self.outputs_written = True\n\n    def write(self, filename, s, env=None):\n        filename = \'{}/{}\'.format(self.install_dir, filename)\n        if isinstance(s, CodeTemplate):\n            assert env is not None\n            env[\'generated_comment\'] = ""@"" + ""generated by aten/src/ATen/gen.py""\n            s = s.substitute(env)\n        self._write_if_changed(filename, s)\n        if filename not in self.filenames:\n            self.undeclared_files.append(filename)\n        else:\n            self.filenames.remove(filename)\n\n    def check_all_files_written(self):\n        if len(self.undeclared_files) > 0:\n            raise Exception(\n                ""trying to write files {} which are not "".format(self.undeclared_files) +\n                ""in the list of outputs this script produces. "" +\n                ""use will_write to add them."")\n        if len(self.filenames) > 0:\n            raise Exception(""Outputs declared with \'will_write\' were "" +\n                            ""never written: {}"".format(self.filenames))\n\n\nTEMPLATE_PATH = options.source_path + ""/templates""\nTYPE_DERIVED_CPP = CodeTemplate.from_file(TEMPLATE_PATH + ""/TypeDerived.cpp"")\nSPARSE_TYPE_DERIVED_CPP = CodeTemplate.from_file(TEMPLATE_PATH + ""/SparseTypeDerived.cpp"")\nTYPE_DERIVED_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/TypeDerived.h"")\nTYPE_DEFAULT_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/TypeDefault.h"")\nTYPE_DEFAULT_CPP = CodeTemplate.from_file(TEMPLATE_PATH + ""/TypeDefault.cpp"")\nOPS_ALREADY_MOVED_TO_C10_CPP = CodeTemplate.from_file(TEMPLATE_PATH + ""/OpsAlreadyMovedToC10.cpp"")\n\nTENSOR_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/TensorBody.h"")\nTENSOR_METHODS_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/TensorMethods.h"")\n\nFUNCTIONS_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/Functions.h"")\n\nLEGACY_TH_FUNCTIONS_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/LegacyTHFunctions.h"")\nLEGACY_TH_FUNCTIONS_CPP = CodeTemplate.from_file(TEMPLATE_PATH + ""/LegacyTHFunctions.cpp"")\n\nNATIVE_FUNCTIONS_H = CodeTemplate.from_file(TEMPLATE_PATH + ""/NativeFunctions.h"")\n\ncore_file_manager = FileManager(core_install_dir)\nfile_manager = FileManager()\ncuda_file_manager = FileManager()\n\ndef backend_to_devicetype(backend):\n    if backend == \'QuantizedCPU\':\n        return \'CPU\'\n    return backend\n\nbackends = [\'CPU\', \'CUDA\']\ndensities = [\'Dense\', \'Sparse\', \'Mkldnn\']  # TODO: layout instead of densities?\n\nquantized_backends = [\'QuantizedCPU\']\n\n# scalar_name, c_type, accreal, is_floating_type\nquantized_scalar_types = [\n    (\'QInt8\', \'qint8\', \'QInt8AccrealNotDefined\', \'QInt8IsFloatingTypeNotDefined\'),\n    (\'QUInt8\', \'quint8\', \'QUInt8AccrealNotDefined\', \'QUInt8IsFloatingTypeNotDefined\'),\n    (\'QInt32\', \'qint32\', \'QInt32AccrealNotDefined\', \'Qint32IsFloatingTypeNotDefined\'),\n]\n\n\n# shared environment for non-derived base classes TensorBody.h Storage.h\ntop_env = {\n    \'cpu_type_headers\': [],\n    \'cuda_type_headers\': [],\n    \'function_registrations\': [],\n    \'list_of_aten_ops\': [],\n    \'type_method_declarations\': [],\n    \'type_method_definitions\': [],\n    \'tensor_method_declarations\': [],\n    \'tensor_method_definitions\': [],\n    \'function_declarations\': [],\n    \'function_definitions\': [],\n    \'type_ids\': [],\n    \'native_function_declarations\': [],\n}\n\n\ndef dict_representer(dumper, data):\n    return dumper.represent_dict(data.items())\n\n\ndef postprocess_output_declarations(output_declarations):\n    # ensure each return has a name associated with it\n    for decl in output_declarations:\n        has_named_ret = False\n        for n, ret in enumerate(decl.returns):\n            if \'name\' not in ret:\n                assert not has_named_ret\n                if decl.inplace:\n                    ret[\'name\'] = \'self\'\n                elif len(decl.returns) == 1:\n                    ret[\'name\'] = \'out\'\n                else:\n                    ret[\'name\'] = \'out\' + str(n)\n            else:\n                has_named_ret = True\n\n    def remove_key_if_none(dictionary, key):\n        if key in dictionary.keys() and dictionary[key] is None:\n            del dictionary[key]\n        return dictionary\n\n    return [remove_key_if_none(decl._asdict(), \'buffers\')\n            for decl in output_declarations]\n\n\ndef format_yaml(data):\n    if options.output_dependencies:\n        # yaml formatting is slow so don\'t do it if we will ditch it.\n        return """"\n    noalias_dumper = yaml.dumper.SafeDumper\n    noalias_dumper.ignore_aliases = lambda self, data: True\n    # Support serializing OrderedDict\n    noalias_dumper.add_representer(OrderedDict, dict_representer)\n    # Some yaml parsers (e.g. Haskell\'s) don\'t understand line breaks.\n    # width=float(\'Inf\') turns off optional line breaks and improves\n    # the portability of the outputted yaml.\n    return yaml.dump(data, default_flow_style=False, Dumper=noalias_dumper, width=float(\'Inf\'))\n\n\ndef generate_storage_type_and_tensor(backend, density, declarations):\n    env = {}\n    density_tag = density if density != \'Dense\' else \'\'\n    env[\'Density\'] = density\n    env[\'Type\'] = ""{}{}Type"".format(density_tag, backend)\n    env[\'DeviceType\'] = backend_to_devicetype(backend)\n    env[\'Backend\'] = density_tag + backend\n    env[\'storage_tensor_headers\'] = []\n    if density != \'Sparse\':\n        env[\'storage_tensor_headers\'] = [\'#include <c10/core/TensorImpl.h>\']\n\n    # used for generating switch logic for external functions\n    tag = density_tag + backend\n    env[\'TypeID\'] = \'TypeID::\' + tag\n    top_env[\'type_ids\'].append(tag + \',\')\n\n    env[\'legacy_th_headers\'] = []\n    if backend == \'CUDA\':\n        env[\'extra_cuda_headers\'] = []\n        env[\'extra_cuda_headers\'].append(\'#include <ATen/DeviceGuard.h>\')\n        if options.rocm:\n            env[\'th_headers\'] = [\n                \'#include <THH/THH.h>\',\n                \'#include <THH/THHTensor.hpp>\',\n                \'#include <THHUNN/THHUNN.h>\',\n                \'#undef THNN_\',\n                \'#undef THCIndexTensor_\',\n            ]\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/hip/ATenHIPGeneral.h>\')\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/hip/HIPDevice.h>\')\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/hip/HIPContext.h>\')\n        else:\n            env[\'th_headers\'] = [\n                \'#include <THC/THC.h>\',\n                \'#include <THC/THCTensor.hpp>\',\n                \'#include <THCUNN/THCUNN.h>\',\n                \'#undef THNN_\',\n                \'#undef THCIndexTensor_\',\n            ]\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/cuda/ATenCUDAGeneral.h>\')\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/cuda/CUDADevice.h>\')\n            env[\'extra_cuda_headers\'].append(\'#include <ATen/cuda/CUDAContext.h>\')\n        env[\'state\'] = [\'globalContext().getTHCState()\']\n        env[\'isCUDA\'] = \'true\'\n        env[\'storage_device\'] = \'return storage->device;\'\n        env[\'Generator\'] = \'CUDAGenerator\'\n        env[\'allocator\'] = \'at::cuda::getCUDADeviceAllocator()\'\n    else:\n        env[\'th_headers\'] = [\n            \'#include <TH/TH.h>\',\n            \'#include <TH/THTensor.hpp>\',\n            \'#include <THNN/THNN.h>\',\n            \'#undef THNN_\',\n        ]\n        env[\'extra_cuda_headers\'] = []\n        env[\'state\'] = []\n        env[\'isCUDA\'] = \'false\'\n        env[\'storage_device\'] = \'throw std::runtime_error(""CPU storage has no device"");\'\n        env[\'Generator\'] = \'CPUGenerator\'\n        env[\'allocator\'] = \'getCPUAllocator()\'\n\n    declarations, definitions, registrations, th_declarations, th_definitions = function_wrapper.create_derived(\n        env, declarations)\n    env[\'type_derived_method_declarations\'] = declarations\n    env[\'type_derived_method_definitions\'] = definitions\n    env[\'function_registrations\'] = registrations\n    env[\'legacy_th_declarations\'] = th_declarations\n    env[\'legacy_th_definitions\'] = th_definitions\n\n    fm = file_manager\n    if env[\'DeviceType\'] == \'CUDA\':\n        fm = cuda_file_manager\n\n    if env[\'Backend\'] == \'CPU\' or env[\'Backend\'] == \'CUDA\':\n        env[\'namespace\'] = env[\'Backend\'].lower()\n        env[\'legacy_th_headers\'].append(\'#include <ATen/LegacyTHFunctions\' + env[\'Backend\'] + "".h>"")\n        fm.write(\'LegacyTHFunctions\' + env[\'Backend\'] + "".h"", LEGACY_TH_FUNCTIONS_H, env)\n        fm.write(\'LegacyTHFunctions\' + env[\'Backend\'] + "".cpp"", LEGACY_TH_FUNCTIONS_CPP, env)\n\n    if density != \'Sparse\':\n        fm.write(env[\'Type\'] + "".cpp"", TYPE_DERIVED_CPP, env)\n    else:\n        fm.write(env[\'Type\'] + "".cpp"", SPARSE_TYPE_DERIVED_CPP, env)\n    fm.write(env[\'Type\'] + "".h"", TYPE_DERIVED_H, env)\n\n    if env[\'DeviceType\'] == \'CPU\':\n        top_env[\'cpu_type_headers\'].append(\n            \'#include ""ATen/{}.h""\'.format(env[\'Type\']))\n    else:\n        assert env[\'DeviceType\'] == \'CUDA\'\n        top_env[\'cuda_type_headers\'].append(\n            \'#include ""ATen/{}.h""\'.format(env[\'Type\']))\n\n\n# yields (backend, density) tuples\ndef iterate_types():\n    for backend in backends:\n        for density in densities:\n            if density == \'Mkldnn\' and backend != \'CPU\':\n                continue\n            else:\n                yield (backend, density)\n    for backend in quantized_backends:\n        yield (backend, \'Dense\')\n\n\n###################\n# declare what files will be output _before_ we do any work\n# so that the script runs quickly when we are just querying the\n# outputs\ndef declare_outputs():\n    core_files = [\'TensorBody.h\', \'TensorMethods.h\', \'OpsAlreadyMovedToC10.cpp\']\n    for f in core_files:\n        core_file_manager.will_write(f)\n    files = [\'Declarations.yaml\', \'TypeDefault.cpp\', \'TypeDefault.h\',\n             \'Functions.h\', \'NativeFunctions.h\']\n    for f in files:\n        file_manager.will_write(f)\n    for backend, density in iterate_types():\n        full_backend = backend if density == ""Dense"" else density + backend\n        fm = file_manager\n        if backend == \'CUDA\':\n            fm = cuda_file_manager\n        for kind in [""Type""]:\n            if kind != \'Type\' and density == ""Sparse"":\n                # No Storage or Tensor for sparse\n                continue\n            fm.will_write(""{}{}.h"".format(full_backend, kind))\n            fm.will_write(""{}{}.cpp"".format(full_backend, kind))\n        if backend == \'CPU\' or backend == \'CUDA\':\n            fm.will_write(""LegacyTHFunctions{}.h"".format(backend))\n            fm.will_write(""LegacyTHFunctions{}.cpp"".format(backend))\n\n\ndef filter_by_extension(files, *extensions):\n    filtered_files = []\n    for file in files:\n        for extension in extensions:\n            if file.endswith(extension):\n                filtered_files.append(file)\n    return filtered_files\n\n\ndef is_namedtensor_only_decl(decl):\n    if \'Dimname\' in decl[\'schema_string\']:\n        return True\n    if decl[\'name\'] == \'align_tensors\' or decl[\'name\'] == \'align_as\':\n        return True\n    return False\n\n\ndef generate_outputs():\n    cwrap_files = filter_by_extension(options.files, \'.cwrap\')\n    nn_files = filter_by_extension(options.files, \'nn.yaml\', \'.h\')\n    native_files = filter_by_extension(options.files, \'native_functions.yaml\')\n\n    declarations = [d\n                    for file in cwrap_files\n                    for d in cwrap_parser.parse(file)]\n\n    declarations += nn_parse.run(nn_files)\n    declarations += native_parse.run(native_files)\n    declarations = preprocess_declarations.run(declarations)\n\n    # note: this will fill in top_env[\'type/tensor_method_declarations/definitions\']\n    # and modify the declarations to include any information that will all_backends\n    # be used by function_wrapper.create_derived\n    output_declarations = function_wrapper.create_generic(top_env, declarations)\n    output_declarations = postprocess_output_declarations(output_declarations)\n    file_manager.write(""Declarations.yaml"", format_yaml(output_declarations))\n\n    # Filter out named-tensor only declarations.\n    # They are necessary in create_generic because that generates Type.h, TensorBody.h,\n    # and TensorMethods.h, all of which are checked in to the codebase and therefore\n    # need to be consistent whether or not BUILD_NAMEDTENSOR is on/off.\n    if not BUILD_NAMEDTENSOR:\n        declarations = [decl for decl in declarations\n                        if not is_namedtensor_only_decl(decl)]\n\n    for backend, density in iterate_types():\n        generate_storage_type_and_tensor(backend, density, declarations)\n\n    core_files = {\n        \'TensorBody.h\': TENSOR_H,\n        \'TensorMethods.h\': TENSOR_METHODS_H,\n        \'OpsAlreadyMovedToC10.cpp\': OPS_ALREADY_MOVED_TO_C10_CPP,\n    }\n\n    for core_file, core_template_file in core_files.items():\n        core_file_manager.write(core_file, core_template_file, top_env)\n\n    file_manager.write(\'TypeDefault.h\', TYPE_DEFAULT_H, top_env)\n    file_manager.write(\'TypeDefault.cpp\', TYPE_DEFAULT_CPP, top_env)\n\n    file_manager.write(\'Functions.h\', FUNCTIONS_H, top_env)\n\n    file_manager.write(\'NativeFunctions.h\', NATIVE_FUNCTIONS_H, top_env)\n\n    file_manager.check_all_files_written()\n    cuda_file_manager.check_all_files_written()\n\ndeclare_outputs()\nif options.output_dependencies is not None:\n    file_manager.write_outputs(options.output_dependencies)\n    core_file_manager.write_outputs(options.output_dependencies + ""-core"")\n    cuda_file_manager.write_outputs(options.output_dependencies + ""-cuda"")\nelse:\n    generate_outputs()\n'"
aten/src/ATen/native_parse.py,0,"b'from __future__ import print_function\nimport re\nimport yaml\nimport pprint\nimport sys\nimport copy\n\ntry:\n    # use faster C loader if available\n    from yaml import CLoader as Loader\nexcept ImportError:\n    from yaml import Loader\n\n# [temp translations]\n# We\'re currently incrementally moving from the custom func schema to the\n# JIT signature schema incrementally. This will reduce overall complexity\n# and increase compliance between these components. So for now we do simple\n# type translations to continue to emit the legacy func schema for further\n# processing by downstream tools. This will helps us avoid having to prematurely\n# change all downstream tools to detect these new types.\ndef type_argument_translations(arg):\n    type_and_name = [a.strip() for a in arg.rsplit(\' \', 1)]\n    name = \'\'\n    if len(type_and_name) > 1:\n        name = type_and_name[1]\n    t = type_and_name[0]\n    name = name.split(\'=\')\n    default = None\n    nullable = False\n    size = None  # Only applies to int[\\d+] and Tensor[\\d+] arguments\n    if len(name) > 1:\n        default = name[1]\n    name = name[0]\n\n    match = re.match(r\'(Tensor.*)\\((.+)\\)(.*)\', t)\n    annotation = None\n    if match:\n        t = match.group(1) + match.group(3)\n        annotation = match.group(2)\n\n    # XXX: is_nullable flag can only annotate entire type as optional type,\n    # need to special case Generator? logic to make ? only available in jit\n    # TODO: deprecate is_nullable global flag, and parse the type\n    # to support annotating complicated types with optional annotation\n    nullable = (t != \'Generator?\' and \'?\' in t)\n\n    # This enables ""Generator? x = None and translates to legacy\n    # ""Generator* x = nullptr"". See [temp translations].\n    if t == \'Generator?\' and default == \'None\':\n        t = \'Generator*\'\n        default = \'nullptr\'\n    # Enables Generator? by translating to legacy Generator*.\n    elif t == ""Generator?"":\n        t = \'Generator*\'\n    # Enables Tensor[] by translating to legacy TensorList.\n    elif t == \'Tensor[]\' or t == \'Tensor?[]\':\n        t = \'TensorList\'\n    # Enables int[] by translating to legacy IntArrayRef.\n    elif t == \'int[]\':\n        t = \'IntArrayRef\'\n    # Enables int by translating to legacy int64_t.\n    elif t == \'int\':\n        t = \'int64_t\'\n    elif t == \'int?\':\n        t = \'int64_t?\'\n    elif t == \'int64_t\':\n        raise RuntimeError(""Please use int and not int64_t. ""\n                           ""See [temp translations] for details."")\n    elif t == \'int64_t?\':\n        raise RuntimeError(""Please use int? and not int64_t?. ""\n                           ""See [temp translations] for details."")\n    # Enables Dimname[] by translating to legacy DimnameList.\n    elif t == \'Dimname[]\':\n        t = \'DimnameList\'\n    elif t == \'Dimname[]?\':\n        t = \'DimnameList?\'\n    # Enables float by translating to legacy double.\n    elif t == \'float\':\n        t = \'double\'\n    # Enables str by translating to legacy std::string.\n    elif t == \'str\':\n        t = \'std::string\'\n    elif t == \'double\':\n        raise RuntimeError(""Please use float and not double. ""\n                           ""See [temp translations] for details."")\n    # Enables int[x] by translating to legacy IntArrayRef[x]. See [temp translations]\n    elif re.match(r\'int\\[(\\d+)\\]\', t):\n        match = re.match(r\'int\\[(\\d+)\\]\', t)\n        t = \'IntArrayRef\'\n        size = int(match.group(1))\n    # Enables bool[x] by translating to legacy std::array<bool,x>. See [temp translations]\n    elif re.match(r\'bool\\[(\\d+)\\]\', t):\n        match = re.match(r\'bool\\[(\\d+)\\]\', t)\n        t = \'std::array<bool,{}>\'.format(match.group(1))\n    elif re.match(r\'std::array\', t):\n        raise RuntimeError(""Please use array notation, e.g. bool[3] and not std::array.""\n                           ""See [temp translations] for details."")\n    # Enables Dimname[x] by translating to DimnameList[x]. See [temp translations]\n    elif re.match(r\'Dimname\\[(\\d+)\\]\', t):\n        match = re.match(r\'Dimname\\[(\\d+)\\]\', t)\n        t = \'DimnameList\'\n        size = int(match.group(1))\n\n    # Legacy type sanitization. TODO: Do we really need this?\n    if t == \'Generator*\':\n        t = \'Generator *\'\n\n    if not default:\n        pass\n    # This enables Tensor? x=None and translates to legacy\n    # ""Tensor? x={}"". See [temp translations].\n    elif t.startswith(\'Tensor?\') and default == \'None\':\n        default = ""{}""\n    elif default == \'True\':\n        default = True\n    elif default == \'False\':\n        default = False\n    elif default == \'true\':\n        raise RuntimeError(""Please use True and not true. ""\n                           ""See [temp translations] for details."")\n    elif default == \'false\':\n        raise RuntimeError(""Please use False and not false. ""\n                           ""See [temp translations] for details."")\n    # Enables default argument [] by translating to legacy {}.\n    # See [temp translations]\n    elif default == \'[]\':\n        default = \'{}\'\n    # Enables lists by translating to legacy {.*}.\n    # See [temp translations]\n    elif re.match(r\'\\[.*\\]\', default):\n        default = ""{"" + default[1:-1] + ""}""\n    elif default == \'None\':\n        default = \'c10::nullopt\'\n    # The JIT signature schema uses Mean, but in particular C++ needs\n    # the legacy at::Reduction::Mean. So we\'ll continue emiting that until\n    # we change this at either a JIT schema or C++ level.\n    elif default == \'Mean\':\n        default = \'at::Reduction::Mean\'\n    elif default == \'contiguous_format\':\n        default = \'MemoryFormat::Contiguous\'\n    elif default == \'per_tensor_affine\':\n        default = \'QScheme::PER_TENSOR_AFFINE\'\n    else:\n        try:\n            default = int(default)\n        except ValueError:\n            try:\n                default = float(default)\n            except ValueError:\n                pass\n\n    return t, name, default, nullable, size, annotation\n\n\ndef parse_arguments(args, func_variants, declaration, func_return):\n    arguments = []\n    kwarg_only = False\n\n    if len(args.strip()) == 0:\n        return arguments\n\n    # TODO: Use a real parser here; this will get bamboozled\n    # by signatures that contain things like std::array<bool, 2> (note the space)\n    for arg_idx, arg in enumerate(args.split(\', \')):\n        type_and_name = [a.strip() for a in arg.rsplit(\' \', 1)]\n        if type_and_name == [\'*\']:\n            assert not kwarg_only\n            kwarg_only = True\n            continue\n\n        t, name, default, nullable, size, annotation = type_argument_translations(arg)\n\n        argument_dict = {\'type\': t.rstrip(\'?\'), \'name\': name, \'is_nullable\': nullable, \'annotation\': annotation}\n        if size:\n            argument_dict[\'size\'] = size\n        if default is not None:\n            argument_dict[\'default\'] = default\n        if kwarg_only:\n            argument_dict[\'kwarg_only\'] = True\n        arguments.append(argument_dict)\n\n    is_out_fn = False\n    arguments_out = []\n    arguments_other = []\n    for argument in arguments:\n        if argument[\'type\'] == ""Tensor"" and \\\n                argument[\'annotation\'] and \\\n                re.match(r\'^(.*!)$\', argument[\'annotation\']) and \\\n                argument.get(\'kwarg_only\'):\n            argument[\'output\'] = True\n            argument[\'kwarg_only\'] = False\n            arguments_out.append(argument)\n            is_out_fn = True\n        else:\n            arguments_other.append(argument)\n\n    arguments = arguments_out + arguments_other\n\n    name = declaration[\'name\']\n    if is_out_fn:\n        declaration[\'name\'] += ""_out""\n\n    # Reverse splat of TensorOptions\n    # As we move towards the JIT function schema for native_functions.yaml we need to support\n    # the expanded version of TensorOptions. For now we discover whether there are three\n    # types and names of keyword arguments: ""ScalarType dtype"", ""Layout layout"" and ""Device device""\n    # Each, if set, must have default arguments set to long or float, strided and ""cpu"" respectively.\n    # They must appear in this order and in this order only in order for us to be able to process them.\n    # In the future we will get rid of this specific processing as downstream consumers start relying\n    # less on the content of Declarations.yaml. If you want to support more than this you\'ll\n    # potentially have to extend the JIT.\n\n    supported_topt_arguments = [\n        [\n            {\'name\': \'dtype\', \'type\': \'ScalarType\', \'is_nullable\': False, \'annotation\': None},\n            {\'name\': \'layout\', \'type\': \'Layout\', \'is_nullable\': False, \'annotation\': None},\n            {\'name\': \'device\', \'type\': \'Device\', \'is_nullable\': False, \'annotation\': None},\n            {\'name\': \'pin_memory\', \'type\': \'bool\', \'is_nullable\': False, \'annotation\': None, \'default\': False},\n        ]\n    ]\n    supported_topt_arguments.append(copy.deepcopy(supported_topt_arguments[0]))\n    for arg in supported_topt_arguments[1]:\n        arg.update({\'kwarg_only\': True})\n    supported_topt_arguments.append(copy.deepcopy(supported_topt_arguments[1]))\n    for arg in supported_topt_arguments[2]:\n        arg.update({\'default\': \'c10::nullopt\', \'is_nullable\': True})\n    # add explicit support for what is needed for tril_indices / triu_indices\n    supported_topt_arguments.append(\n        [\n            {\'name\': \'dtype\', \'type\': \'ScalarType\', \'annotation\': None, \'kwarg_only\': True,\n             \'default\': \'long\', \'is_nullable\': True},\n            {\'name\': \'layout\', \'type\': \'Layout\', \'annotation\': None, \'kwarg_only\': True,\n             \'default\': \'c10::nullopt\', \'is_nullable\': True},\n            {\'name\': \'device\', \'type\': \'Device\', \'annotation\': None, \'kwarg_only\': True,\n             \'default\': \'c10::nullopt\', \'is_nullable\': True},\n            {\'name\': \'pin_memory\', \'type\': \'bool\', \'annotation\': None, \'kwarg_only\': True,\n             \'default\': \'c10::nullopt\', \'is_nullable\': True},\n        ]\n    )\n\n    corresponding_topts = [\n        {\'type\': \'TensorOptions\', \'name\': \'options\', \'is_nullable\': False, \'annotation\': None},\n    ]\n    corresponding_topts.append(corresponding_topts[0].copy())\n    corresponding_topts[1][\'kwarg_only\'] = True\n    corresponding_topts.append(corresponding_topts[1].copy())\n    corresponding_topts[2][\'default\'] = \'{}\'\n    corresponding_topts.append(\n        {\'type\': \'TensorOptions\', \'name\': \'options\', \'is_nullable\': False, \'annotation\': None,\n         \'kwarg_only\': True, \'default\': \'at::kLong\'})\n\n    def check_topt_representation(topt_representation):\n        for idx, supported_topt in enumerate(supported_topt_arguments):\n            matches = all(topt_representation[i] == topt for i, topt in enumerate(supported_topt))\n            if matches:\n                return corresponding_topts[idx]\n        return None\n\n    def is_tensor_option(argument):\n        return argument[\'name\'] in [\'dtype\', \'layout\', \'device\', \'pin_memory\']\n\n    new_arguments = []\n    idx = 0\n    while idx < len(arguments):\n        argument = arguments[idx]\n        number_of_arguments = len(supported_topt_arguments[0])\n        if is_tensor_option(argument) and len(arguments) - idx >= number_of_arguments:\n            topt_representation = []\n            for i in range(number_of_arguments):\n                argument = arguments[idx]\n                if not is_tensor_option(argument):\n                    break\n                topt_representation.append(argument)\n                idx += 1\n            if len(topt_representation) == number_of_arguments:\n                merged_argument = check_topt_representation(topt_representation)\n                assert merged_argument, \\\n                    ""Unsupported combination of TensorOptions {}, the only currently supported combinations are {}""\\\n                    .format(str(topt_representation), str(supported_topt_arguments))\n                new_arguments.append(merged_argument)\n            else:\n                new_arguments += topt_representation\n        else:\n            new_arguments.append(argument)\n            idx += 1\n\n    arguments = new_arguments\n\n    # Sanity checks\n\n    # TODO: convention is that the ith-argument correspond to the i-th return, but it would\n    # be better if we just named everything and matched by name.\n    for arg_idx, argument in enumerate(arguments_out):\n        assert argument[\'annotation\'] == func_return[arg_idx][\'annotation\'], \\\n            ""For func {} writeable keyword Tensor arguments need to have a matching return Tensor. Further, "" \\\n            ""the ith-argument needs to correspond to the i-th return."".format(name)\n\n    assert len(arguments_out) <= len(func_return), ""func {} must return at least as many Tensors "" \\\n        ""as can be passed as output."".format(name)\n\n    if name.endswith(\'_out\'):\n        raise RuntimeError(""Native function {} may not be suffixed with _out as we transition to a unified schema. ""\n                           ""Otherwise you will cause confusion amongst consumers of native functions."".format(name))\n\n    if is_out_fn and func_variants not in [[], \'function\', [\'function\']]:\n        raise RuntimeError(""Native functions with output MUST be declared with only the function variant; ""\n                           ""e.g., variants: function; otherwise you will tickle a Python argument binding bug ""\n                           ""(which usually manifests itself as the result variable being undefined.) ""\n                           ""The culprit was: {}"".format(name))\n    if not is_out_fn:\n        assert len(arguments_out) == 0, ""func {} is not marked as output yet contains output "" \\\n            ""keyword arguments"".format(name)\n\n    # TODO: Explicit checking for void is a hack and should disappear after a more\n    # functionally complete implementation of Tensor aliases.\n    if declaration[\'inplace\'] and len(func_return) > 0:\n        found_self = False\n        for arg_idx, argument in enumerate(arguments):\n            if argument[\'name\'] == ""self"":\n                assert argument[\'annotation\'] and argument[\'annotation\'].endswith(""!""), \\\n                    ""Inplace function \\""{}\\"" needs to annotate Tensor argument named self "" \\\n                    ""as mutable."".format(name)\n                found_self = True\n                assert argument[\'annotation\'] == func_return[arg_idx][\'annotation\'], \\\n                    ""Inplace function annotations of function {} need to match between "" \\\n                    ""input and correponding output."".format(name)\n                assert argument[\'name\'] == func_return[arg_idx][\'name\'] or \\\n                    argument[\'name\'] == func_return[arg_idx][\'name\'] + ""_return""\n                assert argument[\'type\'] == func_return[arg_idx][\'type\']\n        assert found_self, ""Inplace function \\""{}\\"" needs Tensor argument named self."".format(name)\n\n    return arguments\n\n\ndef parse_return_arguments(return_decl, inplace, func_decl):\n    arguments = []\n    if return_decl == \'()\':\n        return arguments\n\n    # TODO: Use a real parser here; this will get bamboozled\n    # by signatures that contain things like std::array<bool, 2> (note the space)\n    if return_decl[0] == \'(\' and return_decl[-1] == \')\':\n        return_decl = return_decl[1:-1]\n\n    multiple_args = len(return_decl.split(\', \')) > 1\n    for arg_idx, arg in enumerate(return_decl.split(\', \')):\n        t, name, default, nullable, size, annotation = type_argument_translations(arg)\n        # name of arguments and name of return sometimes have collision\n        # in this case, we rename the return name to <name>_return.\n        return_name = name\n        if name in func_decl[\'func\'].split(\'->\')[0]:\n            return_name = name + ""_return""\n        argument_dict = {\'type\': t, \'name\': return_name, \'annotation\': annotation}\n        if name:\n            # See Note [field_name versus name]\n            argument_dict[\'field_name\'] = name\n        else:\n            if t == ""Tensor"" and inplace:\n                assert annotation and annotation.endswith(""!""), \\\n                    ""Return Tensor of function \\""{}\\"" flagged as inplace needs to be "" \\\n                    ""annotated as mutable"".format(func_decl[\'func\'])\n                argument_dict[\'name\'] = \'self\'\n            else:\n                argument_dict[\'name\'] = \'result\' if not multiple_args else \'result\' + str(arg_idx)\n        argument_dict[\'output\'] = True\n        arguments.append(argument_dict)\n    return arguments\n\n\ndef parse_native_yaml(path):\n    with open(path, \'r\') as f:\n        return yaml.load(f, Loader=Loader)\n\n\ndef propagate_field_names(output_arguments, return_arguments):\n    if output_arguments:\n        for i, r in enumerate(return_arguments):\n            if \'field_name\' in r:\n                output_arguments[i][\'field_name\'] = r[\'field_name\']\n\ndef is_named_tensor_only(declaration):\n    return any([\'Dimname\' in arg[\'type\'] for arg in declaration[\'arguments\']])\n\n\ndef run(paths):\n    declarations = []\n    for path in paths:\n        for func in parse_native_yaml(path):\n            declaration = {\'mode\': \'native\'}\n            try:\n                declaration[\'schema_string\'] = ""aten::"" + func[\'func\']\n                if \'->\' in func[\'func\']:\n                    func_decl, return_decl = [x.strip() for x in func[\'func\'].split(\'->\')]\n                else:\n                    raise Exception(\'Expected return declaration\')\n                fn_name, arguments = func_decl.split(\'(\', 1)\n                if \'.\' in fn_name:\n                    fn_name, overload_name = fn_name.split(\'.\', 1)\n                else:\n                    overload_name = \'\'\n                assert arguments[-1] == "")"", ""Expecting closing ) for {}"".format(func[\'func\'])\n                arguments = arguments[:-1]  # Expect closing )\n                declaration[\'name\'] = func.get(\'name\', fn_name)\n                declaration[\'operator_name\'] = func.get(\'name\', fn_name)\n                declaration[\'overload_name\'] = func.get(\'overload_name\', overload_name)\n                declaration[\'inplace\'] = re.search(\'(^__i|[^_]_$)\', fn_name) is not None\n                return_arguments = parse_return_arguments(return_decl, declaration[\'inplace\'], func)\n                arguments = parse_arguments(arguments, func.get(\'variants\', []), declaration, return_arguments)\n                output_arguments = [x for x in arguments if x.get(\'output\')]\n                propagate_field_names(output_arguments, return_arguments)\n                declaration[\'return\'] = return_arguments if len(output_arguments) == 0 else output_arguments\n                declaration[\'variants\'] = func.get(\'variants\', [\'function\'])\n                declaration[\'requires_tensor\'] = func.get(\'requires_tensor\', False)\n                declaration[\'matches_jit_signature\'] = func.get(\'matches_jit_signature\', True)\n                declaration[\'cpu_half\'] = func.get(\'cpu_half\', False)\n                declaration[\'cpu_bfloat16\'] = func.get(\'cpu_bfloat16\', False)\n                declaration[\'cuda_bfloat16\'] = func.get(\'cuda_bfloat16\', False)\n                declaration[\'cpu_bool\'] = func.get(\'cpu_bool\', False)\n                declaration[\'cuda_bool\'] = func.get(\'cuda_bool\', False)\n                declaration[\'deprecated\'] = func.get(\'deprecated\', False)\n                declaration[\'device_guard\'] = func.get(\'device_guard\', True)\n                declaration[\'supports_named_tensor\'] = func.get(\'supports_named_tensor\', False)\n                declaration[\'use_c10_dispatcher\'] = func.get(\'use_c10_dispatcher\', \'unboxed_only\')\n                assert declaration[\'use_c10_dispatcher\'] in [\'unboxed_only\', \'full\']\n                declaration[\'category_override\'] = func.get(\'category_override\', \'\')\n                declaration[\'arguments\'] = func.get(\'arguments\', arguments)\n                declaration[\'type_method_definition_dispatch\'] = func.get(\'dispatch\', declaration[\'name\'])\n                declaration[\'python_module\'] = func.get(\'python_module\', \'\')\n                declarations.append(declaration)\n            except Exception as e:\n                msg = \'\'\'Exception raised in processing function:\n{func}\nGenerated partial declaration:\n{decl}\'\'\'.format(func=pprint.pformat(func), decl=pprint.pformat(declaration))\n                print(msg, file=sys.stderr)\n                raise e\n\n    return declarations\n'"
aten/src/ATen/nn_parse.py,0,"b'import copy\nimport re\nimport common_with_cwrap\nimport yaml\nfrom collections import OrderedDict, defaultdict\n\ntry:\n    # use faster C loader if available\n    from yaml import CLoader as Loader\nexcept ImportError:\n    from yaml import Loader\n\n\n# matches `name`, `params` in `name(params)`\nNAME_PARAM_REGEX = r\'(\\w+)\\((.*)\\)\'\n\n\ndef argument_to_declaration(param, func=None):\n    arg = {}\n    arg[\'type\'], name = param.split(\' \')\n    if (arg[\'type\'].endswith(\'?\')):\n        arg[\'is_nullable\'] = True\n        arg[\'type\'] = arg[\'type\'].rstrip(\'?\')\n    if arg[\'type\'] == \'Tensor\':\n        arg[\'type\'] = \'THTensor*\'\n    elif arg[\'type\'] == \'LongTensor\':\n        arg[\'type\'] = \'THIndexTensor*\'\n    elif arg[\'type\'] == \'Scalar\':\n        arg[\'type\'] = \'accreal\'\n    elif arg[\'type\'] == \'Generator*\':\n        arg[\'type\'] = \'THGenerator*\'\n\n    match = re.match(r\'IntArrayRef\\[(\\d+)\\]\', arg[\'type\'])\n    if match:\n        arg[\'type\'] = \'IntArrayRef\'\n        arg[\'size\'] = int(match.group(1))\n\n    if \'=\' in name:\n        name, default = name.split(\'=\')\n        arg[\'optional\'] = True\n        arg[\'default\'] = default\n    arg[\'name\'] = name\n\n    if func is not None:\n        wrap_dims = func.get(\'wrap_dim\', {})\n        if name in wrap_dims:\n            arg[\'wrap_dim\'] = wrap_dims[name]\n\n    return arg\n\n\ndef output_arguments(thnn_function):\n    cname = thnn_function.name\n    output_args = []\n\n    # function_wrapper expects everything in a declaration to be in\n    # the base type (i.e. THTensor*), but if we pull a THCUNN only\n    # implementation, it will have THCTensor* as the arg type. So we\n    # strip the THC here before returning\n    def map_to_th_type(t):\n        if t.startswith(\'THC\'):\n            t = t.replace(\'THC\', \'TH\')\n        return t\n\n    def is_output_arg(arg_name, func_name):\n        if arg_name == \'output\' and \'updateOutput\' in cname:\n            return True\n        if name in {\'gradInput\', \'gradWeight\', \'gradBias\', \'gradGrid\'}:\n            return True\n        if arg_name == \'indices\' and \'updateOutput\' in cname and \'Unpool\' not in cname:\n            # indices is an output argument in pooling and an input in unpooling\n            return True\n        return False\n\n    for arg in thnn_function.arguments:\n        name = arg.name\n        if is_output_arg(name, cname):\n            desc = {\n                \'type\': map_to_th_type(arg.type),\n                \'name\': camel_to_snake(name),\n                \'output\': True,\n            }\n            if name.startswith(\'grad_\'):\n                desc[\'is_nullable\'] = True\n            output_args.append(desc)\n    return output_args\n\n\ndef get_return(args):\n    indices = [str(idx) for idx, arg in enumerate(args) if arg.get(\'output\')]\n    return \'argument {}\'.format(\',\'.join(indices))\n\n\nARGUMENT_MAPPINGS = {\n    \'k\': \'kernel_size\',\n    \'d\': \'stride\',\n    \'pad\': \'padding\',\n    \'p\': \'padding\',\n    \'o\': \'output_size\',\n    \'osize\': \'output_size\',\n    \'output\': \'output_size\',  # as a prefix e.g. outputW\n    \'isize\': \'input_size\',\n    \'dilation\': \'dilation\',\n    \'adj\': \'output_padding\',\n    \'a\': \'output_padding\',\n}\n\nDIMENSION_OFFSET = {\n    \'width\': -1,\n    \'height\': -2,\n    \'B\': 0,\n    \'C\': 1,\n    \'W\': -1,\n    \'H\': -2,\n    \'T\': -3,\n    \'left\': 0,\n    \'right\': 1,\n    \'top\': 2,\n    \'bottom\': 3,\n    \'front\': 4,\n    \'back\': 5,\n}\n\nSUBSTITUTIONS = {\n    \'input\': \'self\',\n    \'weights\': \'weight\',\n    \'train\': \'training\',\n    \'val\': \'value\',\n    \'lambda\': \'lambd\',\n    \'negval\': \'negative_slope\',\n}\n\n\ndef camel_to_snake(name):\n    # from https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n    s1 = re.sub(\'(.)([A-Z][a-z]+)\', r\'\\1_\\2\', name)\n    return re.sub(\'([a-z0-9])([A-Z])\', r\'\\1_\\2\', s1).lower()\n\n\ndef get_thnn_args(thnn_function, params, inplace):\n    params_by_name = {p[\'name\']: p for p in params}\n\n    def arg_expr(prefix, suffix):\n        # e.g kW, kH\n        name = ARGUMENT_MAPPINGS[prefix]\n        if name not in params_by_name:\n            raise RuntimeError(\'missing arg ""{}"" in {}\'.format(name, thnn_function.name))\n        param = params_by_name[name]\n        if param[\'type\'] == \'IntArrayRef\' and \'size\' in param:\n            name = name + \'_\'\n        # NB: We calculate the dimension based on the name of\n        # the argument, not its positional order.  This means\n        # that we may reorder arguments to get them in\n        # the right place; e.g., if a THNN implementation\n        # has arguments in the order kernelW, kernelH, we\n        # will generate a caller that is kernel[1], kernel[0]\n        # to order them in the correct way.\n        index = DIMENSION_OFFSET[suffix]\n        if index < 0:\n            index += param[\'size\']\n        expr = \'{}[{}]\'.format(name, index)\n        return {\'type\': \'EXPRESSION\', \'name\': expr}\n\n    thnn_args = []\n    for arg in thnn_function.arguments:\n        name = arg.name\n        if name == \'state\':\n            continue\n        if inplace and name == \'output\':\n            name = \'self\'\n        aten_name = camel_to_snake(SUBSTITUTIONS.get(name, name))\n        parts = aten_name.split(\'_\')\n        if aten_name in params_by_name:\n            param = params_by_name[aten_name]\n            if arg.is_optional:\n                param[\'is_nullable\'] = True\n            thnn_args.append(copy.deepcopy(param))\n        elif len(parts) == 2 and parts[0] in ARGUMENT_MAPPINGS and parts[1] in DIMENSION_OFFSET:\n            # e.g. pad_left\n            thnn_args.append(arg_expr(parts[0], parts[1]))\n        elif name[-1] in DIMENSION_OFFSET and name[:-1] in ARGUMENT_MAPPINGS:\n            # e.g kW, kH\n            thnn_args.append(arg_expr(name[:-1], name[-1]))\n        elif name == \'owidth\' or name == \'oheight\':\n            thnn_args.append(arg_expr(name[0], name[1:]))\n        elif name == \'scale\':\n            thnn_args.append({\'type\': \'EXPRESSION\', \'name\': \'1\'})\n        elif name == \'inplace\':\n            thnn_args.append({\'type\': \'EXPRESSION\', \'name\': str(inplace).lower()})\n        else:\n            raise RuntimeError(""{}: can\'t find binding for \'{}\'""\n                               .format(thnn_function.name, name))\n    return thnn_args\n\n\ndef remove_unused_args(args, thnn_args):\n    """"""Returns the subset of args whose name appears in thnn_args""""""\n    def clean_name(name):\n        name = name[:name.index(\'[\')] if \'[\' in name else name\n        if name.endswith(\'_\'):\n            name = name[:-1]\n        return name\n    uses = set([clean_name(arg[\'name\']) for arg in thnn_args])\n    uses.add(\'output_mask\')\n    args = [arg for arg in args if arg[\'name\'] in uses]\n    for arg in args:\n        if \'default\' in arg:\n            del arg[\'default\']\n    return args\n\n\ndef unique_args(argslist):\n    result = []\n    seen = set()\n    for args in argslist:\n        for arg in args:\n            if arg[\'name\'] in seen:\n                continue\n            seen.add(arg[\'name\'])\n            result.append(arg)\n    return result\n\n\ndef function_info(name, arguments, cimpls, buffers, backends, inplace, scalar_check, backend_types):\n    """"""\n    cimpls contains information use to call into THNN:\n        cname: THNN function name\n        arguments: arguments to functional call\n        condition: [optional] guard around call\n    """"""\n    return {\n        \'mode\': \'NN\',\n        \'name\': name,\n        \'cpu_bfloat16\': True if backend_types is not None and \'CPU\' in backend_types and\n                \'BFloat16\' in backend_types[\'CPU\'] else False,\n        \'backend_types\': backend_types,\n        \'arguments\': arguments,\n        \'return\': \'argument 0\' if inplace else get_return(arguments),\n        \'buffers\': buffers,\n        \'backends\': backends,\n        \'cimpls\': cimpls,\n        \'scalar_check\': scalar_check,\n        \'variants\': [\'function\'],\n    }\n\ndef base_declaration(func, thnn_function, backends, backend_types, inplace=False):\n    """"""Creates the NN function without any buffers in it\'s signature""""""\n    name, params = re.match(NAME_PARAM_REGEX, func[\'name\']).groups()\n    if inplace:\n        name += \'_\'\n    params = params.split(\', \')\n    arguments = [argument_to_declaration(a, func) for a in params]\n    if not inplace:\n        arguments += output_arguments(thnn_function)\n    buffers = [argument_to_declaration(\'Tensor \' + buf)\n               for buf in func.get(\'buffers\', [])]\n\n    return function_info(name, arguments, None, buffers, backends, inplace, func.get(\'scalar_check\'), backend_types)\n\ndef forward_declaration(base, thnn_function, backend_types, inplace=False):\n    name = \'{}_forward\'.format(base[\'name\'])\n    if inplace:\n        name += \'_\'\n\n    arguments = [copy.deepcopy(arg) for arg in base[\'arguments\']\n                 if not arg.get(\'output\')]\n\n    arguments += output_arguments(thnn_function)\n    for buffer in base[\'buffers\']:\n        buffer = copy.deepcopy(buffer)\n        buffer[\'output\'] = True\n        arguments.append(buffer)\n\n    thnn_args = get_thnn_args(thnn_function, arguments, inplace)\n    arguments = remove_unused_args(arguments, thnn_args)\n    cimpl = {\'cname\': thnn_function.name, \'arguments\': thnn_args}\n\n    scalar_check = base[\'scalar_check\']\n    if scalar_check is not None:\n        output_arg_names = [arg[\'name\'] for arg in arguments if arg.get(\'output\', False)]\n        scalar_check = {k: v for (k, v) in scalar_check.items() if k in output_arg_names}\n\n    return function_info(name, arguments, [cimpl], [], base[\'backends\'], inplace, scalar_check, backend_types)\n\ndef backward_declaration(base, thnn_functions, backend_types):\n    name = \'{}_backward\'.format(base[\'name\'])\n\n    arguments = []\n    arguments.append({\'type\': \'THTensor*\', \'name\': \'grad_output\'})\n    arguments += [copy.deepcopy(arg) for arg in base[\'arguments\']\n                  if arg[\'name\'] != \'inplace\']\n    arguments += base[\'buffers\']\n\n    if \'upsample\' in base[\'name\']:\n        # Add input_size as parameter to upsample backwards functions\n        # Note that input_size is 4-dim for upsample_xxx2d\n        size = 2 + int(re.search(r\'(\\d+)d\', base[\'name\']).group(1))\n        input_size_arg = {\'type\': \'IntArrayRef\', \'name\': \'input_size\', \'size\': size}\n        for output_size_idx, arg in enumerate(arguments):\n            if arg[\'name\'] == \'output_size\':\n                break\n        arguments.insert(output_size_idx + 1, input_size_arg)\n\n    if \'im2col\' in base[\'name\']:\n        # Add input_size as parameter to im2col backwards function\n        input_size_arg = {\'type\': \'IntArrayRef\', \'name\': \'input_size\', \'size\': 2}\n        arguments.insert(2, input_size_arg)\n\n    # outputs from the forward may be inputs to the backwards\n    for arg in arguments:\n        if \'output\' in arg:\n            del arg[\'output\']\n\n    arguments += unique_args([output_arguments(f) for f in thnn_functions])\n\n    def initialize_output_arg(arg):\n        # the mask array<bool, N> specifies which return values to compute\n        arg[\'mask\'] = True\n        arg[\'is_nullable\'] = True\n\n        # grad_weight and grad_bias need to be resized and zeroed\n        if arg[\'name\'] == \'grad_weight\':\n            arg[\'resize\'] = \'weight\'\n            arg[\'zero\'] = True\n        if arg[\'name\'] == \'grad_bias\':\n            dim = 1 if \'transpose\' in name else 0\n            arg[\'resize\'] = [(\'weight\', dim)]\n            arg[\'zero\'] = True\n\n    is_batch_norm_backward = \'_backward\' in thnn_functions[0].name\n    grad_params = []\n    if len(thnn_functions) > 1 or is_batch_norm_backward:\n        for arg in arguments:\n            if arg.get(\'output\', False):\n                initialize_output_arg(arg)\n            if \'Tensor\' in arg[\'type\'] and arg[\'name\'].startswith(\'grad_\') and \\\n                    \'input\' not in arg[\'name\'] and \'output\' not in arg[\'name\']:\n                grad_params.append(arg[\'name\'])\n\n    thnn_args = [get_thnn_args(f, arguments, False) for f in thnn_functions]\n    arguments = remove_unused_args(arguments, unique_args(thnn_args))\n    cimpls = []\n\n    def get_condition(func):\n        # only call into the THNN functions if the output args are not null\n        if \'_updateGradInput\' in func.name:\n            return \'grad_input_\'\n        if \'_accGradParameters\' in func.name:\n            return \' || \'.join(p + \'_\' for p in grad_params)\n        return None\n\n    for func, args in zip(thnn_functions, thnn_args):\n        cimpl = {\'cname\': func.name, \'arguments\': args}\n        if len(thnn_functions) > 1:\n            cimpl[\'condition\'] = get_condition(func)\n        cimpls.append(cimpl)\n\n    output_args = [arg for arg in arguments if arg.get(\'output\', False)]\n    scalar_check_arg = base[\'scalar_check\'] if base[\'scalar_check\'] is not None else dict()\n    scalar_check = {k: v for (k, v) in scalar_check_arg.items() if k in (a[\'name\'] for a in output_args)}\n    for arg in output_args:\n        # resize automatically sets scalar_check\n        if scalar_check.get(arg[\'name\']) is not None or arg.get(\'resize\', False):\n            pass\n        else:\n            base_name = arg[\'name\'][len(\'grad_\'):] if arg[\'name\'] != \'grad_input\' else \'self\'\n            if base_name in (a[\'name\'] for a in arguments):\n                scalar_check[arg[\'name\']] = base_name + \'_->dim() == 0\'\n            else:\n                raise ValueError((""Could not infer scalar_check for {} argument of func {} because {} ""\n                                  ""does not exist.  Please explicitly specify scalar_check.""\n                                  .format(arg[\'name\'], name, base_name)))\n\n    return function_info(name, arguments, cimpls, [], base[\'backends\'], False, scalar_check, backend_types)\n\n\ndef parse_nn_yaml(filename):\n    with open(filename, \'r\') as f:\n        return yaml.load(f, Loader=Loader)\n\n\ninclude_only = \'(updateOutput|updateGradInput|accGradParameters|backward)$\'\nexclude = \'LookupTable\'\n\n\ndef run(paths):\n    function_backends = defaultdict(list)\n    header_functions = OrderedDict()\n\n    headers = [p for p in paths if p.endswith(\'.h\')]\n    yamls = [p for p in paths if p.endswith(\'.yaml\')]\n\n    for path in headers:\n        backend = \'CUDA\' if re.search(\'THCU\', path) else \'CPU\'\n        for func in common_with_cwrap.parse_header(path):\n            if re.search(include_only, func.name) is None or re.search(exclude, func.name) is not None:\n                continue\n            function_backends[func.name].append(backend)\n            if func.name not in header_functions:\n                header_functions[func.name] = func\n\n    bwd_suffixes = [\'_updateGradInput\', \'_accGradParameters\', \'_backward\']\n\n    declarations = []\n    for path in yamls:\n        for func in parse_nn_yaml(path):\n            cname = func[\'cname\']\n            backends = function_backends[cname + \'_updateOutput\']\n\n            fwd_function = header_functions[cname + \'_updateOutput\']\n            bwd_functions = []\n            for suffix in bwd_suffixes:\n                if cname + suffix in header_functions:\n                    bwd_functions.append(header_functions[cname + suffix])\n\n            default_scalar_types = [\'Float\', \'Double\', \'Half\']  # Half will be stripped for CPU backend\n            forward_backend_types = {}\n            backward_backend_types = {}\n            for backend in backends:\n                backend_props = func.get(backend, {})\n                forward_backend_types[backend] = backend_props.get(\'forward_scalar_types\', default_scalar_types)\n                backward_backend_types[backend] = backend_props.get(\'backward_scalar_types\', default_scalar_types)\n\n            base = base_declaration(func, fwd_function, backends, None)\n            declarations.append(forward_declaration(base, fwd_function, forward_backend_types))\n            if bwd_functions:\n                declarations.append(backward_declaration(base, bwd_functions, backward_backend_types))\n\n\n            if func.get(\'has_inplace\', False):\n                declarations.append(base_declaration(func, fwd_function, backends, forward_backend_types, True))\n                declarations.append(forward_declaration(base, fwd_function, forward_backend_types, True))\n\n    return declarations\n'"
aten/src/ATen/preprocess_declarations.py,0,"b'import re\nfrom copy import deepcopy\nfrom function_wrapper import TYPE_FORMAL_GENERIC\nimport common_with_cwrap\n\ntype_map = {\n    \'floating_point\': [\n        \'Float\',\n        \'Double\',\n        \'Half\',\n        \'BFloat16\',\n    ],\n    \'integral\': [\n        \'Byte\',\n        \'Char\',\n        \'Short\',\n        \'Int\',\n        \'Long\',\n        \'Bool\',\n    ],\n    \'quantized\': [\n        \'QInt8\',\n        \'QUInt8\',\n        \'QInt32\',\n    ]\n}\n\nall_types = type_map[\'floating_point\'] + type_map[\'integral\'] + type_map[\'quantized\']\ntype_map[\'all\'] = all_types\n\nall_backends = [\'CPU\', \'CUDA\', \'SparseCPU\', \'SparseCUDA\', \'MkldnnCPU\', \'QuantizedCPU\']\ndefault_backends = [\'CPU\', \'CUDA\']\n\n\ndef process_types_and_backends(option):\n    # if specific pairs were not listed, then enumerate them\n    # based on the backend and type attributes\n    # if backend or type is not defined, it is assumed to be all of them\n    if \'backend_types\' not in option:\n        backends = option.get(\'backends\', default_backends)\n        if isinstance(option.get(\'type_method_definition_dispatch\'), dict):\n            backends = option.get(\'type_method_definition_dispatch\').keys()\n        backends = set(backends)\n\n        backend_types = {}\n        for backend in backends:\n            if backend == \'QuantizedCPU\':\n                backend_types[backend] = type_map[\'quantized\']\n            else:\n                backend_types[backend] = option.get(\'types\', all_types)\n    else:\n        backend_types = option[\'backend_types\']\n\n    # expand type alias (integral, floating_point, all)\n    def expand(types):\n        ret = []\n        for t in types:\n            if t in type_map:\n                ret.extend(type_map[t])\n            else:\n                assert(t in all_types)\n                ret.append(t)\n        return ret\n\n    for backend in backend_types.keys():\n        assert(backend in all_backends)\n        backend_types[backend] = set(expand(backend_types[backend]))\n\n    # special case remove Half for cpu unless it is explicitly enabled\n    if not option.get(\'cpu_half\', False):\n        if \'CPU\' in backend_types:\n            backend_types[\'CPU\'].discard(\'Half\')\n\n    # special case remove BFloat16 for cpu and cuda unless it is explicitly enabled\n    if not option.get(\'cpu_bfloat16\', False):\n        if \'CPU\' in backend_types:\n            backend_types[\'CPU\'].discard(\'BFloat16\')\n\n    if not option.get(\'cuda_bfloat16\', False):\n        if \'CUDA\' in backend_types:\n            backend_types[\'CUDA\'].discard(\'BFloat16\')\n\n    # special cases remove bool for cpu and cuda unless it is explicitly enabled\n    if not option.get(\'cpu_bool\', False):\n        if \'CPU\' in backend_types:\n            backend_types[\'CPU\'].discard(\'Bool\')\n\n    if not option.get(\'cuda_bool\', False):\n        if \'CUDA\' in backend_types:\n            backend_types[\'CUDA\'].discard(\'Bool\')\n\n    # sort the result for easy reading\n    for backend in backend_types.keys():\n        backend_types[backend] = sorted([type for type in backend_types[backend]])\n    option[\'backend_types\'] = backend_types\n\n\ndef exclude(declaration):\n    return \'only_register\' in declaration or declaration.get(\'name\') == \'ndimension\'\n\n\ndef add_variants(option):\n    option.setdefault(\'variants\', [\'method\'])\n\n# if we have \'output\' arguments, generate a variant where\n# we mark oututs as allocate = True, and where the method variant\n# is disabled...\n\n\ndef handle_outputs_taken_as_arguments(options):\n    new_options = []\n\n    def is_nullable(arg):\n        return (arg[\'type\'] in {\'THIntegerTensor*\', \'THTensor*\'} and\n                arg.get(\'default\', \'\') in {None, \'NULL\', \'nullptr\'})\n\n    def should_generate_out_variant(option):\n        if \'function\' in option[\'variants\'] and option[\'mode\'] != \'native\':\n            # don\'t generate _out variants for in-place functions\n            return re.search(\'(^__i|[^_]_$)\', option[\'api_name\']) is None\n        return False\n\n    for option in options:\n        for arg in option[\'arguments\']:\n            # mark arguments which can be null\n            if is_nullable(arg):\n                arg[\'is_nullable\'] = True\n\n        if any(\'output\' in arg for arg in option[\'arguments\']):\n            allocate_option = deepcopy(option)\n            # the allocating option needs to be marked\n            for arg in allocate_option[\'arguments\']:\n                if \'output\' in arg:\n                    arg[\'allocate\'] = True\n\n            # the original option, which takes arguments for the results,\n            # is no longer a method, and has _out added to indicte it takes\n            # output arguments\n            if should_generate_out_variant(option):\n                if \'method\' in option[\'variants\']:\n                    option[\'variants\'].remove(\'method\')\n                option[\'api_name\'] += \'_out\'\n                new_options.append(option)\n\n            new_options.append(allocate_option)\n        else:\n            new_options.append(option)\n    return new_options\n\n\ndef sanitize_return(option):\n    ret = option[\'return\']\n    m = re.match(r\'argument (\\d+(,\\d+)*)\', ret)\n    if m is not None:\n        arguments = [int(x) for x in m.group(1).split(\',\')]\n        option[\'return\'] = {\'kind\': \'arguments\', \'arguments\': arguments}\n    elif ret == \'self\':\n        option[\'return\'] = {\'kind\': \'arguments\', \'arguments\': []}\n        for i, x in enumerate(option[\'arguments\']):\n            if x[\'name\'] == \'self\':\n                option[\'return\'][\'arguments\'].append(i)\n                break\n    else:\n        option[\'return\'] = {\'kind\': \'type\', \'type\': option[\'return\']}\n\n\ndef set_mode(option):\n    option[\'mode\'] = option.get(\'mode\', \'TH\')\n\n# To enable 0-dim support in TH operations\n# we find all places where a single Scalar replaced with a Tensor\n# as an argument is still a valid function\n# we then mark the tensor variant with a key zero_dim_dispatch_when_scalar: name\n# where \'name\' is the name of the argument that should be a scalar\n# during dispatch, if that argument is marked internally as holding a scalar\n# then the method will dispatch to that function.\n\n\ndef discover_zero_dim_tensor_operations(declaration):\n    def signature(option, i=None, value=None):\n        elements = [TYPE_FORMAL_GENERIC.get(arg[\'type\'], arg[\'type\'])\n                    if i is None or j != i else value\n                    for j, arg in enumerate(option[\'arguments\'])]\n        return \'#\'.join(elements)\n    signature_to_option = {signature(option): option\n                           for option in declaration[\'options\']}\n\n    for option in declaration[\'options\']:\n        for i, arg in enumerate(option[\'arguments\']):\n            if arg[\'type\'] == \'real\':\n                signature_of_tensor_version = signature(option, i, \'Tensor &\')\n                if signature_of_tensor_version in signature_to_option:\n                    tensor_version = \\\n                        signature_to_option[signature_of_tensor_version]\n                    names = [arg[\'name\'] for arg in tensor_version[\'arguments\']]\n                    tensor_version[\'zero_dim_dispatch_when_scalar\'] = names[i]\n                    # print(""FOUND ""+str(i)   )\n                    # print(""Scalar Version ===== "")\n                    # print(yaml.dump(option))\n                    # print(""Tensor Version ===== "")\n                    # print(yaml.dump(tensor_version))\n                    # print(""SHARED ""+names[i])\n\n\ndef is_extended_method(option):\n    if \'method\' in option[\'variants\']:\n        return False\n    else:\n        return True\n\n\ndef run(declarations):\n    declarations = [d for d in declarations if not exclude(d)]\n    non_extended_methods = set()\n    for declaration in declarations:\n        common_with_cwrap.set_declaration_defaults(declaration)\n        declaration[\'options\'] = [deepcopy(o) for o in declaration[\'options\']]\n        declaration[\'options\'] = common_with_cwrap.filter_unique_options(\n            declaration[\'options\'],\n            allow_kwarg=False,\n            type_to_signature=TYPE_FORMAL_GENERIC,\n            remove_self=True)\n\n        common_with_cwrap.sort_by_number_of_args(declaration)\n\n        discover_zero_dim_tensor_operations(declaration)\n\n        for option in declaration[\'options\']:\n            set_mode(option)\n            if option[\'mode\'] != \'native\':\n                sanitize_return(option)\n            process_types_and_backends(option)\n            add_variants(option)\n            if not is_extended_method(option):\n                non_extended_methods.add(option[\'api_name\'])\n        declaration[\'options\'] = handle_outputs_taken_as_arguments(\n            declaration[\'options\'])\n    # We (very unfortunately) have overloaded virtual methods. Because\n    # of C++\'s rules, we cannot move one overload without doing some\n    # extra work to make sure that overload in a superclass and an\n    # overload in a subclass resolve together. I\'ve chosen to resolve\n    # this problem simply by moving ALL overloads of a method which\n    # occurs in Tensor to Type.  This is why we have to first compute\n    # which methods *names* go on type, and then move ALL overloads\n    # of this name to Type.\n    for declaration in declarations:\n        for option in declaration[\'options\']:\n            option[\'extended_method\'] = option[\'api_name\'] not in non_extended_methods\n    return declarations\n'"
aten/src/ATen/native/quantized/cpu/qnnpack/configure.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport confu\nfrom confu import arm, x86\n\n\nparser = confu.standard_parser()\n\n\ndef main(args):\n    options = parser.parse_args(args)\n    build = confu.Build.from_options(options)\n\n    build.export_cpath(""include"", [""q8gemm.h""])\n\n    with build.options(\n        source_dir=""src"",\n        deps=[\n            build.deps.cpuinfo,\n            build.deps.clog,\n            build.deps.psimd,\n            build.deps.fxdiv,\n            build.deps.pthreadpool,\n            build.deps.FP16,\n        ],\n        extra_include_dirs=""src"",\n    ):\n\n        requantization_objects = [\n            build.cc(""requantization/precise-scalar.c""),\n            build.cc(""requantization/fp32-scalar.c""),\n            build.cc(""requantization/q31-scalar.c""),\n            build.cc(""requantization/gemmlowp-scalar.c""),\n        ]\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            requantization_objects += [\n                build.cc(""requantization/precise-psimd.c""),\n                build.cc(""requantization/fp32-psimd.c""),\n            ]\n        if build.target.is_x86 or build.target.is_x86_64:\n            with build.options(isa=x86.sse2):\n                requantization_objects += [\n                    build.cc(""requantization/precise-sse2.c""),\n                    build.cc(""requantization/fp32-sse2.c""),\n                    build.cc(""requantization/q31-sse2.c""),\n                    build.cc(""requantization/gemmlowp-sse2.c""),\n                ]\n            with build.options(isa=x86.ssse3):\n                requantization_objects += [\n                    build.cc(""requantization/precise-ssse3.c""),\n                    build.cc(""requantization/q31-ssse3.c""),\n                    build.cc(""requantization/gemmlowp-ssse3.c""),\n                ]\n            with build.options(isa=x86.sse4_1):\n                requantization_objects += [\n                    build.cc(""requantization/precise-sse4.c""),\n                    build.cc(""requantization/q31-sse4.c""),\n                    build.cc(""requantization/gemmlowp-sse4.c""),\n                ]\n        if build.target.is_arm or build.target.is_arm64:\n            with build.options(isa=arm.neon if build.target.is_arm else None):\n                requantization_objects += [\n                    build.cc(""requantization/precise-neon.c""),\n                    build.cc(""requantization/fp32-neon.c""),\n                    build.cc(""requantization/q31-neon.c""),\n                    build.cc(""requantization/gemmlowp-neon.c""),\n                ]\n\n        qnnpytorch_pack_objects = [\n            # Common parts\n            build.cc(""init.c""),\n            build.cc(""operator-delete.c""),\n            build.cc(""operator-run.c""),\n            # Operators\n            build.cc(""add.c""),\n            build.cc(""average-pooling.c""),\n            build.cc(""channel-shuffle.c""),\n            build.cc(""clamp.c""),\n            build.cc(""convolution.c""),\n            build.cc(""indirection.c""),\n            build.cc(""deconvolution.c""),\n            build.cc(""fully-connected.c""),\n            build.cc(""global-average-pooling.c""),\n            build.cc(""leaky-relu.c""),\n            build.cc(""max-pooling.c""),\n            build.cc(""sigmoid.c""),\n            build.cc(""softargmax.c""),\n            # Scalar micro-kernels\n            build.cc(""u8lut32norm/scalar.c""),\n            build.cc(""x8lut/scalar.c""),\n        ]\n\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            qnnpytorch_pack_objects += [\n                build.cc(""sconv/6x8-psimd.c""),\n                build.cc(""sdwconv/up4x9-psimd.c""),\n                build.cc(""sgemm/6x8-psimd.c""),\n            ]\n\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            if build.target.is_arm or build.target.is_arm64:\n                qnnpytorch_pack_objects += [\n                    build.cc(""q8avgpool/mp8x9p8q-neon.c""),\n                    build.cc(""q8avgpool/up8x9-neon.c""),\n                    build.cc(""q8avgpool/up8xm-neon.c""),\n                    build.cc(""q8conv/4x8-neon.c""),\n                    build.cc(""q8conv/8x8-neon.c""),\n                    build.cc(""q8dwconv/mp8x25-neon.c""),\n                    build.cc(""q8dwconv/up8x9-neon.c""),\n                    build.cc(""q8gavgpool/mp8x7p7q-neon.c""),\n                    build.cc(""q8gavgpool/up8x7-neon.c""),\n                    build.cc(""q8gavgpool/up8xm-neon.c""),\n                    build.cc(""q8gemm/4x-sumrows-neon.c""),\n                    build.cc(""q8gemm/4x8-neon.c""),\n                    build.cc(""q8gemm/4x8c2-xzp-neon.c""),\n                    build.cc(""q8gemm/6x4-neon.c""),\n                    build.cc(""q8gemm/8x8-neon.c""),\n                    build.cc(""q8vadd/neon.c""),\n                    build.cc(""sgemm/5x8-neon.c""),\n                    build.cc(""sgemm/6x8-neon.c""),\n                    build.cc(""u8clamp/neon.c""),\n                    build.cc(""u8maxpool/16x9p8q-neon.c""),\n                    build.cc(""u8maxpool/sub16-neon.c""),\n                    build.cc(""u8rmax/neon.c""),\n                    build.cc(""x8zip/x2-neon.c""),\n                    build.cc(""x8zip/x3-neon.c""),\n                    build.cc(""x8zip/x4-neon.c""),\n                    build.cc(""x8zip/xm-neon.c""),\n                ]\n            if build.target.is_arm:\n                qnnpytorch_pack_objects += [\n                    build.cc(""hgemm/8x8-aarch32-neonfp16arith.S""),\n                    build.cc(""q8conv/4x8-aarch32-neon.S""),\n                    build.cc(""q8dwconv/up8x9-aarch32-neon.S""),\n                    build.cc(""q8gemm/4x8-aarch32-neon.S""),\n                    build.cc(""q8gemm/4x8c2-xzp-aarch32-neon.S""),\n                ]\n            if build.target.is_arm64:\n                qnnpytorch_pack_objects += [\n                    build.cc(""q8gemm/8x8-aarch64-neon.S""),\n                    build.cc(""q8conv/8x8-aarch64-neon.S""),\n                ]\n            if build.target.is_x86 or build.target.is_x86_64:\n                with build.options(isa=x86.sse2):\n                    qnnpytorch_pack_objects += [\n                        build.cc(""q8avgpool/mp8x9p8q-sse2.c""),\n                        build.cc(""q8avgpool/up8x9-sse2.c""),\n                        build.cc(""q8avgpool/up8xm-sse2.c""),\n                        build.cc(""q8conv/4x4c2-sse2.c""),\n                        build.cc(""q8dwconv/mp8x25-sse2.c""),\n                        build.cc(""q8dwconv/up8x9-sse2.c""),\n                        build.cc(""q8gavgpool/mp8x7p7q-sse2.c""),\n                        build.cc(""q8gavgpool/up8x7-sse2.c""),\n                        build.cc(""q8gavgpool/up8xm-sse2.c""),\n                        build.cc(""q8gemm/2x4c8-sse2.c""),\n                        build.cc(""q8gemm/4x4c2-sse2.c""),\n                        build.cc(""q8vadd/sse2.c""),\n                        build.cc(""u8clamp/sse2.c""),\n                        build.cc(""u8maxpool/16x9p8q-sse2.c""),\n                        build.cc(""u8maxpool/sub16-sse2.c""),\n                        build.cc(""u8rmax/sse2.c""),\n                        build.cc(""x8zip/x2-sse2.c""),\n                        build.cc(""x8zip/x3-sse2.c""),\n                        build.cc(""x8zip/x4-sse2.c""),\n                        build.cc(""x8zip/xm-sse2.c""),\n                    ]\n            build.static_library(""qnnpack"", qnnpytorch_pack_objects)\n\n    with build.options(\n        source_dir=""test"",\n        deps={\n            (\n                build,\n                build.deps.cpuinfo,\n                build.deps.clog,\n                build.deps.pthreadpool,\n                build.deps.FP16,\n                build.deps.googletest,\n            ): any,\n            ""log"": build.target.is_android,\n        },\n        extra_include_dirs=[""src"", ""test""],\n    ):\n\n        build.unittest(""hgemm-test"", build.cxx(""hgemm.cc""))\n        build.unittest(""q8avgpool-test"", build.cxx(""q8avgpool.cc""))\n        build.unittest(""q8conv-test"", build.cxx(""q8conv.cc""))\n        build.unittest(""q8dwconv-test"", build.cxx(""q8dwconv.cc""))\n        build.unittest(""q8gavgpool-test"", build.cxx(""q8gavgpool.cc""))\n        build.unittest(""q8gemm-test"", build.cxx(""q8gemm.cc""))\n        build.unittest(""q8vadd-test"", build.cxx(""q8vadd.cc""))\n        build.unittest(""sconv-test"", build.cxx(""sconv.cc""))\n        build.unittest(""sgemm-test"", build.cxx(""sgemm.cc""))\n        build.unittest(""u8clamp-test"", build.cxx(""u8clamp.cc""))\n        build.unittest(""u8lut32norm-test"", build.cxx(""u8lut32norm.cc""))\n        build.unittest(""u8maxpool-test"", build.cxx(""u8maxpool.cc""))\n        build.unittest(""u8rmax-test"", build.cxx(""u8rmax.cc""))\n        build.unittest(""x8lut-test"", build.cxx(""x8lut.cc""))\n        build.unittest(""x8zip-test"", build.cxx(""x8zip.cc""))\n\n        build.unittest(""add-test"", build.cxx(""add.cc""))\n        build.unittest(""average-pooling-test"", build.cxx(""average-pooling.cc""))\n        build.unittest(""channel-shuffle-test"", build.cxx(""channel-shuffle.cc""))\n        build.unittest(""clamp-test"", build.cxx(""clamp.cc""))\n        build.unittest(""convolution-test"", build.cxx(""convolution.cc""))\n        build.unittest(""deconvolution-test"", build.cxx(""deconvolution.cc""))\n        build.unittest(""fully-connected-test"", build.cxx(""fully-connected.cc""))\n        build.unittest(\n            ""global-average-pooling-test"", build.cxx(""global-average-pooling.cc"")\n        )\n        build.unittest(""leaky-relu-test"", build.cxx(""leaky-relu.cc""))\n        build.unittest(""max-pooling-test"", build.cxx(""max-pooling.cc""))\n        build.unittest(""sigmoid-test"", build.cxx(""sigmoid.cc""))\n        build.unittest(""softargmax-test"", build.cxx(""softargmax.cc""))\n        build.unittest(\n            ""requantization-test"",\n            [build.cxx(""requantization.cc"")] + requantization_objects,\n        )\n\n    benchmark_isa = None\n    if build.target.is_arm:\n        benchmark_isa = arm.neon\n    elif build.target.is_x86:\n        benchmark_isa = x86.sse4_1\n    with build.options(\n        source_dir=""bench"",\n        deps={\n            (\n                build,\n                build.deps.cpuinfo,\n                build.deps.clog,\n                build.deps.pthreadpool,\n                build.deps.FP16,\n                build.deps.googlebenchmark,\n            ): any,\n            ""log"": build.target.is_android,\n        },\n        isa=benchmark_isa,\n        extra_include_dirs=""src"",\n    ):\n\n        build.benchmark(""add-bench"", build.cxx(""add.cc""))\n        build.benchmark(""average-pooling-bench"", build.cxx(""average-pooling.cc""))\n        build.benchmark(""channel-shuffle-bench"", build.cxx(""channel-shuffle.cc""))\n        build.benchmark(""convolution-bench"", build.cxx(""convolution.cc""))\n        build.benchmark(\n            ""global-average-pooling-bench"", build.cxx(""global-average-pooling.cc"")\n        )\n        build.benchmark(""max-pooling-bench"", build.cxx(""max-pooling.cc""))\n        build.benchmark(""sigmoid-bench"", build.cxx(""sigmoid.cc""))\n        build.benchmark(""softargmax-bench"", build.cxx(""softargmax.cc""))\n\n        build.benchmark(""q8gemm-bench"", build.cxx(""q8gemm.cc""))\n        build.benchmark(""hgemm-bench"", build.cxx(""hgemm.cc""))\n        build.benchmark(""sgemm-bench"", build.cxx(""sgemm.cc""))\n        build.benchmark(\n            ""requantization-bench"",\n            [build.cxx(""requantization.cc"")] + requantization_objects,\n        )\n\n    return build\n\n\nif __name__ == ""__main__"":\n    import sys\n\n    main(sys.argv[1:]).generate()\n'"
aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\nimport os\n\n\nQNNPACK_SOURCES = {\n    # Generic functions\n    None: [\n        ""requantization/fp32-psimd.c"",\n        ""requantization/fp32-scalar.c"",\n        ""requantization/gemmlowp-scalar.c"",\n        ""requantization/precise-psimd.c"",\n        ""requantization/precise-scalar.c"",\n        ""requantization/q31-scalar.c"",\n        ""sgemm/6x8-psimd.c"",\n        ""u8lut32norm/scalar.c"",\n        ""x8lut/scalar.c"",\n    ],\n    # AArch32/AArch64-specific uKernels\n    ""defined(__arm__) || defined(__aarch64__)"": [\n        ""q8avgpool/mp8x9p8q-neon.c"",\n        ""q8avgpool/up8x9-neon.c"",\n        ""q8avgpool/up8xm-neon.c"",\n        ""q8conv/4x8-neon.c"",\n        ""q8conv/8x8-neon.c"",\n        ""q8dwconv/mp8x25-neon.c"",\n        ""q8dwconv/up8x9-neon.c"",\n        ""q8gavgpool/mp8x7p7q-neon.c"",\n        ""q8gavgpool/up8x7-neon.c"",\n        ""q8gavgpool/up8xm-neon.c"",\n        ""q8gemm/4x-sumrows-neon.c"",\n        ""q8gemm/4x8-neon.c"",\n        ""q8gemm/4x8c2-xzp-neon.c"",\n        ""q8gemm/6x4-neon.c"",\n        ""q8gemm/8x8-neon.c"",\n        ""q8vadd/neon.c"",\n        ""requantization/fp32-neon.c"",\n        ""requantization/gemmlowp-neon.c"",\n        ""requantization/precise-neon.c"",\n        ""requantization/q31-neon.c"",\n        ""sgemm/5x8-neon.c"",\n        ""sgemm/6x8-neon.c"",\n        ""u8clamp/neon.c"",\n        ""u8maxpool/16x9p8q-neon.c"",\n        ""u8maxpool/sub16-neon.c"",\n        ""u8rmax/neon.c"",\n        ""x8zip/x2-neon.c"",\n        ""x8zip/x3-neon.c"",\n        ""x8zip/x4-neon.c"",\n        ""x8zip/xm-neon.c"",\n    ],\n    # x86/x86-64-specific uKernels\n    ""defined(__i386__) || defined(__i686__) || defined(__x86_64__)"": [\n        ""q8avgpool/mp8x9p8q-sse2.c"",\n        ""q8avgpool/up8x9-sse2.c"",\n        ""q8avgpool/up8xm-sse2.c"",\n        ""q8conv/4x4c2-sse2.c"",\n        ""q8dwconv/mp8x25-sse2.c"",\n        ""q8dwconv/up8x9-sse2.c"",\n        ""q8gavgpool/mp8x7p7q-sse2.c"",\n        ""q8gavgpool/up8x7-sse2.c"",\n        ""q8gavgpool/up8xm-sse2.c"",\n        ""q8gemm/2x4c8-sse2.c"",\n        ""q8gemm/4x4c2-sse2.c"",\n        ""q8vadd/sse2.c"",\n        ""requantization/fp32-sse2.c"",\n        ""requantization/gemmlowp-sse2.c"",\n        ""requantization/gemmlowp-sse4.c"",\n        ""requantization/gemmlowp-ssse3.c"",\n        ""requantization/precise-sse2.c"",\n        ""requantization/precise-sse4.c"",\n        ""requantization/precise-ssse3.c"",\n        ""requantization/q31-sse2.c"",\n        ""requantization/q31-sse4.c"",\n        ""requantization/q31-ssse3.c"",\n        ""u8clamp/sse2.c"",\n        ""u8maxpool/16x9p8q-sse2.c"",\n        ""u8maxpool/sub16-sse2.c"",\n        ""u8rmax/sse2.c"",\n        ""x8zip/x2-sse2.c"",\n        ""x8zip/x3-sse2.c"",\n        ""x8zip/x4-sse2.c"",\n        ""x8zip/xm-sse2.c"",\n    ],\n    # AArch32-specific uKernels\n    ""defined(__arm__)"": [\n        ""hgemm/8x8-aarch32-neonfp16arith.S"",\n        ""q8conv/4x8-aarch32-neon.S"",\n        ""q8dwconv/up8x9-aarch32-neon.S"",\n        ""q8gemm/4x8-aarch32-neon.S"",\n        ""q8gemm/4x8c2-xzp-aarch32-neon.S"",\n    ],\n    # AArch64-specific uKernels\n    ""defined(__aarch64__)"": [\n        ""q8conv/8x8-aarch64-neon.S"",\n        ""q8gemm/8x8-aarch64-neon.S"",\n    ],\n}\n\nBANNER = ""/* Auto-generated by generate-wrappers.py script. Do not modify */""\n\n\nif __name__ == ""__main__"":\n    for condition, filenames in QNNPACK_SOURCES.items():\n        for filename in filenames:\n            filepath = os.path.join(""wrappers"", filename)\n            if not os.path.isdir(os.path.dirname(filepath)):\n                os.makedirs(os.path.dirname(filepath))\n            with open(filepath, ""w"") as wrapper:\n                print(BANNER, file=wrapper)\n                print(file=wrapper)\n\n                # Architecture- or platform-dependent preprocessor flags can be\n                # defined here. Note: platform_preprocessor_flags can\'t be used\n                # because they are ignored by arc focus & buck project.\n\n                if condition is None:\n                    print(""#include <%s>"" % filename, file=wrapper)\n                else:\n                    # Include source file only if condition is satisfied\n                    print(""#if %s"" % condition, file=wrapper)\n                    print(""#include <%s>"" % filename, file=wrapper)\n                    print(""#endif /* %s */"" % condition, file=wrapper)\n'"
aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/configure.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport confu\nparser = confu.standard_parser(""clog configuration script"")\n\n\ndef main(args):\n    options = parser.parse_args(args)\n    build = confu.Build.from_options(options)\n\n    build.export_cpath(""include"", [""clog.h""])\n\n    with build.options(source_dir=""src"", extra_include_dirs=""src""):\n        build.static_library(""clog"", build.cc(""clog.c""))\n\n    with build.options(source_dir=""test"", deps={\n            (build, build.deps.googletest): all,\n            ""log"": build.target.is_android}):\n        build.unittest(""clog-test"", build.cxx(""clog.cc""))\n\n    return build\n\nif __name__ == ""__main__"":\n    import sys\n    main(sys.argv[1:]).generate()\n'"
