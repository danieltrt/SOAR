file_path,api_count,code
hubconf.py,1,"b""dependencies = ['torch']\n\nfrom resnest.torch.resnest import resnest50, resnest101, resnest200, resnest269\n"""
setup.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport io\nimport os\nimport subprocess\n\nfrom setuptools import setup, find_packages\n\ncwd = os.path.dirname(os.path.abspath(__file__))\n\nversion = \'0.0.4\'\ntry:\n    if not os.getenv(\'RELEASE\'):\n        from datetime import date\n        today = date.today()\n        day = today.strftime(""b%Y%m%d"")\n        version += day\nexcept Exception:\n    pass\n\ndef create_version_file():\n    global version, cwd\n    print(\'-- Building version \' + version)\n    version_path = os.path.join(cwd, \'resnest\', \'version.py\')\n    with open(version_path, \'w\') as f:\n        f.write(\'""""""This is resnest version file.""""""\\n\')\n        f.write(""__version__ = \'{}\'\\n"".format(version))\n\nrequirements = [\n    \'numpy\',\n    \'tqdm\',\n    \'nose\',\n    \'torch>=1.0.0\',\n    \'Pillow\',\n    \'scipy\',\n    \'requests\',\n]\n\nif __name__ == \'__main__\':\n    create_version_file()\n    setup(\n        name=""resnest"",\n        version=version,\n        author=""Hang Zhang"",\n        author_email=""zhanghang0704@gmail.com"",\n        url=""https://github.com/zhanghang1989/ResNeSt"",\n        description=""ResNeSt"",\n        long_description=open(\'README.md\').read(),\n        long_description_content_type=\'text/markdown\',\n        license=\'Apache-2.0\',\n        install_requires=requirements,\n        packages=find_packages(exclude=[""scripts"", ""examples"", ""tests""]),\n        package_data={\'resnest\': [\n            \'LICENSE\',\n        ]},\n    )\n\n'"
resnest/__init__.py,0,b''
resnest/transforms.py,0,"b'# code adapted from:\n# https://github.com/kakaobrain/fast-autoaugment\n# https://github.com/rpmcruz/autoaugment\nimport math\nimport random\n\nimport numpy as np\nfrom collections import defaultdict\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nfrom PIL import Image\n\nrandom_mirror = True\n\nRESAMPLE_MODE=Image.BICUBIC\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, v, 0, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, v, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[0]\n    return img.transform(img.size, Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[1]\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         RESAMPLE_MODE)\n\n\ndef TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         RESAMPLE_MODE)\n\n\ndef Rotate(img, v):  # [-30, 30]\n    assert -30 <= v <= 30\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Flip(img, _):  # not from the paper\n    return PIL.ImageOps.mirror(img)\n\n\ndef Solarize(img, v):  # [0, 256]\n    assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\n\ndef SolarizeAdd(img, addition=0, threshold=128):\n    img_np = np.array(img).astype(np.int)\n    img_np = img_np + addition\n    img_np = np.clip(img_np, 0, 255)\n    img_np = img_np.astype(np.uint8)\n    img = Image.fromarray(img_np)\n    return PIL.ImageOps.solarize(img, threshold)\n\n\ndef Posterize(img, v):  # [4, 8]\n    #assert 4 <= v <= 8\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\ndef Contrast(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Color(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Brightness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\ndef CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n    # assert 0 <= v <= 20\n    if v < 0:\n        return img\n    w, h = img.size\n    x0 = np.random.uniform(w)\n    y0 = np.random.uniform(h)\n\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = min(w, x0 + v)\n    y1 = min(h, y0 + v)\n\n    xy = (x0, y0, x1, y1)\n    color = (125, 123, 114)\n    # color = (0, 0, 0)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n    assert 0.0 <= v <= 0.2\n    if v <= 0.:\n        return img\n\n    v = v * img.size[0]\n    return CutoutAbs(img, v)\n\n\n\ndef TranslateYAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         resample=RESAMPLE_MODE)\n\n\ndef TranslateXAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         resample=RESAMPLE_MODE)\n\n\ndef Posterize2(img, v):  # [0, 4]\n    assert 0 <= v <= 4\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\n\ndef SamplePairing(imgs):  # [0, 0.4]\n    def f(img1, v):\n        i = np.random.choice(len(imgs))\n        img2 = Image.fromarray(imgs[i])\n        return Image.blend(img1, img2, v)\n\n    return f\n\n\ndef augment_list(for_autoaug=True):  # 16 oeprations and their ranges\n    l = [\n        (ShearX, -0.3, 0.3),  # 0\n        (ShearY, -0.3, 0.3),  # 1\n        (TranslateX, -0.45, 0.45),  # 2\n        (TranslateY, -0.45, 0.45),  # 3\n        (Rotate, -30, 30),  # 4\n        (AutoContrast, 0, 1),  # 5\n        (Invert, 0, 1),  # 6\n        (Equalize, 0, 1),  # 7\n        (Solarize, 0, 256),  # 8\n        (Posterize, 4, 8),  # 9\n        (Contrast, 0.1, 1.9),  # 10\n        (Color, 0.1, 1.9),  # 11\n        (Brightness, 0.1, 1.9),  # 12\n        (Sharpness, 0.1, 1.9),  # 13\n        (Cutout, 0, 0.2),  # 14\n        # (SamplePairing(imgs), 0, 0.4),  # 15\n    ]\n    if for_autoaug:\n        l += [\n            (CutoutAbs, 0, 20),  # compatible with auto-augment\n            (Posterize2, 0, 4),  # 9\n            (TranslateXAbs, 0, 10),  # 9\n            (TranslateYAbs, 0, 10),  # 9\n        ]\n    return l\n\n\naugment_dict = {fn.__name__: (fn, v1, v2) for fn, v1, v2 in augment_list()}\n\nPARAMETER_MAX = 10\n\n\ndef float_parameter(level, maxval):\n    return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n    return int(float_parameter(level, maxval))\n\n\ndef autoaug2fastaa(f):\n    def autoaug():\n        mapper = defaultdict(lambda: lambda x: x)\n        mapper.update({\n            \'ShearX\': lambda x: float_parameter(x, 0.3),\n            \'ShearY\': lambda x: float_parameter(x, 0.3),\n            \'TranslateX\': lambda x: int_parameter(x, 10),\n            \'TranslateY\': lambda x: int_parameter(x, 10),\n            \'Rotate\': lambda x: int_parameter(x, 30),\n            \'Solarize\': lambda x: 256 - int_parameter(x, 256),\n            \'Posterize2\': lambda x: 4 - int_parameter(x, 4),\n            \'Contrast\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Color\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Brightness\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Sharpness\': lambda x: float_parameter(x, 1.8) + .1,\n            \'CutoutAbs\': lambda x: int_parameter(x, 20)\n        })\n\n        def low_high(name, prev_value):\n            _, low, high = get_augment(name)\n            return float(prev_value - low) / (high - low)\n\n        policies = f()\n        new_policies = []\n        for policy in policies:\n            new_policies.append([(name, pr, low_high(name, mapper[name](level))) for name, pr, level in policy])\n        return new_policies\n\n    return autoaug\n\n\n@autoaug2fastaa\ndef autoaug_imagenet_policies():\n    return [\n        [(\'Posterize2\', 0.4, 8), (\'Rotate\', 0.6, 9)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n        [(\'Posterize2\', 0.6, 7), (\'Posterize2\', 0.6, 6)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Equalize\', 0.4, 4), (\'Rotate\', 0.8, 8)],\n        [(\'Solarize\', 0.6, 3), (\'Equalize\', 0.6, 7)],\n        [(\'Posterize2\', 0.8, 5), (\'Equalize\', 1.0, 2)],\n        [(\'Rotate\', 0.2, 3), (\'Solarize\', 0.6, 8)],\n        [(\'Equalize\', 0.6, 8), (\'Posterize2\', 0.4, 6)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 0.4, 0)],\n        [(\'Rotate\', 0.4, 9), (\'Equalize\', 0.6, 2)],\n        [(\'Equalize\', 0.0, 7), (\'Equalize\', 0.8, 8)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 1.0, 0)],\n        [(\'Color\', 0.8, 8), (\'Solarize\', 0.8, 7)],\n        [(\'Sharpness\', 0.4, 7), (\'Invert\', 0.6, 8)],\n        [(\'ShearX\', 0.6, 5), (\'Equalize\', 1.0, 9)],\n        [(\'Color\', 0.4, 0), (\'Equalize\', 0.6, 3)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n    ]\n\n\ndef get_augment(name):\n    return augment_dict[name]\n\n\ndef apply_augment(img, name, level):\n    augment_fn, low, high = get_augment(name)\n    return augment_fn(img.copy(), level * (high - low) + low)\n\n\ndef rand_augment_list():  # 16 oeprations and their ranges\n    l = [\n        (AutoContrast, 0, 1),\n        (Equalize, 0, 1),\n        (Invert, 0, 1),\n        (Rotate, 0, 30),\n        (Posterize, 0, 4),\n        (Solarize, 0, 256),\n        (SolarizeAdd, 0, 110),\n        (Color, 0.1, 1.9),\n        (Contrast, 0.1, 1.9),\n        (Brightness, 0.1, 1.9),\n        (Sharpness, 0.1, 1.9),\n        (ShearX, 0., 0.3),\n        (ShearY, 0., 0.3),\n        (CutoutAbs, 0, 40),\n        (TranslateXabs, 0., 100),\n        (TranslateYabs, 0., 100),\n    ]\n\n    return l\n\n\n\nclass ERandomCrop:\n    # pylint: disable=misplaced-comparison-constant\n    def __init__(self, imgsize, min_covered=0.1, aspect_ratio_range=(3./4, 4./3),\n                 area_range=(0.1, 1.0), max_attempts=10):\n        assert 0.0 < min_covered\n        assert 0 < aspect_ratio_range[0] <= aspect_ratio_range[1]\n        assert 0 < area_range[0] <= area_range[1]\n        assert 1 <= max_attempts\n\n        self.min_covered = min_covered\n        self.aspect_ratio_range = aspect_ratio_range\n        self.area_range = area_range\n        self.max_attempts = max_attempts\n        self._fallback = ECenterCrop(imgsize)\n\n    def __call__(self, img):\n        # https://github.com/tensorflow/tensorflow/blob/9274bcebb31322370139467039034f8ff852b004/tensorflow/core/kernels/sample_distorted_bounding_box_op.cc#L111\n        original_width, original_height = img.size\n        min_area = self.area_range[0] * (original_width * original_height)\n        max_area = self.area_range[1] * (original_width * original_height)\n\n        for _ in range(self.max_attempts):\n            aspect_ratio = random.uniform(*self.aspect_ratio_range)\n            height = int(round(math.sqrt(min_area / aspect_ratio)))\n            max_height = int(round(math.sqrt(max_area / aspect_ratio)))\n\n            if max_height * aspect_ratio > original_width:\n                max_height = (original_width + 0.5 - 1e-7) / aspect_ratio\n                max_height = int(max_height)\n                if max_height * aspect_ratio > original_width:\n                    max_height -= 1\n\n            if max_height > original_height:\n                max_height = original_height\n\n            if height >= max_height:\n                height = max_height\n\n            height = int(round(random.uniform(height, max_height)))\n            width = int(round(height * aspect_ratio))\n            area = width * height\n\n            if area < min_area or area > max_area:\n                continue\n            if width > original_width or height > original_height:\n                continue\n            if area < self.min_covered * (original_width * original_height):\n                continue\n            if width == original_width and height == original_height:\n                return self._fallback(img)\n\n            x = random.randint(0, original_width - width)\n            y = random.randint(0, original_height - height)\n            return img.crop((x, y, x + width, y + height))\n\n        return self._fallback(img)\n\nclass ECenterCrop:\n    """"""Crop the given PIL Image and resize it to desired size.\n    Args:\n        img (PIL Image): Image to be cropped. (0,0) denotes the top left corner of the image.\n        output_size (sequence or int): (height, width) of the crop box. If int,\n            it is used for both directions\n    Returns:\n        PIL Image: Cropped image.\n    """"""\n    def __init__(self, imgsize):\n        self.imgsize = imgsize\n        import torchvision.transforms as pth_transforms\n        self.resize_method = pth_transforms.Resize((imgsize, imgsize), interpolation=RESAMPLE_MODE)\n\n    def __call__(self, img):\n        image_width, image_height = img.size\n        image_short = min(image_width, image_height)\n\n        crop_size = float(self.imgsize) / (self.imgsize + 32) * image_short\n\n        crop_height, crop_width = crop_size, crop_size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        img = img.crop((crop_left, crop_top, crop_left + crop_width, crop_top + crop_height))\n        return self.resize_method(img)\n\n\n'"
resnest/utils.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport os\nfrom pathlib import Path\nimport requests\nimport errno\nimport shutil\nimport hashlib\nimport zipfile\nimport logging\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'unzip\', \'download\', \'mkdir\', \'check_sha1\', \'raise_num_file\']\n\ndef unzip(zip_file_path, root=os.path.expanduser(\'./\')):\n    """"""Unzips files located at `zip_file_path` into parent directory specified by `root`.\n    """"""\n    folders = []\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n        for name in zf.namelist():\n            folder = Path(name).parts[0]\n            if folder not in folders:\n                folders.append(folder)\n    folders = folders[0] if len(folders) == 1 else tuple(folders)\n    return folders\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download files from a given URL.\n\n    Parameters\n    ----------\n    url : str\n        URL where file is located\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if one already exists at this location.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits (will ignore existing file when hash is specified\n        but doesn\'t match).\n\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        logger.info(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname\n\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash\n\n\ndef mkdir(path):\n    """"""Make directory at the specified local path with special error handling.\n    """"""\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n'"
tests/test_radix_major.py,12,"b'import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\nfrom torch.nn.modules.utils import _pair\n\nfrom resnest.torch.splat import SplAtConv2d\n\nclass RadixMajorNaiveImp(Module):\n    """"""Split-Attention Conv2d\n    """"""\n    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, bias=True,\n                 radix=2, reduction_factor=4,\n                 rectify=False, rectify_avg=False, norm_layer=None,\n                 dropblock_prob=0.0, **kwargs):\n        super(RadixMajorNaiveImp, self).__init__()\n        padding = _pair(padding)\n        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n        self.rectify_avg = rectify_avg\n        inter_channels = max(in_channels*radix//reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n        self.dropblock_prob = dropblock_prob\n        if self.rectify:\n            from rfconv import RFConv2d\n            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n        else:\n            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                               groups=groups*radix, bias=bias, **kwargs)\n        self.use_bn = norm_layer is not None\n        assert not self.use_bn\n\n        self.relu = ReLU(inplace=True)\n        cardinal_group_width = channels // groups\n        cardinal_inter_channels = inter_channels // groups\n\n        self.fc1 = nn.ModuleList([nn.Linear(cardinal_group_width, cardinal_inter_channels) for _ in range(groups)])\n        self.fc2 = nn.ModuleList([nn.Linear(cardinal_inter_channels, cardinal_group_width*radix) for _ in range(groups)])\n\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock2D(dropblock_prob, 3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, channel = x.shape[:2]\n        cardinality = self.cardinality\n        radix = self.radix\n\n        tiny_group_width = channel//radix//cardinality\n        all_groups = torch.split(x, tiny_group_width, dim=1)\n\n        out = []\n        for k in range(cardinality):\n            U_k = [all_groups[r * cardinality + k] for r in range(radix)]\n            U_k = sum(U_k)\n            gap_k = F.adaptive_avg_pool2d(U_k, 1).squeeze()\n            atten_k = self.fc2[k](self.fc1[k](gap_k))\n            if radix > 1:\n                x_k = [all_groups[r * cardinality + k] for r in range(radix)]\n                x_k = torch.cat(x_k, dim=1)\n                atten_k = atten_k.view(batch, radix, -1)\n                atten_k = F.softmax(atten_k, dim=1)\n            else:\n                x_k = all_groups[k]\n                atten_k = F.sigmoid(atten_k)\n            attended_k = x_k * atten_k.view(batch, -1, 1, 1)\n            out_k = sum(torch.split(attended_k, attended_k.size(1)//self.radix, dim=1))\n            out.append(out_k)\n \n        return torch.cat(out, dim=1).contiguous()\n\n@torch.no_grad()\ndef sync_weigths(m1, m2):\n    m1.conv.weight.copy_(torch.from_numpy(m2.conv.weight.data.numpy()))\n    nn.init.ones_(m1.fc1.weight)\n    nn.init.ones_(m1.fc2.weight)\n    nn.init.zeros_(m1.fc1.bias)\n    nn.init.zeros_(m1.fc2.bias)\n    for m in m2.fc1:\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n    for m in m2.fc2:\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n\ndef _AssertTensorClose(a, b, atol=1e-3, rtol=1e-3):\n    npa, npb = a.cpu().detach().numpy(), b.cpu().detach().numpy()\n    assert np.allclose(npa, npb, atol=atol), \\\n        \'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}\'.format(\n            a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n\ndef test_radix_major():\n    device = torch.device(\'cuda:0\') if torch.cuda.is_available() else torch.device(\'cpu\')\n    def compare_two_imp(batch, height, width,\n                        in_channels, channels,\n                        kernel_size, stride, padding,\n                        radix, groups):\n        layer1 = SplAtConv2d(in_channels, channels, kernel_size, stride, padding, radix=radix, groups=groups, bias=False)\n        layer2 = RadixMajorNaiveImp(in_channels, channels, kernel_size, stride, padding, radix=radix, groups=groups, bias=False)\n        sync_weigths(layer1, layer2)\n        layer1 = layer1.to(device)\n        layer2 = layer2.to(device)\n        x = torch.rand(batch, in_channels, height, width).to(device)\n        y1 = layer1(x)\n        y2 = layer2(x)\n        _AssertTensorClose(y1, y2)\n\n    for batch in [2, 4, 8, 32]:\n        for height in [7, 14, 28, 56]:\n            width = height\n            for in_channels in [16, 64, 128]:\n                channels = in_channels\n                for kernel_size in [3, 5]:\n                     padding = kernel_size // 2\n                     for stride in [1, 2]:\n                        for radix in [1, 2, 4]:\n                            for groups in [1, 2, 4]:\n                                compare_two_imp(\n                                    batch, height, width, in_channels,\n                                    channels, kernel_size, stride, padding,\n                                    radix, groups)\n\nif __name__ == ""__main__"":\n    import nose\n    nose.runmodule()\n'"
tests/test_torch.py,1,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport importlib\nimport inspect\n\ndef test_model_inference():\n    # get all models\n    import resnest.torch as module\n    functions = inspect.getmembers(module, inspect.isfunction)\n    model_list = [f[0] for f in functions]\n\n    get_model = importlib.import_module(\'resnest.torch\')\n    x = torch.rand(1, 3, 224, 224)\n    for model_name in model_list:\n        if \'fast\' in model_name: continue\n        print(\'Doing: \', model_name)\n        net = getattr(get_model, model_name)\n        model = net(pretrained=True)\n        model.eval()\n        y = model(x)\n\nif __name__ == ""__main__"":\n    import nose\n    nose.runmodule()\n'"
resnest/gluon/__init__.py,0,b'from .resnest import *\nfrom .ablation import *\nfrom .model_zoo import get_model\n'
resnest/gluon/ablation.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n""""""Ablation Study Models for ResNeSt""""""\nfrom .resnet import ResNet, Bottleneck\nfrom mxnet import cpu\n\n__all__ = [\'resnest50_fast_1s1x64d\', \'resnest50_fast_2s1x64d\', \'resnest50_fast_4s1x64d\',\n           \'resnest50_fast_1s2x40d\', \'resnest50_fast_2s2x40d\', \'resnest50_fast_4s2x40d\',\n           \'resnest50_fast_1s4x24d\']\n\ndef resnest50_fast_1s1x64d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, cardinality=1, bottleneck_width=64,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_1s1x64d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_2s1x64d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, cardinality=1, bottleneck_width=64,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_2s1x64d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_4s1x64d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, cardinality=1, bottleneck_width=64,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_4s1x64d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_1s2x40d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, cardinality=2, bottleneck_width=40,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_1s2x40d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_2s2x40d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, cardinality=2, bottleneck_width=40,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_2s2x40d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_4s2x40d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, cardinality=2, bottleneck_width=40,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_4s2x40d\',\n                                             root=root), ctx=ctx)\n    return model\n\ndef resnest50_fast_1s4x24d(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, cardinality=4, bottleneck_width=24,\n                   deep_stem=True, avg_down=True,\n                   avd=True, avd_first=True,\n                   use_splat=True, dropblock_prob=0.1,\n                   name_prefix=\'resnetv1f_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50_fast_1s4x24d\',\n                                             root=root), ctx=ctx)\n    return model\n\n'"
resnest/gluon/data_utils.py,0,"b'from PIL import Image\n\nimport mxnet as mx\nfrom mxnet.gluon import Block\nfrom ..transforms import *\n\nclass RandAugment(object):\n    def __init__(self, n, m):\n        self.n = n\n        self.m = m\n        self.augment_list = rand_augment_list()\n        self.topil = ToPIL()\n\n    def __call__(self, img):\n        img = self.topil(img)\n        ops = random.choices(self.augment_list, k=self.n)\n        for op, minval, maxval in ops:\n            if random.random() > random.uniform(0.2, 0.8):\n                continue\n            val = (float(self.m) / 30) * float(maxval - minval) + minval\n            img = op(img, val)\n        return img\n\n\nclass ToPIL(object):\n    """"""Convert image from ndarray format to PIL\n    """"""\n    def __call__(self, img):\n        x = Image.fromarray(img.asnumpy())\n        return x\n\nclass ToNDArray(object):\n    def __call__(self, img):\n        x = mx.nd.array(np.array(img), mx.cpu(0))\n        return x\n\nclass AugmentationBlock(Block):\n    r""""""\n    AutoAugment Block\n\n    Example\n    -------\n    >>> from autogluon.utils.augment import AugmentationBlock, autoaug_imagenet_policies\n    >>> aa_transform = AugmentationBlock(autoaug_imagenet_policies())\n    """"""\n    def __init__(self, policies):\n        """"""\n        plicies : list of (name, pr, level)\n        """"""\n        super().__init__()\n        self.policies = policies\n        self.topil = ToPIL()\n        self.tond = ToNDArray()\n\n    def forward(self, img):\n        img = self.topil(img)\n        policy = random.choice(self.policies)\n        for name, pr, level in policy:\n            if random.random() > pr:\n                continue\n            img = apply_augment(img, name, level)\n        img = self.tond(img)\n        return img\n'"
resnest/gluon/dropblock.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport mxnet as mx\nfrom functools import partial\nfrom mxnet.gluon.nn import MaxPool2D, Block, HybridBlock\n\n__all__ = [\'DropBlock\', \'set_drop_prob\', \'DropBlockScheduler\']\n\nclass DropBlock(HybridBlock):\n    def __init__(self, drop_prob, block_size, c, h, w):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.block_size = block_size\n        self.c, self.h, self.w = c, h, w\n        self.numel = c * h * w\n        pad_h = max((block_size - 1), 0)\n        pad_w = max((block_size - 1), 0)\n        self.padding = (pad_h//2, pad_h-pad_h//2, pad_w//2, pad_w-pad_w//2)\n        self.dtype = \'float32\'\n\n    def hybrid_forward(self, F, x):\n        if not mx.autograd.is_training() or self.drop_prob <= 0:\n            return x\n        gamma = self.drop_prob * (self.h * self.w) / (self.block_size ** 2) / \\\n            ((self.w - self.block_size + 1) * (self.h - self.block_size + 1))\n        # generate mask\n        mask = F.random.uniform(0, 1, shape=(1, self.c, self.h, self.w), dtype=self.dtype) < gamma\n        mask = F.Pooling(mask, pool_type=\'max\',\n                         kernel=(self.block_size, self.block_size), pad=self.padding)\n        mask = 1 - mask\n        y = F.broadcast_mul(F.broadcast_mul(x, mask),\n                            (1.0 * self.numel / mask.sum(axis=0, exclude=True).expand_dims(1).expand_dims(1).expand_dims(1)))\n        return y\n\n    def cast(self, dtype):\n        super(DropBlock, self).cast(dtype)\n        self.dtype = dtype\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' + \\\n            \'drop_prob: {}, block_size{}\'.format(self.drop_prob, self.block_size) +\')\'\n        return reprstr\n\ndef set_drop_prob(drop_prob, module):\n    """"""\n    Example:\n        from functools import partial\n        apply_drop_prob = partial(set_drop_prob, 0.1)\n        net.apply(apply_drop_prob)\n    """"""\n    if isinstance(module, DropBlock):\n        module.drop_prob = drop_prob\n\n\nclass DropBlockScheduler(object):\n    def __init__(self, net, start_prob, end_prob, num_epochs):\n        self.net = net\n        self.start_prob = start_prob\n        self.end_prob = end_prob\n        self.num_epochs = num_epochs\n\n    def __call__(self, epoch):\n        ratio = self.start_prob + 1.0 * (self.end_prob - self.start_prob) * (epoch + 1) / self.num_epochs\n        assert (ratio >= 0 and ratio <= 1)\n        apply_drop_prob = partial(set_drop_prob, ratio)\n        self.net.apply(apply_drop_prob)\n        self.net.hybridize()\n\n'"
resnest/gluon/model_store.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""Model store which provides pretrained models.""""""\nfrom __future__ import print_function\n\n__all__ = [\'get_model_file\', \'purge\']\n\nimport os\nimport zipfile\n\nfrom ..utils import download, check_sha1\n\n_model_sha1 = {name: checksum for checksum, name in [\n    (\'bcfefe1dd1dd1ef5cfed5563123c1490ea37b42e\', \'resnest50\'),\n    (\'5da943b3230f071525a98639945a6b3b3a45ac95\', \'resnest101\'),\n    (\'0c5d117df664ace220aa6fc2922c094bb079d381\', \'resnest200\'),\n    (\'11ae7f5da2bcdbad05ba7e84f9b74383e717f3e3\', \'resnest269\'),\n    (\'5e16dbe56f1fba8e1bc2faddd91f874bfbd74193\', \'resnest50_fast_1s1x64d\'),\n    (\'85eb779a5e313d74b5e5390dae02aa8082a0f469\', \'resnest50_fast_2s1x64d\'),\n    (\'3f215532c6d8e07a10df116309993d4479fc3e4b\', \'resnest50_fast_4s1x64d\'),\n    (\'af3514c2ec757a3a9666a75b82f142ed47d55bee\', \'resnest50_fast_1s2x40d\'),\n    (\'2db13245aa4967cf5e8617cb4911880dd41628a4\', \'resnest50_fast_2s2x40d\'),\n    (\'b24d515797832e02da4da9c8a15effd5e44cfb56\', \'resnest50_fast_4s2x40d\'),\n    (\'7318153ddb5e542a20cc6c58192f3c6209cff9ed\', \'resnest50_fast_1s4x24d\'),\n    ]}\n\nencoding_repo_url = \'https://hangzh.s3-us-west-1.amazonaws.com/\'\n_url_format = \'{repo_url}gluon/models/{file_name}.zip\'\n\ndef short_hash(name):\n    if name not in _model_sha1:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha1[name][:8]\n\ndef get_model_file(name, root=os.path.join(\'~\', \'.encoding\', \'models\')):\n    r""""""Return location for the pretrained on local file system.\n    This function will download from online model zoo when model cannot be found or has mismatch.\n    The root directory will be created if it doesn\'t exist.\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    Returns\n    -------\n    file_path\n        Path to the requested pretrained model file.\n    """"""\n    if name not in _model_sha1:\n        import gluoncv as gcv\n        return gcv.model_zoo.model_store.get_model_file(name, root=root)\n    file_name = \'{name}-{short_hash}\'.format(name=name, short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name+\'.params\')\n    sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file {} detected.\' +\n                  \' Downloading again.\'.format(file_path))\n    else:\n        print(\'Model file {} is not found. Downloading.\'.format(file_path))\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name+\'.zip\')\n    repo_url = os.environ.get(\'ENCODING_REPO\', encoding_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\ndef purge(root=os.path.join(\'~\', \'.encoding\', \'models\')):\n    r""""""Purge all pretrained model files in local file store.\n    Parameters\n    ----------\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    """"""\n    root = os.path.expanduser(root)\n    files = os.listdir(root)\n    for f in files:\n        if f.endswith("".params""):\n            os.remove(os.path.join(root, f))\n\ndef pretrained_model_list():\n    return list(_model_sha1.keys())\n\n'"
resnest/gluon/model_zoo.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom .resnest import *\nfrom .ablation import *\n\n_all__ = [\'get_model\', \'get_model_list\']\n\nmodels = {\n    \'resnest50\': resnest50,\n    \'resnest101\': resnest101,\n    \'resnest200\': resnest200,\n    \'resnest269\': resnest269,\n    \'resnest50_fast_1s1x64d\': resnest50_fast_1s1x64d,\n    \'resnest50_fast_2s1x64d\': resnest50_fast_2s1x64d,\n    \'resnest50_fast_4s1x64d\': resnest50_fast_4s1x64d,\n    \'resnest50_fast_1s2x40d\': resnest50_fast_1s2x40d,\n    \'resnest50_fast_2s2x40d\': resnest50_fast_2s2x40d,\n    \'resnest50_fast_4s2x40d\': resnest50_fast_4s2x40d,\n    \'resnest50_fast_1s4x24d\': resnest50_fast_1s4x24d,\n    }\n\ndef get_model(name, **kwargs):\n    """"""Returns a pre-defined model by name\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    pretrained : bool\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    Returns\n    -------\n    Module:\n        The model.\n    """"""\n\n    name = name.lower()\n    if name in models:\n        net = models[name](**kwargs)\n    else:\n        raise ValueError(\'%s\\n\\t%s\' % (str(name), \'\\n\\t\'.join(sorted(models.keys()))))\n    return net\n\ndef get_model_list():\n    """"""Get the entire list of model names in model_zoo.\n    Returns\n    -------\n    list of str\n        Entire list of model names in model_zoo.\n    """"""\n    return models.keys()\n\n'"
resnest/gluon/resnest.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNeSt implemented in Gluon.""""""\n\n__all__ = [\'resnest50\', \'resnest101\',\n           \'resnest200\', \'resnest269\']\n\nfrom .resnet import ResNet, Bottleneck\nfrom mxnet import cpu\n\ndef resnest50(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                      radix=2, cardinality=1, bottleneck_width=64,\n                      deep_stem=True, avg_down=True,\n                      avd=True, avd_first=False,\n                      use_splat=True, dropblock_prob=0.1,\n                      name_prefix=\'resnest_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest50\', root=root), ctx=ctx)\n    return model\n\ndef resnest101(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 23, 3],\n                      radix=2, cardinality=1, bottleneck_width=64,\n                      deep_stem=True, avg_down=True, stem_width=64,\n                      avd=True, avd_first=False, use_splat=True, dropblock_prob=0.1,\n                      name_prefix=\'resnest_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest101\', root=root), ctx=ctx)\n    return model\n\ndef resnest200(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 24, 36, 3], deep_stem=True, avg_down=True, stem_width=64,\n                      avd=True, use_splat=True, dropblock_prob=0.1, final_drop=0.2,\n                      name_prefix=\'resnest_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest200\', root=root), ctx=ctx)\n    return model\n\ndef resnest269(pretrained=False, root=\'~/.mxnet/models\', ctx=cpu(0), **kwargs):\n    model = ResNet(Bottleneck, [3, 30, 48, 8], deep_stem=True, avg_down=True, stem_width=64,\n                      avd=True, use_splat=True, dropblock_prob=0.1, final_drop=0.2,\n                      name_prefix=\'resnest_\', **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        model.load_parameters(get_model_file(\'resnest269\', root=root), ctx=ctx)\n    return model\n'"
resnest/gluon/resnet.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNets, implemented in Gluon.""""""\n# pylint: disable=arguments-differ,unused-argument,missing-docstring\nfrom __future__ import division\n\nimport os\nimport math\nfrom mxnet.context import cpu\nfrom mxnet.gluon.block import HybridBlock\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.nn import BatchNorm\n\nfrom .dropblock import DropBlock\nfrom .splat import SplitAttentionConv\n\n__all__ = [\'ResNet\', \'Bottleneck\']\n\ndef _update_input_size(input_size, stride):\n    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n    ih, iw = (input_size, input_size) if isinstance(input_size, int) else input_size\n    oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n    input_size = (oh, ow)\n    return input_size\n\nclass Bottleneck(HybridBlock):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, channels, cardinality=1, bottleneck_width=64, strides=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None,\n                 norm_kwargs=None, last_gamma=False,\n                 dropblock_prob=0, input_size=None, use_splat=False,\n                 radix=2, avd=False, avd_first=False, in_channels=None, \n                 split_drop_ratio=0, **kwargs):\n        super(Bottleneck, self).__init__()\n        group_width = int(channels * (bottleneck_width / 64.)) * cardinality\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.dropblock_prob = dropblock_prob\n        self.use_splat = use_splat\n        self.avd = avd and (strides > 1 or previous_dilation != dilation)\n        self.avd_first = avd_first\n        if self.dropblock_prob > 0:\n            self.dropblock1 = DropBlock(dropblock_prob, 3, group_width, *input_size)\n            if self.avd:\n                if avd_first:\n                    input_size = _update_input_size(input_size, strides)\n                self.dropblock2 = DropBlock(dropblock_prob, 3, group_width, *input_size)\n                if not avd_first:\n                    input_size = _update_input_size(input_size, strides)\n            else:\n                input_size = _update_input_size(input_size, strides)\n                self.dropblock2 = DropBlock(dropblock_prob, 3, group_width, *input_size)\n            self.dropblock3 = DropBlock(dropblock_prob, 3, channels*4, *input_size)\n        self.conv1 = nn.Conv2D(channels=group_width, kernel_size=1,\n                               use_bias=False, in_channels=in_channels)\n        self.bn1 = norm_layer(in_channels=group_width, **norm_kwargs)\n        self.relu1 = nn.Activation(\'relu\')\n        if self.use_splat:\n            self.conv2 = SplitAttentionConv(channels=group_width, kernel_size=3, strides = 1 if self.avd else strides,\n                                              padding=dilation, dilation=dilation, groups=cardinality, use_bias=False,\n                                              in_channels=group_width, norm_layer=norm_layer, norm_kwargs=norm_kwargs,\n                                              radix=radix, drop_ratio=split_drop_ratio, **kwargs)\n        else:\n            self.conv2 = nn.Conv2D(channels=group_width, kernel_size=3, strides = 1 if self.avd else strides,\n                                   padding=dilation, dilation=dilation, groups=cardinality, use_bias=False,\n                                   in_channels=group_width, **kwargs)\n            self.bn2 = norm_layer(in_channels=group_width, **norm_kwargs)\n            self.relu2 = nn.Activation(\'relu\')\n        self.conv3 = nn.Conv2D(channels=channels*4, kernel_size=1, use_bias=False, in_channels=group_width)\n        if not last_gamma:\n            self.bn3 = norm_layer(in_channels=channels*4, **norm_kwargs)\n        else:\n            self.bn3 = norm_layer(in_channels=channels*4, gamma_initializer=\'zeros\',\n                                  **norm_kwargs)\n        if self.avd:\n            self.avd_layer = nn.AvgPool2D(3, strides, padding=1)\n        self.relu3 = nn.Activation(\'relu\')\n        self.downsample = downsample\n        self.dilation = dilation\n        self.strides = strides\n\n    def hybrid_forward(self, F, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.dropblock_prob > 0:\n            out = self.dropblock1(out)\n        out = self.relu1(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        if self.use_splat:\n            out = self.conv2(out)\n            if self.dropblock_prob > 0:\n                out = self.dropblock2(out)\n        else:\n            out = self.conv2(out)\n            out = self.bn2(out)\n            if self.dropblock_prob > 0:\n                out = self.dropblock2(out)\n            out = self.relu2(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if self.dropblock_prob > 0:\n            out = self.dropblock3(out)\n\n        out = out + residual\n        out = self.relu3(out)\n\n        return out\n\nclass ResNet(HybridBlock):\n    """""" ResNet Variants Definations\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used (default: :class:`mxnet.gluon.nn.BatchNorm`)\n        Can be :class:`mxnet.gluon.nn.BatchNorm` or :class:`mxnet.gluon.contrib.nn.SyncBatchNorm`.\n    last_gamma : bool, default False\n        Whether to initialize the gamma of the last BatchNorm layer in each bottleneck to zero.\n    deep_stem : bool, default False\n        Whether to replace the 7x7 conv1 with 3 3x3 convolution layers.\n    avg_down : bool, default False\n        Whether to use average pooling for projection skip connection between stages/downsample.\n    final_drop : float, default 0.0\n        Dropout ratio before the final classification layer.\n    use_global_stats : bool, default False\n        Whether forcing BatchNorm to use global statistics instead of minibatch statistics;\n        optionally set to True if finetuning using ImageNet classification pretrained models.\n    Reference:\n        - He, Kaiming, et al. ""Deep residual learning for image recognition.""\n        Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, cardinality=1, bottleneck_width=64,\n                 classes=1000, dilated=False, dilation=1, norm_layer=BatchNorm,\n                 norm_kwargs=None, last_gamma=False, deep_stem=False, stem_width=32,\n                 avg_down=False, final_drop=0.0, use_global_stats=False,\n                 name_prefix=\'\', dropblock_prob=0, input_size=224,\n                 use_splat=False, radix=2, avd=False, avd_first=False, split_drop_ratio=0):\n        self.cardinality = cardinality\n        self.bottleneck_width = bottleneck_width\n        self.inplanes = stem_width*2 if deep_stem else 64\n        self.radix = radix\n        self.split_drop_ratio = split_drop_ratio\n        self.avd_first = avd_first\n        super(ResNet, self).__init__(prefix=name_prefix)\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        if use_global_stats:\n            norm_kwargs[\'use_global_stats\'] = True\n        self.norm_kwargs = norm_kwargs\n        with self.name_scope():\n            if not deep_stem:\n                self.conv1 = nn.Conv2D(channels=64, kernel_size=7, strides=2,\n                                       padding=3, use_bias=False, in_channels=3)\n            else:\n                self.conv1 = nn.HybridSequential(prefix=\'conv1\')\n                self.conv1.add(nn.Conv2D(channels=stem_width, kernel_size=3, strides=2,\n                                         padding=1, use_bias=False, in_channels=3))\n                self.conv1.add(norm_layer(in_channels=stem_width, **norm_kwargs))\n                self.conv1.add(nn.Activation(\'relu\'))\n                self.conv1.add(nn.Conv2D(channels=stem_width, kernel_size=3, strides=1,\n                                         padding=1, use_bias=False, in_channels=stem_width))\n                self.conv1.add(norm_layer(in_channels=stem_width, **norm_kwargs))\n                self.conv1.add(nn.Activation(\'relu\'))\n                self.conv1.add(nn.Conv2D(channels=stem_width*2, kernel_size=3, strides=1,\n                                         padding=1, use_bias=False, in_channels=stem_width))\n            input_size = _update_input_size(input_size, 2)\n            self.bn1 = norm_layer(in_channels=64 if not deep_stem else stem_width*2,\n                                  **norm_kwargs)\n            self.relu = nn.Activation(\'relu\')\n            self.maxpool = nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n            input_size = _update_input_size(input_size, 2)\n            self.layer1 = self._make_layer(1, block, 64, layers[0], avg_down=avg_down,\n                                           norm_layer=norm_layer, last_gamma=last_gamma, use_splat=use_splat,\n                                           avd=avd)\n            self.layer2 = self._make_layer(2, block, 128, layers[1], strides=2, avg_down=avg_down,\n                                           norm_layer=norm_layer, last_gamma=last_gamma, use_splat=use_splat,\n                                           avd=avd)\n            input_size = _update_input_size(input_size, 2)\n            if dilated or dilation==4:\n                self.layer3 = self._make_layer(3, block, 256, layers[2], strides=1, dilation=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n                self.layer4 = self._make_layer(4, block, 512, layers[3], strides=1, dilation=4, pre_dilation=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n            elif dilation==3:\n                # special\n                self.layer3 = self._make_layer(3, block, 256, layers[2], strides=1, dilation=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n                self.layer4 = self._make_layer(4, block, 512, layers[3], strides=2, dilation=2, pre_dilation=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n            elif dilation==2:\n                self.layer3 = self._make_layer(3, block, 256, layers[2], strides=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n                self.layer4 = self._make_layer(4, block, 512, layers[3], strides=1, dilation=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n            else:\n                self.layer3 = self._make_layer(3, block, 256, layers[2], strides=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n                input_size = _update_input_size(input_size, 2)\n                self.layer4 = self._make_layer(4, block, 512, layers[3], strides=2,\n                                               avg_down=avg_down, norm_layer=norm_layer,\n                                               last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                               input_size=input_size, use_splat=use_splat, avd=avd)\n                input_size = _update_input_size(input_size, 2)\n            self.avgpool = nn.GlobalAvgPool2D()\n            self.flat = nn.Flatten()\n            self.drop = None\n            if final_drop > 0.0:\n                self.drop = nn.Dropout(final_drop)\n            self.fc = nn.Dense(in_units=512 * block.expansion, units=classes)\n\n    def _make_layer(self, stage_index, block, planes, blocks, strides=1, dilation=1,\n                    pre_dilation=1, avg_down=False, norm_layer=None,\n                    last_gamma=False,\n                    dropblock_prob=0, input_size=224, use_splat=False, avd=False):\n        downsample = None\n        if strides != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.HybridSequential(prefix=\'down%d_\'%stage_index)\n            with downsample.name_scope():\n                if avg_down:\n                    if pre_dilation == 1:\n                        downsample.add(nn.AvgPool2D(pool_size=strides, strides=strides,\n                                                    ceil_mode=True, count_include_pad=False))\n                    elif strides==1:\n                        downsample.add(nn.AvgPool2D(pool_size=1, strides=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                    else:\n                        downsample.add(nn.AvgPool2D(pool_size=pre_dilation*strides, strides=strides, padding=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                    downsample.add(nn.Conv2D(channels=planes * block.expansion, kernel_size=1,\n                                             strides=1, use_bias=False, in_channels=self.inplanes))\n                    downsample.add(norm_layer(in_channels=planes * block.expansion,\n                                              **self.norm_kwargs))\n                else:\n                    downsample.add(nn.Conv2D(channels=planes * block.expansion,\n                                             kernel_size=1, strides=strides, use_bias=False,\n                                             in_channels=self.inplanes))\n                    downsample.add(norm_layer(in_channels=planes * block.expansion,\n                                              **self.norm_kwargs))\n\n        layers = nn.HybridSequential(prefix=\'layers%d_\'%stage_index)\n        with layers.name_scope():\n            if dilation in (1, 2):\n                layers.add(block(planes, cardinality=self.cardinality,\n                                 bottleneck_width=self.bottleneck_width,\n                                 strides=strides, dilation=pre_dilation,\n                                 downsample=downsample, previous_dilation=dilation,\n                                 norm_layer=norm_layer, norm_kwargs=self.norm_kwargs,\n                                 last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                 input_size=input_size, use_splat=use_splat, avd=avd, avd_first=self.avd_first,\n                                 radix=self.radix, in_channels=self.inplanes,\n                                 split_drop_ratio=self.split_drop_ratio))\n            elif dilation == 4:\n                layers.add(block(planes, cardinality=self.cardinality,\n                                 bottleneck_width=self.bottleneck_width,\n                                 strides=strides, dilation=pre_dilation,\n                                 downsample=downsample, previous_dilation=dilation,\n                                 norm_layer=norm_layer, norm_kwargs=self.norm_kwargs,\n                                 last_gamma=last_gamma, dropblock_prob=dropblock_prob,\n                                 input_size=input_size, use_splat=use_splat, avd=avd, avd_first=self.avd_first,\n                                 radix=self.radix, in_channels=self.inplanes,\n                                 split_drop_ratio=self.split_drop_ratio))\n            else:\n                raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n            input_size = _update_input_size(input_size, strides)\n            self.inplanes = planes * block.expansion\n            for i in range(1, blocks):\n                layers.add(block(planes, cardinality=self.cardinality,\n                                 bottleneck_width=self.bottleneck_width, dilation=dilation,\n                                 previous_dilation=dilation, norm_layer=norm_layer,\n                                 norm_kwargs=self.norm_kwargs, last_gamma=last_gamma,\n                                 dropblock_prob=dropblock_prob, input_size=input_size,\n                                 use_splat=use_splat, avd=avd, avd_first=self.avd_first,\n                                 radix=self.radix, in_channels=self.inplanes,\n                                 split_drop_ratio=self.split_drop_ratio))\n\n        return layers\n\n    def hybrid_forward(self, F, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = self.flat(x)\n        if self.drop is not None:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x\n'"
resnest/gluon/splat.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.nn import Conv2D, Block, HybridBlock, Dense, BatchNorm, Activation\n\n__all__ = ['SplitAttentionConv']\n\nUSE_BN = True\n\nclass SplitAttentionConv(HybridBlock):\n    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, radix=2, *args, in_channels=None, r=2,\n                 norm_layer=BatchNorm, norm_kwargs=None, drop_ratio=0, **kwargs):\n        super().__init__()\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        inter_channels = max(in_channels*radix//2//r, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.conv = Conv2D(channels*radix, kernel_size, strides, padding, dilation,\n                           groups=groups*radix, *args, in_channels=in_channels, **kwargs)\n        if USE_BN:\n            self.bn = norm_layer(in_channels=channels*radix, **norm_kwargs)\n        self.relu = Activation('relu')\n        self.fc1 = Conv2D(inter_channels, 1, in_channels=channels, groups=self.cardinality)\n        if USE_BN:\n            self.bn1 = norm_layer(in_channels=inter_channels, **norm_kwargs)\n        self.relu1 = Activation('relu')\n        if drop_ratio > 0:\n            self.drop = nn.Dropout(drop_ratio)\n        else:\n            self.drop = None\n        self.fc2 = Conv2D(channels*radix, 1, in_channels=inter_channels, groups=self.cardinality)\n        self.channels = channels\n        self.rsoftmax = rSoftMax(radix, groups)\n\n    def hybrid_forward(self, F, x):\n        x = self.conv(x)\n        if USE_BN:\n            x = self.bn(x)\n        x = self.relu(x)\n        if self.radix > 1:\n            splited = F.split(x, self.radix, axis=1)\n            gap = sum(splited)\n        else:\n            gap = x\n        gap = F.contrib.AdaptiveAvgPooling2D(gap, 1)\n        gap = self.fc1(gap)\n        if USE_BN:\n            gap = self.bn1(gap)\n        atten = self.relu1(gap)\n        if self.drop:\n            atten = self.drop(atten)\n        atten = self.fc2(atten).reshape((0, self.radix, self.channels))\n        atten = self.rsoftmax(atten).reshape((0, -1, 1, 1))\n        if self.radix > 1:\n            atten = F.split(atten, self.radix, axis=1)\n            outs = [F.broadcast_mul(att, split) for (att, split) in zip(atten, splited)]\n            out = sum(outs)\n        else:\n            out = F.broadcast_mul(atten, x)\n        return out\n\n\nclass rSoftMax(nn.HybridBlock):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def hybrid_forward(self, F, x):\n        if self.radix > 1:\n            x = x.reshape((0, self.cardinality, self.radix, -1)).swapaxes(1, 2)\n            x = F.softmax(x, axis=1)\n            x = x.reshape((0, -1))\n        else:\n            x = F.sigmoid(x)\n        return x\n\n"""
resnest/torch/__init__.py,0,b'from .resnest import *\nfrom .ablation import *\n'
resnest/torch/ablation.py,7,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNeSt ablation study models""""""\n\nimport torch\nfrom .resnet import ResNet, Bottleneck\n\n__all__ = [\'resnest50_fast_1s1x64d\', \'resnest50_fast_2s1x64d\', \'resnest50_fast_4s1x64d\',\n           \'resnest50_fast_1s2x40d\', \'resnest50_fast_2s2x40d\', \'resnest50_fast_4s2x40d\',\n           \'resnest50_fast_1s4x24d\']\n\n_url_format = \'https://hangzh.s3.amazonaws.com/encoding/models/{}-{}.pth\'\n\n_model_sha256 = {name: checksum for checksum, name in [\n    (\'d8fbf808\', \'resnest50_fast_1s1x64d\'),\n    (\'44938639\', \'resnest50_fast_2s1x64d\'),\n    (\'f74f3fc3\', \'resnest50_fast_4s1x64d\'),\n    (\'32830b84\', \'resnest50_fast_1s2x40d\'),\n    (\'9d126481\', \'resnest50_fast_2s2x40d\'),\n    (\'41d14ed0\', \'resnest50_fast_4s2x40d\'),\n    (\'d4a4f76f\', \'resnest50_fast_1s4x24d\'),\n    ]}\n\ndef short_hash(name):\n    if name not in _model_sha256:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha256[name][:8]\n\nresnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n    name in _model_sha256.keys()\n}\n\ndef resnest50_fast_1s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_2s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_2s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_4s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_4s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_1s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_2s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_2s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_4s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_4s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_1s4x24d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=4, bottleneck_width=24,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s4x24d\'], progress=True, check_hash=True))\n    return model\n'"
resnest/torch/resnest.py,4,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNeSt models""""""\n\nimport torch\nfrom .resnet import ResNet, Bottleneck\n\n__all__ = [\'resnest50\', \'resnest101\', \'resnest200\', \'resnest269\']\n\n_url_format = \'https://hangzh.s3.amazonaws.com/encoding/models/{}-{}.pth\'\n\n_model_sha256 = {name: checksum for checksum, name in [\n    (\'528c19ca\', \'resnest50\'),\n    (\'22405ba7\', \'resnest101\'),\n    (\'75117900\', \'resnest200\'),\n    (\'0cc87c48\', \'resnest269\'),\n    ]}\n\ndef short_hash(name):\n    if name not in _model_sha256:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha256[name][:8]\n\nresnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n    name in _model_sha256.keys()\n}\n\ndef resnest50(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50\'], progress=True, check_hash=True))\n    return model\n\ndef resnest101(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 23, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest101\'], progress=True, check_hash=True))\n    return model\n\ndef resnest200(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 24, 36, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest200\'], progress=True, check_hash=True))\n    return model\n\ndef resnest269(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 30, 48, 8],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest269\'], progress=True, check_hash=True))\n    return model\n'"
resnest/torch/resnet.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNet variants""""""\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom .splat import SplAtConv2d\n\n__all__ = [\'ResNet\', \'Bottleneck\']\n\nclass DropBlock2D(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n\nclass Bottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 radix=1, cardinality=1, bottleneck_width=64,\n                 avd=False, avd_first=False, dilation=1, is_first=False,\n                 rectified_conv=False, rectify_avg=False,\n                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.dropblock_prob = dropblock_prob\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n            stride = 1\n\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv2d(\n                group_width, group_width, kernel_size=3,\n                stride=stride, padding=dilation,\n                dilation=dilation, groups=cardinality, bias=False,\n                radix=radix, rectify=rectified_conv,\n                rectify_avg=rectify_avg,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob)\n        elif rectified_conv:\n            from rfconv import RFConv2d\n            self.conv2 = RFConv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False,\n                average_mode=rectify_avg)\n            self.bn2 = norm_layer(group_width)\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(\n            group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n            zeros_(self.bn3.weight)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    """"""ResNet Variants\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n\n    Reference:\n\n        - He, Kaiming, et al. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n                 num_classes=1000, dilated=False, dilation=1,\n                 deep_stem=False, stem_width=64, avg_down=False,\n                 rectified_conv=False, rectify_avg=False,\n                 avd=False, avd_first=False,\n                 final_drop=0.0, dropblock_prob=0,\n                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n        self.cardinality = groups\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width*2 if deep_stem else 64\n        self.avg_down = avg_down\n        self.last_gamma = last_gamma\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        super(ResNet, self).__init__()\n        self.rectified_conv = rectified_conv\n        self.rectify_avg = rectify_avg\n        if rectified_conv:\n            from rfconv import RFConv2d\n            conv_layer = RFConv2d\n        else:\n            conv_layer = nn.Conv2d\n        conv_kwargs = {\'average_mode\': rectify_avg} if rectified_conv else {}\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n            )\n        else:\n            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False, **conv_kwargs)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        elif dilation==2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        self.avgpool = GlobalAvgPool2d()\n        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n                    dropblock_prob=0.0, is_first=True):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n                                                    ceil_mode=True, count_include_pad=False))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=1, bias=False))\n            else:\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=stride, bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.Sequential(*down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=dilation, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        #x = x.view(x.size(0), -1)\n        x = torch.flatten(x, 1)\n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x\n'"
resnest/torch/splat.py,6,"b'""""""Split-Attention""""""\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\nfrom torch.nn.modules.utils import _pair\n\n__all__ = [\'SplAtConv2d\']\n\nclass SplAtConv2d(Module):\n    """"""Split-Attention Conv2d\n    """"""\n    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, bias=True,\n                 radix=2, reduction_factor=4,\n                 rectify=False, rectify_avg=False, norm_layer=None,\n                 dropblock_prob=0.0, **kwargs):\n        super(SplAtConv2d, self).__init__()\n        padding = _pair(padding)\n        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n        self.rectify_avg = rectify_avg\n        inter_channels = max(in_channels*radix//reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n        self.dropblock_prob = dropblock_prob\n        if self.rectify:\n            from rfconv import RFConv2d\n            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n        else:\n            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                               groups=groups*radix, bias=bias, **kwargs)\n        self.use_bn = norm_layer is not None\n        if self.use_bn:\n            self.bn0 = norm_layer(channels*radix)\n        self.relu = ReLU(inplace=True)\n        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n        if self.use_bn:\n            self.bn1 = norm_layer(inter_channels)\n        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock2D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            splited = torch.split(x, rchannel//self.radix, dim=1)\n            gap = sum(splited) \n        else:\n            gap = x\n        gap = F.adaptive_avg_pool2d(gap, 1)\n        gap = self.fc1(gap)\n\n        if self.use_bn:\n            gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n\n        if self.radix > 1:\n            attens = torch.split(atten, rchannel//self.radix, dim=1)\n            out = sum([att*split for (att, split) in zip(attens, splited)])\n        else:\n            out = atten * x\n        return out.contiguous()\n\nclass rSoftMax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\n'"
scripts/dataset/prepare_imagenet.py,0,"b'""""""Prepare the ImageNet dataset""""""\nimport os\nimport argparse\nimport tarfile\nimport pickle\nimport gzip\nimport subprocess\nfrom tqdm import tqdm\nimport subprocess\nfrom resnest.utils import check_sha1, download, mkdir\n\n_TARGET_DIR = os.path.expanduser(\'~/.encoding/data/ILSVRC2012\')\n_TRAIN_TAR = \'ILSVRC2012_img_train.tar\'\n_TRAIN_TAR_SHA1 = \'43eda4fe35c1705d6606a6a7a633bc965d194284\'\n_VAL_TAR = \'ILSVRC2012_img_val.tar\'\n_VAL_TAR_SHA1 = \'5f3f73da3395154b60528b2b2a2caf2374f5f178\'\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Setup the ImageNet dataset.\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', required=True,\n                        help=""The directory that contains downloaded tar files"")\n    parser.add_argument(\'--target-dir\', default=_TARGET_DIR,\n                        help=""The directory to store extracted images"")\n    parser.add_argument(\'--checksum\', action=\'store_true\',\n                        help=""If check integrity before extracting."")\n    parser.add_argument(\'--with-rec\', action=\'store_true\',\n                        help=""If build image record files."")\n    parser.add_argument(\'--num-thread\', type=int, default=1,\n                        help=""Number of threads to use when building image record file."")\n    args = parser.parse_args()\n    return args\n\ndef check_file(filename, checksum, sha1):\n    if not os.path.exists(filename):\n        raise ValueError(\'File not found: \'+filename)\n    if checksum and not check_sha1(filename, sha1):\n        raise ValueError(\'Corrupted file: \'+filename)\n\ndef extract_train(tar_fname, target_dir, with_rec=False, num_thread=1):\n    mkdir(target_dir)\n    with tarfile.open(tar_fname) as tar:\n        print(""Extracting ""+tar_fname+""..."")\n        # extract each class one-by-one\n        pbar = tqdm(total=len(tar.getnames()))\n        for class_tar in tar:\n            pbar.set_description(\'Extract \'+class_tar.name)\n            tar.extract(class_tar, target_dir)\n            class_fname = os.path.join(target_dir, class_tar.name)\n            class_dir = os.path.splitext(class_fname)[0]\n            os.mkdir(class_dir)\n            with tarfile.open(class_fname) as f:\n                f.extractall(class_dir)\n            os.remove(class_fname)\n            pbar.update(1)\n        pbar.close()\n\ndef extract_val(tar_fname, target_dir, with_rec=False, num_thread=1):\n    mkdir(target_dir)\n    print(\'Extracting \' + tar_fname)\n    with tarfile.open(tar_fname) as tar:\n        tar.extractall(target_dir)\n    # build rec file before images are moved into subfolders\n    # move images to proper subfolders\n    subprocess.call([""wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash""],\n                    cwd=target_dir, shell=True)\n    \n\ndef main():\n    args = parse_args()\n\n    target_dir = os.path.expanduser(args.target_dir)\n    #if os.path.exists(target_dir):\n    #    raise ValueError(\'Target dir [\'+target_dir+\'] exists. Remove it first\')\n\n    download_dir = os.path.expanduser(args.download_dir)\n    train_tar_fname = os.path.join(download_dir, _TRAIN_TAR)\n    check_file(train_tar_fname, args.checksum, _TRAIN_TAR_SHA1)\n    val_tar_fname = os.path.join(download_dir, _VAL_TAR)\n    check_file(val_tar_fname, args.checksum, _VAL_TAR_SHA1)\n\n    build_rec = args.with_rec\n    if build_rec:\n        os.makedirs(os.path.join(target_dir, \'rec\'))\n    extract_train(train_tar_fname, os.path.join(target_dir, \'train\'), build_rec, args.num_thread)\n    extract_val(val_tar_fname, os.path.join(target_dir, \'val\'), build_rec, args.num_thread)\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/gluon/train.py,0,"b'# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\n\n# disable autotune\nos.environ[\'MXNET_CUDNN_AUTOTUNE_DEFAULT\'] = \'1\'\n#os.environ[\'MXNET_GPU_MEM_POOL_TYPE\'] = \'Round\'\nos.environ[\'MXNET_GPU_MEM_POOL_ROUND_LINEAR_CUTOFF\'] = \'26\'\nos.environ[\'MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_FWD\'] = \'999\'\nos.environ[\'MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_BWD\'] = \'25\'\nos.environ[\'MXNET_GPU_COPY_NTHREADS\'] = \'1\'\nos.environ[\'MXNET_OPTIMIZER_AGGREGATION_SIZE\'] = \'54\'\n\nimport argparse\nimport logging\nimport math\nimport time\nimport random\nfrom PIL import Image\n\nimport horovod.mxnet as hvd\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import autograd, gluon, lr_scheduler\nfrom mxnet.io import DataBatch, DataIter\nfrom mxnet.gluon.data.vision import transforms\n\nfrom resnest.gluon import get_model\nfrom resnest.utils import mkdir\nfrom resnest.transforms import ERandomCrop, ECenterCrop\nfrom torchvision.transforms import transforms as pth_transforms\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    logging.info(\'mpi4py is not installed. Use ""pip install --no-cache mpi4py"" to install\')\n    MPI = None\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'MXNet ImageNet Example\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--use-rec\', action=\'store_true\', default=False,\n                    help=\'use image record iter for data input (default: False)\')\nparser.add_argument(\'--data-nthreads\', type=int, default=8,\n                    help=\'number of threads for data decoding (default: 2)\')\nparser.add_argument(\'--rec-train\', type=str, default=\'\',\n                    help=\'the training data\')\nparser.add_argument(\'--rec-val\', type=str, default=\'\',\n                    help=\'the validation data\')\nparser.add_argument(\'--batch-size\', type=int, default=128,\n                    help=\'training batch size per device (default: 128)\')\nparser.add_argument(\'--dtype\', type=str, default=\'float32\',\n                    help=\'data type for training (default: float32)\')\nparser.add_argument(\'--num-epochs\', type=int, default=90,\n                    help=\'number of training epochs (default: 90)\')\nparser.add_argument(\'--lr\', type=float, default=0.05,\n                    help=\'learning rate for a single GPU (default: 0.05)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9,\n                    help=\'momentum value for optimizer (default: 0.9)\')\nparser.add_argument(\'--wd\', type=float, default=0.0001,\n                    help=\'weight decay rate (default: 0.0001)\')\nparser.add_argument(\'--warmup-lr\', type=float, default=0.0,\n                    help=\'starting warmup learning rate (default: 0.0)\')\nparser.add_argument(\'--warmup-epochs\', type=int, default=10,\n                    help=\'number of warmup epochs (default: 10)\')\nparser.add_argument(\'--last-gamma\', action=\'store_true\', default=False,\n                    help=\'whether to init gamma of the last BN layer in \\\n                    each bottleneck to 0 (default: False)\')\nparser.add_argument(\'--mixup\', action=\'store_true\',\n                    help=\'whether train the model with mix-up. default is false.\')\nparser.add_argument(\'--mixup-alpha\', type=float, default=0.2,\n                    help=\'beta distribution parameter for mixup sampling, default is 0.2.\')\nparser.add_argument(\'--mixup-off-epoch\', type=int, default=0,\n                    help=\'how many last epochs to train without mixup, default is 0.\')\nparser.add_argument(\'--label-smoothing\', action=\'store_true\',\n                    help=\'use label smoothing or not in training. default is false.\')\nparser.add_argument(\'--no-wd\', action=\'store_true\',\n                    help=\'whether to remove weight decay on bias, and beta/gamma for batchnorm layers.\')\n\nparser.add_argument(\'--model\', type=str, default=\'resnet50_v1\',\n                    help=\'type of model to use. see vision_model for options.\')\nparser.add_argument(\'--use-pretrained\', action=\'store_true\', default=False,\n                    help=\'load pretrained model weights (default: False)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training (default: False)\')\nparser.add_argument(\'--eval-frequency\', type=int, default=0,\n                    help=\'frequency of evaluating validation accuracy \\\n                    when training with gluon mode (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=40,\n                    help=\'number of batches to wait before logging (default: 40)\')\nparser.add_argument(\'--save-frequency\', type=int, default=20,\n                    help=\'frequency of model saving (default: 0)\')\nparser.add_argument(\'--save-dir\', type=str, default=\'params\',\n                    help=\'directory of saved models\')\n# data\nparser.add_argument(\'--input-size\', type=int, default=224,\n                    help=\'size of the input image size. default is 224\')\nparser.add_argument(\'--crop-ratio\', type=float, default=0.875,\n                    help=\'Crop ratio during validation. default is 0.875\')\n# resume\nparser.add_argument(\'--resume-epoch\', type=int, default=0,\n                    help=\'epoch to resume training from.\')\nparser.add_argument(\'--resume-params\', type=str, default=\'\',\n                    help=\'path of parameters to load from.\')\nparser.add_argument(\'--resume-states\', type=str, default=\'\',\n                    help=\'path of trainer state to load from.\')\n# new tricks\nparser.add_argument(\'--dropblock-prob\', type=float, default=0,\n                    help=\'DropBlock prob. default is 0.\')\nparser.add_argument(\'--auto_aug\', action=\'store_true\',\n                    help=\'use auto_aug. default is false.\')\nargs = parser.parse_args()\n\n# Horovod: initialize Horovod\nhvd.init()\nnum_workers = hvd.size()\nrank = hvd.rank()\nlocal_rank = hvd.local_rank()\n\nif rank==0:\n    logging.basicConfig(level=logging.INFO)\n    logging.info(args)\n\nnum_classes = 1000\nnum_training_samples = 1281167\nbatch_size = args.batch_size\nepoch_size = \\\n    int(math.ceil(int(num_training_samples // num_workers) / batch_size))\n\n\nlr_sched = lr_scheduler.CosineScheduler(\n    args.num_epochs * epoch_size,\n    base_lr=(args.lr * num_workers),\n    warmup_steps=(args.warmup_epochs * epoch_size),\n    warmup_begin_lr=args.warmup_lr\n)\n\n\nclass SplitSampler(mx.gluon.data.sampler.Sampler):\n    """""" Split the dataset into `num_parts` parts and sample from the part with\n    index `part_index`\n \n    Parameters\n    ----------\n    length: int\n      Number of examples in the dataset\n    num_parts: int\n      Partition the data into multiple parts\n    part_index: int\n      The index of the part to read from\n    """"""\n    def __init__(self, length, num_parts=1, part_index=0, random=True):\n        # Compute the length of each partition\n        self.part_len = length // num_parts\n        # Compute the start index for this partition\n        self.start = self.part_len * part_index\n        # Compute the end index for this partition\n        self.end = self.start + self.part_len\n        self.random = random\n \n    def __iter__(self):\n        # Extract examples between `start` and `end`, shuffle and return them.\n        indices = list(range(self.start, self.end))\n        if self.random:\n            random.shuffle(indices)\n        return iter(indices)\n \n    def __len__(self):\n        return self.part_len\n\ndef get_train_data(rec_train, batch_size, data_nthreads, input_size, crop_ratio, args):\n    def train_batch_fn(batch, ctx):\n        data = batch[0].as_in_context(ctx)\n        label = batch[1].as_in_context(ctx)\n        return data, label\n\n    jitter_param = 0.4\n    lighting_param = 0.1\n    resize = int(math.ceil(input_size / crop_ratio))\n\n    train_transforms = []\n    if args.auto_aug:\n        print(\'Using AutoAugment\')\n        from resnest.gluon.data_utils import AugmentationBlock, autoaug_imagenet_policies\n        train_transforms.append(AugmentationBlock(autoaug_imagenet_policies()))\n\n    if input_size >= 320:\n        train_transforms.extend([\n            ERandomCrop(input_size),\n            pth_transforms.Resize((input_size, input_size), interpolation=Image.BICUBIC),\n            pth_transforms.RandomHorizontalFlip(),\n            pth_transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n            transforms.RandomLighting(lighting_param),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    else:\n        train_transforms.extend([\n            transforms.RandomResizedCrop(input_size),\n            transforms.RandomFlipLeftRight(),\n            transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                         saturation=jitter_param),\n            transforms.RandomLighting(lighting_param),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        \n    transform_train = transforms.Compose(train_transforms)\n\n    train_set = mx.gluon.data.vision.ImageRecordDataset(rec_train).transform_first(transform_train)\n    train_sampler = SplitSampler(len(train_set), num_parts=num_workers, part_index=rank)\n\n    train_data = gluon.data.DataLoader(train_set, batch_size=batch_size,# shuffle=True,\n                                       last_batch=\'discard\', num_workers=data_nthreads,\n                                       sampler=train_sampler)\n    return train_data, train_batch_fn\n\n\ndef get_val_data(rec_val, batch_size, data_nthreads, input_size, crop_ratio):\n    def val_batch_fn(batch, ctx):\n        data = batch[0].as_in_context(ctx)\n        label = batch[1].as_in_context(ctx)\n        return data, label\n\n    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    crop_ratio = crop_ratio if crop_ratio > 0 else 0.875\n    resize = int(math.ceil(input_size/crop_ratio))\n\n\n    if input_size >= 320:\n        transform_test = transforms.Compose([\n            pth_transforms.ToPIL(),\n            ECenterCrop(input_size),\n            pth_transforms.Resize((input_size, input_size), interpolation=Image.BICUBIC),\n            pth_transforms.ToNDArray(),\n            transforms.ToTensor(),\n            normalize\n        ])\n    else:\n        transform_test = transforms.Compose([\n            transforms.Resize(resize, keep_ratio=True),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            normalize\n        ])\n\n    val_set = mx.gluon.data.vision.ImageRecordDataset(rec_val).transform_first(transform_test)\n\n    val_sampler = SplitSampler(len(val_set), num_parts=num_workers, part_index=rank)\n    val_data = gluon.data.DataLoader(val_set, batch_size=batch_size,\n                                     num_workers=data_nthreads,\n                                     sampler=val_sampler)\n\n    return val_data, val_batch_fn\n\n# Horovod: pin GPU to local rank\ncontext = mx.cpu(local_rank) if args.no_cuda else mx.gpu(local_rank)\n\ntrain_data, train_batch_fn = get_train_data(args.rec_train, batch_size, args.data_nthreads,\n                                            args.input_size, args.crop_ratio, args)\nval_data, val_batch_fn = get_val_data(args.rec_val, batch_size, args.data_nthreads, args.input_size,\n                                      args.crop_ratio)\n\n# Get model from GluonCV model zoo\n# https://gluon-cv.mxnet.io/model_zoo/index.html\nkwargs = {\'ctx\': context,\n          \'pretrained\': args.use_pretrained,\n          \'classes\': num_classes,\n          \'input_size\': args.input_size}\n\nif args.last_gamma:\n    kwargs[\'last_gamma\'] = True\n\nif args.dropblock_prob > 0:\n        kwargs[\'dropblock_prob\'] = args.dropblock_prob\n\nnet = get_model(args.model, **kwargs)\nnet.cast(args.dtype)\n\nfrom resnest.gluon.dropblock import DropBlockScheduler\n# does not impact normal model\ndrop_scheduler = DropBlockScheduler(net, 0, 0.1, args.num_epochs)\n\nif rank==0:\n    logging.info(net)\n\n# Create initializer\ninitializer = mx.init.Xavier(rnd_type=\'gaussian\', factor_type=""in"", magnitude=2)\n\ndef train_gluon():\n    if args.save_dir:\n        save_dir = args.save_dir\n        save_dir = os.path.expanduser(save_dir)\n        mkdir(save_dir)\n    else:\n        save_dir = \'./\'\n        save_frequency = 0\n\n    def evaluate(epoch):\n        acc_top1 = mx.metric.Accuracy()\n        acc_top5 = mx.metric.TopKAccuracy(5)\n        for _, batch in enumerate(val_data):\n            data, label = val_batch_fn(batch, context)\n            output = net(data.astype(args.dtype, copy=False))\n            acc_top1.update([label], [output])\n            acc_top5.update([label], [output])\n\n        top1_name, top1_acc = acc_top1.get()\n        top5_name, top5_acc = acc_top5.get()\n        if MPI is not None:\n            comm = MPI.COMM_WORLD\n            res1 = comm.gather(top1_acc, root=0)\n            res2 = comm.gather(top5_acc, root=0)\n        if rank==0:\n            if MPI is not None:\n                #logging.info(\'MPI gather res1: {}\'.format(res1))\n                top1_acc = sum(res1) / len(res1)\n                top5_acc = sum(res2) / len(res2)\n            logging.info(\'Epoch[%d] Rank[%d]\\tValidation-%s=%f\\tValidation-%s=%f\',\n                         epoch, rank, top1_name, top1_acc, top5_name, top5_acc)\n\n    # Hybridize and initialize model\n    net.hybridize()\n    if args.resume_params is not \'\':\n        net.load_parameters(args.resume_params, ctx = context)\n\n    else:\n        net.initialize(initializer, ctx=context)\n\n    if args.no_wd:\n        for k, v in net.collect_params(\'.*beta|.*gamma|.*bias\').items():\n            v.wd_mult = 0.0\n\n    # Horovod: fetch and broadcast parameters\n    params = net.collect_params()\n    if params is not None:\n        hvd.broadcast_parameters(params, root_rank=0)\n\n    # Create optimizer\n    optimizer = \'nag\'\n    optimizer_params = {\'wd\': args.wd,\n                        \'momentum\': args.momentum,\n                        \'lr_scheduler\': lr_sched}\n    if args.dtype == \'float16\':\n        optimizer_params[\'multi_precision\'] = True\n    opt = mx.optimizer.create(optimizer, **optimizer_params)\n\n    # Horovod: create DistributedTrainer, a subclass of gluon.Trainer\n    trainer = hvd.DistributedTrainer(params, opt)\n    if args.resume_states is not \'\':\n        trainer.load_states(args.resume_states)\n\n    # Create loss function and train metric\n    if args.label_smoothing or args.mixup:\n        sparse_label_loss = False\n    else:\n        sparse_label_loss = True\n\n    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n    if args.mixup:\n        train_metric = mx.metric.RMSE()\n    else:\n        train_metric = mx.metric.Accuracy()\n\n    def mixup_transform(label, classes, lam=1, eta=0.0):\n        if isinstance(label, mx.nd.NDArray):\n            label = [label]\n        res = []\n        for l in label:\n            y1 = l.one_hot(classes, on_value = 1 - eta + eta/classes, off_value = eta/classes)\n            y2 = l[::-1].one_hot(classes, on_value = 1 - eta + eta/classes, off_value = eta/classes)\n            res.append(lam*y1 + (1-lam)*y2)\n        return res\n\n    def smooth(label, classes, eta=0.1):\n        if isinstance(label, mx.NDArray):\n            label = [label]\n        smoothed = []\n        for l in label:\n            res = l.one_hot(classes, on_value = 1 - eta + eta/classes, off_value = eta/classes)\n            smoothed.append(res)\n        return smoothed\n\n    # Train model\n    for epoch in range(args.resume_epoch, args.num_epochs):\n        drop_scheduler(epoch)\n        tic = time.time()\n        train_metric.reset()\n\n        btic = time.time()\n        for nbatch, batch in enumerate(train_data, start=1):\n            data, label = train_batch_fn(batch, context)\n            data, label = [data], [label]\n            if args.mixup:\n                lam = np.random.beta(args.mixup_alpha, args.mixup_alpha)\n                if epoch >= args.num_epochs - args.mixup_off_epoch:\n                    lam = 1\n                data = [lam*X + (1-lam)*X[::-1] for X in data]\n\n                if args.label_smoothing:\n                    eta = 0.1\n                else:\n                    eta = 0.0\n                label = mixup_transform(label, num_classes, lam, eta)\n\n            elif args.label_smoothing:\n                hard_label = label\n                label = smooth(label, num_classes)\n\n            with autograd.record():\n                outputs = [net(X.astype(args.dtype, copy=False)) for X in data]\n                loss = [loss_fn(yhat, y.astype(args.dtype, copy=False)) for yhat, y in zip(outputs, label)]\n            for l in loss:\n                l.backward()\n            trainer.step(batch_size)\n\n            if args.mixup:\n                output_softmax = [mx.nd.SoftmaxActivation(out.astype(\'float32\', copy=False)) \\\n                                  for out in outputs]\n                train_metric.update(label, output_softmax)\n            else:\n                if args.label_smoothing:\n                    train_metric.update(hard_label, outputs)\n                else:\n                    train_metric.update(label, outputs)\n\n            if args.log_interval and nbatch % args.log_interval == 0:\n                if rank == 0:\n                    logging.info(\'Epoch[%d] Batch[%d] Loss[%.3f]\', epoch, nbatch,\n                                 loss[0].mean().asnumpy()[0])\n\n                    train_metric_name, train_metric_score = train_metric.get()\n                    logging.info(\'Epoch[%d] Rank[%d] Batch[%d]\\t%s=%f\\tlr=%f\',\n                                 epoch, rank, nbatch, train_metric_name, train_metric_score, trainer.learning_rate)\n                btic = time.time()\n\n        # Report metrics\n        elapsed = time.time() - tic\n        _, acc = train_metric.get()\n        if rank == 0:\n            logging.info(\'Epoch[%d] Rank[%d] Batch[%d]\\tTime cost=%.2f\\tTrain-metric=%f\',\n                         epoch, rank, nbatch, elapsed, acc)\n            epoch_speed = num_workers * batch_size * nbatch / elapsed\n            logging.info(\'Epoch[%d]\\tSpeed: %.2f samples/sec\', epoch, epoch_speed)\n\n        # Evaluate performance\n        if args.eval_frequency and (epoch + 1) % args.eval_frequency == 0:\n            evaluate(epoch)\n\n        # Save model\n        if args.save_frequency and (epoch + 1) % args.save_frequency == 0:\n            net.save_parameters(\'%s/imagenet-%s-%d.params\'%(save_dir, args.model, epoch))\n            trainer.save_states(\'%s/imagenet-%s-%d.states\'%(save_dir, args.model, epoch))\n\n    # Evaluate performance at the end of training\n    evaluate(epoch)\n\n    net.save_parameters(\'%s/imagenet-%s-%d.params\'%(save_dir, args.model, args.num_epochs-1))\n    trainer.save_states(\'%s/imagenet-%s-%d.states\'%(save_dir, args.model, args.num_epochs-1))\n\nif __name__ == \'__main__\':\n    train_gluon()\n'"
scripts/gluon/verify.py,0,"b""\n#os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'\n\nimport argparse, os, math, time, sys\n\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import *\n\nfrom resnest.gluon import get_model\n\nfrom PIL import Image\n\n# CLI\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a model for image classification.')\n    parser.add_argument('--data-dir', type=str, default='~/.encoding/data/ILSVRC2012/',\n                        help='Imagenet directory for validation.')\n    parser.add_argument('--rec-dir', type=str, default=None,\n                        help='recio directory for validation.')\n    parser.add_argument('--batch-size', type=int, default=32,\n                        help='training batch size per device (CPU/GPU).')\n    parser.add_argument('--num-gpus', type=int, default=8,\n                        help='number of gpus to use.')\n    parser.add_argument('-j', '--num-data-workers', dest='num_workers', default=32, type=int,\n                        help='number of preprocessing workers')\n    parser.add_argument('--model', type=str, default='model', required=False,\n                        help='type of model to use. see vision_model for options.')\n    parser.add_argument('--resume', type=str, default=None,\n                        help='put the path to resuming file if needed')\n    parser.add_argument('--crop-size', type=int, default=224,\n                        help='input shape of the image, default is 224.')\n    parser.add_argument('--crop-ratio', type=float, default=0.875,\n                        help='The ratio for crop and input size, for validation dataset only')\n    parser.add_argument('--dtype', type=str,\n                        help='training data type')\n    parser.add_argument('--dilation', type=int, default=1,\n                        help='network dilation. default 1 (no-dilation)')\n    opt = parser.parse_args()\n    return opt\n\ndef test(network, ctx, val_data, batch_fn):\n    acc_top1 = mx.metric.Accuracy()\n    acc_top5 = mx.metric.TopKAccuracy(5)\n    acc_top1.reset()\n    acc_top5.reset()\n    num_batch = len(val_data)\n    num = 0\n    start = time.time()\n \n    iterator = enumerate(val_data)\n    next_i, next_batch = next(iterator)\n    next_data, next_label = batch_fn(next_batch, ctx)\n    stop = False\n    while not stop:\n        i = next_i\n        data = next_data\n        label = next_label\n        outputs = [network(X.astype(opt.dtype, copy=False)) for X in data]\n        try:\n            next_i, next_batch = next(iterator)\n            next_data, next_label = batch_fn(next_batch, ctx)\n            if next_i == 5:\n                # warm-up\n                num = 0\n                mx.nd.waitall()\n                start = time.time()\n        except StopIteration:\n            stop = True\n        acc_top1.update(label, outputs)\n        acc_top5.update(label, outputs)\n        _, top1 = acc_top1.get()\n        _, top5 = acc_top5.get()\n        print('%d / %d : %.8f, %.8f'%(i, num_batch, 1-top1, 1-top5))\n        num += batch_size\n\n    end = time.time()\n    speed = num / (end - start)\n    print('Throughput is %f img/sec.'% speed)\n\n    _, top1 = acc_top1.get()\n    _, top5 = acc_top5.get()\n    return (1-top1, 1-top5)\n\n\nif __name__ == '__main__':\n    opt = parse_args()\n\n    batch_size = opt.batch_size\n    classes = 1000\n\n    num_gpus = opt.num_gpus\n    if num_gpus > 0:\n        batch_size *= num_gpus\n    ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n    num_workers = opt.num_workers\n\n    input_size = opt.crop_size\n    model_name = opt.model\n    pretrained = True if not opt.resume else False\n\n    kwargs = {'ctx': ctx, 'pretrained': pretrained, 'classes': classes}\n\n    if opt.dilation > 1:\n        kwargs['dilation'] = opt.dilation\n\n    net = get_model(model_name, **kwargs)\n    net.cast(opt.dtype)\n    if opt.resume:\n        net.load_parameters(opt.resume, ctx=ctx)\n    else:\n        net.hybridize()\n\n    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    crop_ratio = opt.crop_ratio if opt.crop_ratio > 0 else 0.875\n    resize = int(math.ceil(input_size/crop_ratio))\n\n    if input_size >= 320:\n        from resnest.transforms import ECenterCrop\n        from resnest.gluon.data_utils import ToPIL, ToNDArray\n        transform_test = transforms.Compose([\n            ToPIL(),\n            ECenterCrop(input_size),\n            ToNDArray(),\n            transforms.ToTensor(),\n            normalize\n        ])\n    else:\n        transform_test = transforms.Compose([\n            transforms.Resize(resize, keep_ratio=True),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            normalize\n        ])\n\n    if not opt.rec_dir:\n        from gluoncv.data import imagenet\n        val_data = gluon.data.DataLoader(\n            imagenet.classification.ImageNet(opt.data_dir, train=False).transform_first(transform_test),\n            batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    else:\n        imgrec = os.path.join(opt.rec_dir, 'val.rec')\n        imgidx = os.path.join(opt.rec_dir, 'val.idx')\n        val_data = gluon.data.DataLoader(\n            mx.gluon.data.vision.ImageRecordDataset(imgrec).transform_first(transform_test),\n            batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    def batch_fn(batch, ctx):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        return data, label\n\n    err_top1_val, err_top5_val = test(net, ctx, val_data, batch_fn)\n    print(err_top1_val, err_top5_val)\n"""
scripts/torch/verify.py,11,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom __future__ import print_function\nimport os\nimport argparse\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\n\nimport PIL\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport warnings\nwarnings.filterwarnings(""ignore"", ""(Possibly )?corrupt EXIF data"", UserWarning)\n\nclass Options():\n    def __init__(self):\n        # data settings\n        parser = argparse.ArgumentParser(description=\'Deep Encoding\')\n        parser.add_argument(\'--base-size\', type=int, default=None,\n                            help=\'base image size\')\n        parser.add_argument(\'--crop-size\', type=int, default=224,\n                            help=\'crop image size\')\n        # model params \n        parser.add_argument(\'--model\', type=str, default=\'densenet\',\n                            help=\'network model type (default: densenet)\')\n        # training hyper params\n        parser.add_argument(\'--batch-size\', type=int, default=128, metavar=\'N\',\n                            help=\'batch size for training (default: 128)\')\n        parser.add_argument(\'--workers\', type=int, default=32,\n                            metavar=\'N\', help=\'dataloader threads\')\n        # cuda, seed and logging\n        parser.add_argument(\'--no-cuda\', action=\'store_true\', \n                            default=False, help=\'disables CUDA training\')\n        parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                            help=\'random seed (default: 1)\')\n        # checking point\n        parser.add_argument(\'--resume\', type=str, default=None,\n                            help=\'put the path to resuming file if needed\')\n        parser.add_argument(\'--verify\', type=str, default=None,\n                            help=\'put the path to resuming file if needed\')\n        self.parser = parser\n\n    def parse(self):\n        args = self.parser.parse_args()\n        return args\n\n\ndef main():\n    # init the args\n    args = Options().parse()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    print(args)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n    # init dataloader\n    interp = PIL.Image.BILINEAR if args.crop_size < 320 else PIL.Image.BICUBIC\n    base_size = args.base_size if args.base_size is not None else int(1.0 * args.crop_size / 0.875)\n    transform_val = transforms.Compose([\n        ECenterCrop(args.crop_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ])\n    valset = ImageNetDataset(transform=transform_val, train=False)\n    val_loader = torch.utils.data.DataLoader(\n        valset, batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True if args.cuda else False)\n    \n    # init the model\n    model_kwargs = {}\n\n    assert args.model in torch.hub.list(\'zhanghang1989/ResNeSt\', force_reload=True)\n    model = torch.hub.load(\'zhanghang1989/ResNeSt\', args.model, pretrained=True)\n    print(model)\n\n    if args.cuda:\n        model.cuda()\n        # Please use CUDA_VISIBLE_DEVICES to control the number of gpus\n        model = nn.DataParallel(model)\n\n    # checkpoint\n    if args.verify:\n        if os.path.isfile(args.verify):\n            print(""=> loading checkpoint \'{}\'"".format(args.verify))\n            model.module.load_state_dict(torch.load(args.verify))\n        else:\n            raise RuntimeError (""=> no verify checkpoint found at \'{}\'"".\\\n                format(args.verify))\n    elif args.resume is not None:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            model.module.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            raise RuntimeError (""=> no resume checkpoint found at \'{}\'"".\\\n                format(args.resume))\n\n    model.eval()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    is_best = False\n    tbar = tqdm(val_loader, desc=\'\\r\')\n    for batch_idx, (data, target) in enumerate(tbar):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        with torch.no_grad():\n            output = model(data)\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            top1.update(acc1[0], data.size(0))\n            top5.update(acc5[0], data.size(0))\n\n        tbar.set_description(\'Top1: %.3f | Top5: %.3f\'%(top1.avg, top5.avg))\n\n    print(\'Top1 Acc: %.3f | Top5 Acc: %.3f \'%(top1.avg, top5.avg))\n\nclass ECenterCrop:\n    """"""Crop the given PIL Image and resize it to desired size.\n    Args:\n        img (PIL Image): Image to be cropped. (0,0) denotes the top left corner of the image.\n        output_size (sequence or int): (height, width) of the crop box. If int,\n            it is used for both directions\n    Returns:\n        PIL Image: Cropped image.\n    """"""\n    def __init__(self, imgsize):\n        self.imgsize = imgsize\n        self.resize_method = transforms.Resize((imgsize, imgsize), interpolation=PIL.Image.BICUBIC)\n\n    def __call__(self, img):\n        image_width, image_height = img.size\n        image_short = min(image_width, image_height)\n\n        crop_size = float(self.imgsize) / (self.imgsize + 32) * image_short\n\n        crop_height, crop_width = crop_size, crop_size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        img = img.crop((crop_left, crop_top, crop_left + crop_width, crop_top + crop_height))\n        return self.resize_method(img)\n\nclass ImageNetDataset(datasets.ImageFolder):\n    BASE_DIR = ""ILSVRC2012""\n    def __init__(self, root=os.path.expanduser(\'~/.encoding/data\'), transform=None,\n                 target_transform=None, train=True, **kwargs):\n        split=\'train\' if train == True else \'val\'\n        root = os.path.join(root, self.BASE_DIR, split)\n        super(ImageNetDataset, self).__init__(root, transform, target_transform)\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        #self.val = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        #self.val = val\n        self.sum += val * n\n        self.count += n\n\n    @property\n    def avg(self):\n        avg = 0 if self.count == 0 else self.sum / self.count\n        return avg\n\nif __name__ == ""__main__"":\n    main()\n\n'"
