file_path,api_count,code
setup.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#!/usr/bin/env python\n\nimport glob\nimport os\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [""torch"", ""torchvision""]\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""maskrcnn_benchmark"", ""csrc"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""maskrcnn_benchmark._C"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""maskrcnn_benchmark"",\n    version=""0.1"",\n    author=""fmassa"",\n    url=""https://github.com/facebookresearch/maskrnn-benchmark"",\n    description=""object detection in pytorch"",\n    packages=find_packages(exclude=(""configs"", ""tests"",)),\n    # install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
demo/predictor.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport cv2\nimport torch\nfrom torchvision import transforms as T\n\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\nfrom maskrcnn_benchmark.modeling.roi_heads.mask_head.inference import Masker\n\n# COCO categories for pretty print\nCOCO_CATEGORIES = [\n    ""__background"",\n    ""person"",\n    ""bicycle"",\n    ""car"",\n    ""motorcycle"",\n    ""airplane"",\n    ""bus"",\n    ""train"",\n    ""truck"",\n    ""boat"",\n    ""traffic light"",\n    ""fire hydrant"",\n    ""stop sign"",\n    ""parking meter"",\n    ""bench"",\n    ""bird"",\n    ""cat"",\n    ""dog"",\n    ""horse"",\n    ""sheep"",\n    ""cow"",\n    ""elephant"",\n    ""bear"",\n    ""zebra"",\n    ""giraffe"",\n    ""backpack"",\n    ""umbrella"",\n    ""handbag"",\n    ""tie"",\n    ""suitcase"",\n    ""frisbee"",\n    ""skis"",\n    ""snowboard"",\n    ""sports ball"",\n    ""kite"",\n    ""baseball bat"",\n    ""baseball glove"",\n    ""skateboard"",\n    ""surfboard"",\n    ""tennis racket"",\n    ""bottle"",\n    ""wine glass"",\n    ""cup"",\n    ""fork"",\n    ""knife"",\n    ""spoon"",\n    ""bowl"",\n    ""banana"",\n    ""apple"",\n    ""sandwich"",\n    ""orange"",\n    ""broccoli"",\n    ""carrot"",\n    ""hot dog"",\n    ""pizza"",\n    ""donut"",\n    ""cake"",\n    ""chair"",\n    ""couch"",\n    ""potted plant"",\n    ""bed"",\n    ""dining table"",\n    ""toilet"",\n    ""tv"",\n    ""laptop"",\n    ""mouse"",\n    ""remote"",\n    ""keyboard"",\n    ""cell phone"",\n    ""microwave"",\n    ""oven"",\n    ""toaster"",\n    ""sink"",\n    ""refrigerator"",\n    ""book"",\n    ""clock"",\n    ""vase"",\n    ""scissors"",\n    ""teddy bear"",\n    ""hair drier"",\n    ""toothbrush"",\n]\nVOC_CATEGORIES = [\n    ""__background"",\n    ""aeroplane"",\n    ""bicycle"",\n    ""bird"",\n    ""boat"",\n    ""bottle"",\n    ""bus"",\n    ""car"",\n    ""cat"",\n    ""chair"",\n    ""cow"",\n    ""diningtable"",\n    ""dog"",\n    ""horse"",\n    ""motorbike"",\n    ""person"",\n    ""pottedplant"",\n    ""sheep"",\n    ""sofa"",\n    ""train"",\n    ""tvmonitor"",\n]\n\n\nclass COCODemo(object):\n    def __init__(\n        self,\n        cfg,\n        confidence_threshold=0.7,\n        show_mask_heatmaps=False,\n        masks_per_dim=2,\n        min_image_size=224,\n    ):\n        self.cfg = cfg.clone()\n        self.model = build_detection_model(cfg)\n        self.model.eval()\n        self.device = torch.device(cfg.MODEL.DEVICE)\n        self.model.to(self.device)\n        self.min_image_size = min_image_size\n\n        checkpointer = DetectronCheckpointer(cfg, self.model)\n        _ = checkpointer.load(cfg.MODEL.WEIGHT)\n\n        self.checkpointer = checkpointer\n        self.transforms = self.build_transform()\n\n        mask_threshold = -1 if show_mask_heatmaps else 0.5\n        self.masker = Masker(threshold=mask_threshold, padding=1)\n\n        # used to make colors for each class\n        self.palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n\n        self.cpu_device = torch.device(""cpu"")\n        self.confidence_threshold = confidence_threshold\n        self.show_mask_heatmaps = show_mask_heatmaps\n        self.masks_per_dim = masks_per_dim\n\n        self.CATEGORIES = COCO_CATEGORIES if cfg.DATASETS.TEST[0][:4] == \'coco\' else VOC_CATEGORIES\n\n    def build_transform(self):\n        """"""\n        Creates a basic transformation that was used to train the models\n        """"""\n        cfg = self.cfg\n\n        # we are loading images with OpenCV, so we don\'t need to convert them\n        # to BGR, they are already! So all we need to do is to normalize\n        # by 255 if we want to convert to BGR255 format, or flip the channels\n        # if we want it to be in RGB in [0-1] range.\n        if cfg.INPUT.TO_BGR255:\n            to_bgr_transform = T.Lambda(lambda x: x * 255)\n        else:\n            to_bgr_transform = T.Lambda(lambda x: x[[2, 1, 0]])\n\n        normalize_transform = T.Normalize(\n            mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n        )\n\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.Resize(self.min_image_size),\n                T.ToTensor(),\n                to_bgr_transform,\n                normalize_transform,\n            ]\n        )\n        return transform\n\n    def run_on_opencv_image(self, image):\n        """"""\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        """"""\n        predictions = self.compute_prediction(image)\n        top_predictions = self.select_top_predictions(predictions)\n\n        result = image.copy()\n        if self.show_mask_heatmaps:\n            return self.create_mask_montage(result, top_predictions)\n        result = self.overlay_boxes(result, top_predictions)\n        if self.cfg.MODEL.MASK_ON:\n            result = self.overlay_mask(result, top_predictions)\n        result = self.overlay_class_names(result, top_predictions)\n\n        return result\n\n    def compute_prediction(self, original_image):\n        """"""\n        Arguments:\n            original_image (np.ndarray): an image as returned by OpenCV\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        """"""\n        # apply pre-processing to image\n        image = self.transforms(original_image)\n        # convert to an ImageList, padded so that it is divisible by\n        # cfg.DATALOADER.SIZE_DIVISIBILITY\n        image_list = to_image_list(image, self.cfg.DATALOADER.SIZE_DIVISIBILITY)\n        image_list = image_list.to(self.device)\n        # compute predictions\n        with torch.no_grad():\n            predictions = self.model(image_list)\n        predictions = [o.to(self.cpu_device) for o in predictions]\n\n        # always single image is passed at a time\n        prediction = predictions[0]\n\n        # reshape prediction (a BoxList) into the original image size\n        height, width = original_image.shape[:-1]\n        prediction = prediction.resize((width, height))\n\n        if prediction.has_field(""mask""):\n            # if we have masks, paste the masks in the right position\n            # in the image, as defined by the bounding boxes\n            masks = prediction.get_field(""mask"")\n            masks = self.masker(masks, prediction)\n            prediction.add_field(""mask"", masks)\n        return prediction\n\n    def select_top_predictions(self, predictions):\n        """"""\n        Select only predictions which have a `score` > self.confidence_threshold,\n        and returns the predictions in descending order of score\n\n        Arguments:\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `scores`.\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        """"""\n        scores = predictions.get_field(""scores"")\n        keep = torch.nonzero(scores > self.confidence_threshold).squeeze(1)\n        predictions = predictions[keep]\n        scores = predictions.get_field(""scores"")\n        _, idx = scores.sort(0, descending=True)\n        return predictions[idx]\n\n    def compute_colors_for_labels(self, labels):\n        """"""\n        Simple function that adds fixed colors depending on the class\n        """"""\n        colors = labels[:, None] * self.palette\n        colors = (colors % 255).numpy().astype(""uint8"")\n        return colors\n\n    def overlay_boxes(self, image, predictions):\n        """"""\n        Adds the predicted boxes on top of the image\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `labels`.\n        """"""\n        labels = predictions.get_field(""labels"")\n        boxes = predictions.bbox\n\n        colors = self.compute_colors_for_labels(labels).tolist()\n\n        for box, color in zip(boxes, colors):\n            box = box.to(torch.int64)\n            top_left, bottom_right = box[:2].tolist(), box[2:].tolist()\n            image = cv2.rectangle(\n                image, tuple(top_left), tuple(bottom_right), tuple(color), 1\n            )\n\n        return image\n\n    def overlay_mask(self, image, predictions):\n        """"""\n        Adds the instances contours for each predicted object.\n        Each label has a different color.\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `mask` and `labels`.\n        """"""\n        masks = predictions.get_field(""mask"").numpy()\n        labels = predictions.get_field(""labels"")\n\n        colors = self.compute_colors_for_labels(labels).tolist()\n\n        for mask, color in zip(masks, colors):\n            thresh = mask[0, :, :, None]\n            _, contours, hierarchy = cv2.findContours(\n                thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n            )\n            image = cv2.drawContours(image, contours, -1, color, 3)\n\n        composite = image\n\n        return composite\n\n    def create_mask_montage(self, image, predictions):\n        """"""\n        Create a montage showing the probability heatmaps for each one one of the\n        detected objects\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `mask`.\n        """"""\n        masks = predictions.get_field(""mask"")\n        masks_per_dim = self.masks_per_dim\n        masks = torch.nn.functional.interpolate(\n            masks.float(), scale_factor=1 / masks_per_dim\n        ).byte()\n        height, width = masks.shape[-2:]\n        max_masks = masks_per_dim ** 2\n        masks = masks[:max_masks]\n        # handle case where we have less detections than max_masks\n        if len(masks) < max_masks:\n            masks_padded = torch.zeros(max_masks, 1, height, width, dtype=torch.uint8)\n            masks_padded[: len(masks)] = masks\n            masks = masks_padded\n        masks = masks.reshape(masks_per_dim, masks_per_dim, height, width)\n        result = torch.zeros(\n            (masks_per_dim * height, masks_per_dim * width), dtype=torch.uint8\n        )\n        for y in range(masks_per_dim):\n            start_y = y * height\n            end_y = (y + 1) * height\n            for x in range(masks_per_dim):\n                start_x = x * width\n                end_x = (x + 1) * width\n                result[start_y:end_y, start_x:end_x] = masks[y, x]\n        return cv2.applyColorMap(result.numpy(), cv2.COLORMAP_JET)\n\n    def overlay_class_names(self, image, predictions):\n        """"""\n        Adds detected class names and scores in the positions defined by the\n        top-left corner of the predicted bounding box\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `scores` and `labels`.\n        """"""\n        scores = predictions.get_field(""scores"").tolist()\n        labels = predictions.get_field(""labels"").tolist()\n        labels = [self.CATEGORIES[i] for i in labels]\n        boxes = predictions.bbox\n\n        template = ""{}: {:.2f}""\n        for box, score, label in zip(boxes, scores, labels):\n            x, y = box[:2]\n            s = template.format(label, score)\n            cv2.putText(\n                image, s, (x, y), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 255, 255), 1\n            )\n\n        return image\n'"
demo/webcam.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport argparse\nimport cv2\n\nfrom maskrcnn_benchmark.config import cfg\nfrom predictor import COCODemo\n\nimport time\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Object Detection Webcam Demo"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n    )\n    parser.add_argument(\n        ""--confidence-threshold"",\n        type=float,\n        default=0.7,\n        help=""Minimum score for the prediction to be shown"",\n    )\n    parser.add_argument(\n        ""--min-image-size"",\n        type=int,\n        default=224,\n        help=""Smallest size of the image to feed to the model. ""\n            ""Model was trained with 800, which gives best results"",\n    )\n    parser.add_argument(\n        ""--show-mask-heatmaps"",\n        dest=""show_mask_heatmaps"",\n        help=""Show a heatmap probability for the top masks-per-dim masks"",\n        action=""store_true"",\n    )\n    parser.add_argument(\n        ""--masks-per-dim"",\n        type=int,\n        default=2,\n        help=""Number of heatmaps per dimension to show"",\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify model config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n\n    # load config from file and command-line arguments\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    # prepare object that handles inference plus adds predictions on top of image\n    coco_demo = COCODemo(\n        cfg,\n        confidence_threshold=args.confidence_threshold,\n        show_mask_heatmaps=args.show_mask_heatmaps,\n        masks_per_dim=args.masks_per_dim,\n        min_image_size=args.min_image_size,\n    )\n\n    cam = cv2.VideoCapture(0)\n    while True:\n        start_time = time.time()\n        ret_val, img = cam.read()\n        composite = coco_demo.run_on_opencv_image(img)\n        print(""Time: {:.2f} s / img"".format(time.time() - start_time))\n        cv2.imshow(""COCO detections"", composite)\n        if cv2.waitKey(1) == 27:\n            break  # esc to quit\n    cv2.destroyAllWindows()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
maskrcnn_benchmark/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n'"
nms_recall/__init__.py,0,b'\n'
nms_recall/inference_for_NR.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport numpy as np\nimport datetime\nimport logging\nimport time\nimport os\n\nimport torch\n\nfrom tqdm import tqdm\n\nfrom maskrcnn_benchmark.utils.comm import is_main_process\nfrom maskrcnn_benchmark.utils.comm import scatter_gather\nfrom maskrcnn_benchmark.utils.comm import synchronize\n\n\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\n\n\ndef compute_on_dataset(model, data_loader, device):\n    model.eval()\n    with_results_dict = {}\n    without_results_dict = {}\n    cpu_device = torch.device(""cpu"")\n    for i, batch in tqdm(enumerate(data_loader)):\n        images, targets, image_ids = batch\n        images = images.to(device)\n        with_overlaps = []\n        without_overlaps = []\n        with torch.no_grad():\n            with_outputs, without_outputs = model(images)\n            for output, target in zip(with_outputs, targets):\n                if len(target) == 0:\n                    continue\n                if len(output) == 0:\n                    overlap = torch.zeros(len(target), device=cpu_device, dtype=torch.float)\n                    with_overlaps.append(overlap)\n                else:\n                    overlap = boxlist_iou(output, target.to(device)).max(dim=0).values\n                    with_overlaps.append(overlap.to(cpu_device))\n            for output, target in zip(without_outputs, targets):\n                if len(target) == 0:\n                    continue\n                if len(output) == 0:\n                    overlap = torch.zeros(len(target), device=cpu_device, dtype=torch.float)\n                    without_overlaps.append(overlap)\n                else:\n                    overlap = boxlist_iou(output, target.to(device)).max(dim=0).values\n                    without_overlaps.append(overlap.to(cpu_device))\n        with_results_dict.update(\n            {img_id: result for img_id, result in zip(image_ids, with_overlaps)}\n        )\n        without_results_dict.update(\n            {img_id: result for img_id, result in zip(image_ids, without_overlaps)}\n        )\n    return with_results_dict, without_results_dict\n\n\ndef _accumulate_predictions_from_multiple_gpus(predictions_per_gpu):\n    all_predictions = scatter_gather(predictions_per_gpu)\n    if not is_main_process():\n        return\n    # merge the list of dicts\n    predictions = {}\n    for p in all_predictions:\n        predictions.update(p)\n    # convert a dict where the key is the index in a list\n    image_ids = list(sorted(predictions.keys()))\n    if len(image_ids) != image_ids[-1] + 1:\n        logger = logging.getLogger(""maskrcnn_benchmark.eval_IR"")\n        logger.warning(\n            ""Number of images that were gathered from multiple processes is not ""\n            ""a contiguous set. Some images might be missing from the evaluation""\n        )\n\n    # convert to a list\n    predictions = [predictions[i] for i in image_ids]\n    return predictions\n\n\ndef evaluate_iou_average_recall(with_overlaps, without_overlaps, output_folder):\n    results = []\n\n    with_overlaps = torch.cat(with_overlaps).numpy()\n    without_overlaps = torch.cat(without_overlaps).numpy()\n\n    iouThrs = np.arange(0.5, 0.95, 0.05)\n\n    matched = (with_overlaps > iouThrs[:, None]).astype(np.int64)\n    with_iou_recalls = matched.sum(1)\n\n    matched = (without_overlaps > iouThrs[:, None]).astype(np.int64)\n    without_iou_recalls = matched.sum(1)\n    nms_iou_recalls = with_iou_recalls / without_iou_recalls.clip(min=1)\n    torch.save([iouThrs, nms_iou_recalls], os.path.join(output_folder, \'iou_recalls.pth\'))\n    print(\'NMS Recall (NR) [0.5:0.9] = {:0.3f}\'.format(float(nms_iou_recalls.mean())))\n    for t, NR in zip(iouThrs[0::2], nms_iou_recalls[0::2]):\n        results.append(NR)\n        print(\'NMS Recall (NR) [IoU > {:0.2f}] = {:0.3f}\'.format(t, float(NR)))\n    return results\n\n\ndef inference(\n    model,\n    data_loader,\n    iou_types=(""bbox"",),\n    box_only=False,\n    device=""cuda"",\n    expected_results=(),\n    expected_results_sigma_tol=4,\n    without_nms=False,\n    output_folder=None,\n):\n\n    # convert to a torch.device for efficiency\n    device = torch.device(device)\n    num_devices = (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_initialized()\n        else 1\n    )\n    logger = logging.getLogger(""maskrcnn_benchmark.eval_IR"")\n    dataset = data_loader.dataset\n    logger.info(""Start evaluation on {} images"".format(len(dataset)))\n    start_time = time.time()\n    with_overlaps, without_overlaps = compute_on_dataset(model, data_loader, device)\n    # wait for all processes to complete before measuring the time\n    synchronize()\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=total_time))\n    logger.info(\n        ""Total inference time: {} ({} s / img per device, on {} devices)"".format(\n            total_time_str, total_time * num_devices / len(dataset), num_devices\n        )\n    )\n\n    with_overlaps = _accumulate_predictions_from_multiple_gpus(with_overlaps)\n    without_overlaps = _accumulate_predictions_from_multiple_gpus(without_overlaps)\n    if not is_main_process():\n        return\n\n    logger.info(""Evaluating IoU average Recall (IR)"")\n    results = evaluate_iou_average_recall(with_overlaps, without_overlaps, output_folder)\n    logger.info(results)\n'"
nms_recall/without_nms_postprocessor.py,11,"b'import torch\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_nms\nfrom maskrcnn_benchmark.structures.boxlist_ops import remove_small_boxes\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.modeling.utils import cat\n\n\nclass RetinaNetPostProcessor(torch.nn.Module):\n    """"""\n    Performs post-processing on the outputs of the RetinaNet boxes.\n    This is only used in the testing.\n    """"""\n    def __init__(\n        self,\n        pre_nms_thresh,\n        pre_nms_top_n,\n        nms_thresh,\n        fpn_post_nms_top_n,\n        min_size,\n        box_coder=None,\n    ):\n        """"""\n        Arguments:\n            pre_nms_thresh (float)\n            pre_nms_top_n (int)\n            nms_thresh (float)\n            fpn_post_nms_top_n (int)\n            min_size (int)\n            box_coder (BoxCoder)\n        """"""\n        super(RetinaNetPostProcessor, self).__init__()\n        self.pre_nms_thresh = pre_nms_thresh\n        self.pre_nms_top_n = pre_nms_top_n\n        self.nms_thresh = nms_thresh\n        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n        self.min_size = min_size\n\n        if box_coder is None:\n            box_coder = BoxCoder(weights=(10., 10., 5., 5.))\n        self.box_coder = box_coder\n\n    def forward_for_single_feature_map_without(self, anchors, box_cls, box_regression,\n                                               pre_nms_thresh):\n        """"""\n        Arguments:\n            anchors: list[BoxList]\n            box_cls: tensor of size N, A * C, H, W\n            box_regression: tensor of size N, A * 4, H, W\n        """"""\n        N, _ , H, W = box_cls.shape\n        A = int(box_regression.size(1) / 4)\n        C = int(box_cls.size(1) / A)\n\n        # put in the same format as anchors\n        box_cls = box_cls.view(N, -1, C, H, W).permute(0, 3, 4, 1, 2)\n        box_cls = box_cls.reshape(N, -1, C)\n        box_cls = box_cls.sigmoid()\n\n        box_regression = box_regression.view(N, -1, 4, H, W)\n        box_regression = box_regression.permute(0, 3, 4, 1, 2)\n        box_regression = box_regression.reshape(N, -1, 4)\n\n        results = [[] for _ in range(N)]\n        candidate_inds = box_cls > pre_nms_thresh\n\n        for batch_idx, (per_box_cls, per_box_regression, per_candidate_inds, per_anchors) in enumerate(zip(\n            box_cls,\n            box_regression,\n            candidate_inds,\n            anchors\n        )):\n            # Sort and select TopN\n            per_box_cls = per_box_cls[per_candidate_inds]\n            per_candidate_nonzeros = per_candidate_inds.nonzero()\n            per_box_loc = per_candidate_nonzeros[:, 0]\n            per_class = per_candidate_nonzeros[:, 1]\n            per_class += 1\n\n            detections = self.box_coder.decode(\n                per_box_regression[per_box_loc, :].view(-1, 4),\n                per_anchors.bbox[per_box_loc, :].view(-1, 4)\n            )\n\n            boxlist = BoxList(detections, per_anchors.size, mode=""xyxy"")\n            boxlist.add_field(""labels"", per_class)\n            boxlist.add_field(""scores"", per_box_cls)\n            boxlist = boxlist.clip_to_image(remove_empty=False)\n            boxlist = remove_small_boxes(boxlist, self.min_size)\n            results[batch_idx] = boxlist\n\n        return results\n\n    def forward_without(self, anchors, box_cls, box_regression):\n        """"""\n        Arguments:\n            anchors: list[list[BoxList]]\n            box_cls: list[tensor]\n            box_regression: list[tensor]\n\n        Returns:\n            boxlists (list[BoxList]): the post-processed anchors, after\n                applying box decoding and NMS\n        """"""\n        sampled_boxes = []\n        anchors = list(zip(*anchors))\n        for layer, (a, o, b) in enumerate(\n            zip(anchors, box_cls, box_regression)):\n            sampled_boxes.append(\n                self.forward_for_single_feature_map_without(\n                    a, o, b, self.pre_nms_thresh\n                )\n            )\n\n        boxlists = list(zip(*sampled_boxes))\n        boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]\n        return boxlists\n\n    def forward_for_single_feature_map_with(self, anchors, box_cls, box_regression,\n                                      pre_nms_thresh):\n        """"""\n        Arguments:\n            anchors: list[BoxList]\n            box_cls: tensor of size N, A * C, H, W\n            box_regression: tensor of size N, A * 4, H, W\n        """"""\n        device = box_cls.device\n        N, _ , H, W = box_cls.shape\n        A = int(box_regression.size(1) / 4)\n        C = int(box_cls.size(1) / A)\n\n        # put in the same format as anchors\n        box_cls = box_cls.view(N, -1, C, H, W).permute(0, 3, 4, 1, 2)\n        box_cls = box_cls.reshape(N, -1, C)\n        box_cls = box_cls.sigmoid()\n\n        box_regression = box_regression.view(N, -1, 4, H, W)\n        box_regression = box_regression.permute(0, 3, 4, 1, 2)\n        box_regression = box_regression.reshape(N, -1, 4)\n\n        num_anchors = A * H * W\n\n        results = [[] for _ in range(N)]\n        candidate_inds = box_cls > pre_nms_thresh\n        if candidate_inds.sum().item() == 0:\n            empty_boxlists = []\n            for a in anchors:\n                empty_boxlist = BoxList(torch.Tensor(0, 4).to(device), a.size)\n                empty_boxlist.add_field(\n                    ""labels"", torch.LongTensor([]).to(device))\n                empty_boxlist.add_field(\n                    ""scores"", torch.Tensor([]).to(device))\n                empty_boxlists.append(empty_boxlist)\n            return empty_boxlists\n\n        pre_nms_top_n = candidate_inds.view(N, -1).sum(1)\n        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n\n        for batch_idx, (per_box_cls, per_box_regression, per_pre_nms_top_n, \\\n        per_candidate_inds, per_anchors) in enumerate(zip(\n            box_cls,\n            box_regression,\n            pre_nms_top_n,\n            candidate_inds,\n            anchors)):\n\n            # Sort and select TopN\n            per_box_cls = per_box_cls[per_candidate_inds]\n            per_candidate_nonzeros = per_candidate_inds.nonzero()\n            per_box_loc = per_candidate_nonzeros[:, 0]\n            per_class = per_candidate_nonzeros[:, 1]\n            per_class += 1\n            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n                per_box_cls, top_k_indices = \\\n                        per_box_cls.topk(per_pre_nms_top_n, sorted=False)\n                per_box_loc = per_box_loc[top_k_indices]\n                per_class = per_class[top_k_indices]\n\n            detections = self.box_coder.decode(\n                per_box_regression[per_box_loc, :].view(-1, 4),\n                per_anchors.bbox[per_box_loc, :].view(-1, 4)\n            )\n\n            boxlist = BoxList(detections, per_anchors.size, mode=""xyxy"")\n            boxlist.add_field(""labels"", per_class)\n            boxlist.add_field(""scores"", per_box_cls)\n            boxlist = boxlist.clip_to_image(remove_empty=False)\n            boxlist = remove_small_boxes(boxlist, self.min_size)\n            results[batch_idx] = boxlist\n\n        return results\n\n    def forward_with(self, anchors, box_cls, box_regression):\n        """"""\n        Arguments:\n            anchors: list[list[BoxList]]\n            box_cls: list[tensor]\n            box_regression: list[tensor]\n\n        Returns:\n            boxlists (list[BoxList]): the post-processed anchors, after\n                applying box decoding and NMS\n        """"""\n        sampled_boxes = []\n        num_levels = len(box_cls)\n        anchors = list(zip(*anchors))\n        for layer, (a, o, b) in enumerate(\n            zip(anchors, box_cls, box_regression)):\n            sampled_boxes.append(\n                self.forward_for_single_feature_map_with(\n                    a, o, b,\n                    self.pre_nms_thresh\n                )\n            )\n\n        boxlists = list(zip(*sampled_boxes))\n        boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]\n\n        boxlists = self.select_over_all_levels_with(boxlists)\n\n        return boxlists\n\n    def select_over_all_levels_with(self, boxlists):\n        num_images = len(boxlists)\n        results = []\n        for i in range(num_images):\n            scores = boxlists[i].get_field(""scores"")\n            labels = boxlists[i].get_field(""labels"")\n            boxes = boxlists[i].bbox\n            boxlist = boxlists[i]\n            result = []\n            # skip the background\n            for j in range(1, 81):\n                inds = (labels == j).nonzero().view(-1)\n                if len(inds) == 0:\n                    continue\n\n                scores_j = scores[inds]\n                boxes_j = boxes[inds, :].view(-1, 4)\n                boxlist_for_class = BoxList(boxes_j, boxlist.size, mode=""xyxy"")\n                boxlist_for_class.add_field(""scores"", scores_j)\n                boxlist_for_class = boxlist_nms(\n                    boxlist_for_class, self.nms_thresh,\n                    score_field=""scores""\n                )\n                num_labels = len(boxlist_for_class)\n                boxlist_for_class.add_field(\n                    ""labels"", torch.full((num_labels,), j,\n                                         dtype=torch.int64,\n                                         device=scores.device)\n                )\n                result.append(boxlist_for_class)\n\n            if len(result) > 0:\n                result = cat_boxlist(result)\n                number_of_detections = len(result)\n\n                # Limit to max_per_image detections **over all classes**\n                if number_of_detections > self.fpn_post_nms_top_n > 0:\n                    cls_scores = result.get_field(""scores"")\n                    image_thresh, _ = torch.kthvalue(\n                        cls_scores.cpu(),\n                        number_of_detections - self.fpn_post_nms_top_n + 1\n                    )\n                    keep = cls_scores >= image_thresh.item()\n                    keep = torch.nonzero(keep).squeeze(1)\n                    result = result[keep]\n                results.append(result)\n            else:\n                empty_boxlist = BoxList(torch.zeros(1, 4).to(\'cuda\'), boxlist.size)\n                empty_boxlist.add_field(\n                    ""labels"", torch.LongTensor([1]).to(\'cuda\'))\n                empty_boxlist.add_field(\n                    ""scores"", torch.Tensor([0.01]).to(\'cuda\'))\n                results.append(empty_boxlist)\n        return results\n\n    def forward(self, *args, **kwargs):\n        return self.forward_with(*args, **kwargs), self.forward_without(*args, **kwargs)\n\n\ndef make_retinanet_postprocessor(\n    config, fpn_post_nms_top_n, rpn_box_coder):\n\n    pre_nms_thresh = 0.05\n    # pre_nms_top_n = config.RETINANET.PRE_NMS_TOP_N\n    # fpn_post_nms_top_n = fpn_post_nms_top_n\n\n    pre_nms_top_n = 100000000000000\n    fpn_post_nms_top_n = 1000000000000000\n\n    nms_thresh = config.MODEL.RPN.NMS_THRESH\n    min_size = config.MODEL.RPN.MIN_SIZE\n    box_selector = RetinaNetPostProcessor(\n        pre_nms_thresh=pre_nms_thresh,\n        pre_nms_top_n=pre_nms_top_n,\n        nms_thresh=nms_thresh,\n        fpn_post_nms_top_n=fpn_post_nms_top_n,\n        box_coder=rpn_box_coder,\n        min_size=min_size\n    )\n    return box_selector\n'"
tests/checkpoint.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom collections import OrderedDict\nimport os\nfrom tempfile import TemporaryDirectory\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\nfrom maskrcnn_benchmark.utils.checkpoint import Checkpointer\n\n\nclass TestCheckpointer(unittest.TestCase):\n    def create_model(self):\n        return nn.Sequential(nn.Linear(2, 3), nn.Linear(3, 1))\n\n    def create_complex_model(self):\n        m = nn.Module()\n        m.block1 = nn.Module()\n        m.block1.layer1 = nn.Linear(2, 3)\n        m.layer2 = nn.Linear(3, 2)\n        m.res = nn.Module()\n        m.res.layer2 = nn.Linear(3, 2)\n\n        state_dict = OrderedDict()\n        state_dict[""layer1.weight""] = torch.rand(3, 2)\n        state_dict[""layer1.bias""] = torch.rand(3)\n        state_dict[""layer2.weight""] = torch.rand(2, 3)\n        state_dict[""layer2.bias""] = torch.rand(2)\n        state_dict[""res.layer2.weight""] = torch.rand(2, 3)\n        state_dict[""res.layer2.bias""] = torch.rand(2)\n\n        return m, state_dict\n\n    def test_from_last_checkpoint_model(self):\n        # test that loading works even if they differ by a prefix\n        for trained_model, fresh_model in [\n            (self.create_model(), self.create_model()),\n            (nn.DataParallel(self.create_model()), self.create_model()),\n            (self.create_model(), nn.DataParallel(self.create_model())),\n            (\n                nn.DataParallel(self.create_model()),\n                nn.DataParallel(self.create_model()),\n            ),\n        ]:\n\n            with TemporaryDirectory() as f:\n                checkpointer = Checkpointer(\n                    trained_model, save_dir=f, save_to_disk=True\n                )\n                checkpointer.save(""checkpoint_file"")\n\n                # in the same folder\n                fresh_checkpointer = Checkpointer(fresh_model, save_dir=f)\n                self.assertTrue(fresh_checkpointer.has_checkpoint())\n                self.assertEqual(\n                    fresh_checkpointer.get_checkpoint_file(),\n                    os.path.join(f, ""checkpoint_file.pth""),\n                )\n                _ = fresh_checkpointer.load()\n\n            for trained_p, loaded_p in zip(\n                trained_model.parameters(), fresh_model.parameters()\n            ):\n                # different tensor references\n                self.assertFalse(id(trained_p) == id(loaded_p))\n                # same content\n                self.assertTrue(trained_p.equal(loaded_p))\n\n    def test_from_name_file_model(self):\n        # test that loading works even if they differ by a prefix\n        for trained_model, fresh_model in [\n            (self.create_model(), self.create_model()),\n            (nn.DataParallel(self.create_model()), self.create_model()),\n            (self.create_model(), nn.DataParallel(self.create_model())),\n            (\n                nn.DataParallel(self.create_model()),\n                nn.DataParallel(self.create_model()),\n            ),\n        ]:\n            with TemporaryDirectory() as f:\n                checkpointer = Checkpointer(\n                    trained_model, save_dir=f, save_to_disk=True\n                )\n                checkpointer.save(""checkpoint_file"")\n\n                # on different folders\n                with TemporaryDirectory() as g:\n                    fresh_checkpointer = Checkpointer(fresh_model, save_dir=g)\n                    self.assertFalse(fresh_checkpointer.has_checkpoint())\n                    self.assertEqual(fresh_checkpointer.get_checkpoint_file(), """")\n                    _ = fresh_checkpointer.load(os.path.join(f, ""checkpoint_file.pth""))\n\n            for trained_p, loaded_p in zip(\n                trained_model.parameters(), fresh_model.parameters()\n            ):\n                # different tensor references\n                self.assertFalse(id(trained_p) == id(loaded_p))\n                # same content\n                self.assertTrue(trained_p.equal(loaded_p))\n\n    def test_complex_model_loaded(self):\n        for add_data_parallel in [False, True]:\n            model, state_dict = self.create_complex_model()\n            if add_data_parallel:\n                model = nn.DataParallel(model)\n\n            load_state_dict(model, state_dict)\n            for loaded, stored in zip(model.state_dict().values(), state_dict.values()):\n                # different tensor references\n                self.assertFalse(id(loaded) == id(stored))\n                # same content\n                self.assertTrue(loaded.equal(stored))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_data_samplers.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\nimport random\nimport unittest\n\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data.sampler import SequentialSampler\nfrom torch.utils.data.sampler import RandomSampler\n\nfrom maskrcnn_benchmark.data.samplers import GroupedBatchSampler\nfrom maskrcnn_benchmark.data.samplers import IterationBasedBatchSampler\n\n\nclass SubsetSampler(Sampler):\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass TestGroupedBatchSampler(unittest.TestCase):\n    def test_respect_order_simple(self):\n        drop_uneven = False\n        dataset = [i for i in range(40)]\n        group_ids = [i // 10 for i in dataset]\n        sampler = SequentialSampler(dataset)\n        for batch_size in [1, 3, 5, 6]:\n            batch_sampler = GroupedBatchSampler(\n                sampler, group_ids, batch_size, drop_uneven\n            )\n            result = list(batch_sampler)\n            merged_result = list(itertools.chain.from_iterable(result))\n            self.assertEqual(merged_result, dataset)\n\n    def test_respect_order(self):\n        drop_uneven = False\n        dataset = [i for i in range(10)]\n        group_ids = [0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n        sampler = SequentialSampler(dataset)\n\n        expected = [\n            [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]],\n            [[0, 1, 3], [2, 4, 5], [6, 9], [7, 8]],\n            [[0, 1, 3, 6], [2, 4, 5, 7], [8], [9]],\n        ]\n\n        for idx, batch_size in enumerate([1, 3, 4]):\n            batch_sampler = GroupedBatchSampler(\n                sampler, group_ids, batch_size, drop_uneven\n            )\n            result = list(batch_sampler)\n            self.assertEqual(result, expected[idx])\n\n    def test_respect_order_drop_uneven(self):\n        batch_size = 3\n        drop_uneven = True\n        dataset = [i for i in range(10)]\n        group_ids = [0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n        sampler = SequentialSampler(dataset)\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n\n        result = list(batch_sampler)\n\n        expected = [[0, 1, 3], [2, 4, 5]]\n        self.assertEqual(result, expected)\n\n    def test_subset_sampler(self):\n        batch_size = 3\n        drop_uneven = False\n        dataset = [i for i in range(10)]\n        group_ids = [0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n        sampler = SubsetSampler([0, 3, 5, 6, 7, 8])\n\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n        result = list(batch_sampler)\n\n        expected = [[0, 3, 6], [5, 7, 8]]\n        self.assertEqual(result, expected)\n\n    def test_permute_subset_sampler(self):\n        batch_size = 3\n        drop_uneven = False\n        dataset = [i for i in range(10)]\n        group_ids = [0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n        sampler = SubsetSampler([5, 0, 6, 1, 3, 8])\n\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n        result = list(batch_sampler)\n\n        expected = [[5, 8], [0, 6, 1], [3]]\n        self.assertEqual(result, expected)\n\n    def test_permute_subset_sampler_drop_uneven(self):\n        batch_size = 3\n        drop_uneven = True\n        dataset = [i for i in range(10)]\n        group_ids = [0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n        sampler = SubsetSampler([5, 0, 6, 1, 3, 8])\n\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n        result = list(batch_sampler)\n\n        expected = [[0, 6, 1]]\n        self.assertEqual(result, expected)\n\n    def test_len(self):\n        batch_size = 3\n        drop_uneven = True\n        dataset = [i for i in range(10)]\n        group_ids = [random.randint(0, 1) for _ in dataset]\n        sampler = RandomSampler(dataset)\n\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n        result = list(batch_sampler)\n        self.assertEqual(len(result), len(batch_sampler))\n        self.assertEqual(len(result), len(batch_sampler))\n\n        batch_sampler = GroupedBatchSampler(sampler, group_ids, batch_size, drop_uneven)\n        batch_sampler_len = len(batch_sampler)\n        result = list(batch_sampler)\n        self.assertEqual(len(result), batch_sampler_len)\n        self.assertEqual(len(result), len(batch_sampler))\n\n\nclass TestIterationBasedBatchSampler(unittest.TestCase):\n    def test_number_of_iters_and_elements(self):\n        for batch_size in [2, 3, 4]:\n            for num_iterations in [4, 10, 20]:\n                for drop_last in [False, True]:\n                    dataset = [i for i in range(10)]\n                    sampler = SequentialSampler(dataset)\n                    batch_sampler = BatchSampler(\n                        sampler, batch_size, drop_last=drop_last\n                    )\n\n                    iter_sampler = IterationBasedBatchSampler(\n                        batch_sampler, num_iterations\n                    )\n                    assert len(iter_sampler) == num_iterations\n                    for i, batch in enumerate(iter_sampler):\n                        start = (i % len(batch_sampler)) * batch_size\n                        end = min(start + batch_size, len(dataset))\n                        expected = [x for x in range(start, end)]\n                        self.assertEqual(batch, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tools/eval_NR.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Set up custom environment before nearly anything else is imported\n# NOTE: this should be the first import (no not reorder)\n\nimport argparse\nimport os\n\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.data import make_data_loader\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.utils.collect_env import collect_env_info\nfrom maskrcnn_benchmark.utils.comm import synchronize, get_rank\nfrom maskrcnn_benchmark.utils.logger import setup_logger\nfrom maskrcnn_benchmark.utils.miscellaneous import mkdir\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom maskrcnn_benchmark.modeling.rpn.anchor_generator import make_anchor_generator_retinanet\nfrom maskrcnn_benchmark.modeling.rpn.retinanet import RetinaNetHead\nfrom nms_recall.inference_for_NR import inference\nfrom nms_recall.without_nms_postprocessor import make_retinanet_postprocessor\n\n\nclass RetinaNetModule(torch.nn.Module):\n\t""""""\n\tModule for RetinaNet computation. Takes feature maps from the backbone and RPN\n\tproposals and losses.\n\t""""""\n\n\tdef __init__(self, cfg):\n\t\tsuper(RetinaNetModule, self).__init__()\n\n\t\tself.cfg = cfg.clone()\n\n\t\tanchor_generator = make_anchor_generator_retinanet(cfg)\n\t\thead = RetinaNetHead(cfg)\n\t\tbox_coder = BoxCoder(weights=(10., 10., 5., 5.))\n\n\t\tif self.cfg.MODEL.SPARSE_MASK_ON:\n\t\t\traise NotImplementedError\n\t\telse:\n\t\t\tbox_selector_test = make_retinanet_postprocessor(\n\t\t\t\tcfg, 100, box_coder)\n\n\t\tself.anchor_generator = anchor_generator\n\t\tself.head = head\n\t\tself.box_selector_test = box_selector_test\n\n\tdef forward(self, images, features, target=None):\n\t\tbox_cls, box_regression = self.head(features)\n\t\tanchors = self.anchor_generator(images, features)\n\n\t\tif self.training:\n\t\t\traise NotImplementedError\n\t\telse:\n\t\t\treturn self._forward_test(anchors, box_cls, box_regression)\n\n\tdef _forward_test(self, anchors, box_cls, box_regression):\n\t\tboxes = self.box_selector_test(anchors, box_cls, box_regression)\n\t\treturn (anchors, boxes), {}\n\n\ndef main():\n\tparser = argparse.ArgumentParser(description=""PyTorch Object Detection Inference"")\n\tparser.add_argument(\n\t\t""--config-file"",\n\t\tdefault=""configs/free_anchor_R-50-FPN_8gpu_1x.yaml"",\n\t\tmetavar=""FILE"",\n\t\thelp=""path to config file"",\n\t)\n\tparser.add_argument(""--local_rank"", type=int, default=0)\n\tparser.add_argument(\n\t\t""opts"",\n\t\thelp=""Modify config options using the command-line"",\n\t\tdefault=None,\n\t\tnargs=argparse.REMAINDER,\n\t)\n\n\targs = parser.parse_args()\n\n\tnum_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n\tdistributed = num_gpus > 1\n\n\tif distributed:\n\t\ttorch.cuda.set_device(args.local_rank)\n\t\ttorch.distributed.init_process_group(\n\t\t\tbackend=""nccl"", init_method=""env://""\n\t\t)\n\n\tcfg.merge_from_file(args.config_file)\n\tcfg.merge_from_list(args.opts)\n\t# cfg.merge_from_list([\'TEST.IMS_PER_BATCH\', 1])\n\tcfg.freeze()\n\n\tsave_dir = """"\n\tlogger = setup_logger(""maskrcnn_benchmark"", save_dir, get_rank())\n\tlogger.info(""Using {} GPUs"".format(num_gpus))\n\tlogger.info(cfg)\n\n\tlogger.info(""Collecting env info (might take some time)"")\n\tlogger.info(""\\n"" + collect_env_info())\n\n\tmodel = build_detection_model(cfg)\n\n\tmodel.rpn = RetinaNetModule(cfg)\n\n\tmodel.to(cfg.MODEL.DEVICE)\n\n\tcheckpointer = DetectronCheckpointer(cfg, model)\n\t_ = checkpointer.load(cfg.MODEL.WEIGHT)\n\n\tiou_types = (""bbox"",)\n\toutput_folders = [None] * len(cfg.DATASETS.TEST)\n\tif cfg.OUTPUT_DIR:\n\t\tdataset_names = cfg.DATASETS.TEST\n\t\tfor idx, dataset_name in enumerate(dataset_names):\n\t\t\toutput_folder = os.path.join(cfg.OUTPUT_DIR, ""NR"", dataset_name)\n\t\t\tmkdir(output_folder)\n\t\t\toutput_folders[idx] = output_folder\n\tdata_loaders_val = make_data_loader(cfg, is_train=False, is_distributed=distributed)\n\tfor output_folder, data_loader_val in zip(output_folders, data_loaders_val):\n\t\tinference(\n\t\t\tmodel,\n\t\t\tdata_loader_val,\n\t\t\tiou_types=iou_types,\n\t\t\t# box_only=cfg.MODEL.RPN_ONLY,\n\t\t\tbox_only=False if cfg.RETINANET.RETINANET_ON else cfg.MODEL.RPN_ONLY,\n\t\t\tdevice=cfg.MODEL.DEVICE,\n\t\t\texpected_results=cfg.TEST.EXPECTED_RESULTS,\n\t\t\texpected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,\n\t\t\toutput_folder=output_folder,\n\t\t)\n\t\tsynchronize()\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
tools/multi_scale_test.py,2,"b'import argparse\nimport os\n\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.data import make_data_loader\nfrom maskrcnn_benchmark.engine.inference import inference\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.modeling.detector.multi_scale_wrapper import MultiScaleRetinaNet\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.utils.collect_env import collect_env_info\nfrom maskrcnn_benchmark.utils.comm import synchronize, get_rank\nfrom maskrcnn_benchmark.utils.logger import setup_logger\nfrom maskrcnn_benchmark.utils.miscellaneous import mkdir\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Object Detection Inference"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/free_anchor_X-101-FPN_j2x.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n    )\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    distributed = num_gpus > 1\n\n    if distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(\n            backend=""nccl"", init_method=""env://""\n        )\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.merge_from_list([\'TEST.IMS_PER_BATCH\', num_gpus])\n    cfg.freeze()\n\n    save_dir = """"\n    logger = setup_logger(""maskrcnn_benchmark"", save_dir, get_rank())\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(cfg)\n\n    logger.info(""Collecting env info (might take some time)"")\n    logger.info(""\\n"" + collect_env_info())\n\n    model = MultiScaleRetinaNet(build_detection_model(cfg), cfg.TEST.MULTI_SCLAES)\n    model.to(cfg.MODEL.DEVICE)\n\n    checkpointer = DetectronCheckpointer(cfg, model)\n    _ = checkpointer.load(cfg.MODEL.WEIGHT)\n\n    iou_types = (""bbox"",)\n    if cfg.MODEL.MASK_ON:\n        iou_types = iou_types + (""segm"",)\n    output_folders = [None] * len(cfg.DATASETS.TEST)\n    if cfg.OUTPUT_DIR:\n        dataset_names = cfg.DATASETS.TEST\n        for idx, dataset_name in enumerate(dataset_names):\n            output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"", dataset_name)\n            mkdir(output_folder)\n            output_folders[idx] = output_folder\n    data_loaders_val = make_data_loader(cfg, is_train=False, is_distributed=distributed)\n    for output_folder, data_loader_val in zip(output_folders, data_loaders_val):\n        inference(\n            model,\n            data_loader_val,\n            iou_types=iou_types,\n            #box_only=cfg.MODEL.RPN_ONLY,\n            box_only=False if cfg.RETINANET.RETINANET_ON else cfg.MODEL.RPN_ONLY,\n            device=cfg.MODEL.DEVICE,\n            expected_results=cfg.TEST.EXPECTED_RESULTS,\n            expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,\n            output_folder=output_folder,\n        )\n        synchronize()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/parse_log.py,0,"b'import re\nimport argparse\nimport numpy as np\n\ndef parse(log_path):\n    with open(log_path) as f:\n       text = f.read()\n\n    float_pattern = r\'\\d+\\.\\d+\'\n    mean_pattern = r\'AdjustSmoothL1\\(mean\\): ({}), ({}), ({}), ({})\'.format(\n        float_pattern, float_pattern, float_pattern, float_pattern)\n    var_pattern = r\'AdjustSmoothL1\\(var\\): ({}), ({}), ({}), ({})\'.format(\n        float_pattern, float_pattern, float_pattern, float_pattern)\n    pattern = mean_pattern + r\'.*\\n.*\' + var_pattern + r\'.*\\n.*\' + \\\n        r\'iter: (\\d+)  \' + \\\n        r\'loss: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'loss_retina_cls: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'loss_retina_reg: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'loss_mask: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'time: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'data: ({}) \\(({})\\)  \'.format(float_pattern, float_pattern) + \\\n        r\'lr: ({})  \'.format(float_pattern) + \\\n        r\'max mem: (\\d+)\'\n    reg_exp = re.compile(pattern)\n\n    headers = [\'smooth_l1_mean\', \'smooth_l1_var\', \'iter\', \'loss\',\n               \'loss_retina_cls\', \'loss_retina_reg\', \'loss_mask\',\n               \'time\', \'data\', \'lr\', \'max_mem\']\n\n    iterations = list()\n    means = list()\n    variations = list()\n    running_losses = list()\n    for args in reg_exp.findall(text):\n        mean = [float(v) for v in args[0:4]]\n        var = [float(v) for v in args[5:8]]\n        iteration = int(args[8])\n        point_loss = float(args[9])\n        running_loss = float(args[10])\n        point_loss_retina_cls = float(args[11])\n        running_loss_retina_cls = float(args[12])\n        point_loss_retina_reg = float(args[13])\n        running_loss_retina_reg = float(args[14])\n        point_loss_mask = float(args[15])\n        running_loss_mask = float(args[16])\n        point_time = float(args[17])\n        running_time = float(args[18])\n        point_data = float(args[19])\n        running_data = float(args[20])\n        lr = float(args[21])\n        max_mem = int(args[22])\n\n        iterations.append(iteration)\n        means.append(mean)\n        variations.append(var)\n        running_losses.append(running_loss)\n\n    iterations = np.asarray(iterations)\n    means = np.asarray(means)\n    variations = np.asarray(variations)\n    running_losses = np.asarray(running_losses)\n    print(iterations)\n    print(means)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Parse log file\')\n    parser.add_argument(\'log_path\', metavar=\'P\', help=\'path to the log file\')\n    args = parser.parse_args()\n\n    parse(args.log_path)\n\n'"
tools/test_net.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Set up custom environment before nearly anything else is imported\n# NOTE: this should be the first import (no not reorder)\nfrom maskrcnn_benchmark.utils.env import setup_environment  # noqa F401 isort:skip\n\nimport argparse\nimport os\n\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.data import make_data_loader\nfrom maskrcnn_benchmark.engine.inference import inference\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.utils.collect_env import collect_env_info\nfrom maskrcnn_benchmark.utils.comm import synchronize, get_rank\nfrom maskrcnn_benchmark.utils.logger import setup_logger\nfrom maskrcnn_benchmark.utils.miscellaneous import mkdir\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Object Detection Inference"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/free_anchor_R-50-FPN_8gpu_1x.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n    )\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    distributed = num_gpus > 1\n\n    if distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(\n            backend=""nccl"", init_method=""env://""\n        )\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    save_dir = """"\n    logger = setup_logger(""maskrcnn_benchmark"", save_dir, get_rank())\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(cfg)\n\n    logger.info(""Collecting env info (might take some time)"")\n    logger.info(""\\n"" + collect_env_info())\n\n    model = build_detection_model(cfg)\n    model.to(cfg.MODEL.DEVICE)\n\n    checkpointer = DetectronCheckpointer(cfg, model)\n    _ = checkpointer.load(cfg.MODEL.WEIGHT)\n\n    iou_types = (""bbox"",)\n    if cfg.MODEL.MASK_ON:\n        iou_types = iou_types + (""segm"",)\n    output_folders = [None] * len(cfg.DATASETS.TEST)\n    if cfg.OUTPUT_DIR:\n        dataset_names = cfg.DATASETS.TEST\n        for idx, dataset_name in enumerate(dataset_names):\n            output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"", dataset_name)\n            mkdir(output_folder)\n            output_folders[idx] = output_folder\n    data_loaders_val = make_data_loader(cfg, is_train=False, is_distributed=distributed)\n    for output_folder, data_loader_val in zip(output_folders, data_loaders_val):\n        inference(\n            model,\n            data_loader_val,\n            iou_types=iou_types,\n            #box_only=cfg.MODEL.RPN_ONLY,\n            box_only=False if cfg.RETINANET.RETINANET_ON else cfg.MODEL.RPN_ONLY,\n            device=cfg.MODEL.DEVICE,\n            expected_results=cfg.TEST.EXPECTED_RESULTS,\n            expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,\n            output_folder=output_folder,\n        )\n        synchronize()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/train_net.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nr""""""\nBasic training script for PyTorch\n""""""\n\n# Set up custom environment before nearly anything else is imported\n# NOTE: this should be the first import (no not reorder)\nfrom maskrcnn_benchmark.utils.env import setup_environment  # noqa F401 isort:skip\n\nimport argparse\nimport os\n\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.data import make_data_loader\nfrom maskrcnn_benchmark.solver import make_lr_scheduler\nfrom maskrcnn_benchmark.solver import make_optimizer\nfrom maskrcnn_benchmark.engine.inference import inference\nfrom maskrcnn_benchmark.engine.trainer import do_train\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.utils.collect_env import collect_env_info\nfrom maskrcnn_benchmark.utils.comm import synchronize, get_rank\nfrom maskrcnn_benchmark.utils.imports import import_file\nfrom maskrcnn_benchmark.utils.logger import setup_logger\nfrom maskrcnn_benchmark.utils.miscellaneous import mkdir\n\n\ndef train(cfg, local_rank, distributed):\n    model = build_detection_model(cfg)\n    device = torch.device(cfg.MODEL.DEVICE)\n    model.to(device)\n\n    optimizer = make_optimizer(cfg, model)\n    scheduler = make_lr_scheduler(cfg, optimizer)\n\n    if distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[local_rank], output_device=local_rank,\n            # this should be removed if we update BatchNorm stats\n            broadcast_buffers=False,\n        )\n\n    arguments = {}\n    arguments[""iteration""] = 0\n\n    output_dir = cfg.OUTPUT_DIR\n\n    save_to_disk = get_rank() == 0\n    checkpointer = DetectronCheckpointer(\n        cfg, model, optimizer, scheduler, output_dir, save_to_disk\n    )\n    extra_checkpoint_data = checkpointer.load(cfg.MODEL.WEIGHT)\n    arguments.update(extra_checkpoint_data)\n\n    data_loader = make_data_loader(\n        cfg,\n        is_train=True,\n        is_distributed=distributed,\n        start_iter=arguments[""iteration""],\n    )\n\n    checkpoint_period = cfg.SOLVER.CHECKPOINT_PERIOD\n\n    do_train(\n        model,\n        data_loader,\n        optimizer,\n        scheduler,\n        checkpointer,\n        device,\n        checkpoint_period,\n        arguments,\n    )\n\n    return model\n\n\ndef test(cfg, model, distributed):\n    if distributed:\n        model = model.module\n    torch.cuda.empty_cache()  # TODO check if it helps\n    iou_types = (""bbox"",)\n    if cfg.MODEL.MASK_ON:\n        iou_types = iou_types + (""segm"",)\n    output_folders = [None] * len(cfg.DATASETS.TEST)\n    if cfg.OUTPUT_DIR:\n        dataset_names = cfg.DATASETS.TEST\n        for idx, dataset_name in enumerate(dataset_names):\n            output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"", dataset_name)\n            mkdir(output_folder)\n            output_folders[idx] = output_folder\n    data_loaders_val = make_data_loader(cfg, is_train=False, is_distributed=distributed)\n    for output_folder, data_loader_val in zip(output_folders, data_loaders_val):\n        inference(\n            model,\n            data_loader_val,\n            iou_types=iou_types,\n            #box_only=cfg.MODEL.RPN_ONLY,\n            box_only=False if cfg.RETINANET.RETINANET_ON else cfg.MODEL.RPN_ONLY,\n            device=cfg.MODEL.DEVICE,\n            expected_results=cfg.TEST.EXPECTED_RESULTS,\n            expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,\n            output_folder=output_folder,\n        )\n        synchronize()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Object Detection Training"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/free_anchor_R-50-FPN_8gpu_1x.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\n        ""--skip-test"",\n        dest=""skip_test"",\n        help=""Do not test the final model"",\n        action=""store_true"",\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    args.distributed = num_gpus > 1\n\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank % torch.cuda.device_count())\n        torch.distributed.init_process_group(\n            backend=""nccl"", init_method=""env://""\n        )\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    output_dir = cfg.OUTPUT_DIR\n    if output_dir:\n        mkdir(output_dir)\n\n    logger = setup_logger(""maskrcnn_benchmark"", output_dir, get_rank())\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(args)\n\n    logger.info(""Collecting env info (might take some time)"")\n    logger.info(""\\n"" + collect_env_info())\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    model = train(cfg, args.local_rank, args.distributed)\n\n    if not args.skip_test:\n        test(cfg, model, args.distributed)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
maskrcnn_benchmark/config/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .defaults import _C as cfg\n'"
maskrcnn_benchmark/config/defaults.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport os\n\nfrom yacs.config import CfgNode as CN\n\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n# or _TEST for a test-specific parameter.\n# For example, the number of images during training will be\n# IMAGES_PER_BATCH_TRAIN, while the number of images for testing will be\n# IMAGES_PER_BATCH_TEST\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n_C.DEBUG = False\n_C.MODEL = CN()\n_C.MODEL.RPN_ONLY = False\n_C.MODEL.MASK_ON = False\n_C.MODEL.SPARSE_MASK_ON = False\n_C.MODEL.DEVICE = ""cuda""\n_C.MODEL.META_ARCHITECTURE = ""GeneralizedRCNN""\n_C.MODEL.USE_GN = False\n# If the WEIGHT starts with a catalog://, like :R-50, the code will look for\n# the path in paths_catalog. Else, it will use it as the specified absolute\n# path\n_C.MODEL.WEIGHT = """"\n\n\n# -----------------------------------------------------------------------------\n# INPUT\n# -----------------------------------------------------------------------------\n_C.INPUT = CN()\n# Size of the smallest side of the image during training\n_C.INPUT.MIN_SIZE_TRAIN = (800,) # 800\n# Maximum size of the side of the image during training\n_C.INPUT.MAX_SIZE_TRAIN = 1333\n# Size of the smallest side of the image during testing\n_C.INPUT.MIN_SIZE_TEST = 800\n# Maximum size of the side of the image during testing\n_C.INPUT.MAX_SIZE_TEST = 1333\n# Values to be used for image normalization\n_C.INPUT.PIXEL_MEAN = [102.9801, 115.9465, 122.7717]\n# Values to be used for image normalization\n_C.INPUT.PIXEL_STD = [1., 1., 1.]\n# Convert image to BGR format (for Caffe2 models), in range 0-255\n_C.INPUT.TO_BGR255 = True\n\n\n# -----------------------------------------------------------------------------\n# Dataset\n# -----------------------------------------------------------------------------\n_C.DATASETS = CN()\n# List of the dataset names for training, as present in paths_catalog.py\n_C.DATASETS.TRAIN = ()\n# List of the dataset names for testing, as present in paths_catalog.py\n_C.DATASETS.TEST = ()\n\n# -----------------------------------------------------------------------------\n# DataLoader\n# -----------------------------------------------------------------------------\n_C.DATALOADER = CN()\n# Number of data loading threads\n_C.DATALOADER.NUM_WORKERS = 4\n# If > 0, this enforces that each collated batch should have a size divisible\n# by SIZE_DIVISIBILITY\n_C.DATALOADER.SIZE_DIVISIBILITY = 0\n# If True, each batch should contain only images for which the aspect ratio\n# is compatible. This groups portrait images together, and landscape images\n# are not batched with portrait images.\n_C.DATALOADER.ASPECT_RATIO_GROUPING = True\n\n# ---------------------------------------------------------------------------- #\n# Backbone options\n# ---------------------------------------------------------------------------- #\n_C.MODEL.BACKBONE = CN()\n\n# The backbone conv body to use\n# The string must match a function that is imported in modeling.model_builder\n# (e.g., \'FPN.add_fpn_ResNet101_conv5_body\' to specify a ResNet-101-FPN\n# backbone)\n_C.MODEL.BACKBONE.CONV_BODY = ""R-50-C4""\n\n# Add StopGrad at a specified stage so the bottom layers are frozen\n_C.MODEL.BACKBONE.FREEZE_CONV_BODY_AT = 2\n_C.MODEL.BACKBONE.OUT_CHANNELS = 256 * 4\n\n\n# ---------------------------------------------------------------------------- #\n# RPN options\n# ---------------------------------------------------------------------------- #\n_C.MODEL.RPN = CN()\n_C.MODEL.RPN.USE_FPN = False\n# Base RPN anchor sizes given in absolute pixels w.r.t. the scaled network input\n_C.MODEL.RPN.ANCHOR_SIZES = (32, 64, 128, 256, 512)\n# Stride of the feature map that RPN is attached.\n# For FPN, number of strides should match number of scales\n_C.MODEL.RPN.ANCHOR_STRIDE = (16,)\n# RPN anchor aspect ratios\n_C.MODEL.RPN.ASPECT_RATIOS = (0.5, 1.0, 2.0)\n# Remove RPN anchors that go outside the image by RPN_STRADDLE_THRESH pixels\n# Set to -1 or a large value, e.g. 100000, to disable pruning anchors\n_C.MODEL.RPN.STRADDLE_THRESH = 0\n# Minimum overlap required between an anchor and ground-truth box for the\n# (anchor, gt box) pair to be a positive example (IoU >= FG_IOU_THRESHOLD\n# ==> positive RPN example)\n_C.MODEL.RPN.FG_IOU_THRESHOLD = 0.7\n# Maximum overlap allowed between an anchor and ground-truth box for the\n# (anchor, gt box) pair to be a negative examples (IoU < BG_IOU_THRESHOLD\n# ==> negative RPN example)\n_C.MODEL.RPN.BG_IOU_THRESHOLD = 0.3\n# Total number of RPN examples per image\n_C.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 256\n# Target fraction of foreground (positive) examples per RPN minibatch\n_C.MODEL.RPN.POSITIVE_FRACTION = 0.5\n# Number of top scoring RPN proposals to keep before applying NMS\n# When FPN is used, this is *per FPN level* (not total)\n_C.MODEL.RPN.PRE_NMS_TOP_N_TRAIN = 12000\n_C.MODEL.RPN.PRE_NMS_TOP_N_TEST = 6000\n# Number of top scoring RPN proposals to keep after applying NMS\n_C.MODEL.RPN.POST_NMS_TOP_N_TRAIN = 2000\n_C.MODEL.RPN.POST_NMS_TOP_N_TEST = 1000\n# NMS threshold used on RPN proposals\n_C.MODEL.RPN.NMS_THRESH = 0.7\n# Proposal height and width both need to be greater than RPN_MIN_SIZE\n# (a the scale used during training or inference)\n_C.MODEL.RPN.MIN_SIZE = 0\n# Number of top scoring RPN proposals to keep after combining proposals from\n# all FPN levels\n_C.MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN = 2000\n_C.MODEL.RPN.FPN_POST_NMS_TOP_N_TEST = 2000\n\n\n# ---------------------------------------------------------------------------- #\n# ROI HEADS options\n# ---------------------------------------------------------------------------- #\n_C.MODEL.ROI_HEADS = CN()\n_C.MODEL.ROI_HEADS.USE_FPN = False\n# Overlap threshold for an RoI to be considered foreground (if >= FG_IOU_THRESHOLD)\n_C.MODEL.ROI_HEADS.FG_IOU_THRESHOLD = 0.5\n# Overlap threshold for an RoI to be considered background\n# (class = 0 if overlap in [0, BG_IOU_THRESHOLD))\n_C.MODEL.ROI_HEADS.BG_IOU_THRESHOLD = 0.5\n# Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets\n# These are empirically chosen to approximately lead to unit variance targets\n_C.MODEL.ROI_HEADS.BBOX_REG_WEIGHTS = (10., 10., 5., 5.)\n# RoI minibatch size *per image* (number of regions of interest [ROIs])\n# Total number of RoIs per training minibatch =\n#   TRAIN.BATCH_SIZE_PER_IM * TRAIN.IMS_PER_BATCH * NUM_GPUS\n# E.g., a common configuration is: 512 * 2 * 8 = 8192\n_C.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n# Target fraction of RoI minibatch that is labeled foreground (i.e. class > 0)\n_C.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.25\n\n# Only used on test mode\n\n# Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to\n# balance obtaining high recall with not having too many low precision\n# detections that will slow down inference post processing steps (like NMS)\n_C.MODEL.ROI_HEADS.SCORE_THRESH = 0.05\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n_C.MODEL.ROI_HEADS.NMS = 0.5\n# Maximum number of detections to return per image (100 is based on the limit\n# established for the COCO dataset)\n_C.MODEL.ROI_HEADS.DETECTIONS_PER_IMG = 100\n\n\n_C.MODEL.ROI_BOX_HEAD = CN()\n_C.MODEL.ROI_BOX_HEAD.FEATURE_EXTRACTOR = ""ResNet50Conv5ROIFeatureExtractor""\n_C.MODEL.ROI_BOX_HEAD.PREDICTOR = ""FastRCNNPredictor""\n_C.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 14\n_C.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO = 0\n_C.MODEL.ROI_BOX_HEAD.POOLER_SCALES = (1.0 / 16,)\n_C.MODEL.ROI_BOX_HEAD.NUM_CLASSES = 81\n# Hidden layer dimension when using an MLP for the RoI box head\n_C.MODEL.ROI_BOX_HEAD.MLP_HEAD_DIM = 1024\n\n\n_C.MODEL.ROI_MASK_HEAD = CN()\n_C.MODEL.ROI_MASK_HEAD.FEATURE_EXTRACTOR = ""ResNet50Conv5ROIFeatureExtractor""\n_C.MODEL.ROI_MASK_HEAD.PREDICTOR = ""MaskRCNNC4Predictor""\n_C.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 14\n_C.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO = 0\n_C.MODEL.ROI_MASK_HEAD.POOLER_SCALES = (1.0 / 16,)\n_C.MODEL.ROI_MASK_HEAD.MLP_HEAD_DIM = 1024\n_C.MODEL.ROI_MASK_HEAD.CONV_LAYERS = (256, 256, 256, 256)\n_C.MODEL.ROI_MASK_HEAD.RESOLUTION = 14\n_C.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR = True\n_C.MODEL.ROI_MASK_HEAD.CANONICAL_LEVEL = 4\n# ---------------------------------------------------------------------------- #\n# ResNe[X]t options (ResNets = {ResNet, ResNeXt}\n# Note that parts of a resnet may be used for both the backbone and the head\n# These options apply to both\n# ---------------------------------------------------------------------------- #\n_C.MODEL.RESNETS = CN()\n\n# Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt\n_C.MODEL.RESNETS.NUM_GROUPS = 1\n\n# Baseline width of each group\n_C.MODEL.RESNETS.WIDTH_PER_GROUP = 64\n\n# Place the stride 2 conv on the 1x1 filter\n# Use True only for the original MSRA ResNet; use False for C2 and Torch models\n_C.MODEL.RESNETS.STRIDE_IN_1X1 = True\n\n# Residual transformation function\n_C.MODEL.RESNETS.TRANS_FUNC = ""BottleneckWithFixedBatchNorm""\n# ResNet\'s stem function (conv1 and pool1)\n_C.MODEL.RESNETS.STEM_FUNC = ""StemWithFixedBatchNorm""\n\n# Apply dilation in stage ""res5""\n_C.MODEL.RESNETS.RES5_DILATION = 1\n\n_C.MODEL.RESNETS.RES2_OUT_CHANNELS = 256\n_C.MODEL.RESNETS.STEM_OUT_CHANNELS = 64\n\n\n# ---------------------------------------------------------------------------- #\n# RetinaNet Options (Follow the Detectron version)\n# ---------------------------------------------------------------------------- #\n_C.RETINANET = CN()\n\n# RetinaNet is used (instead of Fast/er/Mask R-CNN/R-FCN/RPN) if True\n_C.RETINANET.RETINANET_ON = False\n\n# This is the number of foreground classes, background is not included.\n_C.RETINANET.NUM_CLASSES = 81\n\n# Anchor aspect ratios to use\n_C.RETINANET.ANCHOR_SIZES = (32, 64, 128, 256, 512)\n_C.RETINANET.ASPECT_RATIOS = (0.5, 1.0, 2.0)\n_C.RETINANET.ANCHOR_STRIDES = (8, 16, 32, 64, 128)\n_C.RETINANET.STRADDLE_THRESH = 0\n\n# Anchor scales per octave\n_C.RETINANET.OCTAVE = 2.0\n_C.RETINANET.SCALES_PER_OCTAVE = 3\n\n# Convolutions to use in the cls and bbox tower\n# NOTE: this doesn\'t include the last conv for logits\n_C.RETINANET.NUM_CONVS = 4\n\n# Weight for bbox_regression loss\n_C.RETINANET.BBOX_REG_WEIGHT = 1.0\n\n# Smooth L1 loss beta for bbox regression\n_C.RETINANET.BBOX_REG_BETA = 0.11\n\n# During inference, #locs to select based on cls score before NMS is performed\n# per FPN level\n_C.RETINANET.PRE_NMS_TOP_N = 1000\n\n# IoU overlap ratio for labeling an anchor as positive\n# Anchors with >= iou overlap are labeled positive\n_C.RETINANET.POSITIVE_OVERLAP = 0.5\n\n# IoU overlap ratio for labeling an anchor as negative\n# Anchors with < iou overlap are labeled negative\n_C.RETINANET.NEGATIVE_OVERLAP = 0.4\n\n# Focal loss parameter: alpha\n_C.RETINANET.LOSS_ALPHA = 0.25\n\n# Focal loss parameter: gamma\n_C.RETINANET.LOSS_GAMMA = 2.0\n\n# Prior prob for the positives at the beginning of training. This is used to set\n# the bias init for the logits layer\n_C.RETINANET.PRIOR_PROB = 0.01\n\n# Whether classification and bbox branch tower should be shared or not\n_C.RETINANET.SHARE_CLS_BBOX_TOWER = False\n\n# Use class specific bounding box regression instead of the default class\n# agnostic regression\n_C.RETINANET.CLASS_SPECIFIC_BBOX = False\n\n# Whether softmax should be used in classification branch training\n_C.RETINANET.SOFTMAX = False\n\n# Inference cls score threshold, anchors with score > INFERENCE_TH are\n# considered for inference\n_C.RETINANET.INFERENCE_TH = 0.05\n\n# ""p3p7"": Use feature p3p7 for object detection and p3-p5 for mask prediction.\n# ""p2p7"": Use feature p3p7 for object detection and p2-p5 for mask prediction.\n_C.RETINANET.BACKBONE = ""p3p7""\n\n_C.RETINANET.NUM_MASKS_TEST = 50\n\n_C.RETINANET.LOW_QUALITY_MATCHES = True\n_C.RETINANET.LOW_QUALITY_THRESHOLD = 0.0\n\n# ---------------------------------------------------------------------------- #\n# FreeAnchor Options (Follow the Detectron version)\n# ---------------------------------------------------------------------------- #\n_C.FREEANCHOR = CN()\n_C.FREEANCHOR.FREEANCHOR_ON = False\n_C.FREEANCHOR.PRE_ANCHOR_TOPK = 50\n_C.FREEANCHOR.BBOX_REG_WEIGHT = 0.75\n_C.FREEANCHOR.BBOX_REG_BETA = 0.11\n_C.FREEANCHOR.BBOX_THRESHOLD = 0.6\n_C.FREEANCHOR.FOCAL_LOSS_ALPHA = 0.5\n_C.FREEANCHOR.FOCAL_LOSS_GAMMA = 2.0\n\n# ---------------------------------------------------------------------------- #\n# SparseMask Options (Follow the Detectron version)\n# ---------------------------------------------------------------------------- #\n_C.MODEL.SPARSE_MASK_HEAD = CN()\n_C.MODEL.SPARSE_MASK_HEAD.PREDICTOR = """"\n_C.MODEL.SPARSE_MASK_HEAD.FEATURE_EXTRACTOR = ""SparseMaskFPNFeatureExtractor""\n_C.MODEL.SPARSE_MASK_HEAD.CONV_LAYERS = (256, 256, 256, 256)\n_C.MODEL.SPARSE_MASK_HEAD.RESOLUTION = 14\n\n\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CN()\n_C.SOLVER.MAX_ITER = 40000\n\n_C.SOLVER.BASE_LR = 0.001\n_C.SOLVER.BIAS_LR_FACTOR = 2\n\n_C.SOLVER.MOMENTUM = 0.9\n\n_C.SOLVER.WEIGHT_DECAY = 0.0005\n_C.SOLVER.WEIGHT_DECAY_BIAS = 0\n\n_C.SOLVER.GAMMA = 0.1\n_C.SOLVER.STEPS = (30000,)\n\n_C.SOLVER.WARMUP_FACTOR = 1.0 / 3\n_C.SOLVER.WARMUP_ITERS = 500\n_C.SOLVER.WARMUP_METHOD = ""linear""\n\n_C.SOLVER.CHECKPOINT_PERIOD = 2500\n\n# Number of images per batch\n# This is global, so if we have 8 GPUs and IMS_PER_BATCH = 16, each GPU will\n# see 2 images per batch\n_C.SOLVER.IMS_PER_BATCH = 16\n\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CN()\n_C.TEST.EXPECTED_RESULTS = []\n_C.TEST.EXPECTED_RESULTS_SIGMA_TOL = 4\n# Number of images per batch\n# This is global, so if we have 8 GPUs and IMS_PER_BATCH = 16, each GPU will\n# see 2 images per batch\n_C.TEST.IMS_PER_BATCH = 8\n_C.TEST.MULTI_SCLAES = ((480, 800), (640, 1067), (800, 1333), (960, 1600), (1120, 1867), (1280, 2133))\n_C.TEST.DETECTIONS_PER_IMG =100\n# Misc options\n# ---------------------------------------------------------------------------- #\n_C.OUTPUT_DIR = "".""\n\n_C.PATHS_CATALOG = os.path.join(os.path.dirname(__file__), ""paths_catalog.py"")\n'"
maskrcnn_benchmark/config/paths_catalog.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""Centralized catalog of paths.""""""\n\nimport os\n\n\nclass DatasetCatalog(object):\n    DATA_DIR = ""/gdata/""\n\n    DATASETS = {\n        ""coco_test-dev"": (\n            ""MSCOCO2017/images"",\n            ""MSCOCO2017/annotations/image_info_test-dev2017.json"",\n        ),\n        ""coco_2017_test"": (\n            ""MSCOCO2017/images"",\n            ""MSCOCO2017/annotations/image_info_test2017.json"",\n        ),\n        ""coco_2017_train"": (\n            ""MSCOCO2017/images"",\n            ""MSCOCO2017/annotations/instances_train2017.json"",\n        ),\n        ""coco_2017_val"": (\n            ""MSCOCO2017/images"",\n            ""MSCOCO2017/annotations/instances_val2017.json"",\n        ),\n        ""coco_2014_train"": (\n            ""coco/train2014"",\n            ""coco/annotations/instances_train2014.json"",\n        ),\n        ""coco_2014_val"": (""coco/val2014"", ""coco/annotations/instances_val2014.json""),\n        ""coco_2014_minival"": (\n            ""coco/val2014"",\n            ""coco/annotations/instances_minival2014.json"",\n        ),\n        ""coco_2014_valminusminival"": (\n            ""coco/val2014"",\n            ""coco/annotations/instances_valminusminival2014.json"",\n        ),\n        ""voc_2007_train"": (\n            ""VOC2007/JPEGImages"",\n            ""VOC2007/annotations/voc_2007_train.json"",\n        ),\n        ""voc_2007_val"": (\n            ""VOC2007/JPEGImages"",\n            ""VOC2007/annotations/voc_2007_val.json"",\n        ),\n        ""voc_2007_test"": (\n            ""VOC2007/JPEGImages"",\n            ""VOC2007/annotations/voc_2007_test.json"",\n        ),\n        ""voc_2012_train"": (\n            ""VOC2012/JPEGImages"",\n            ""VOC2012/annotations/voc_2012_train.json"",\n        ),\n        ""voc_2012_val"": (\n            ""VOC2012/JPEGImages"",\n            ""VOC2012/annotations/voc_2012_val.json"",\n        ),\n    }\n\n    @staticmethod\n    def get(name):\n        if ""coco"" in name or ""voc"" in name:\n            data_dir = DatasetCatalog.DATA_DIR\n            attrs = DatasetCatalog.DATASETS[name]\n            args = dict(\n                root=os.path.join(data_dir, attrs[0]),\n                ann_file=os.path.join(data_dir, attrs[1]),\n            )\n            return dict(\n                factory=""COCODataset"",\n                args=args,\n            )\n        raise RuntimeError(""Dataset not available: {}"".format(name))\n\n\nclass ModelCatalog(object):\n    S3_C2_DETECTRON_URL = ""https://dl.fbaipublicfiles.com/detectron""\n    C2_IMAGENET_MODELS = {\n        ""MSRA/R-50"": ""ImageNetPretrained/MSRA/R-50.pkl"",\n        ""MSRA/R-50-GN"": ""ImageNetPretrained/47261647/R-50-GN.pkl"",\n        ""MSRA/R-101"": ""ImageNetPretrained/MSRA/R-101.pkl"",\n        ""MSRA/R-101-GN"": ""ImageNetPretrained/47592356/R-101-GN.pkl"",\n        ""FAIR/20171220/X-101-32x8d"": ""ImageNetPretrained/20171220/X-101-32x8d.pkl"",\n        ""FBResNeXt/X-101-64x4d"": ""ImageNetPretrained/FBResNeXt/X-101-64x4d.pkl"",\n    }\n\n    C2_DETECTRON_SUFFIX = ""output/train/{}coco_2014_train%3A{}coco_2014_valminusminival/generalized_rcnn/model_final.pkl""\n    C2_DETECTRON_MODELS = {\n        ""35857197/e2e_faster_rcnn_R-50-C4_1x"": ""01_33_49.iAX0mXvW"",\n        ""35857345/e2e_faster_rcnn_R-50-FPN_1x"": ""01_36_30.cUF7QR7I"",\n        ""35857890/e2e_faster_rcnn_R-101-FPN_1x"": ""01_38_50.sNxI7sX7"",\n        ""36761737/e2e_faster_rcnn_X-101-32x8d-FPN_1x"": ""06_31_39.5MIHi1fZ"",\n        ""35858791/e2e_mask_rcnn_R-50-C4_1x"": ""01_45_57.ZgkA7hPB"",\n        ""35858933/e2e_mask_rcnn_R-50-FPN_1x"": ""01_48_14.DzEQe4wC"",\n        ""35861795/e2e_mask_rcnn_R-101-FPN_1x"": ""02_31_37.KqyEK4tT"",\n        ""36761843/e2e_mask_rcnn_X-101-32x8d-FPN_1x"": ""06_35_59.RZotkLKI"",\n        ""37129812/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x"": ""09_35_36.8pzTQKYK"",\n        # keypoints\n        ""37697547/e2e_keypoint_rcnn_R-50-FPN_1x"": ""08_42_54.kdzV35ao""\n    }\n\n    @staticmethod\n    def get(name):\n        if name.startswith(""Caffe2Detectron/COCO""):\n            return ModelCatalog.get_c2_detectron_12_2017_baselines(name)\n        if name.startswith(""ImageNetPretrained""):\n            return ModelCatalog.get_c2_imagenet_pretrained(name)\n        raise RuntimeError(""model not present in the catalog {}"".format(name))\n\n    @staticmethod\n    def get_c2_imagenet_pretrained(name):\n        prefix = ModelCatalog.S3_C2_DETECTRON_URL\n        name = name[len(""ImageNetPretrained/""):]\n        name = ModelCatalog.C2_IMAGENET_MODELS[name]\n        url = ""/"".join([prefix, name])\n        return url\n\n    @staticmethod\n    def get_c2_detectron_12_2017_baselines(name):\n        # Detectron C2 models are stored following the structure\n        # prefix/<model_id>/2012_2017_baselines/<model_name>.yaml.<signature>/suffix\n        # we use as identifiers in the catalog Caffe2Detectron/COCO/<model_id>/<model_name>\n        prefix = ModelCatalog.S3_C2_DETECTRON_URL\n        dataset_tag = ""keypoints_"" if ""keypoint"" in name else """"\n        suffix = ModelCatalog.C2_DETECTRON_SUFFIX.format(dataset_tag, dataset_tag)\n        # remove identification prefix\n        name = name[len(""Caffe2Detectron/COCO/""):]\n        # split in <model_id> and <model_name>\n        model_id, model_name = name.split(""/"")\n        # parsing to make it match the url address from the Caffe2 models\n        model_name = ""{}.yaml"".format(model_name)\n        signature = ModelCatalog.C2_DETECTRON_MODELS[name]\n        unique_name = ""."".join([model_name, signature])\n        url = ""/"".join([prefix, model_id, ""12_2017_baselines"", unique_name, suffix])\n        return url\n'"
maskrcnn_benchmark/data/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .build import make_data_loader\n'"
maskrcnn_benchmark/data/build.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport bisect\nimport copy\nimport logging\n\nimport torch.utils.data\nfrom maskrcnn_benchmark.utils.comm import get_world_size\nfrom maskrcnn_benchmark.utils.imports import import_file\n\nfrom . import datasets as D\nfrom . import samplers\n\nfrom .collate_batch import BatchCollator\nfrom .transforms import build_transforms\n\n\ndef build_dataset(dataset_list, transforms, dataset_catalog, is_train=True):\n    """"""\n    Arguments:\n        dataset_list (list[str]): Contains the names of the datasets, i.e.,\n            coco_2014_trian, coco_2014_val, etc\n        transforms (callable): transforms to apply to each (image, target) sample\n        dataset_catalog (DatasetCatalog): contains the information on how to\n            construct a dataset.\n        is_train (bool): whether to setup the dataset for training or testing\n    """"""\n    if not isinstance(dataset_list, (list, tuple)):\n        raise RuntimeError(\n                ""dataset_list should be a list of strings, got {}"".format(dataset_list))\n    datasets = []\n    for dataset_name in dataset_list:\n        data = dataset_catalog.get(dataset_name)\n        factory = getattr(D, data[""factory""])\n        args = data[""args""]\n        # for COCODataset, we want to remove images without annotations\n        # during training\n        if data[""factory""] == ""COCODataset"":\n            args[""remove_images_without_annotations""] = is_train\n        args[""transforms""] = transforms\n        # make dataset from factory\n        dataset = factory(**args)\n        datasets.append(dataset)\n\n    # for testing, return a list of datasets\n    if not is_train:\n        return datasets\n\n    # for training, concatenate all datasets into a single one\n    dataset = datasets[0]\n    if len(datasets) > 1:\n        dataset = D.ConcatDataset(datasets)\n\n    return [dataset]\n\n\ndef make_data_sampler(dataset, shuffle, distributed):\n    if distributed:\n        return samplers.DistributedSampler(dataset, shuffle=shuffle)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef _quantize(x, bins):\n    bins = copy.copy(bins)\n    bins = sorted(bins)\n    quantized = list(map(lambda y: bisect.bisect_right(bins, y), x))\n    return quantized\n\n\ndef _compute_aspect_ratios(dataset):\n    aspect_ratios = []\n    for i in range(len(dataset)):\n        img_info = dataset.get_img_info(i)\n        aspect_ratio = float(img_info[""height""]) / float(img_info[""width""])\n        aspect_ratios.append(aspect_ratio)\n    return aspect_ratios\n\n\ndef make_batch_data_sampler(\n    dataset, sampler, aspect_grouping, images_per_batch, num_iters=None, start_iter=0\n):\n    if aspect_grouping:\n        if not isinstance(aspect_grouping, (list, tuple)):\n            aspect_grouping = [aspect_grouping]\n        aspect_ratios = _compute_aspect_ratios(dataset)\n        group_ids = _quantize(aspect_ratios, aspect_grouping)\n        batch_sampler = samplers.GroupedBatchSampler(\n            sampler, group_ids, images_per_batch, drop_uneven=False\n        )\n    else:\n        batch_sampler = torch.utils.data.sampler.BatchSampler(\n            sampler, images_per_batch, drop_last=False\n        )\n    if num_iters is not None:\n        batch_sampler = samplers.IterationBasedBatchSampler(batch_sampler, num_iters, start_iter)\n    return batch_sampler\n\n\ndef make_data_loader(cfg, is_train=True, is_distributed=False, start_iter=0):\n    num_gpus = get_world_size()\n    if is_train:\n        images_per_batch = cfg.SOLVER.IMS_PER_BATCH\n        assert (\n            images_per_batch % num_gpus == 0\n        ), ""SOLVER.IMS_PER_BATCH ({}) must be divisible by the number ""\n        ""of GPUs ({}) used."".format(images_per_batch, num_gpus)\n        images_per_gpu = images_per_batch // num_gpus\n        shuffle = True\n        num_iters = cfg.SOLVER.MAX_ITER\n    else:\n        images_per_batch = cfg.TEST.IMS_PER_BATCH\n        assert (\n            images_per_batch % num_gpus == 0\n        ), ""TEST.IMS_PER_BATCH ({}) must be divisible by the number ""\n        ""of GPUs ({}) used."".format(images_per_batch, num_gpus)\n        images_per_gpu = images_per_batch // num_gpus\n        shuffle = False if not is_distributed else True\n        num_iters = None\n        start_iter = 0\n\n    if images_per_gpu > 1:\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            ""When using more than one image per GPU you may encounter ""\n            ""an out-of-memory (OOM) error if your GPU does not have ""\n            ""sufficient memory. If this happens, you can reduce ""\n            ""SOLVER.IMS_PER_BATCH (for training) or ""\n            ""TEST.IMS_PER_BATCH (for inference). For training, you must ""\n            ""also adjust the learning rate and schedule length according ""\n            ""to the linear scaling rule. See for example: ""\n            ""https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14""\n        )\n\n    # group images which have similar aspect ratio. In this case, we only\n    # group in two cases: those with width / height > 1, and the other way around,\n    # but the code supports more general grouping strategy\n    aspect_grouping = [1] if cfg.DATALOADER.ASPECT_RATIO_GROUPING else []\n\n    paths_catalog = import_file(\n        ""maskrcnn_benchmark.config.paths_catalog"", cfg.PATHS_CATALOG, True\n    )\n    DatasetCatalog = paths_catalog.DatasetCatalog\n    dataset_list = cfg.DATASETS.TRAIN if is_train else cfg.DATASETS.TEST\n\n    transforms = build_transforms(cfg, is_train)\n    datasets = build_dataset(dataset_list, transforms, DatasetCatalog, is_train)\n\n    data_loaders = []\n    for dataset in datasets:\n        sampler = make_data_sampler(dataset, shuffle, is_distributed)\n        batch_sampler = make_batch_data_sampler(\n            dataset, sampler, aspect_grouping, images_per_gpu, num_iters, start_iter\n        )\n        collator = BatchCollator(cfg.DATALOADER.SIZE_DIVISIBILITY)\n        num_workers = cfg.DATALOADER.NUM_WORKERS\n        data_loader = torch.utils.data.DataLoader(\n            dataset,\n            num_workers=num_workers,\n            batch_sampler=batch_sampler,\n            collate_fn=collator,\n        )\n        data_loaders.append(data_loader)\n    if is_train:\n        # during training, a single (possibly concatenated) data_loader is returned\n        assert len(data_loaders) == 1\n        return data_loaders[0]\n    return data_loaders\n'"
maskrcnn_benchmark/data/collate_batch.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\n\n\nclass BatchCollator(object):\n    """"""\n    From a list of samples from the dataset,\n    returns the batched images and targets.\n    This should be passed to the DataLoader\n    """"""\n\n    def __init__(self, size_divisible=0):\n        self.size_divisible = size_divisible\n\n    def __call__(self, batch):\n        transposed_batch = list(zip(*batch))\n        images = to_image_list(transposed_batch[0], self.size_divisible)\n        targets = transposed_batch[1]\n        img_ids = transposed_batch[2]\n        return images, targets, img_ids\n'"
maskrcnn_benchmark/engine/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n'"
maskrcnn_benchmark/engine/inference.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport datetime\nimport logging\nimport tempfile\nimport time\nimport os\nfrom collections import OrderedDict\n\nimport torch\n\nfrom tqdm import tqdm\n\nfrom ..structures.bounding_box import BoxList\nfrom ..utils.comm import is_main_process\nfrom ..utils.comm import scatter_gather\nfrom ..utils.comm import synchronize\n\n\nfrom maskrcnn_benchmark.modeling.roi_heads.mask_head.inference import Masker\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\n\n\ndef compute_on_dataset(model, data_loader, device):\n    model.eval()\n    results_dict = {}\n    cpu_device = torch.device(""cpu"")\n    for i, batch in tqdm(enumerate(data_loader)):\n        images, targets, image_ids = batch\n        images = images.to(device)\n        with torch.no_grad():\n            output = model(images)\n            output = [o.to(cpu_device) for o in output]\n        results_dict.update(\n            {img_id: result for img_id, result in zip(image_ids, output)}\n        )\n    return results_dict\n\n\ndef prepare_for_coco_detection(predictions, dataset):\n    # assert isinstance(dataset, COCODataset)\n    coco_results = []\n    for image_id, prediction in enumerate(predictions):\n        original_id = dataset.id_to_img_map[image_id]\n        if len(prediction) == 0:\n            continue\n\n        # TODO replace with get_img_info?\n        image_width = dataset.coco.imgs[original_id][""width""]\n        image_height = dataset.coco.imgs[original_id][""height""]\n        prediction = prediction.resize((image_width, image_height))\n        prediction = prediction.convert(""xywh"")\n\n        boxes = prediction.bbox.tolist()\n        scores = prediction.get_field(""scores"").tolist()\n        labels = prediction.get_field(""labels"").tolist()\n\n        mapped_labels = [dataset.contiguous_category_id_to_json_id[i] for i in labels]\n\n        coco_results.extend(\n            [\n                {\n                    ""image_id"": original_id,\n                    ""category_id"": mapped_labels[k],\n                    ""bbox"": box,\n                    ""score"": scores[k],\n                }\n                for k, box in enumerate(boxes)\n            ]\n        )\n    return coco_results\n\n\ndef prepare_for_coco_segmentation(predictions, dataset):\n    import pycocotools.mask as mask_util\n    import numpy as np\n\n    masker = Masker(threshold=0.5, padding=1)\n    # assert isinstance(dataset, COCODataset)\n    coco_results = []\n    for image_id, prediction in tqdm(enumerate(predictions)):\n        original_id = dataset.id_to_img_map[image_id]\n        if len(prediction) == 0:\n            continue\n\n        # TODO replace with get_img_info?\n        image_width = dataset.coco.imgs[original_id][""width""]\n        image_height = dataset.coco.imgs[original_id][""height""]\n        prediction = prediction.resize((image_width, image_height))\n        masks = prediction.get_field(""mask"")\n        # t = time.time()\n        masks = masker(masks, prediction)\n        # logger.info(\'Time mask: {}\'.format(time.time() - t))\n        # prediction = prediction.convert(\'xywh\')\n\n        # boxes = prediction.bbox.tolist()\n        scores = prediction.get_field(""scores"").tolist()\n        labels = prediction.get_field(""labels"").tolist()\n\n        # rles = prediction.get_field(\'mask\')\n\n        rles = [\n            mask_util.encode(np.array(mask[0, :, :, np.newaxis], order=""F""))[0]\n            for mask in masks\n        ]\n        for rle in rles:\n            rle[""counts""] = rle[""counts""].decode(""utf-8"")\n\n        mapped_labels = [dataset.contiguous_category_id_to_json_id[i] for i in labels]\n\n        coco_results.extend(\n            [\n                {\n                    ""image_id"": original_id,\n                    ""category_id"": mapped_labels[k],\n                    ""segmentation"": rle,\n                    ""score"": scores[k],\n                }\n                for k, rle in enumerate(rles)\n            ]\n        )\n    return coco_results\n\n\n# inspired from Detectron\ndef evaluate_box_proposals(\n    predictions, dataset, thresholds=None, area=""all"", limit=None\n):\n    """"""Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    """"""\n    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\n        ""all"": 0,\n        ""small"": 1,\n        ""medium"": 2,\n        ""large"": 3,\n        ""96-128"": 4,\n        ""128-256"": 5,\n        ""256-512"": 6,\n        ""512-inf"": 7,\n    }\n    area_ranges = [\n        [0 ** 2, 1e5 ** 2],  # all\n        [0 ** 2, 32 ** 2],  # small\n        [32 ** 2, 96 ** 2],  # medium\n        [96 ** 2, 1e5 ** 2],  # large\n        [96 ** 2, 128 ** 2],  # 96-128\n        [128 ** 2, 256 ** 2],  # 128-256\n        [256 ** 2, 512 ** 2],  # 256-512\n        [512 ** 2, 1e5 ** 2],\n    ]  # 512-inf\n    assert area in areas, ""Unknown area range: {}"".format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = []\n    num_pos = 0\n\n    for image_id, prediction in enumerate(predictions):\n        original_id = dataset.id_to_img_map[image_id]\n\n        # TODO replace with get_img_info?\n        image_width = dataset.coco.imgs[original_id][""width""]\n        image_height = dataset.coco.imgs[original_id][""height""]\n        prediction = prediction.resize((image_width, image_height))\n\n        # sort predictions in descending order\n        # TODO maybe remove this and make it explicit in the documentation\n        inds = prediction.get_field(""objectness"").sort(descending=True)[1]\n        prediction = prediction[inds]\n\n        ann_ids = dataset.coco.getAnnIds(imgIds=original_id)\n        anno = dataset.coco.loadAnns(ann_ids)\n        gt_boxes = [obj[""bbox""] for obj in anno if obj[""iscrowd""] == 0]\n        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n        gt_boxes = BoxList(gt_boxes, (image_width, image_height), mode=""xywh"").convert(\n            ""xyxy""\n        )\n        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])\n\n        if len(gt_boxes) == 0:\n            continue\n\n        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n        gt_boxes = gt_boxes[valid_gt_inds]\n\n        num_pos += len(gt_boxes)\n\n        if len(gt_boxes) == 0:\n            continue\n\n        if len(prediction) == 0:\n            continue\n\n        if limit is not None and len(prediction) > limit:\n            prediction = prediction[:limit]\n\n        overlaps = boxlist_iou(prediction, gt_boxes)\n\n        _gt_overlaps = torch.zeros(len(gt_boxes))\n        for j in range(min(len(prediction), len(gt_boxes))):\n            # find which proposal box maximally covers each gt box\n            # and get the iou amount of coverage for each gt box\n            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n\n            # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n            assert gt_ovr >= 0\n            # find the proposal box that covers the best covered gt box\n            box_ind = argmax_overlaps[gt_ind]\n            # record the iou coverage of this gt box\n            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n            assert _gt_overlaps[j] == gt_ovr\n            # mark the proposal box and the gt box as used\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n\n        # append recorded iou coverage level\n        gt_overlaps.append(_gt_overlaps)\n    gt_overlaps = torch.cat(gt_overlaps, dim=0)\n    gt_overlaps, _ = torch.sort(gt_overlaps)\n\n    if thresholds is None:\n        step = 0.05\n        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n    recalls = torch.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {\n        ""ar"": ar,\n        ""recalls"": recalls,\n        ""thresholds"": thresholds,\n        ""gt_overlaps"": gt_overlaps,\n        ""num_pos"": num_pos,\n    }\n\n\ndef evaluate_predictions_on_coco(\n    coco_gt, coco_results, json_result_file, iou_type=""bbox""\n):\n    import json\n\n    with open(json_result_file, ""w"") as f:\n        json.dump(coco_results, f)\n\n    from pycocotools.cocoeval import COCOeval\n\n    coco_dt = coco_gt.loadRes(str(json_result_file))\n    # coco_dt = coco_gt.loadRes(coco_results)\n    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    return coco_eval\n\n\ndef _accumulate_predictions_from_multiple_gpus(predictions_per_gpu):\n    all_predictions = scatter_gather(predictions_per_gpu)\n    if not is_main_process():\n        return\n    # merge the list of dicts\n    predictions = {}\n    for p in all_predictions:\n        predictions.update(p)\n    # convert a dict where the key is the index in a list\n    image_ids = list(sorted(predictions.keys()))\n    if len(image_ids) != image_ids[-1] + 1:\n        logger = logging.getLogger(""maskrcnn_benchmark.inference"")\n        logger.warning(\n            ""Number of images that were gathered from multiple processes is not ""\n            ""a contiguous set. Some images might be missing from the evaluation""\n        )\n\n    # convert to a list\n    predictions = [predictions[i] for i in image_ids]\n    return predictions\n\n\nclass COCOResults(object):\n    METRICS = {\n        ""bbox"": [""AP"", ""AP50"", ""AP75"", ""APs"", ""APm"", ""APl""],\n        ""segm"": [""AP"", ""AP50"", ""AP75"", ""APs"", ""APm"", ""APl""],\n        ""box_proposal"": [\n            ""AR@100"",\n            ""ARs@100"",\n            ""ARm@100"",\n            ""ARl@100"",\n            ""AR@1000"",\n            ""ARs@1000"",\n            ""ARm@1000"",\n            ""ARl@1000"",\n        ],\n        ""keypoint"": [""AP"", ""AP50"", ""AP75"", ""APm"", ""APl""],\n    }\n\n    def __init__(self, *iou_types):\n        allowed_types = (""box_proposal"", ""bbox"", ""segm"")\n        assert all(iou_type in allowed_types for iou_type in iou_types)\n        results = OrderedDict()\n        for iou_type in iou_types:\n            results[iou_type] = OrderedDict(\n                [(metric, -1) for metric in COCOResults.METRICS[iou_type]]\n            )\n        self.results = results\n\n    def update(self, coco_eval):\n        if coco_eval is None:\n            return\n        from pycocotools.cocoeval import COCOeval\n\n        assert isinstance(coco_eval, COCOeval)\n        s = coco_eval.stats\n        iou_type = coco_eval.params.iouType\n        res = self.results[iou_type]\n        metrics = COCOResults.METRICS[iou_type]\n        for idx, metric in enumerate(metrics):\n            res[metric] = s[idx]\n\n    def __repr__(self):\n        # TODO make it pretty\n        return repr(self.results)\n\n\ndef check_expected_results(results, expected_results, sigma_tol):\n    if not expected_results:\n        return\n\n    logger = logging.getLogger(""maskrcnn_benchmark.inference"")\n    for task, metric, (mean, std) in expected_results:\n        actual_val = results.results[task][metric]\n        lo = mean - sigma_tol * std\n        hi = mean + sigma_tol * std\n        ok = (lo < actual_val) and (actual_val < hi)\n        msg = (\n            ""{} > {} sanity check (actual vs. expected): ""\n            ""{:.3f} vs. mean={:.4f}, std={:.4}, range=({:.4f}, {:.4f})""\n        ).format(task, metric, actual_val, mean, std, lo, hi)\n        if not ok:\n            msg = ""FAIL: "" + msg\n            logger.error(msg)\n        else:\n            msg = ""PASS: "" + msg\n            logger.info(msg)\n\n\ndef inference(\n    model,\n    data_loader,\n    iou_types=(""bbox"",),\n    box_only=False,\n    device=""cuda"",\n    expected_results=(),\n    expected_results_sigma_tol=4,\n    output_folder=None,\n):\n\n    # convert to a torch.device for efficiency\n    device = torch.device(device)\n    num_devices = (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_initialized()\n        else 1\n    )\n    logger = logging.getLogger(""maskrcnn_benchmark.inference"")\n    dataset = data_loader.dataset\n    logger.info(""Start evaluation on {} images"".format(len(dataset)))\n    start_time = time.time()\n    predictions = compute_on_dataset(model, data_loader, device)\n    # wait for all processes to complete before measuring the time\n    synchronize()\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=total_time))\n    logger.info(\n        ""Total inference time: {} ({} s / img per device, on {} devices)"".format(\n            total_time_str, total_time * num_devices / len(dataset), num_devices\n        )\n    )\n\n    predictions = _accumulate_predictions_from_multiple_gpus(predictions)\n    if not is_main_process():\n        return\n\n    if output_folder:\n        torch.save(predictions, os.path.join(output_folder, ""predictions.pth""))\n\n    if box_only:\n        logger.info(""Evaluating bbox proposals"")\n        areas = {""all"": """", ""small"": ""s"", ""medium"": ""m"", ""large"": ""l""}\n        res = COCOResults(""box_proposal"")\n        for limit in [100, 1000]:\n            for area, suffix in areas.items():\n                stats = evaluate_box_proposals(\n                    predictions, dataset, area=area, limit=limit\n                )\n                key = ""AR{}@{:d}"".format(suffix, limit)\n                res.results[""box_proposal""][key] = stats[""ar""].item()\n        logger.info(res)\n        check_expected_results(res, expected_results, expected_results_sigma_tol)\n        if output_folder:\n            torch.save(res, os.path.join(output_folder, ""box_proposals.pth""))\n        return\n    logger.info(""Preparing results for COCO format"")\n    coco_results = {}\n    if ""bbox"" in iou_types:\n        logger.info(""Preparing bbox results"")\n        coco_results[""bbox""] = prepare_for_coco_detection(predictions, dataset)\n    if ""segm"" in iou_types:\n        logger.info(""Preparing segm results"")\n        coco_results[""segm""] = prepare_for_coco_segmentation(predictions, dataset)\n\n    results = COCOResults(*iou_types)\n    logger.info(""Evaluating predictions"")\n    for iou_type in iou_types:\n        with tempfile.NamedTemporaryFile() as f:\n            file_path = f.name\n            if output_folder:\n                file_path = os.path.join(output_folder, iou_type + "".json"")\n            res = evaluate_predictions_on_coco(\n                dataset.coco, coco_results[iou_type], file_path, iou_type\n            )\n            results.update(res)\n    logger.info(results)\n    check_expected_results(results, expected_results, expected_results_sigma_tol)\n    if output_folder:\n        torch.save(results, os.path.join(output_folder, ""coco_results.pth""))\n'"
maskrcnn_benchmark/engine/trainer.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport datetime\nimport logging\nimport time\n\nimport torch\nimport torch.distributed as dist\n\nfrom maskrcnn_benchmark.utils.comm import get_world_size, get_rank\nfrom maskrcnn_benchmark.utils.metric_logger import MetricLogger\n\n\ndef reduce_loss_dict(loss_dict):\n    """"""\n    Reduce the loss dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    loss_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return loss_dict\n    with torch.no_grad():\n        loss_names = []\n        all_losses = []\n        for k, v in loss_dict.items():\n            loss_names.append(k)\n            all_losses.append(v)\n        all_losses = torch.stack(all_losses, dim=0)\n        dist.reduce(all_losses, dst=0)\n        if dist.get_rank() == 0:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            all_losses /= world_size\n        reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\n    return reduced_losses\n\n\ndef do_train(\n    model,\n    data_loader,\n    optimizer,\n    scheduler,\n    checkpointer,\n    device,\n    checkpoint_period,\n    arguments,\n):\n    logger = logging.getLogger(""maskrcnn_benchmark.trainer"")\n    logger.info(""Start training"")\n    meters = MetricLogger(delimiter=""  "")\n    max_iter = len(data_loader)\n    start_iter = arguments[""iteration""]\n    model.train()\n    start_training_time = time.time()\n    end = time.time()\n    for iteration, (images, targets, _) in enumerate(data_loader, start_iter):\n        data_time = time.time() - end\n        arguments[""iteration""] = iteration\n\n        scheduler.step()\n\n        images = images.to(device)\n        targets = [target.to(device) for target in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = reduce_loss_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n        meters.update(loss=losses_reduced, **loss_dict_reduced)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        batch_time = time.time() - end\n        end = time.time()\n        meters.update(time=batch_time, data=data_time)\n\n        eta_seconds = meters.time.global_avg * (max_iter - iteration)\n        eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\n        if (iteration + 1) % 20 == 0 or (iteration + 1) == max_iter:\n            logger.info(\n                meters.delimiter.join(\n                    [\n                        ""eta: {eta}"",\n                        ""iter: {iter}"",\n                        ""{meters}"",\n                        ""lr: {lr:.6f}"",\n                        ""max mem: {memory:.0f}"",\n                    ]\n                ).format(\n                    eta=eta_string,\n                    iter=iteration + 1,\n                    meters=str(meters),\n                    lr=optimizer.param_groups[0][""lr""],\n                    memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0,\n                )\n            )\n        if (iteration + 1) % checkpoint_period == 0 or (iteration + 1) == max_iter:\n            checkpointer.save(""model_{:07d}"".format(iteration + 1), **arguments)\n\n    total_training_time = time.time() - start_training_time\n    total_time_str = str(datetime.timedelta(seconds=total_training_time))\n    logger.info(\n        ""Total training time: {} ({:.4f} s / it)"".format(\n            total_time_str, total_training_time / (max_iter)\n        )\n    )\n'"
maskrcnn_benchmark/layers/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nfrom .batch_norm import FrozenBatchNorm2d\nfrom .misc import Conv2d\nfrom .misc import ConvTranspose2d\nfrom .misc import interpolate\nfrom .nms import nms\nfrom .roi_align import ROIAlign\nfrom .roi_align import roi_align\nfrom .roi_pool import ROIPool\nfrom .roi_pool import roi_pool\nfrom .smooth_l1_loss import smooth_l1_loss, SmoothL1Loss\nfrom .sigmoid_focal_loss import SigmoidFocalLoss\n\n__all__ = [""nms"", ""roi_align"", ""ROIAlign"", ""roi_pool"", ""ROIPool"",\n           ""smooth_l1_loss"", ""SmoothL1Loss"", ""Conv2d"", ""ConvTranspose2d"",\n           ""interpolate"", ""FrozenBatchNorm2d"", ""SigmoidFocalLoss""]\n'"
maskrcnn_benchmark/layers/_utils.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport glob\nimport os.path\n\nimport torch\n\ntry:\n    from torch.utils.cpp_extension import load as load_ext\n    from torch.utils.cpp_extension import CUDA_HOME\nexcept ImportError:\n    raise ImportError(""The cpp layer extensions requires PyTorch 0.4 or higher"")\n\n\ndef _load_C_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir = os.path.dirname(this_dir)\n    this_dir = os.path.join(this_dir, ""csrc"")\n\n    main_file = glob.glob(os.path.join(this_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(this_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(this_dir, ""cuda"", ""*.cu""))\n\n    source = main_file + source_cpu\n\n    extra_cflags = []\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        source.extend(source_cuda)\n        extra_cflags = [""-DWITH_CUDA""]\n    source = [os.path.join(this_dir, s) for s in source]\n    extra_include_paths = [this_dir]\n    return load_ext(\n        ""torchvision"",\n        source,\n        extra_cflags=extra_cflags,\n        extra_include_paths=extra_include_paths,\n    )\n\n\n_C = _load_C_extensions()\n'"
maskrcnn_benchmark/layers/batch_norm.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\n\n\nclass FrozenBatchNorm2d(nn.Module):\n    """"""\n    BatchNorm2d where the batch statistics and the affine parameters\n    are fixed\n    """"""\n\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(""weight"", torch.ones(n))\n        self.register_buffer(""bias"", torch.zeros(n))\n        self.register_buffer(""running_mean"", torch.zeros(n))\n        self.register_buffer(""running_var"", torch.ones(n))\n\n    def forward(self, x):\n        scale = self.weight * self.running_var.rsqrt()\n        bias = self.bias - self.running_mean * scale\n        scale = scale.reshape(1, -1, 1, 1)\n        bias = bias.reshape(1, -1, 1, 1)\n        return x * scale + bias\n'"
maskrcnn_benchmark/layers/misc.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nhelper class that supports empty tensors on some nn functions.\n\nIdeally, add support directly in PyTorch to empty tensors in\nthose functions.\n\nThis can be removed once https://github.com/pytorch/pytorch/issues/12013\nis implemented\n""""""\n\nimport math\nimport torch\nfrom torch.nn.modules.utils import _ntuple\n\n\nclass _NewEmptyTensorOp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape\n        return _NewEmptyTensorOp.apply(grad, shape), None\n\n\n\nclass Conv2d(torch.nn.Conv2d):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(Conv2d, self).forward(x)\n        # get output shape\n\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(\n                x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride\n            )\n        ]\n        output_shape = [x.shape[0], self.weight.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\nclass ConvTranspose2d(torch.nn.ConvTranspose2d):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(ConvTranspose2d, self).forward(x)\n        # get output shape\n\n        output_shape = [\n            (i - 1) * d - 2 * p + (di * (k - 1) + 1) + op\n            for i, p, di, k, d, op in zip(\n                x.shape[-2:],\n                self.padding,\n                self.dilation,\n                self.kernel_size,\n                self.stride,\n                self.output_padding,\n            )\n        ]\n        output_shape = [x.shape[0], self.bias.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\ndef interpolate(\n    input, size=None, scale_factor=None, mode=""nearest"", align_corners=None\n):\n    if input.numel() > 0:\n        return torch.nn.functional.interpolate(\n            input, size, scale_factor, mode, align_corners\n        )\n\n    def _check_size_scale_factor(dim):\n        if size is None and scale_factor is None:\n            raise ValueError(""either size or scale_factor should be defined"")\n        if size is not None and scale_factor is not None:\n            raise ValueError(""only one of size or scale_factor should be defined"")\n        if (\n            scale_factor is not None\n            and isinstance(scale_factor, tuple)\n            and len(scale_factor) != dim\n        ):\n            raise ValueError(\n                ""scale_factor shape must match input shape. ""\n                ""Input is {}D, scale_factor size is {}"".format(dim, len(scale_factor))\n            )\n\n    def _output_size(dim):\n        _check_size_scale_factor(dim)\n        if size is not None:\n            return size\n        scale_factors = _ntuple(dim)(scale_factor)\n        # math.floor might return float in py2.7\n        return [\n            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)\n        ]\n\n    output_shape = tuple(_output_size(2))\n    output_shape = input.shape[:-2] + output_shape\n    return _NewEmptyTensorOp.apply(input, output_shape)\n'"
maskrcnn_benchmark/layers/nms.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# from ._utils import _C\nfrom maskrcnn_benchmark import _C\n\nnms = _C.nms\n# nms.__doc__ = """"""\n# This function performs Non-maximum suppresion""""""\n'"
maskrcnn_benchmark/layers/roi_align.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom maskrcnn_benchmark import _C\n\n\nclass _ROIAlign(Function):\n    @staticmethod\n    def forward(ctx, input, roi, output_size, spatial_scale, sampling_ratio):\n        ctx.save_for_backward(roi)\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.sampling_ratio = sampling_ratio\n        ctx.input_shape = input.size()\n        output = _C.roi_align_forward(\n            input, roi, spatial_scale, output_size[0], output_size[1], sampling_ratio\n        )\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        rois, = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        sampling_ratio = ctx.sampling_ratio\n        bs, ch, h, w = ctx.input_shape\n        grad_input = _C.roi_align_backward(\n            grad_output,\n            rois,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n            sampling_ratio,\n        )\n        return grad_input, None, None, None, None\n\n\nroi_align = _ROIAlign.apply\n\n\nclass ROIAlign(nn.Module):\n    def __init__(self, output_size, spatial_scale, sampling_ratio):\n        super(ROIAlign, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n        self.sampling_ratio = sampling_ratio\n\n    def forward(self, input, rois):\n        return roi_align(\n            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio\n        )\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "", sampling_ratio="" + str(self.sampling_ratio)\n        tmpstr += "")""\n        return tmpstr\n'"
maskrcnn_benchmark/layers/roi_pool.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom maskrcnn_benchmark import _C\n\n\nclass _ROIPool(Function):\n    @staticmethod\n    def forward(ctx, input, roi, output_size, spatial_scale):\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.input_shape = input.size()\n        output, argmax = _C.roi_pool_forward(\n            input, roi, spatial_scale, output_size[0], output_size[1]\n        )\n        ctx.save_for_backward(input, roi, argmax)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, rois, argmax = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        bs, ch, h, w = ctx.input_shape\n        grad_input = _C.roi_pool_backward(\n            grad_output,\n            input,\n            rois,\n            argmax,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n        )\n        return grad_input, None, None, None\n\n\nroi_pool = _ROIPool.apply\n\n\nclass ROIPool(nn.Module):\n    def __init__(self, output_size, spatial_scale):\n        super(ROIPool, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n\n    def forward(self, input, rois):\n        return roi_pool(input, rois, self.output_size, self.spatial_scale)\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "")""\n        return tmpstr\n'"
maskrcnn_benchmark/layers/sigmoid_focal_loss.py,2,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\n\nfrom maskrcnn_benchmark import _C\n\n\nclass _SigmoidFocalLoss(Function):\n    @staticmethod\n    def forward(ctx, logits, targets, num_classes, gamma, alpha):\n        ctx.save_for_backward(logits, targets);\n        ctx.num_classes = num_classes\n        ctx.gamma = gamma\n        ctx.alpha = alpha\n\n        losses = _C.sigmoid_focalloss_forward(\n            logits, targets, num_classes, gamma, alpha\n        )\n        return losses\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, d_loss):\n        logits, targets = ctx.saved_tensors\n        num_classes = ctx.num_classes\n        gamma = ctx.gamma\n        alpha = ctx.alpha\n        d_loss = d_loss.contiguous()\n        d_logits = _C.sigmoid_focalloss_backward(\n            logits, targets, d_loss, num_classes, gamma, alpha\n        )\n        return d_logits, None, None, None, None\n\n\nsigmoid_focalloss = _SigmoidFocalLoss.apply\n\n\nclass SigmoidFocalLoss(nn.Module):\n    def __init__(self, num_classes, gamma, alpha):\n        super(SigmoidFocalLoss, self).__init__()\n        self.num_classes = num_classes\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, logits, targets):\n        loss = sigmoid_focalloss(\n            logits, targets, self.num_classes, self.gamma, self.alpha\n        )\n        return loss.sum()\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""num_classes="" + str(self.num_classes)\n        tmpstr += "", gamma="" + str(self.gamma)\n        tmpstr += "", alpha="" + str(self.alpha)\n        tmpstr += "")""\n        return tmpstr\n'"
maskrcnn_benchmark/layers/smooth_l1_loss.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\n\nclass SmoothL1Loss(torch.nn.Module):\n    def __init__(self, beta=1. /9):\n        super(SmoothL1Loss, self).__init__()\n        self.beta = beta\n\n    def forward(self, input, target, size_average=True):\n        return smooth_l1_loss(input, target, self.beta, size_average)\n\n\n# TODO maybe push this to nn?\ndef smooth_l1_loss(input, target, beta=1. / 9, size_average=True):\n    """"""\n    very similar to the smooth_l1_loss from pytorch, but with\n    the extra beta parameter\n    """"""\n    n = torch.abs(input - target)\n    cond = n < beta\n    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n    if size_average:\n        return loss.mean()\n    return loss.sum()\n'"
maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\n\nclass BalancedPositiveNegativeSampler(object):\n    """"""\n    This class samples batches, ensuring that they contain a fixed proportion of positives\n    """"""\n\n    def __init__(self, batch_size_per_image, positive_fraction):\n        """"""\n        Arguments:\n            batch_size_per_image (int): number of elements to be selected per image\n            positive_fraction (float): percentace of positive elements per batch\n        """"""\n        self.batch_size_per_image = batch_size_per_image\n        self.positive_fraction = positive_fraction\n\n    def __call__(self, matched_idxs):\n        """"""\n        Arguments:\n            matched idxs: list of tensors containing -1, 0 or positive values.\n                Each tensor corresponds to a specific image.\n                -1 values are ignored, 0 are considered as negatives and > 0 as\n                positives.\n\n        Returns:\n            pos_idx (list[tensor])\n            neg_idx (list[tensor])\n\n        Returns two lists of binary masks for each image.\n        The first list contains the positive elements that were selected,\n        and the second list the negative example.\n        """"""\n        pos_idx = []\n        neg_idx = []\n        for matched_idxs_per_image in matched_idxs:\n            positive = torch.nonzero(matched_idxs_per_image >= 1).squeeze(1)\n            negative = torch.nonzero(matched_idxs_per_image == 0).squeeze(1)\n\n            num_pos = int(self.batch_size_per_image * self.positive_fraction)\n            # protect against not enough positive examples\n            num_pos = min(positive.numel(), num_pos)\n            num_neg = self.batch_size_per_image - num_pos\n            # protect against not enough negative examples\n            num_neg = min(negative.numel(), num_neg)\n\n            # randomly select positive and negative examples\n            perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n            perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n\n            pos_idx_per_image = positive[perm1]\n            neg_idx_per_image = negative[perm2]\n\n            # create binary mask from indices\n            pos_idx_per_image_mask = torch.zeros_like(\n                matched_idxs_per_image, dtype=torch.uint8\n            )\n            neg_idx_per_image_mask = torch.zeros_like(\n                matched_idxs_per_image, dtype=torch.uint8\n            )\n            pos_idx_per_image_mask[pos_idx_per_image] = 1\n            neg_idx_per_image_mask[neg_idx_per_image] = 1\n\n            pos_idx.append(pos_idx_per_image_mask)\n            neg_idx.append(neg_idx_per_image_mask)\n\n        return pos_idx, neg_idx\n'"
maskrcnn_benchmark/modeling/box_coder.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport math\n\nimport torch\n\n\nclass BoxCoder(object):\n    """"""\n    This class encodes and decodes a set of bounding boxes into\n    the representation used for training the regressors.\n    """"""\n\n    def __init__(self, weights, bbox_xform_clip=math.log(1000. / 16)):\n        """"""\n        Arguments:\n            weights (4-element tuple)\n            bbox_xform_clip (float)\n        """"""\n        self.weights = weights\n        self.bbox_xform_clip = bbox_xform_clip\n\n    def encode(self, reference_boxes, proposals):\n        """"""\n        Encode a set of proposals with respect to some\n        reference boxes\n\n        Arguments:\n            reference_boxes (Tensor): reference boxes\n            proposals (Tensor): boxes to be encoded\n        """"""\n\n        TO_REMOVE = 1  # TODO remove\n        ex_widths = proposals[..., 2] - proposals[..., 0] + TO_REMOVE\n        ex_heights = proposals[..., 3] - proposals[..., 1] + TO_REMOVE\n        ex_ctr_x = proposals[..., 0] + 0.5 * ex_widths\n        ex_ctr_y = proposals[..., 1] + 0.5 * ex_heights\n\n        gt_widths = reference_boxes[..., 2] - reference_boxes[..., 0] + TO_REMOVE\n        gt_heights = reference_boxes[..., 3] - reference_boxes[..., 1] + TO_REMOVE\n        gt_ctr_x = reference_boxes[..., 0] + 0.5 * gt_widths\n        gt_ctr_y = reference_boxes[..., 1] + 0.5 * gt_heights\n\n        wx, wy, ww, wh = self.weights\n        targets_dx = wx * (gt_ctr_x - ex_ctr_x) / ex_widths\n        targets_dy = wy * (gt_ctr_y - ex_ctr_y) / ex_heights\n        targets_dw = ww * torch.log(gt_widths / ex_widths)\n        targets_dh = wh * torch.log(gt_heights / ex_heights)\n\n        targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=-1)\n        return targets\n\n    def decode(self, rel_codes, boxes):\n        """"""\n        From a set of original boxes and encoded relative box offsets,\n        get the decoded boxes.\n\n        Arguments:\n            rel_codes (Tensor): encoded boxes\n            boxes (Tensor): reference boxes.\n        """"""\n\n        boxes = boxes.to(rel_codes.dtype)\n\n        TO_REMOVE = 1  # TODO remove\n        widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE\n        heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = rel_codes[:, 0::4] / wx\n        dy = rel_codes[:, 1::4] / wy\n        dw = rel_codes[:, 2::4] / ww\n        dh = rel_codes[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(rel_codes)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n        # y2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n        return pred_boxes\n'"
maskrcnn_benchmark/modeling/matcher.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\n\nclass Matcher(object):\n    """"""\n    This class assigns to each predicted ""element"" (e.g., a box) a ground-truth\n    element. Each predicted element will have exactly zero or one matches; each\n    ground-truth element may be assigned to zero or more predicted elements.\n\n    Matching is based on the MxN match_quality_matrix, that characterizes how well\n    each (ground-truth, predicted)-pair match. For example, if the elements are\n    boxes, the matrix may contain box IoU overlap values.\n\n    The matcher returns a tensor of size N containing the index of the ground-truth\n    element m that matches to prediction n. If there is no match, a negative value\n    is returned.\n    """"""\n\n    BELOW_LOW_THRESHOLD = -1\n    BETWEEN_THRESHOLDS = -2\n\n    def __init__(self, high_threshold, low_threshold,\n                 allow_low_quality_matches=False, low_quality_threshold=0.0):\n        """"""\n        Args:\n            high_threshold (float): quality values greater than or equal to\n                this value are candidate matches.\n            low_threshold (float): a lower quality threshold used to stratify\n                matches into three levels:\n                1) matches >= high_threshold\n                2) BETWEEN_THRESHOLDS matches in [low_threshold, high_threshold)\n                3) BELOW_LOW_THRESHOLD matches in [0, low_threshold)\n            allow_low_quality_matches (bool): if True, produce additional matches\n                for predictions that have only low-quality match candidates. See\n                set_low_quality_matches_ for more details.\n        """"""\n        assert low_threshold <= high_threshold\n        self.high_threshold = high_threshold\n        self.low_threshold = low_threshold\n        self.allow_low_quality_matches = allow_low_quality_matches\n        self.low_quality_threshold = low_quality_threshold\n\n    def __call__(self, match_quality_matrix):\n        """"""\n        Args:\n            match_quality_matrix (Tensor[float]): an MxN tensor, containing the\n            pairwise quality between M ground-truth elements and N predicted elements.\n\n        Returns:\n            matches (Tensor[int64]): an N tensor where N[i] is a matched gt in\n            [0, M - 1] or a negative value indicating that prediction i could not\n            be matched.\n        """"""\n        if match_quality_matrix.numel() == 0:\n            # handle empty case\n            device = match_quality_matrix.device\n            return torch.empty((0,), dtype=torch.int64, device=device)\n\n        # match_quality_matrix is M (gt) x N (predicted)\n        # Max over gt elements (dim 0) to find best gt candidate for each prediction\n        matched_vals, matches = match_quality_matrix.max(dim=0)\n        if self.allow_low_quality_matches:\n            all_matches = matches.clone()\n\n        # Assign candidate matches with low quality to negative (unassigned) values\n        below_low_threshold = matched_vals < self.low_threshold\n        between_thresholds = (matched_vals >= self.low_threshold) & (\n            matched_vals < self.high_threshold\n        )\n        matches[below_low_threshold] = Matcher.BELOW_LOW_THRESHOLD\n        matches[between_thresholds] = Matcher.BETWEEN_THRESHOLDS\n\n        if self.allow_low_quality_matches:\n            self.set_low_quality_matches_(matches, all_matches, match_quality_matrix)\n\n        return matches\n\n    def set_low_quality_matches_(self, matches, all_matches, match_quality_matrix):\n        """"""\n        Produce additional matches for predictions that have only low-quality matches.\n        Specifically, for each ground-truth find the set of predictions that have\n        maximum overlap with it (including ties); for each prediction in that set, if\n        it is unmatched, then match it to the ground-truth with which it has the highest\n        quality value.\n        """"""\n        # For each gt, find the prediction with which it has highest quality\n        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)\n\n        if self.low_quality_threshold > 0.0:\n            select = highest_quality_foreach_gt >= self.low_quality_threshold\n            highest_quality_foreach_gt = highest_quality_foreach_gt[select]\n            match_quality_matrix = match_quality_matrix[select]\n        # Find highest quality match available, even if it is low, including ties\n        gt_pred_pairs_of_highest_quality = torch.nonzero(\n            match_quality_matrix == highest_quality_foreach_gt[:, None]\n        )\n        # Example gt_pred_pairs_of_highest_quality:\n        #   tensor([[    0, 39796],\n        #           [    1, 32055],\n        #           [    1, 32070],\n        #           [    2, 39190],\n        #           [    2, 40255],\n        #           [    3, 40390],\n        #           [    3, 41455],\n        #           [    4, 45470],\n        #           [    5, 45325],\n        #           [    5, 46390]])\n        # Each row is a (gt index, prediction index)\n        # Note how gt items 1, 2, 3, and 5 each have two ties\n\n        pred_inds_to_update = gt_pred_pairs_of_highest_quality[:, 1]\n        matches[pred_inds_to_update] = all_matches[pred_inds_to_update]\n'"
maskrcnn_benchmark/modeling/poolers.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom maskrcnn_benchmark.layers import ROIAlign\n\nfrom .utils import cat\n\n\nclass LevelMapper(object):\n    """"""Determine which FPN level each RoI in a set of RoIs should map to based\n    on the heuristic in the FPN paper.\n    """"""\n\n    def __init__(self, k_min, k_max, canonical_scale=224, canonical_level=4, eps=1e-6):\n        """"""\n        Arguments:\n            k_min (int)\n            k_max (int)\n            canonical_scale (int)\n            canonical_level (int)\n            eps (float)\n        """"""\n        self.k_min = k_min\n        self.k_max = k_max\n        self.s0 = canonical_scale\n        self.lvl0 = canonical_level\n        self.eps = eps\n\n    def __call__(self, boxlists):\n        """"""\n        Arguments:\n            boxlists (list[BoxList])\n        """"""\n        # Compute level ids\n        s = torch.sqrt(cat([boxlist.area() for boxlist in boxlists]))\n\n        # Eqn.(1) in FPN paper\n        target_lvls = torch.floor(self.lvl0 + torch.log2(s / self.s0 + self.eps))\n        target_lvls = torch.clamp(target_lvls, min=self.k_min, max=self.k_max)\n        return target_lvls.to(torch.int64) - self.k_min\n\n\nclass Pooler(nn.Module):\n    """"""\n    Pooler for Detection with or without FPN.\n    It currently hard-code ROIAlign in the implementation,\n    but that can be made more generic later on.\n    Also, the requirement of passing the scales is not strictly necessary, as they\n    can be inferred from the size of the feature map / size of original image,\n    which is available thanks to the BoxList.\n    """"""\n\n    def __init__(self, output_size, scales, sampling_ratio, canonical_level=4):\n        """"""\n        Arguments:\n            output_size (list[tuple[int]] or list[int]): output size for the pooled region\n            scales (list[float]): scales for each Pooler\n            sampling_ratio (int): sampling ratio for ROIAlign\n        """"""\n        super(Pooler, self).__init__()\n        poolers = []\n        for scale in scales:\n            poolers.append(\n                ROIAlign(\n                    output_size, spatial_scale=scale, sampling_ratio=sampling_ratio\n                )\n            )\n        self.poolers = nn.ModuleList(poolers)\n        self.output_size = output_size\n        # get the levels in the feature map by leveraging the fact that the network always\n        # downsamples by a factor of 2 at each level.\n        lvl_min = -torch.log2(torch.tensor(scales[0], dtype=torch.float32)).item()\n        lvl_max = -torch.log2(torch.tensor(scales[-1], dtype=torch.float32)).item()\n        self.map_levels = LevelMapper(\n            lvl_min, lvl_max, canonical_level=canonical_level\n        )\n\n    def convert_to_roi_format(self, boxes):\n        concat_boxes = cat([b.bbox for b in boxes], dim=0)\n        device, dtype = concat_boxes.device, concat_boxes.dtype\n        ids = cat(\n            [\n                torch.full((len(b), 1), i, dtype=dtype, device=device)\n                for i, b in enumerate(boxes)\n            ],\n            dim=0,\n        )\n        rois = torch.cat([ids, concat_boxes], dim=1)\n        return rois\n\n    def forward(self, x, boxes):\n        """"""\n        Arguments:\n            x (list[Tensor]): feature maps for each level\n            boxes (list[BoxList]): boxes to be used to perform the pooling operation.\n        Returns:\n            result (Tensor)\n        """"""\n        num_levels = len(self.poolers)\n        rois = self.convert_to_roi_format(boxes)\n        if num_levels == 1:\n            return self.poolers[0](x[0], rois)\n\n        levels = self.map_levels(boxes)\n\n        num_rois = len(rois)\n        num_channels = x[0].shape[1]\n        output_size = self.output_size[0]\n\n        dtype, device = x[0].dtype, x[0].device\n        result = torch.zeros(\n            (num_rois, num_channels, output_size, output_size),\n            dtype=dtype,\n            device=device,\n        )\n        for level, (per_level_feature, pooler) in enumerate(zip(x, self.poolers)):\n            idx_in_level = torch.nonzero(levels == level).squeeze(1)\n            rois_per_level = rois[idx_in_level]\n            result[idx_in_level] = pooler(per_level_feature, rois_per_level)\n\n        return result\n'"
maskrcnn_benchmark/modeling/utils.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nMiscellaneous utility functions\n""""""\n\nimport torch\n\n\ndef cat(tensors, dim=0):\n    """"""\n    Efficient version of torch.cat that avoids a copy if there is only a single element in a list\n    """"""\n    assert isinstance(tensors, (list, tuple))\n    if len(tensors) == 1:\n        return tensors[0]\n    return torch.cat(tensors, dim)\n'"
maskrcnn_benchmark/solver/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .build import make_optimizer\nfrom .build import make_lr_scheduler\nfrom .lr_scheduler import WarmupMultiStepLR\n'"
maskrcnn_benchmark/solver/build.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nfrom .lr_scheduler import WarmupMultiStepLR\n\n\ndef make_optimizer(cfg, model):\n    params = []\n    for key, value in model.named_parameters():\n        if not value.requires_grad:\n            continue\n        lr = cfg.SOLVER.BASE_LR\n        weight_decay = cfg.SOLVER.WEIGHT_DECAY\n        if ""bias"" in key:\n            lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n        params += [{""params"": [value], ""lr"": lr, ""weight_decay"": weight_decay}]\n\n    optimizer = torch.optim.SGD(params, lr, momentum=cfg.SOLVER.MOMENTUM)\n    return optimizer\n\n\ndef make_lr_scheduler(cfg, optimizer):\n    return WarmupMultiStepLR(\n        optimizer,\n        cfg.SOLVER.STEPS,\n        cfg.SOLVER.GAMMA,\n        warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n        warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n        warmup_method=cfg.SOLVER.WARMUP_METHOD,\n    )\n'"
maskrcnn_benchmark/solver/lr_scheduler.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom bisect import bisect_right\n\nimport torch\n\n\n# FIXME ideally this would be achieved with a CombinedLRScheduler,\n# separating MultiStepLR with WarmupLR\n# but the current LRScheduler design doesn\'t allow it\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        milestones,\n        gamma=0.1,\n        warmup_factor=1.0 / 3,\n        warmup_iters=500,\n        warmup_method=""linear"",\n        last_epoch=-1,\n    ):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"",\n                milestones,\n            )\n\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted""\n                ""got {}"".format(warmup_method)\n            )\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == ""constant"":\n                warmup_factor = self.warmup_factor\n            elif self.warmup_method == ""linear"":\n                alpha = self.last_epoch / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [\n            base_lr\n            * warmup_factor\n            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            for base_lr in self.base_lrs\n        ]\n'"
maskrcnn_benchmark/structures/bounding_box.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\n# transpose\nFLIP_LEFT_RIGHT = 0\nFLIP_TOP_BOTTOM = 1\n\n\nclass BoxList(object):\n    """"""\n    This class represents a set of bounding boxes.\n    The bounding boxes are represented as a Nx4 Tensor.\n    In order to uniquely determine the bounding boxes with respect\n    to an image, we also store the corresponding image dimensions.\n    They can contain extra information that is specific to each bounding box, such as\n    labels.\n    """"""\n\n    def __init__(self, bbox, image_size, mode=""xyxy""):\n        device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device(""cpu"")\n        bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)\n        if bbox.ndimension() != 2:\n            raise ValueError(\n                ""bbox should have 2 dimensions, got {}"".format(bbox.ndimension())\n            )\n        if bbox.size(-1) != 4:\n            raise ValueError(\n                ""last dimenion of bbox should have a ""\n                ""size of 4, got {}"".format(bbox.size(-1))\n            )\n        if mode not in (""xyxy"", ""xywh""):\n            raise ValueError(""mode should be \'xyxy\' or \'xywh\'"")\n\n        self.bbox = bbox\n        self.size = image_size  # (image_width, image_height)\n        self.mode = mode\n        self.extra_fields = {}\n\n    def add_field(self, field, field_data):\n        self.extra_fields[field] = field_data\n\n    def get_field(self, field):\n        return self.extra_fields[field]\n\n    def has_field(self, field):\n        return field in self.extra_fields\n\n    def fields(self):\n        return list(self.extra_fields.keys())\n\n    def _copy_extra_fields(self, bbox):\n        for k, v in bbox.extra_fields.items():\n            self.extra_fields[k] = v\n\n    def convert(self, mode):\n        if mode not in (""xyxy"", ""xywh""):\n            raise ValueError(""mode should be \'xyxy\' or \'xywh\'"")\n        if mode == self.mode:\n            return self\n        # we only have two modes, so don\'t need to check\n        # self.mode\n        xmin, ymin, xmax, ymax = self._split_into_xyxy()\n        if mode == ""xyxy"":\n            bbox = torch.cat((xmin, ymin, xmax, ymax), dim=-1)\n            bbox = BoxList(bbox, self.size, mode=mode)\n        else:\n            TO_REMOVE = 1\n            bbox = torch.cat(\n                (xmin, ymin, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE), dim=-1\n            )\n            bbox = BoxList(bbox, self.size, mode=mode)\n        bbox._copy_extra_fields(self)\n        return bbox\n\n    def _split_into_xyxy(self):\n        if self.mode == ""xyxy"":\n            xmin, ymin, xmax, ymax = self.bbox.split(1, dim=-1)\n            return xmin, ymin, xmax, ymax\n        elif self.mode == ""xywh"":\n            TO_REMOVE = 1\n            xmin, ymin, w, h = self.bbox.split(1, dim=-1)\n            return (\n                xmin,\n                ymin,\n                xmin + (w - TO_REMOVE).clamp(min=0),\n                ymin + (h - TO_REMOVE).clamp(min=0),\n            )\n        else:\n            raise RuntimeError(""Should not be here"")\n\n    def resize(self, size, *args, **kwargs):\n        """"""\n        Returns a resized copy of this bounding box\n\n        :param size: The requested size in pixels, as a 2-tuple:\n            (width, height).\n        """"""\n\n        ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(size, self.size))\n        if ratios[0] == ratios[1]:\n            ratio = ratios[0]\n            scaled_box = self.bbox * ratio\n            bbox = BoxList(scaled_box, size, mode=self.mode)\n            # bbox._copy_extra_fields(self)\n            for k, v in self.extra_fields.items():\n                if not isinstance(v, torch.Tensor):\n                    v = v.resize(size, *args, **kwargs)\n                bbox.add_field(k, v)\n            return bbox\n\n        ratio_width, ratio_height = ratios\n        xmin, ymin, xmax, ymax = self._split_into_xyxy()\n        scaled_xmin = xmin * ratio_width\n        scaled_xmax = xmax * ratio_width\n        scaled_ymin = ymin * ratio_height\n        scaled_ymax = ymax * ratio_height\n        scaled_box = torch.cat(\n            (scaled_xmin, scaled_ymin, scaled_xmax, scaled_ymax), dim=-1\n        )\n        bbox = BoxList(scaled_box, size, mode=""xyxy"")\n        # bbox._copy_extra_fields(self)\n        for k, v in self.extra_fields.items():\n            if not isinstance(v, torch.Tensor):\n                v = v.resize(size, *args, **kwargs)\n            bbox.add_field(k, v)\n\n        return bbox.convert(self.mode)\n\n    def transpose(self, method):\n        """"""\n        Transpose bounding box (flip or rotate in 90 degree steps)\n        :param method: One of :py:attr:`PIL.Image.FLIP_LEFT_RIGHT`,\n          :py:attr:`PIL.Image.FLIP_TOP_BOTTOM`, :py:attr:`PIL.Image.ROTATE_90`,\n          :py:attr:`PIL.Image.ROTATE_180`, :py:attr:`PIL.Image.ROTATE_270`,\n          :py:attr:`PIL.Image.TRANSPOSE` or :py:attr:`PIL.Image.TRANSVERSE`.\n        """"""\n        if method not in (FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM):\n            raise NotImplementedError(\n                ""Only FLIP_LEFT_RIGHT and FLIP_TOP_BOTTOM implemented""\n            )\n\n        image_width, image_height = self.size\n        xmin, ymin, xmax, ymax = self._split_into_xyxy()\n        if method == FLIP_LEFT_RIGHT:\n            TO_REMOVE = 1\n            transposed_xmin = image_width - xmax - TO_REMOVE\n            transposed_xmax = image_width - xmin - TO_REMOVE\n            transposed_ymin = ymin\n            transposed_ymax = ymax\n        elif method == FLIP_TOP_BOTTOM:\n            transposed_xmin = xmin\n            transposed_xmax = xmax\n            transposed_ymin = image_height - ymax\n            transposed_ymax = image_height - ymin\n\n        transposed_boxes = torch.cat(\n            (transposed_xmin, transposed_ymin, transposed_xmax, transposed_ymax), dim=-1\n        )\n        bbox = BoxList(transposed_boxes, self.size, mode=""xyxy"")\n        # bbox._copy_extra_fields(self)\n        for k, v in self.extra_fields.items():\n            if not isinstance(v, torch.Tensor):\n                v = v.transpose(method)\n            bbox.add_field(k, v)\n        return bbox.convert(self.mode)\n\n    def crop(self, box):\n        """"""\n        Cropss a rectangular region from this bounding box. The box is a\n        4-tuple defining the left, upper, right, and lower pixel\n        coordinate.\n        """"""\n        xmin, ymin, xmax, ymax = self._split_into_xyxy()\n        w, h = box[2] - box[0], box[3] - box[1]\n        cropped_xmin = (xmin - box[0]).clamp(min=0, max=w)\n        cropped_ymin = (ymin - box[1]).clamp(min=0, max=h)\n        cropped_xmax = (xmax - box[0]).clamp(min=0, max=w)\n        cropped_ymax = (ymax - box[1]).clamp(min=0, max=h)\n\n        # TODO should I filter empty boxes here?\n        if False:\n            is_empty = (cropped_xmin == cropped_xmax) | (cropped_ymin == cropped_ymax)\n\n        cropped_box = torch.cat(\n            (cropped_xmin, cropped_ymin, cropped_xmax, cropped_ymax), dim=-1\n        )\n        bbox = BoxList(cropped_box, (w, h), mode=""xyxy"")\n        # bbox._copy_extra_fields(self)\n        for k, v in self.extra_fields.items():\n            if not isinstance(v, torch.Tensor):\n                v = v.crop(box)\n            bbox.add_field(k, v)\n        return bbox.convert(self.mode)\n\n    # Tensor-like methods\n\n    def to(self, device):\n        bbox = BoxList(self.bbox.to(device), self.size, self.mode)\n        for k, v in self.extra_fields.items():\n            if hasattr(v, ""to""):\n                v = v.to(device)\n            bbox.add_field(k, v)\n        return bbox\n\n    def __getitem__(self, item):\n        bbox = BoxList(self.bbox[item], self.size, self.mode)\n        for k, v in self.extra_fields.items():\n            bbox.add_field(k, v[item])\n        return bbox\n\n    def __len__(self):\n        return self.bbox.shape[0]\n\n    def clip_to_image(self, remove_empty=True):\n        TO_REMOVE = 1\n        self.bbox[:, 0].clamp_(min=0, max=self.size[0] - TO_REMOVE)\n        self.bbox[:, 1].clamp_(min=0, max=self.size[1] - TO_REMOVE)\n        self.bbox[:, 2].clamp_(min=0, max=self.size[0] - TO_REMOVE)\n        self.bbox[:, 3].clamp_(min=0, max=self.size[1] - TO_REMOVE)\n        if remove_empty:\n            box = self.bbox\n            keep = (box[:, 3] > box[:, 1]) & (box[:, 2] > box[:, 0])\n            return self[keep]\n        return self\n\n    def area(self):\n        if self.mode == \'xyxy\':\n            TO_REMOVE = 1\n            box = self.bbox\n            area = (box[:, 2] - box[:, 0] + TO_REMOVE) * (box[:, 3] - box[:, 1] + TO_REMOVE)\n        elif self.mode == \'xywh\':\n            box = self.bbox\n            area = box[:, 2] * box[:, 3]\n        else:\n            raise RuntimeError(""Should not be here"")\n            \n        return area\n\n    def copy_with_fields(self, fields):\n        bbox = BoxList(self.bbox, self.size, self.mode)\n        if not isinstance(fields, (list, tuple)):\n            fields = [fields]\n        for field in fields:\n            bbox.add_field(field, self.get_field(field))\n        return bbox\n\n    def __repr__(self):\n        s = self.__class__.__name__ + ""(""\n        s += ""num_boxes={}, "".format(len(self))\n        s += ""image_width={}, "".format(self.size[0])\n        s += ""image_height={}, "".format(self.size[1])\n        s += ""mode={})"".format(self.mode)\n        return s\n\n\nif __name__ == ""__main__"":\n    bbox = BoxList([[0, 0, 10, 10], [0, 0, 5, 5]], (10, 10))\n    s_bbox = bbox.resize((5, 5))\n    print(s_bbox)\n    print(s_bbox.bbox)\n\n    t_bbox = bbox.transpose(0)\n    print(t_bbox)\n    print(t_bbox.bbox)\n'"
maskrcnn_benchmark/structures/boxlist_ops.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nfrom .bounding_box import BoxList\n\nfrom maskrcnn_benchmark.layers import nms as _box_nms\n\n\ndef boxlist_nms(boxlist, nms_thresh, max_proposals=-1, score_field=""score""):\n    """"""\n    Performs non-maximum suppression on a boxlist, with scores specified\n    in a boxlist field via score_field.\n\n    Arguments:\n        boxlist(BoxList)\n        nms_thresh (float)\n        max_proposals (int): if > 0, then only the top max_proposals are kept\n            after non-maxium suppression\n        score_field (str)\n    """"""\n    if nms_thresh <= 0:\n        return boxlist\n    mode = boxlist.mode\n    boxlist = boxlist.convert(""xyxy"")\n    boxes = boxlist.bbox\n    score = boxlist.get_field(score_field)\n    keep = _box_nms(boxes, score, nms_thresh)\n    if max_proposals > 0:\n        keep = keep[: max_proposals]\n    boxlist = boxlist[keep]\n    return boxlist.convert(mode)\n\n\ndef remove_small_boxes(boxlist, min_size):\n    """"""\n    Only keep boxes with both sides >= min_size\n\n    Arguments:\n        boxlist (Boxlist)\n        min_size (int)\n    """"""\n    # TODO maybe add an API for querying the ws / hs\n    xywh_boxes = boxlist.convert(""xywh"").bbox\n    _, _, ws, hs = xywh_boxes.unbind(dim=1)\n    keep = (\n        (ws >= min_size) & (hs >= min_size)\n    ).nonzero().squeeze(1)\n    return boxlist[keep]\n\n\n# implementation from https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/box.py\n# with slight modifications\ndef boxlist_iou(boxlist1, boxlist2):\n    """"""Compute the intersection over union of two set of boxes.\n    The box order must be (xmin, ymin, xmax, ymax).\n\n    Arguments:\n      box1: (BoxList) bounding boxes, sized [N,4].\n      box2: (BoxList) bounding boxes, sized [M,4].\n\n    Returns:\n      (tensor) iou, sized [N,M].\n\n    Reference:\n      https://github.com/chainer/chainercv/blob/master/chainercv/utils/bbox/bbox_iou.py\n    """"""\n    if boxlist1.size != boxlist2.size:\n        raise RuntimeError(\n                ""boxlists should have same image size, got {}, {}"".format(boxlist1, boxlist2))\n\n    N = len(boxlist1)\n    M = len(boxlist2)\n\n    area1 = boxlist1.area()\n    area2 = boxlist2.area()\n\n    box1, box2 = boxlist1.bbox, boxlist2.bbox\n\n    lt = torch.max(box1[:, None, :2], box2[:, :2])  # [N,M,2]\n    rb = torch.min(box1[:, None, 2:], box2[:, 2:])  # [N,M,2]\n\n    TO_REMOVE = 1\n\n    wh = (rb - lt + TO_REMOVE).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n\n    iou = inter / (area1[:, None] + area2 - inter)\n    return iou\n\n\ndef boxlist_block_iou(boxlist1, boxlist2):\n    """"""Compute the block overlap of two set of boxes.\n    The box order must be (xmin, ymin, xmax, ymax).\n\n    Arguments:\n      box1: (BoxList) bounding boxes, sized [N,4].\n      box2: (BoxList) bounding boxes, sized [M,4].\n\n    Returns:\n      (tensor) iou, sized [N,M].\n\n    Reference:\n      https://github.com/chainer/chainercv/blob/master/chainercv/utils/bbox/bbox_iou.py\n    """"""\n    if boxlist1.size != boxlist2.size:\n        raise RuntimeError(\n                ""boxlists should have same image size, got {}, {}"".format(boxlist1, boxlist2))\n\n    box1, box2 = boxlist1.bbox, boxlist2.bbox\n\n    lt = torch.max(box1[:, None, :2], box2[:, :2])  # [N,M,2]\n    rb = torch.min(box1[:, None, 2:], box2[:, 2:])  # [N,M,2]\n\n    TO_REMOVE = 1\n\n    wh_inter = (rb - lt + TO_REMOVE).clamp(min=0)  # [N,M,2]\n    wh1 = box1[:, 2:] - box1[:, :2] + TO_REMOVE\n    wh2 = box2[:, 2:] - box2[:, :2] + TO_REMOVE\n\n    wh_iou = wh_inter / (wh1[:, None] + wh2 - wh_inter)\n    block_iou = torch.min(wh_iou, dim=2).values\n\n    return block_iou\n\n\n# TODO redundant, remove\ndef _cat(tensors, dim=0):\n    """"""\n    Efficient version of torch.cat that avoids a copy if there is only a single element in a list\n    """"""\n    assert isinstance(tensors, (list, tuple))\n    if len(tensors) == 1:\n        return tensors[0]\n    return torch.cat(tensors, dim)\n\n\ndef cat_boxlist(bboxes):\n    """"""\n    Concatenates a list of BoxList (having the same image size) into a\n    single BoxList\n\n    Arguments:\n        bboxes (list[BoxList])\n    """"""\n    assert isinstance(bboxes, (list, tuple))\n    assert all(isinstance(bbox, BoxList) for bbox in bboxes)\n\n    size = bboxes[0].size\n    assert all(bbox.size == size for bbox in bboxes)\n\n    mode = bboxes[0].mode\n    assert all(bbox.mode == mode for bbox in bboxes)\n\n    fields = set(bboxes[0].fields())\n    assert all(set(bbox.fields()) == fields for bbox in bboxes)\n\n    cat_boxes = BoxList(_cat([bbox.bbox for bbox in bboxes], dim=0), size, mode)\n\n    for field in fields:\n        data = _cat([bbox.get_field(field) for bbox in bboxes], dim=0)\n        cat_boxes.add_field(field, data)\n\n    return cat_boxes\n'"
maskrcnn_benchmark/structures/image_list.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom __future__ import division\n\nimport torch\n\n\nclass ImageList(object):\n    """"""\n    Structure that holds a list of images (of possibly\n    varying sizes) as a single tensor.\n    This works by padding the images to the same size,\n    and storing in a field the original sizes of each image\n    """"""\n\n    def __init__(self, tensors, image_sizes):\n        """"""\n        Arguments:\n            tensors (tensor)\n            image_sizes (list[tuple[int, int]])\n        """"""\n        self.tensors = tensors\n        self.image_sizes = image_sizes\n\n    def to(self, *args, **kwargs):\n        cast_tensor = self.tensors.to(*args, **kwargs)\n        return ImageList(cast_tensor, self.image_sizes)\n\n\ndef to_image_list(tensors, size_divisible=0):\n    """"""\n    tensors can be an ImageList, a torch.Tensor or\n    an iterable of Tensors. It can\'t be a numpy array.\n    When tensors is an iterable of Tensors, it pads\n    the Tensors with zeros so that they have the same\n    shape\n    """"""\n    if isinstance(tensors, torch.Tensor) and size_divisible > 0:\n        tensors = [tensors]\n\n    if isinstance(tensors, ImageList):\n        return tensors\n    elif isinstance(tensors, torch.Tensor):\n        # single tensor shape can be inferred\n        assert tensors.dim() == 4\n        image_sizes = [tensor.shape[-2:] for tensor in tensors]\n        return ImageList(tensors, image_sizes)\n    elif isinstance(tensors, (tuple, list)):\n        max_size = tuple(max(s) for s in zip(*[img.shape for img in tensors]))\n\n        # TODO Ideally, just remove this and let me model handle arbitrary\n        # input sizs\n        if size_divisible > 0:\n            import math\n\n            stride = size_divisible\n            max_size = list(max_size)\n            max_size[1] = int(math.ceil(max_size[1] / stride) * stride)\n            max_size[2] = int(math.ceil(max_size[2] / stride) * stride)\n            max_size = tuple(max_size)\n\n        batch_shape = (len(tensors),) + max_size\n        batched_imgs = tensors[0].new(*batch_shape).zero_()\n        for img, pad_img in zip(tensors, batched_imgs):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n\n        image_sizes = [im.shape[-2:] for im in tensors]\n\n        return ImageList(batched_imgs, image_sizes)\n    else:\n        raise TypeError(""Unsupported type for to_image_list: {}"".format(type(tensors)))\n'"
maskrcnn_benchmark/structures/segmentation_mask.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\n# transpose\nFLIP_LEFT_RIGHT = 0\nFLIP_TOP_BOTTOM = 1\n\n\nclass Mask(object):\n    """"""\n    This class is unfinished and not meant for use yet\n    It is supposed to contain the mask for an object as\n    a 2d tensor\n    """"""\n\n    def __init__(self, masks, size, mode):\n        self.masks = masks\n        self.size = size\n        self.mode = mode\n\n    def transpose(self, method):\n        if method not in (FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM):\n            raise NotImplementedError(\n                ""Only FLIP_LEFT_RIGHT and FLIP_TOP_BOTTOM implemented""\n            )\n\n        width, height = self.size\n        if method == FLIP_LEFT_RIGHT:\n            dim = width\n            idx = 2\n        elif method == FLIP_TOP_BOTTOM:\n            dim = height\n            idx = 1\n\n        flip_idx = list(range(dim)[::-1])\n        flipped_masks = self.masks.index_select(dim, flip_idx)\n        return Mask(flipped_masks, self.size, self.mode)\n\n    def crop(self, box):\n        w, h = box[2] - box[0], box[3] - box[1]\n\n        cropped_masks = self.masks[:, box[1] : box[3], box[0] : box[2]]\n        return Mask(cropped_masks, size=(w, h), mode=self.mode)\n\n    def resize(self, size, *args, **kwargs):\n        pass\n\n\nclass Polygons(object):\n    """"""\n    This class holds a set of polygons that represents a single instance\n    of an object mask. The object can be represented as a set of\n    polygons\n    """"""\n\n    def __init__(self, polygons, size, mode):\n        # assert isinstance(polygons, list), \'{}\'.format(polygons)\n        if isinstance(polygons, list):\n            polygons = [torch.as_tensor(p, dtype=torch.float32) for p in polygons]\n        elif isinstance(polygons, Polygons):\n            polygons = polygons.polygons\n\n        self.polygons = polygons\n        self.size = size\n        self.mode = mode\n\n    def transpose(self, method):\n        if method not in (FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM):\n            raise NotImplementedError(\n                ""Only FLIP_LEFT_RIGHT and FLIP_TOP_BOTTOM implemented""\n            )\n\n        flipped_polygons = []\n        width, height = self.size\n        if method == FLIP_LEFT_RIGHT:\n            dim = width\n            idx = 0\n        elif method == FLIP_TOP_BOTTOM:\n            dim = height\n            idx = 1\n\n        for poly in self.polygons:\n            p = poly.clone()\n            TO_REMOVE = 1\n            p[idx::2] = dim - poly[idx::2] - TO_REMOVE\n            flipped_polygons.append(p)\n\n        return Polygons(flipped_polygons, size=self.size, mode=self.mode)\n\n    def crop(self, box):\n        w, h = box[2] - box[0], box[3] - box[1]\n\n        # TODO chck if necessary\n        w = max(w, 1)\n        h = max(h, 1)\n\n        cropped_polygons = []\n        for poly in self.polygons:\n            p = poly.clone()\n            p[0::2] = p[0::2] - box[0]  # .clamp(min=0, max=w)\n            p[1::2] = p[1::2] - box[1]  # .clamp(min=0, max=h)\n            cropped_polygons.append(p)\n\n        return Polygons(cropped_polygons, size=(w, h), mode=self.mode)\n\n    def resize(self, size, *args, **kwargs):\n        ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(size, self.size))\n        if ratios[0] == ratios[1]:\n            ratio = ratios[0]\n            scaled_polys = [p * ratio for p in self.polygons]\n            return Polygons(scaled_polys, size, mode=self.mode)\n\n        ratio_w, ratio_h = ratios\n        scaled_polygons = []\n        for poly in self.polygons:\n            p = poly.clone()\n            p[0::2] *= ratio_w\n            p[1::2] *= ratio_h\n            scaled_polygons.append(p)\n\n        return Polygons(scaled_polygons, size=size, mode=self.mode)\n\n#     def convert(self, mode):\n#         width, height = self.size\n#         if mode == ""mask"":\n#             rles = mask_utils.frPyObjects(\n#                 [p.numpy() for p in self.polygons], height, width\n#             )\n#             rle = mask_utils.merge(rles)\n#             mask = mask_utils.decode(rle)\n#             mask = torch.from_numpy(mask)\n#             # TODO add squeeze?\n#             return mask\n\n    def __repr__(self):\n        s = self.__class__.__name__ + ""(""\n        s += ""num_polygons={}, "".format(len(self.polygons))\n        s += ""image_width={}, "".format(self.size[0])\n        s += ""image_height={}, "".format(self.size[1])\n        s += ""mode={})"".format(self.mode)\n        return s\n\n\nclass SegmentationMask(object):\n    """"""\n    This class stores the segmentations for all objects in the image\n    """"""\n\n    def __init__(self, polygons, size, mode=None):\n        """"""\n        Arguments:\n            polygons: a list of list of lists of numbers. The first\n                level of the list correspond to individual instances,\n                the second level to all the polygons that compose the\n                object, and the third level to the polygon coordinates.\n        """"""\n        assert isinstance(polygons, list)\n\n        self.polygons = [Polygons(p, size, mode) for p in polygons]\n        self.size = size\n        self.mode = mode\n\n    def transpose(self, method):\n        if method not in (FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM):\n            raise NotImplementedError(\n                ""Only FLIP_LEFT_RIGHT and FLIP_TOP_BOTTOM implemented""\n            )\n\n        flipped = []\n        for polygon in self.polygons:\n            flipped.append(polygon.transpose(method))\n        return SegmentationMask(flipped, size=self.size, mode=self.mode)\n\n    def crop(self, box):\n        w, h = box[2] - box[0], box[3] - box[1]\n        cropped = []\n        for polygon in self.polygons:\n            cropped.append(polygon.crop(box))\n        return SegmentationMask(cropped, size=(w, h), mode=self.mode)\n\n    def resize(self, size, *args, **kwargs):\n        scaled = []\n        for polygon in self.polygons:\n            scaled.append(polygon.resize(size, *args, **kwargs))\n        return SegmentationMask(scaled, size=size, mode=self.mode)\n\n    def to(self, *args, **kwargs):\n        return self\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, slice)):\n            selected_polygons = [self.polygons[item]]\n        else:\n            # advanced indexing on a single dimension\n            selected_polygons = []\n            if isinstance(item, torch.Tensor) and item.dtype == torch.uint8:\n                item = item.nonzero()\n                item = item.squeeze(1) if item.numel() > 0 else item\n                item = item.tolist()\n            for i in item:\n                selected_polygons.append(self.polygons[i])\n        return SegmentationMask(selected_polygons, size=self.size, mode=self.mode)\n\n    def __iter__(self):\n        return iter(self.polygons)\n\n    def __repr__(self):\n        s = self.__class__.__name__ + ""(""\n        s += ""num_instances={}, "".format(len(self.polygons))\n        s += ""image_width={}, "".format(self.size[0])\n        s += ""image_height={})"".format(self.size[1])\n        return s\n'"
maskrcnn_benchmark/utils/c2_model_loading.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport logging\nimport pickle\nfrom collections import OrderedDict\n\nimport torch\n\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\n\n\ndef _rename_basic_resnet_weights(layer_keys):\n    layer_keys = [k.replace(""_"", ""."") for k in layer_keys]\n    layer_keys = [k.replace("".w"", "".weight"") for k in layer_keys]\n    layer_keys = [k.replace("".bn"", ""_bn"") for k in layer_keys]\n    layer_keys = [k.replace("".b"", "".bias"") for k in layer_keys]\n    layer_keys = [k.replace(""_bn.s"", ""_bn.scale"") for k in layer_keys]\n    layer_keys = [k.replace("".biasranch"", "".branch"") for k in layer_keys]\n    layer_keys = [k.replace(""bbox.pred"", ""bbox_pred"") for k in layer_keys]\n    layer_keys = [k.replace(""cls.score"", ""cls_score"") for k in layer_keys]\n    layer_keys = [k.replace(""res.conv1_"", ""conv1_"") for k in layer_keys]\n\n    # RPN / Faster RCNN\n    layer_keys = [k.replace("".biasbox"", "".bbox"") for k in layer_keys]\n    layer_keys = [k.replace(""conv.rpn"", ""rpn.conv"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.bbox.pred"", ""rpn.bbox_pred"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.cls.logits"", ""rpn.cls_logits"") for k in layer_keys]\n\n    # Affine-Channel -> BatchNorm enaming\n    layer_keys = [k.replace(""_bn.scale"", ""_bn.weight"") for k in layer_keys]\n\n    # Make torchvision-compatible\n    layer_keys = [k.replace(""conv1_bn."", ""bn1."") for k in layer_keys]\n\n    layer_keys = [k.replace(""res2."", ""layer1."") for k in layer_keys]\n    layer_keys = [k.replace(""res3."", ""layer2."") for k in layer_keys]\n    layer_keys = [k.replace(""res4."", ""layer3."") for k in layer_keys]\n    layer_keys = [k.replace(""res5."", ""layer4."") for k in layer_keys]\n\n    layer_keys = [k.replace("".branch2a."", "".conv1."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2a_bn."", "".bn1."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2b."", "".conv2."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2b_bn."", "".bn2."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2c."", "".conv3."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2c_bn."", "".bn3."") for k in layer_keys]\n\n    layer_keys = [k.replace("".branch1."", "".downsample.0."") for k in layer_keys]\n    layer_keys = [k.replace("".branch1_bn."", "".downsample.1."") for k in layer_keys]\n\n    return layer_keys\n\ndef _rename_fpn_weights(layer_keys, stage_names):\n    for mapped_idx, stage_name in enumerate(stage_names, 1):\n        suffix = """"\n        if mapped_idx < 4:\n            suffix = "".lateral""\n        layer_keys = [\n            k.replace(""fpn.inner.layer{}.sum{}"".format(stage_name, suffix), ""fpn_inner{}"".format(mapped_idx)) for k in layer_keys\n        ]\n        layer_keys = [k.replace(""fpn.layer{}.sum"".format(stage_name), ""fpn_layer{}"".format(mapped_idx)) for k in layer_keys]\n\n\n    layer_keys = [k.replace(""rpn.conv.fpn2"", ""rpn.conv"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.bbox_pred.fpn2"", ""rpn.bbox_pred"") for k in layer_keys]\n    layer_keys = [\n        k.replace(""rpn.cls_logits.fpn2"", ""rpn.cls_logits"") for k in layer_keys\n    ]\n\n    return layer_keys\n\n\ndef _rename_weights_for_resnet(weights, stage_names):\n    original_keys = sorted(weights.keys())\n    layer_keys = sorted(weights.keys())\n\n    # for X-101, rename output to fc1000 to avoid conflicts afterwards\n    layer_keys = [k if k != ""pred_b"" else ""fc1000_b"" for k in layer_keys]\n    layer_keys = [k if k != ""pred_w"" else ""fc1000_w"" for k in layer_keys]\n\n    # performs basic renaming: _ -> . , etc\n    layer_keys = _rename_basic_resnet_weights(layer_keys)\n\n    # FPN\n    layer_keys = _rename_fpn_weights(layer_keys, stage_names)\n\n    # Mask R-CNN\n    layer_keys = [k.replace(""mask.fcn.logits"", ""mask_fcn_logits"") for k in layer_keys]\n    layer_keys = [k.replace("".[mask].fcn"", ""mask_fcn"") for k in layer_keys]\n    layer_keys = [k.replace(""conv5.mask"", ""conv5_mask"") for k in layer_keys]\n\n    # Keypoint R-CNN\n    layer_keys = [k.replace(""kps.score.lowres"", ""kps_score_lowres"") for k in layer_keys]\n    layer_keys = [k.replace(""kps.score"", ""kps_score"") for k in layer_keys]\n    layer_keys = [k.replace(""conv.fcn"", ""conv_fcn"") for k in layer_keys]\n\n    # Rename for our RPN structure\n    layer_keys = [k.replace(""rpn."", ""rpn.head."") for k in layer_keys]\n\n    key_map = {k: v for k, v in zip(original_keys, layer_keys)}\n\n    logger = logging.getLogger(__name__)\n    logger.info(""Remapping C2 weights"")\n    max_c2_key_size = max([len(k) for k in original_keys if ""_momentum"" not in k])\n\n    new_weights = OrderedDict()\n    for k in original_keys:\n        v = weights[k]\n        if ""_momentum"" in k or k == ""lr"" or k == ""weight_order"":\n            continue\n        # if \'fc1000\' in k:\n        #     continue\n        w = torch.from_numpy(v)\n        # if ""bn"" in k:\n        #     w = w.view(1, -1, 1, 1)\n        logger.info(""C2 name: {: <{}} mapped name: {}"".format(k, max_c2_key_size, key_map[k]))\n        new_weights[key_map[k]] = w\n\n    return new_weights\n\n\ndef _load_c2_pickled_weights(file_path):\n    with open(file_path, ""rb"") as f:\n        if torch._six.PY3:\n            data = pickle.load(f, encoding=""latin1"")\n        else:\n            data = pickle.load(f)\n    if ""blobs"" in data:\n        weights = data[""blobs""]\n    else:\n        weights = data\n    return weights\n\n\n_C2_STAGE_NAMES = {\n    ""R-50"": [""1.2"", ""2.3"", ""3.5"", ""4.2""],\n    ""R-101"": [""1.2"", ""2.3"", ""3.22"", ""4.2""],\n}\n\ndef load_c2_format(cfg, f):\n    # TODO make it support other architectures\n    state_dict = _load_c2_pickled_weights(f)\n    conv_body = cfg.MODEL.BACKBONE.CONV_BODY\n    arch = conv_body.replace(""-C4"", """").replace(""-FPN"", """")\n    stages = _C2_STAGE_NAMES[arch]\n    state_dict = _rename_weights_for_resnet(state_dict, stages)\n    return dict(model=state_dict)\n'"
maskrcnn_benchmark/utils/checkpoint.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport logging\nimport os\n\nimport torch\n\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\nfrom maskrcnn_benchmark.utils.c2_model_loading import load_c2_format\nfrom maskrcnn_benchmark.utils.imports import import_file\nfrom maskrcnn_benchmark.utils.model_zoo import cache_url\n\n\nclass Checkpointer(object):\n    def __init__(\n        self,\n        model,\n        optimizer=None,\n        scheduler=None,\n        save_dir="""",\n        save_to_disk=None,\n        logger=None,\n    ):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.save_dir = save_dir\n        self.save_to_disk = save_to_disk\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        self.logger = logger\n\n    def save(self, name, **kwargs):\n        if not self.save_dir:\n            return\n\n        if not self.save_to_disk:\n            return\n\n        data = {}\n        data[""model""] = self.model.state_dict()\n        if self.optimizer is not None:\n            data[""optimizer""] = self.optimizer.state_dict()\n        if self.scheduler is not None:\n            data[""scheduler""] = self.scheduler.state_dict()\n        data.update(kwargs)\n\n        save_file = os.path.join(self.save_dir, ""{}.pth"".format(name))\n        self.logger.info(""Saving checkpoint to {}"".format(save_file))\n        torch.save(data, save_file)\n        self.tag_last_checkpoint(save_file)\n\n    def load(self, f=None):\n        if self.has_checkpoint():\n            # override argument with existing checkpoint\n            f = self.get_checkpoint_file()\n        if not f:\n            # no checkpoint could be found\n            self.logger.info(""No checkpoint found. Initializing model from scratch"")\n            return {}\n        self.logger.info(""Loading checkpoint from {}"".format(f))\n        checkpoint = self._load_file(f)\n        if not self.has_checkpoint():\n            checkpoint = {""model"": checkpoint[""model""]}\n        self._load_model(checkpoint)\n        if ""optimizer"" in checkpoint and self.optimizer:\n            self.logger.info(""Loading optimizer from {}"".format(f))\n            self.optimizer.load_state_dict(checkpoint.pop(""optimizer""))\n        if ""scheduler"" in checkpoint and self.scheduler:\n            self.logger.info(""Loading scheduler from {}"".format(f))\n            self.scheduler.load_state_dict(checkpoint.pop(""scheduler""))\n\n        # return any further checkpoint data\n        return checkpoint\n\n    def has_checkpoint(self):\n        save_file = os.path.join(self.save_dir, ""last_checkpoint"")\n        return os.path.exists(save_file)\n\n    def get_checkpoint_file(self):\n        save_file = os.path.join(self.save_dir, ""last_checkpoint"")\n        try:\n            with open(save_file, ""r"") as f:\n                last_saved = f.read()\n        except IOError:\n            # if file doesn\'t exist, maybe because it has just been\n            # deleted by a separate process\n            last_saved = """"\n        return last_saved\n\n    def tag_last_checkpoint(self, last_filename):\n        save_file = os.path.join(self.save_dir, ""last_checkpoint"")\n        with open(save_file, ""w"") as f:\n            f.write(last_filename)\n\n    def _load_file(self, f):\n        return torch.load(f, map_location=torch.device(""cpu""))\n\n    def _load_model(self, checkpoint):\n        load_state_dict(self.model, checkpoint.pop(""model""))\n\n\nclass DetectronCheckpointer(Checkpointer):\n    def __init__(\n        self,\n        cfg,\n        model,\n        optimizer=None,\n        scheduler=None,\n        save_dir="""",\n        save_to_disk=None,\n        logger=None,\n    ):\n        super(DetectronCheckpointer, self).__init__(\n            model, optimizer, scheduler, save_dir, save_to_disk, logger\n        )\n        self.cfg = cfg.clone()\n\n    def _load_file(self, f):\n        # catalog lookup\n        if f.startswith(""catalog://""):\n            paths_catalog = import_file(\n                ""maskrcnn_benchmark.config.paths_catalog"", self.cfg.PATHS_CATALOG, True\n            )\n            catalog_f = paths_catalog.ModelCatalog.get(f[len(""catalog://"") :])\n            self.logger.info(""{} points to {}"".format(f, catalog_f))\n            f = catalog_f\n        # download url files\n        if f.startswith(""http""):\n            # if the file is a url path, download it and cache it\n            cached_f = cache_url(f)\n            self.logger.info(""url {} cached in {}"".format(f, cached_f))\n            f = cached_f\n        # convert Caffe2 checkpoint from pkl\n        if f.endswith("".pkl""):\n            return load_c2_format(self.cfg, f)\n        # load native detectron.pytorch checkpoint\n        loaded = super(DetectronCheckpointer, self)._load_file(f)\n        if ""model"" not in loaded:\n            loaded = dict(model=loaded)\n        return loaded\n'"
maskrcnn_benchmark/utils/collect_env.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport PIL\n\nfrom torch.utils.collect_env import get_pretty_env_info\n\n\ndef get_pil_version():\n    return ""\\n        Pillow ({})"".format(PIL.__version__)\n\n\ndef collect_env_info():\n    env_str = get_pretty_env_info()\n    env_str += get_pil_version()\n    return env_str\n'"
maskrcnn_benchmark/utils/comm.py,22,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nThis file contains primitives for multi-gpu communication.\nThis is useful when doing distributed training.\n""""""\n\nimport os\nimport pickle\nimport tempfile\nimport time\n\nimport torch\n\n\ndef get_world_size():\n    if not torch.distributed.is_initialized():\n        return 1\n    return torch.distributed.get_world_size()\n\n\ndef get_rank():\n    if not torch.distributed.is_initialized():\n        return 0\n    return torch.distributed.get_rank()\n\n\ndef is_main_process():\n    if not torch.distributed.is_initialized():\n        return True\n    return torch.distributed.get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize between multiple processes when\n    using distributed training\n    """"""\n    if not torch.distributed.is_initialized():\n        return\n    world_size = torch.distributed.get_world_size()\n    rank = torch.distributed.get_rank()\n    if world_size == 1:\n        return\n\n    def _send_and_wait(r):\n        if rank == r:\n            tensor = torch.tensor(0, device=""cuda"")\n        else:\n            tensor = torch.tensor(1, device=""cuda"")\n        torch.distributed.broadcast(tensor, r)\n        while tensor.item() == 1:\n            time.sleep(1)\n\n    _send_and_wait(0)\n    # now sync on the main process\n    _send_and_wait(1)\n\n\ndef _encode(encoded_data, data):\n    # gets a byte representation for the data\n    encoded_bytes = pickle.dumps(data)\n    # convert this byte string into a byte tensor\n    storage = torch.ByteStorage.from_buffer(encoded_bytes)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n    # encoding: first byte is the size and then rest is the data\n    s = tensor.numel()\n    assert s <= 255, ""Can\'t encode data greater than 255 bytes""\n    # put the encoded data in encoded_data\n    encoded_data[0] = s\n    encoded_data[1 : (s + 1)] = tensor\n\n\ndef _decode(encoded_data):\n    size = encoded_data[0]\n    encoded_tensor = encoded_data[1 : (size + 1)].to(""cpu"")\n    return pickle.loads(bytearray(encoded_tensor.tolist()))\n\n\n# TODO try to use tensor in shared-memory instead of serializing to disk\n# this involves getting the all_gather to work\ndef scatter_gather(data):\n    """"""\n    This function gathers data from multiple processes, and returns them\n    in a list, as they were obtained from each process.\n\n    This function is useful for retrieving data from multiple processes,\n    when launching the code with torch.distributed.launch\n\n    Note: this function is slow and should not be used in tight loops, i.e.,\n    do not use it in the training loop.\n\n    Arguments:\n        data: the object to be gathered from multiple processes.\n            It must be serializable\n\n    Returns:\n        result (list): a list with as many elements as there are processes,\n            where each element i in the list corresponds to the data that was\n            gathered from the process of rank i.\n    """"""\n    # strategy: the main process creates a temporary directory, and communicates\n    # the location of the temporary directory to all other processes.\n    # each process will then serialize the data to the folder defined by\n    # the main process, and then the main process reads all of the serialized\n    # files and returns them in a list\n    if not torch.distributed.is_initialized():\n        return [data]\n    synchronize()\n    # get rank of the current process\n    rank = torch.distributed.get_rank()\n\n    # the data to communicate should be small\n    data_to_communicate = torch.empty(256, dtype=torch.uint8, device=""cuda"")\n    if rank == 0:\n        # manually creates a temporary directory, that needs to be cleaned\n        # afterwards\n        tmp_dir = tempfile.mkdtemp()\n        _encode(data_to_communicate, tmp_dir)\n\n    synchronize()\n    # the main process (rank=0) communicates the data to all processes\n    torch.distributed.broadcast(data_to_communicate, 0)\n\n    # get the data that was communicated\n    tmp_dir = _decode(data_to_communicate)\n\n    # each process serializes to a different file\n    file_template = ""file{}.pth""\n    tmp_file = os.path.join(tmp_dir, file_template.format(rank))\n    torch.save(data, tmp_file)\n\n    # synchronize before loading the data\n    synchronize()\n\n    # only the master process returns the data\n    if rank == 0:\n        data_list = []\n        world_size = torch.distributed.get_world_size()\n        for r in range(world_size):\n            file_path = os.path.join(tmp_dir, file_template.format(r))\n            d = torch.load(file_path)\n            data_list.append(d)\n            # cleanup\n            os.remove(file_path)\n        # cleanup\n        os.rmdir(tmp_dir)\n        return data_list\n'"
maskrcnn_benchmark/utils/env.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport os\n\nfrom maskrcnn_benchmark.utils.imports import import_file\n\n\ndef setup_environment():\n    """"""Perform environment setup work. The default setup is a no-op, but this\n    function allows the user to specify a Python source file that performs\n    custom setup work that may be necessary to their computing environment.\n    """"""\n    custom_module_path = os.environ.get(""TORCH_DETECTRON_ENV_MODULE"")\n    if custom_module_path:\n        setup_custom_environment(custom_module_path)\n    else:\n        # The default setup is a no-op\n        pass\n\n\ndef setup_custom_environment(custom_module_path):\n    """"""Load custom environment setup from a Python source file and run the setup\n    function.\n    """"""\n    module = import_file(""maskrcnn_benchmark.utils.env.custom_module"", custom_module_path)\n    assert hasattr(module, ""setup_environment"") and callable(\n        module.setup_environment\n    ), (\n        ""Custom environment module defined in {} does not have the ""\n        ""required callable attribute \'setup_environment\'.""\n    ).format(\n        custom_module_path\n    )\n    module.setup_environment()\n\n\n# Force environment setup when this module is imported\nsetup_environment()\n'"
maskrcnn_benchmark/utils/imports.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nif torch._six.PY3:\n    import importlib\n    import importlib.util\n    import sys\n\n\n    # from https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n    def import_file(module_name, file_path, make_importable=False):\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        if make_importable:\n            sys.modules[module_name] = module\n        return module\nelse:\n    import imp\n\n    def import_file(module_name, file_path, make_importable=None):\n        module = imp.load_source(module_name, file_path)\n        return module\n'"
maskrcnn_benchmark/utils/logger.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport logging\nimport os\nimport sys\n\n\ndef setup_logger(name, save_dir, distributed_rank):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    # don\'t log results for the non-master process\n    if distributed_rank > 0:\n        return logger\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if save_dir:\n        fh = logging.FileHandler(os.path.join(save_dir, ""log.txt""))\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n'"
maskrcnn_benchmark/utils/metric_logger.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom collections import defaultdict\nfrom collections import deque\n\nimport torch\n\n\nclass SmoothedValue(object):\n    """"""Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    """"""\n\n    def __init__(self, window_size=20):\n        self.deque = deque(maxlen=window_size)\n        self.series = []\n        self.total = 0.0\n        self.count = 0\n\n    def update(self, value):\n        self.deque.append(value)\n        self.series.append(value)\n        self.count += 1\n        self.total += value\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque))\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=""\\t""):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        return object.__getattr__(self, attr)\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                ""{}: {:.4f} ({:.4f})"".format(name, meter.median, meter.global_avg)\n            )\n        return self.delimiter.join(loss_str)\n'"
maskrcnn_benchmark/utils/miscellaneous.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport errno\nimport os\n\n\ndef mkdir(path):\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n'"
maskrcnn_benchmark/utils/model_serialization.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom collections import OrderedDict\nimport logging\n\nimport torch\n\nfrom maskrcnn_benchmark.utils.imports import import_file\n\n\ndef align_and_update_state_dicts(model_state_dict, loaded_state_dict):\n    """"""\n    Strategy: suppose that the models that we will create will have prefixes appended\n    to each of its keys, for example due to an extra level of nesting that the original\n    pre-trained weights from ImageNet won\'t contain. For example, model.state_dict()\n    might return backbone[0].body.res2.conv1.weight, while the pre-trained model contains\n    res2.conv1.weight. We thus want to match both parameters together.\n    For that, we look for each model weight, look among all loaded keys if there is one\n    that is a suffix of the current weight name, and use it if that\'s the case.\n    If multiple matches exist, take the one with longest size\n    of the corresponding name. For example, for the same model as before, the pretrained\n    weight file can contain both res2.conv1.weight, as well as conv1.weight. In this case,\n    we want to match backbone[0].body.conv1.weight to conv1.weight, and\n    backbone[0].body.res2.conv1.weight to res2.conv1.weight.\n    """"""\n    current_keys = sorted(list(model_state_dict.keys()))\n    loaded_keys = sorted(list(loaded_state_dict.keys()))\n    # get a matrix of string matches, where each (i, j) entry correspond to the size of the\n    # loaded_key string, if it matches\n    match_matrix = [\n        len(j) if i.endswith(j) else 0 for i in current_keys for j in loaded_keys\n    ]\n    match_matrix = torch.as_tensor(match_matrix).view(\n        len(current_keys), len(loaded_keys)\n    )\n    max_match_size, idxs = match_matrix.max(1)\n    # remove indices that correspond to no-match\n    idxs[max_match_size == 0] = -1\n\n    # used for logging\n    max_size = max([len(key) for key in current_keys]) if current_keys else 1\n    max_size_loaded = max([len(key) for key in loaded_keys]) if loaded_keys else 1\n    log_str_template = ""{: <{}} loaded from {: <{}} of shape {}""\n    logger = logging.getLogger(__name__)\n    for idx_new, idx_old in enumerate(idxs.tolist()):\n        if idx_old == -1:\n            continue\n        key = current_keys[idx_new]\n        key_old = loaded_keys[idx_old]\n        model_state_dict[key] = loaded_state_dict[key_old]\n        logger.info(\n            log_str_template.format(\n                key,\n                max_size,\n                key_old,\n                max_size_loaded,\n                tuple(loaded_state_dict[key_old].shape),\n            )\n        )\n\n\ndef strip_prefix_if_present(state_dict, prefix):\n    keys = sorted(state_dict.keys())\n    if not all(key.startswith(prefix) for key in keys):\n        return state_dict\n    stripped_state_dict = OrderedDict()\n    for key, value in state_dict.items():\n        stripped_state_dict[key.replace(prefix, """")] = value\n    return stripped_state_dict\n\n\ndef load_state_dict(model, loaded_state_dict):\n    model_state_dict = model.state_dict()\n    # if the state_dict comes from a model that was wrapped in a\n    # DataParallel or DistributedDataParallel during serialization,\n    # remove the ""module"" prefix before performing the matching\n    loaded_state_dict = strip_prefix_if_present(loaded_state_dict, prefix=""module."")\n    align_and_update_state_dicts(model_state_dict, loaded_state_dict)\n\n    # use strict loading\n    model.load_state_dict(model_state_dict)\n'"
maskrcnn_benchmark/utils/model_zoo.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport os\nimport sys\n\nfrom torch.hub import _download_url_to_file\nfrom torch.hub import urlparse\nfrom torch.hub import HASH_REGEX\n\nfrom maskrcnn_benchmark.utils.comm import is_main_process\nfrom maskrcnn_benchmark.utils.comm import synchronize\n\n\n# very similar to https://github.com/pytorch/pytorch/blob/master/torch/utils/model_zoo.py\n# but with a few improvements and modifications\ndef cache_url(url, model_dir=None, progress=True):\n    r""""""Loads the Torch serialized object at the given URL.\n    If the object is already present in `model_dir`, it\'s deserialized and\n    returned. The filename part of the URL should follow the naming convention\n    ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n    digits of the SHA256 hash of the contents of the file. The hash is used to\n    ensure unique names and to verify the contents of the file.\n    The default value of `model_dir` is ``$TORCH_HOME/models`` where\n    ``$TORCH_HOME`` defaults to ``~/.torch``. The default directory can be\n    overridden with the ``$TORCH_MODEL_ZOO`` environment variable.\n    Args:\n        url (string): URL of the object to download\n        model_dir (string, optional): directory in which to save the object\n        progress (bool, optional): whether or not to display a progress bar to stderr\n    Example:\n        >>> cached_file = maskrcnn_benchmark.utils.model_zoo.cache_url(\'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\')\n    """"""\n    if model_dir is None:\n        torch_home = os.path.expanduser(os.getenv(\'TORCH_HOME\', \'~/.torch\'))\n        model_dir = os.getenv(\'TORCH_MODEL_ZOO\', os.path.join(torch_home, \'models\'))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    parts = urlparse(url)\n    filename = os.path.basename(parts.path)\n    if filename == ""model_final.pkl"":\n        # workaround as pre-trained Caffe2 models from Detectron have all the same filename\n        # so make the full path the filename by replacing / with _\n        filename = parts.path.replace(""/"", ""_"")\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file) and is_main_process():\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        hash_prefix = HASH_REGEX.search(filename)\n        if hash_prefix is not None:\n            hash_prefix = hash_prefix.group(1)\n            # workaround: Caffe2 models don\'t have a hash, but follow the R-50 convention,\n            # which matches the hash PyTorch uses. So we skip the hash matching\n            # if the hash_prefix is less than 6 characters\n            if len(hash_prefix) < 6:\n                hash_prefix = None\n        _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n    synchronize()\n    return cached_file\n'"
maskrcnn_benchmark/data/datasets/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .coco import COCODataset\nfrom .concat_dataset import ConcatDataset\n\n__all__ = [""COCODataset"", ""ConcatDataset""]\n'"
maskrcnn_benchmark/data/datasets/coco.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nimport torchvision\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\nfrom maskrcnn_benchmark.structures.segmentation_mask import SegmentationMask\n\n\nclass COCODataset(torchvision.datasets.coco.CocoDetection):\n    def __init__(\n        self, ann_file, root, remove_images_without_annotations, transforms=None\n    ):\n        super(COCODataset, self).__init__(root, ann_file)\n\n        # sort indices for reproducible results\n        self.ids = sorted(self.ids)\n\n        # filter images without detection annotations\n        if remove_images_without_annotations:\n            self.ids = [\n                img_id\n                for img_id in self.ids\n                if len(self.coco.getAnnIds(imgIds=img_id, iscrowd=None)) > 0\n            ]\n\n        self.json_category_id_to_contiguous_id = {\n            v: i + 1 for i, v in enumerate(self.coco.getCatIds())\n        }\n        self.contiguous_category_id_to_json_id = {\n            v: k for k, v in self.json_category_id_to_contiguous_id.items()\n        }\n        self.id_to_img_map = {k: v for k, v in enumerate(self.ids)}\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        img, anno = super(COCODataset, self).__getitem__(idx)\n\n        # filter crowd annotations\n        # TODO might be better to add an extra field\n        anno = [obj for obj in anno if obj[""iscrowd""] == 0]\n\n        boxes = [obj[""bbox""] for obj in anno]\n        boxes = torch.as_tensor(boxes).reshape(-1, 4)  # guard against no boxes\n        target = BoxList(boxes, img.size, mode=""xywh"").convert(""xyxy"")\n\n        classes = [obj[""category_id""] for obj in anno]\n        classes = [self.json_category_id_to_contiguous_id[c] for c in classes]\n        classes = torch.tensor(classes)\n        target.add_field(""labels"", classes)\n\n#         masks = [obj[""segmentation""] for obj in anno]\n#         masks = SegmentationMask(masks, img.size)\n#         target.add_field(""masks"", masks)\n\n        target = target.clip_to_image(remove_empty=True)\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target, idx\n\n    def get_img_info(self, index):\n        img_id = self.id_to_img_map[index]\n        img_data = self.coco.imgs[img_id]\n        return img_data\n'"
maskrcnn_benchmark/data/datasets/concat_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport bisect\n\nfrom torch.utils.data.dataset import ConcatDataset as _ConcatDataset\n\n\nclass ConcatDataset(_ConcatDataset):\n    """"""\n    Same as torch.utils.data.dataset.ConcatDataset, but exposes an extra\n    method for querying the sizes of the image\n    """"""\n\n    def get_idxs(self, idx):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return dataset_idx, sample_idx\n\n    def get_img_info(self, idx):\n        dataset_idx, sample_idx = self.get_idxs(idx)\n        return self.datasets[dataset_idx].get_img_info(sample_idx)\n'"
maskrcnn_benchmark/data/datasets/list_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nSimple dataset class that wraps a list of path names\n""""""\n\nfrom PIL import Image\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\n\nclass ListDataset(object):\n    def __init__(self, image_lists, transforms=None):\n        self.image_lists = image_lists\n        self.transforms = transforms\n\n    def __getitem__(self, item):\n        img = Image.open(self.image_lists[item]).convert(""RGB"")\n\n        # dummy target\n        w, h = img.size\n        target = BoxList([[0, 0, w, h]], img.size, mode=""xyxy"")\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_lists)\n\n    def get_img_info(self, item):\n        """"""\n        Return the image dimensions for the image, without\n        loading and pre-processing it\n        """"""\n        pass\n'"
maskrcnn_benchmark/data/samplers/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .distributed import DistributedSampler\nfrom .grouped_batch_sampler import GroupedBatchSampler\nfrom .iteration_based_batch_sampler import IterationBasedBatchSampler\n\n__all__ = [""DistributedSampler"", ""GroupedBatchSampler"", ""IterationBasedBatchSampler""]\n'"
maskrcnn_benchmark/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed,\n# with a modification in the import to use the deprecated backend\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = True\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
maskrcnn_benchmark/data/samplers/grouped_batch_sampler.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\n\nimport torch\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupedBatchSampler(BatchSampler):\n    """"""\n    Wraps another sampler to yield a mini-batch of indices.\n    It enforces that elements from the same group should appear in groups of batch_size.\n    It also tries to provide mini-batches which follows an ordering which is\n    as close as possible to the ordering from the original sampler.\n\n    Arguments:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_uneven (bool): If ``True``, the sampler will drop the batches whose\n            size is less than ``batch_size``\n\n    """"""\n\n    def __init__(self, sampler, group_ids, batch_size, drop_uneven=False):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                ""sampler should be an instance of ""\n                ""torch.utils.data.Sampler, but got sampler={}"".format(sampler)\n            )\n        self.sampler = sampler\n        self.group_ids = torch.as_tensor(group_ids)\n        assert self.group_ids.dim() == 1\n        self.batch_size = batch_size\n        self.drop_uneven = drop_uneven\n\n        self.groups = torch.unique(self.group_ids).sort(0)[0]\n\n        self._can_reuse_batches = False\n\n    def _prepare_batches(self):\n        dataset_size = len(self.group_ids)\n        # get the sampled indices from the sampler\n        sampled_ids = torch.as_tensor(list(self.sampler))\n        # potentially not all elements of the dataset were sampled\n        # by the sampler (e.g., DistributedSampler).\n        # construct a tensor which contains -1 if the element was\n        # not sampled, and a non-negative number indicating the\n        # order where the element was sampled.\n        # for example. if sampled_ids = [3, 1] and dataset_size = 5,\n        # the order is [-1, 1, -1, 0, -1]\n        order = torch.full((dataset_size,), -1, dtype=torch.int64)\n        order[sampled_ids] = torch.arange(len(sampled_ids))\n\n        # get a mask with the elements that were sampled\n        mask = order >= 0\n\n        # find the elements that belong to each individual cluster\n        clusters = [(self.group_ids == i) & mask for i in self.groups]\n        # get relative order of the elements inside each cluster\n        # that follows the order from the sampler\n        relative_order = [order[cluster] for cluster in clusters]\n        # with the relative order, find the absolute order in the\n        # sampled space\n        permutation_ids = [s[s.sort()[1]] for s in relative_order]\n        # permute each cluster so that they follow the order from\n        # the sampler\n        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]\n\n        # splits each cluster in batch_size, and merge as a list of tensors\n        splits = [c.split(self.batch_size) for c in permuted_clusters]\n        merged = tuple(itertools.chain.from_iterable(splits))\n\n        # now each batch internally has the right order, but\n        # they are grouped by clusters. Find the permutation between\n        # different batches that brings them as close as possible to\n        # the order that we have in the sampler. For that, we will consider the\n        # ordering as coming from the first element of each batch, and sort\n        # correspondingly\n        first_element_of_batch = [t[0].item() for t in merged]\n        # get and inverse mapping from sampled indices and the position where\n        # they occur (as returned by the sampler)\n        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}\n        # from the first element in each batch, get a relative ordering\n        first_index_of_batch = torch.as_tensor(\n            [inv_sampled_ids_map[s] for s in first_element_of_batch]\n        )\n\n        # permute the batches so that they approximately follow the order\n        # from the sampler\n        permutation_order = first_index_of_batch.sort(0)[1].tolist()\n        # finally, permute the batches\n        batches = [merged[i].tolist() for i in permutation_order]\n\n        if self.drop_uneven:\n            kept = []\n            for batch in batches:\n                if len(batch) == self.batch_size:\n                    kept.append(batch)\n            batches = kept\n        return batches\n\n    def __iter__(self):\n        if self._can_reuse_batches:\n            batches = self._batches\n            self._can_reuse_batches = False\n        else:\n            batches = self._prepare_batches()\n        self._batches = batches\n        return iter(batches)\n\n    def __len__(self):\n        if not hasattr(self, ""_batches""):\n            self._batches = self._prepare_batches()\n            self._can_reuse_batches = True\n        return len(self._batches)\n'"
maskrcnn_benchmark/data/samplers/iteration_based_batch_sampler.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom torch.utils.data.sampler import BatchSampler\n\n\nclass IterationBasedBatchSampler(BatchSampler):\n    """"""\n    Wraps a BatchSampler, resampling from it until\n    a specified number of iterations have been sampled\n    """"""\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, ""set_epoch""):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n'"
maskrcnn_benchmark/data/transforms/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .transforms import Compose\nfrom .transforms import Resize\nfrom .transforms import RandomHorizontalFlip\nfrom .transforms import ToTensor\nfrom .transforms import Normalize\n\nfrom .build import build_transforms\n\n'"
maskrcnn_benchmark/data/transforms/build.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom . import transforms as T\n\n\ndef build_transforms(cfg, is_train=True):\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN\n        flip_prob = 0.5  # cfg.INPUT.FLIP_PROB_TRAIN\n        resize = T.MultiScaleResize(min_size, max_size)\n    else:\n        min_size = cfg.INPUT.MIN_SIZE_TEST\n        max_size = cfg.INPUT.MAX_SIZE_TEST\n        flip_prob = 0\n        resize = T.Resize(min_size, max_size)\n\n    to_bgr255 = cfg.INPUT.TO_BGR255\n    normalize_transform = T.Normalize(\n        mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD, to_bgr255=to_bgr255\n    )\n\n    transform = T.Compose(\n        [\n            resize,\n            T.RandomHorizontalFlip(flip_prob),\n            T.ToTensor(),\n            normalize_transform,\n        ]\n    )\n    return transform\n'"
maskrcnn_benchmark/data/transforms/transforms.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport random\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, min_size, max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    # modified from torchvision to add support for max size\n    def get_size(self, image_size):\n        w, h = image_size\n        size = self.min_size\n        max_size = self.max_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(round(max_size * min_original_size / max_original_size))\n\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (h, w)\n\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n\n        return (oh, ow)\n\n    def __call__(self, image, target):\n        size = self.get_size(image.size)\n        image = F.resize(image, size)\n        target = target.resize(image.size)\n        return image, target\n\n\nclass MultiScaleResize(object):\n    def __init__(self, min_sizes, max_size):\n        self.resizers = []\n        for min_size in min_sizes:\n            self.resizers.append(Resize(min_size, max_size))\n\n    def __call__(self, image, target):\n        resizer = random.choice(self.resizers)\n        image, target = resizer(image, target)\n\n        return image, target\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            image = F.hflip(image)\n            target = target.transpose(0)\n        return image, target\n\n\nclass ToTensor(object):\n    def __call__(self, image, target):\n        return F.to_tensor(image), target\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, to_bgr255=True):\n        self.mean = mean\n        self.std = std\n        self.to_bgr255 = to_bgr255\n\n    def __call__(self, image, target):\n        if self.to_bgr255:\n            image = image[[2, 1, 0]] * 255\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, target\n'"
maskrcnn_benchmark/modeling/backbone/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .backbone import build_backbone\n'"
maskrcnn_benchmark/modeling/backbone/backbone.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom collections import OrderedDict\n\nfrom torch import nn\n\nfrom . import fpn as fpn_module\nfrom . import resnet\n\n\ndef build_resnet_backbone(cfg):\n    body = resnet.ResNet(cfg)\n    model = nn.Sequential(OrderedDict([(""body"", body)]))\n    return model\n\n\ndef build_resnet_fpn_backbone(cfg):\n    body = resnet.ResNet(cfg)\n    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n    out_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS\n    fpn = fpn_module.FPN(\n        in_channels_list=[\n            in_channels_stage2,\n            in_channels_stage2 * 2,\n            in_channels_stage2 * 4,\n            in_channels_stage2 * 8,\n        ],\n        out_channels=out_channels,\n        top_blocks=fpn_module.LastLevelMaxPool(),\n        use_gn=cfg.MODEL.USE_GN\n    )\n    model = nn.Sequential(OrderedDict([(""body"", body), (""fpn"", fpn)]))\n    return model\n\n\ndef build_resnet_fpn_p3p7_backbone(cfg):\n    body = resnet.ResNet(cfg)\n    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n    out_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS\n    fpn = fpn_module.FPN(\n        in_channels_list=[\n            0,\n            in_channels_stage2 * 2,\n            in_channels_stage2 * 4,\n            in_channels_stage2 * 8,\n        ],\n        out_channels=out_channels,\n        top_blocks=fpn_module.LastLevelP6P7(out_channels),\n        use_gn=cfg.MODEL.USE_GN\n    )\n    model = nn.Sequential(OrderedDict([(""body"", body), (""fpn"", fpn)]))\n    return model\n\n\n_BACKBONES = {""resnet"": build_resnet_backbone,\n              ""resnet-fpn"": build_resnet_fpn_backbone,\n              ""resnet-fpn-retina"": build_resnet_fpn_p3p7_backbone,\n             }\n\n\ndef build_resnet_fpn_p2p7_backbone(cfg):\n    body = resnet.ResNet(cfg)\n    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n    out_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS\n    fpn = fpn_module.FPN(\n        in_channels_list=[\n            in_channels_stage2,\n            in_channels_stage2 * 2,\n            in_channels_stage2 * 4,\n            in_channels_stage2 * 8,\n        ],\n        out_channels=out_channels,\n        top_blocks=fpn_module.LastLevelP6P7(out_channels),\n        use_gn=cfg.MODEL.USE_GN\n    )\n    model = nn.Sequential(OrderedDict([(""body"", body), (""fpn"", fpn)]))\n    return model\n\n\n_BACKBONES = {""resnet"": build_resnet_backbone,\n              ""resnet-fpn"": build_resnet_fpn_backbone,\n              ""resnet-fpn-retina"": build_resnet_fpn_p3p7_backbone,\n             }\n\n\n\ndef build_backbone(cfg):\n    assert cfg.MODEL.BACKBONE.CONV_BODY.startswith(\n        ""R-""\n    ), ""Only ResNet and ResNeXt models are currently implemented""\n    # Models using FPN end with ""-FPN""\n    if cfg.MODEL.BACKBONE.CONV_BODY.endswith(""-FPN""):\n        if cfg.RETINANET.RETINANET_ON:\n            if cfg.RETINANET.BACKBONE == ""p3p7"":\n                return build_resnet_fpn_p3p7_backbone(cfg)\n            elif cfg.RETINANET.BACKBONE == ""p2p7"":\n                return build_resnet_fpn_p2p7_backbone(cfg)\n            else:\n                raise Exception(""Wrong Setting {}:{}"".format(\n                    \'cfg.RETINANET.BACKBONE\', cfg.RETINANET.BACKBBACKBONE))\n        else:\n            return build_resnet_fpn_backbone(cfg)\n    return build_resnet_backbone(cfg)\n'"
maskrcnn_benchmark/modeling/backbone/fpn.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass FPN(nn.Module):\n    """"""\n    Module that adds FPN on top of a list of feature maps.\n    The feature maps are currently supposed to be in increasing depth\n    order, and must be consecutive\n    """"""\n\n    def __init__(self, in_channels_list, out_channels, top_blocks=None,\n                 use_gn=False):\n        """"""\n        Arguments:\n            in_channels_list (list[int]): number of channels for each feature map that\n                will be fed\n            out_channels (int): number of channels of the FPN representation\n            top_blocks (nn.Module or None): if provided, an extra operation will\n                be performed on the output of the last (smallest resolution)\n                FPN output, and the result will extend the result list\n        """"""\n        super(FPN, self).__init__()\n        self.inner_blocks = []\n        self.layer_blocks = []\n        # If in_channels is 0, it would be used. \n        self.valid_layers = [i > 0 for i in in_channels_list]\n        for idx, in_channels in enumerate(in_channels_list, 1):\n            inner_block = ""fpn_inner{}"".format(idx)\n            layer_block = ""fpn_layer{}"".format(idx)\n\n            if in_channels == 0:\n                continue\n\n            if use_gn:\n                inner_block_module = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, 1),\n                    nn.GroupNorm(32, out_channels))\n                layer_block_module = nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n                    nn.GroupNorm(32, out_channels))\n            else:\n                inner_block_module = nn.Conv2d(in_channels, out_channels, 1)\n                layer_block_module = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n\n            for module in [inner_block_module, layer_block_module]:\n                for m in module.modules():\n                    if isinstance(m, nn.Conv2d):\n                        # Caffe2 implementation uses XavierFill, which in fact\n                        # corresponds to kaiming_uniform_ in PyTorch\n                        nn.init.kaiming_uniform_(m.weight, a=1)\n                        nn.init.constant_(m.bias, 0)\n                    if isinstance(m, nn.GroupNorm):\n                        nn.init.constant_(m.weight, 1.0)\n                        nn.init.constant_(m.bias, 0)\n\n            self.add_module(inner_block, inner_block_module)\n            self.add_module(layer_block, layer_block_module)\n            self.inner_blocks.append(inner_block)\n            self.layer_blocks.append(layer_block)\n        self.top_blocks = top_blocks\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x (list[Tensor]): feature maps for each feature level.\n        Returns:\n            results (tuple[Tensor]): feature maps after FPN layers.\n                They are ordered from highest resolution first.\n        """"""\n        last_inner = getattr(self, self.inner_blocks[-1])(x[-1])\n        results = []\n        results.append(getattr(self, self.layer_blocks[-1])(last_inner))\n        for feature, inner_block, layer_block in zip(\n            x[:-1][::-1], self.inner_blocks[:-1][::-1], self.layer_blocks[:-1][::-1]\n        ):\n            if len(inner_block):\n                inner_top_down = F.interpolate(last_inner, scale_factor=2, mode=""nearest"")\n                inner_lateral = getattr(self, inner_block)(feature)\n                # TODO use size instead of scale to make it robust to different sizes\n                # inner_top_down = F.upsample(last_inner, size=inner_lateral.shape[-2:],\n                # mode=\'bilinear\', align_corners=False)\n                last_inner = inner_lateral + inner_top_down\n                results.insert(0, getattr(self, layer_block)(last_inner))\n\n        if self.top_blocks is not None:\n            last_results = self.top_blocks(results[-1])\n            results.extend(last_results)\n\n        return tuple(results)\n\n\nclass LastLevelMaxPool(nn.Module):\n    def forward(self, x):\n        return [F.max_pool2d(x, 1, 2, 0)]\n\n\nclass LastLevelP6P7(nn.Module):\n    """"""\n    This module is used in RetinaNet to generate extra layers, P6 and P7.\n    """"""\n    def __init__(self, out_channels):\n        super(LastLevelP6P7, self).__init__()\n        self.p6 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n        for module in [self.p6, self.p7]:\n            nn.init.kaiming_uniform_(module.weight, a=1)\n            nn.init.constant_(module.bias, 0)\n\n    def forward(self, x):\n        p6 = self.p6(x)\n        p7 = self.p7(F.relu(p6))\n        return [p6, p7]\n'"
maskrcnn_benchmark/modeling/backbone/resnet.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nVariant of the resnet module that takes cfg as an argument.\nExample usage. Strings may be specified in the config file.\n    model = ResNet(\n        ""StemWithFixedBatchNorm"",\n        ""BottleneckWithFixedBatchNorm"",\n        ""ResNet50StagesTo4"",\n    )\nCustom implementations may be written in user code and hooked in via the\n`register_*` functions.\n""""""\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom maskrcnn_benchmark.layers import FrozenBatchNorm2d\nfrom maskrcnn_benchmark.layers import Conv2d\n\n\n# ResNet stage specification\nStageSpec = namedtuple(\n    ""StageSpec"",\n    [\n        ""index"",  # Index of the stage, eg 1, 2, ..,. 5\n        ""block_count"",  # Numer of residual blocks in the stage\n        ""return_features"",  # True => return the last feature map from this stage\n    ],\n)\n\n# -----------------------------------------------------------------------------\n# Standard ResNet models\n# -----------------------------------------------------------------------------\n# ResNet-50 (including all stages)\nResNet50StagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, False), (2, 4, False), (3, 6, False), (4, 3, True))\n)\n# ResNet-50 up to stage 4 (excludes stage 5)\nResNet50StagesTo4 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, False), (2, 4, False), (3, 6, True))\n)\n# ResNet-50-FPN (including all stages)\nResNet50FPNStagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, True), (2, 4, True), (3, 6, True), (4, 3, True))\n)\n# ResNet-101-FPN (including all stages)\nResNet101FPNStagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, True), (2, 4, True), (3, 23, True), (4, 3, True))\n)\n\n\nclass ResNet(nn.Module):\n    def __init__(self, cfg):\n        super(ResNet, self).__init__()\n\n        # If we want to use the cfg in forward(), then we should make a copy\n        # of it and store it for later use:\n        # self.cfg = cfg.clone()\n\n        # Translate string names to implementations\n        stem_module = _STEM_MODULES[cfg.MODEL.RESNETS.STEM_FUNC]\n        stage_specs = _STAGE_SPECS[cfg.MODEL.BACKBONE.CONV_BODY]\n        transformation_module = _TRANSFORMATION_MODULES[cfg.MODEL.RESNETS.TRANS_FUNC]\n\n        # Construct the stem module\n        self.stem = stem_module(cfg)\n\n        # Constuct the specified ResNet stages\n        num_groups = cfg.MODEL.RESNETS.NUM_GROUPS\n        width_per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP\n        in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS\n        stage2_bottleneck_channels = num_groups * width_per_group\n        stage2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n        self.stages = []\n        self.return_features = {}\n        for stage_spec in stage_specs:\n            name = ""layer"" + str(stage_spec.index)\n            stage2_relative_factor = 2 ** (stage_spec.index - 1)\n            bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor\n            out_channels = stage2_out_channels * stage2_relative_factor\n            module = _make_stage(\n                transformation_module,\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                stage_spec.block_count,\n                num_groups,\n                cfg.MODEL.RESNETS.STRIDE_IN_1X1,\n                first_stride=int(stage_spec.index > 1) + 1,\n            )\n            in_channels = out_channels\n            self.add_module(name, module)\n            self.stages.append(name)\n            self.return_features[name] = stage_spec.return_features\n\n        # Optionally freeze (requires_grad=False) parts of the backbone\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT)\n\n    def _freeze_backbone(self, freeze_at):\n        for stage_index in range(freeze_at):\n            if stage_index == 0:\n                m = self.stem  # stage 0 is the stem\n            else:\n                m = getattr(self, ""layer"" + str(stage_index))\n            for p in m.parameters():\n                p.requires_grad = False\n\n    def forward(self, x):\n        outputs = []\n        x = self.stem(x)\n        for stage_name in self.stages:\n            x = getattr(self, stage_name)(x)\n            if self.return_features[stage_name]:\n                outputs.append(x)\n        return outputs\n\n\nclass ResNetHead(nn.Module):\n    def __init__(\n        self,\n        block_module,\n        stages,\n        num_groups=1,\n        width_per_group=64,\n        stride_in_1x1=True,\n        stride_init=None,\n        res2_out_channels=256,\n    ):\n        super(ResNetHead, self).__init__()\n\n        stage2_relative_factor = 2 ** (stages[0].index - 1)\n        stage2_bottleneck_channels = num_groups * width_per_group\n        out_channels = res2_out_channels * stage2_relative_factor\n        in_channels = out_channels // 2\n        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor\n\n        block_module = _TRANSFORMATION_MODULES[block_module]\n\n        self.stages = []\n        stride = stride_init\n        for stage in stages:\n            name = ""layer"" + str(stage.index)\n            if not stride:\n                stride = int(stage.index > 1) + 1\n            module = _make_stage(\n                block_module,\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                stage.block_count,\n                num_groups,\n                stride_in_1x1,\n                first_stride=stride,\n            )\n            stride = None\n            self.add_module(name, module)\n            self.stages.append(name)\n\n    def forward(self, x):\n        for stage in self.stages:\n            x = getattr(self, stage)(x)\n        return x\n\n\ndef _make_stage(\n    transformation_module,\n    in_channels,\n    bottleneck_channels,\n    out_channels,\n    block_count,\n    num_groups,\n    stride_in_1x1,\n    first_stride,\n):\n    blocks = []\n    stride = first_stride\n    for _ in range(block_count):\n        blocks.append(\n            transformation_module(\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                num_groups,\n                stride_in_1x1,\n                stride,\n            )\n        )\n        stride = 1\n        in_channels = out_channels\n    return nn.Sequential(*blocks)\n\n\nclass BottleneckWithFixedBatchNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        bottleneck_channels,\n        out_channels,\n        num_groups=1,\n        stride_in_1x1=True,\n        stride=1,\n    ):\n        super(BottleneckWithFixedBatchNorm, self).__init__()\n\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                Conv2d(\n                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n                ),\n                FrozenBatchNorm2d(out_channels),\n            )\n\n        # The original MSRA ResNet models have stride in the first 1x1 conv\n        # The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have\n        # stride in the 3x3 conv\n        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)\n\n        self.conv1 = Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=stride_1x1,\n            bias=False,\n        )\n        self.bn1 = FrozenBatchNorm2d(bottleneck_channels)\n        # TODO: specify init for the above\n\n        self.conv2 = Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride_3x3,\n            padding=1,\n            bias=False,\n            groups=num_groups,\n        )\n        self.bn2 = FrozenBatchNorm2d(bottleneck_channels)\n\n        self.conv3 = Conv2d(\n            bottleneck_channels, out_channels, kernel_size=1, bias=False\n        )\n        self.bn3 = FrozenBatchNorm2d(out_channels)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu_(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu_(out)\n\n        out0 = self.conv3(out)\n        out = self.bn3(out0)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu_(out)\n\n        return out\n\n\nclass StemWithFixedBatchNorm(nn.Module):\n    def __init__(self, cfg):\n        super(StemWithFixedBatchNorm, self).__init__()\n\n        out_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS\n\n        self.conv1 = Conv2d(\n            3, out_channels, kernel_size=7, stride=2, padding=3, bias=False\n        )\n        self.bn1 = FrozenBatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu_(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        return x\n\n\n_TRANSFORMATION_MODULES = {""BottleneckWithFixedBatchNorm"": BottleneckWithFixedBatchNorm}\n\n_STEM_MODULES = {""StemWithFixedBatchNorm"": StemWithFixedBatchNorm}\n\n_STAGE_SPECS = {\n    ""R-50-C4"": ResNet50StagesTo4,\n    ""R-50-C5"": ResNet50StagesTo5,\n    ""R-50-FPN"": ResNet50FPNStagesTo5,\n    ""R-101-FPN"": ResNet101FPNStagesTo5,\n}\n\n\ndef register_transformation_module(module_name, module):\n    _register_generic(_TRANSFORMATION_MODULES, module_name, module)\n\n\ndef register_stem_module(module_name, module):\n    _register_generic(_STEM_MODULES, module_name, module)\n\n\ndef register_stage_spec(stage_spec_name, stage_spec):\n    _register_generic(_STAGE_SPECS, stage_spec_name, stage_spec)\n\n\ndef _register_generic(module_dict, module_name, module):\n    assert module_name not in module_dict\n    module_dict[module_name] = module\n'"
maskrcnn_benchmark/modeling/detector/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .detectors import build_detection_model\n'"
maskrcnn_benchmark/modeling/detector/detectors.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom .generalized_rcnn import GeneralizedRCNN\nfrom .retinanet import RetinaNet\n\n_DETECTION_META_ARCHITECTURES = {""GeneralizedRCNN"": GeneralizedRCNN,\n                                 ""RetinaNet"": RetinaNet}\n\n\ndef build_detection_model(cfg):\n    meta_arch = _DETECTION_META_ARCHITECTURES[cfg.MODEL.META_ARCHITECTURE]\n    return meta_arch(cfg)\n'"
maskrcnn_benchmark/modeling/detector/generalized_rcnn.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nImplements the Generalized R-CNN framework\n""""""\n\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\n\nfrom ..backbone import build_backbone\nfrom ..rpn.rpn import build_rpn\nfrom ..rpn.retinanet import build_retinanet\nfrom ..roi_heads.roi_heads import build_roi_heads\n\n\nclass GeneralizedRCNN(nn.Module):\n    """"""\n    Main class for Generalized R-CNN. Currently supports boxes and masks.\n    It consists of three main parts:\n    - backbone\n    = rpn\n    - heads: takes the features + the proposals from the RPN and computes\n        detections / masks from it.\n    """"""\n\n    def __init__(self, cfg):\n        super(GeneralizedRCNN, self).__init__()\n\n        self.backbone = build_backbone(cfg)\n        if not cfg.RETINANET.RETINANET_ON:\n            self.rpn = build_rpn(cfg)\n        else:\n            self.rpn = build_retinanet(cfg)\n        self.roi_heads = build_roi_heads(cfg)\n\n    def forward(self, images, targets=None):\n        """"""\n        Arguments:\n            images (list[Tensor] or ImageList): images to be processed\n            targets (list[BoxList]): ground-truth boxes present in the image (optional)\n\n        Returns:\n            result (list[BoxList] or dict[Tensor]): the output from the model.\n                During training, it returns a dict[Tensor] which contains the losses.\n                During testing, it returns list[BoxList] contains additional fields\n                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n\n        """"""\n        if self.training and targets is None:\n            raise ValueError(""In training mode, targets should be passed"")\n        images = to_image_list(images)\n        features = self.backbone(images.tensors)\n        proposals, proposal_losses = self.rpn(images, features, targets)\n        if self.roi_heads:\n            x, result, detector_losses = self.roi_heads(features, proposals, targets)\n        else:\n            # RPN-only models don\'t have roi_heads\n            x = features\n            result = proposals\n            detector_losses = {}\n\n        if self.training:\n            losses = {}\n            losses.update(detector_losses)\n            losses.update(proposal_losses)\n            return losses\n\n        return result\n'"
maskrcnn_benchmark/modeling/detector/multi_scale_wrapper.py,1,"b'from torch import nn\nfrom torch.nn.functional import interpolate\nfrom maskrcnn_benchmark.data.transforms.transforms import Resize\nfrom maskrcnn_benchmark.structures.image_list import ImageList, to_image_list\n\n\nclass MultiScaleRetinaNet(nn.Module):\n    """"""\n    Main class for RetinaNet\n    It consists of three main parts:\n    - backbone\n    - bbox_heads: BBox prediction.\n    - Mask_heads:\n    """"""\n\n    def __init__(self, retinanet, scales):\n        super(MultiScaleRetinaNet, self).__init__()\n        self.retinanet = retinanet\n        self.resizers = [Resize(min_size, max_size) for (min_size, max_size) in scales]\n\n    def forward(self, images, targets=None):\n        """"""\n        Arguments:\n            images (list[Tensor] or ImageList): images to be processed\n            targets (list[BoxList]): ground-truth boxes present in the image (optional)\n\n        Returns:\n            result (list[BoxList] or dict[Tensor]): the output from the model.\n                During training, it returns a dict[Tensor] which contains the losses.\n                During testing, it returns list[BoxList] contains additional fields\n                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n\n        """"""\n        assert self.training is False\n\n        all_anchors, all_box_cls, all_box_regression = [], [], []\n        for resizer in self.resizers:\n            image_size = images.image_sizes[0]\n            size = resizer.get_size(image_size[::-1])\n            aug_images = interpolate(\n                images.tensors[:, :, :image_size[0], :image_size[1]], size, mode=\'bilinear\', align_corners=True\n            )[0]\n            aug_images = to_image_list(aug_images, size_divisible=self.retinanet.cfg.DATALOADER.SIZE_DIVISIBILITY)\n            features = self.retinanet.backbone(aug_images.tensors)\n            if self.retinanet.cfg.RETINANET.BACKBONE == ""p2p7"":\n                features = features[1:]\n            box_cls, box_regression = self.retinanet.rpn.head(features)\n            anchors = self.retinanet.rpn.anchor_generator(aug_images, features)[0]\n            all_anchors.extend(anchors), all_box_cls.extend(box_cls), all_box_regression.extend(box_regression)\n\n        detections = self.retinanet.rpn.box_selector_test([all_anchors], all_box_cls, all_box_regression)\n\n        return detections\n'"
maskrcnn_benchmark/modeling/detector/retinanet.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nImplements the Generalized R-CNN framework\n""""""\n\nfrom torch import nn\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\nfrom ..backbone import build_backbone\nfrom ..rpn.retinanet import build_retinanet\nimport copy\n\n\nclass RetinaNet(nn.Module):\n    """"""\n    Main class for RetinaNet\n    It consists of three main parts:\n    - backbone\n    - bbox_heads: BBox prediction.\n    - Mask_heads:\n    """"""\n\n    def __init__(self, cfg):\n        super(RetinaNet, self).__init__()\n        self.cfg = copy.deepcopy(cfg)\n        self.backbone = build_backbone(cfg)\n        self.rpn = build_retinanet(cfg)\n\n    def forward(self, images, targets=None):\n        """"""\n        Arguments:\n            images (list[Tensor] or ImageList): images to be processed\n            targets (list[BoxList]): ground-truth boxes present in the image (optional)\n\n        Returns:\n            result (list[BoxList] or dict[Tensor]): the output from the model.\n                During training, it returns a dict[Tensor] which contains the losses.\n                During testing, it returns list[BoxList] contains additional fields\n                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n\n        """"""\n        if self.training and targets is None:\n            raise ValueError(""In training mode, targets should be passed"")\n        images = to_image_list(images)\n        features = self.backbone(images.tensors)\n\n        # Retina RPN Output\n        rpn_features = features\n        if self.cfg.RETINANET.BACKBONE == ""p2p7"":\n            rpn_features = features[1:]\n        (anchors, detections), detector_losses = self.rpn(images, rpn_features, targets)\n        if self.training:\n            return detector_losses\n        else:\n            return detections\n'"
maskrcnn_benchmark/modeling/roi_heads/roi_heads.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nfrom .box_head.box_head import build_roi_box_head\nfrom .mask_head.mask_head import build_roi_mask_head\n\n\nclass CombinedROIHeads(torch.nn.ModuleDict):\n    """"""\n    Combines a set of individual heads (for box prediction or masks) into a single\n    head.\n    """"""\n\n    def __init__(self, cfg, heads):\n        super(CombinedROIHeads, self).__init__(heads)\n        self.cfg = cfg.clone()\n        if cfg.MODEL.MASK_ON and cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:\n            self.mask.feature_extractor = self.box.feature_extractor\n\n    def forward(self, features, proposals, targets=None):\n        losses = {}\n        # TODO rename x to roi_box_features, if it doesn\'t increase memory consumption\n        x, detections, loss_box = self.box(features, proposals, targets)\n        losses.update(loss_box)\n        if self.cfg.MODEL.MASK_ON:\n            mask_features = features\n            # optimization: during training, if we share the feature extractor between\n            # the box and the mask heads, then we can reuse the features already computed\n            if (\n                self.training\n                and self.cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR\n            ):\n                mask_features = x\n            # During training, self.box() will return the unaltered proposals as ""detections""\n            # this makes the API consistent during training and testing\n            x, detections, loss_mask = self.mask(mask_features, detections, targets)\n            losses.update(loss_mask)\n        return x, detections, losses\n\n\ndef build_roi_heads(cfg):\n    # individually create the heads, that will be combined together\n    # afterwards\n    roi_heads = []\n    if not cfg.MODEL.RPN_ONLY:\n        roi_heads.append((""box"", build_roi_box_head(cfg)))\n    if cfg.MODEL.MASK_ON:\n        roi_heads.append((""mask"", build_roi_mask_head(cfg)))\n\n    # combine individual heads in a single module\n    if roi_heads:\n        roi_heads = CombinedROIHeads(cfg, roi_heads)\n\n    return roi_heads\n'"
maskrcnn_benchmark/modeling/rpn/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# from .rpn import build_rpn\n'"
maskrcnn_benchmark/modeling/rpn/anchor_generator.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport math\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\n\nclass BufferList(nn.Module):\n    """"""\n    Similar to nn.ParameterList, but for buffers\n    """"""\n\n    def __init__(self, buffers=None):\n        super(BufferList, self).__init__()\n        if buffers is not None:\n            self.extend(buffers)\n\n    def extend(self, buffers):\n        offset = len(self)\n        for i, buffer in enumerate(buffers):\n            self.register_buffer(str(offset + i), buffer)\n        return self\n\n    def __len__(self):\n        return len(self._buffers)\n\n    def __iter__(self):\n        return iter(self._buffers.values())\n\n\nclass AnchorGenerator(nn.Module):\n    """"""\n    For a set of image sizes and feature maps, computes a set\n    of anchors\n    """"""\n\n    def __init__(\n        self,\n        sizes=(128, 256, 512),\n        aspect_ratios=(0.5, 1.0, 2.0),\n        anchor_strides=(8, 16, 32),\n        straddle_thresh=0,\n    ):\n        super(AnchorGenerator, self).__init__()\n\n        if len(anchor_strides) == 1:\n            anchor_stride = anchor_strides[0]\n            cell_anchors = [\n                generate_anchors(anchor_stride, sizes, aspect_ratios).float()\n            ]\n        else:\n            if len(anchor_strides) != len(sizes):\n                raise RuntimeError(""FPN should have #anchor_strides == #sizes"")\n\n            cell_anchors = [\n                generate_anchors(\n                    anchor_stride,\n                    size if type(size) is tuple else (size,),\n                    aspect_ratios\n                ).float()\n                for anchor_stride, size in zip(anchor_strides, sizes)\n            ]\n        self.strides = anchor_strides\n        self.cell_anchors = BufferList(cell_anchors)\n        self.straddle_thresh = straddle_thresh\n\n    def num_anchors_per_location(self):\n        return [len(cell_anchors) for cell_anchors in self.cell_anchors]\n\n    def grid_anchors(self, grid_sizes):\n        anchors = []\n        for size, stride, base_anchors in zip(\n            grid_sizes, self.strides, self.cell_anchors\n        ):\n            grid_height, grid_width = size\n            device = base_anchors.device\n            shifts_x = torch.arange(\n                0, grid_width * stride, step=stride, dtype=torch.float32, device=device\n            )\n            shifts_y = torch.arange(\n                0, grid_height * stride, step=stride, dtype=torch.float32, device=device\n            )\n            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n            shift_x = shift_x.reshape(-1)\n            shift_y = shift_y.reshape(-1)\n            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)\n\n            anchors.append(\n                (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4)\n            )\n\n        return anchors\n\n    def add_visibility_to(self, boxlist):\n        image_width, image_height = boxlist.size\n        anchors = boxlist.bbox\n        if self.straddle_thresh >= 0:\n            inds_inside = (\n                (anchors[..., 0] >= -self.straddle_thresh)\n                & (anchors[..., 1] >= -self.straddle_thresh)\n                & (anchors[..., 2] < image_width + self.straddle_thresh)\n                & (anchors[..., 3] < image_height + self.straddle_thresh)\n            )\n        else:\n            device = anchors.device\n            inds_inside = torch.ones(anchors.shape[0], dtype=torch.uint8, device=device)\n        boxlist.add_field(""visibility"", inds_inside)\n\n    def forward(self, image_list, feature_maps):\n        grid_height, grid_width = feature_maps[0].shape[-2:]\n        grid_sizes = [feature_map.shape[-2:] for feature_map in feature_maps]\n        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)\n        anchors = []\n        for i, (image_height, image_width) in enumerate(image_list.image_sizes):\n            anchors_in_image = []\n            for anchors_per_feature_map in anchors_over_all_feature_maps:\n                boxlist = BoxList(\n                    anchors_per_feature_map, (image_width, image_height), mode=""xyxy""\n                )\n                self.add_visibility_to(boxlist)\n                anchors_in_image.append(boxlist)\n            anchors.append(anchors_in_image)\n        return anchors\n\n\ndef make_anchor_generator(config):\n    anchor_sizes = config.MODEL.RPN.ANCHOR_SIZES\n    aspect_ratios = config.MODEL.RPN.ASPECT_RATIOS\n    anchor_stride = config.MODEL.RPN.ANCHOR_STRIDE\n    straddle_thresh = config.MODEL.RPN.STRADDLE_THRESH\n\n    if config.MODEL.RPN.USE_FPN:\n        assert len(anchor_stride) == len(\n            anchor_sizes\n        ), ""FPN should have len(ANCHOR_STRIDE) == len(ANCHOR_SIZES)""\n    else:\n        assert len(anchor_stride) == 1, ""Non-FPN should have a single ANCHOR_STRIDE""\n    anchor_generator = AnchorGenerator(\n        anchor_sizes, aspect_ratios, anchor_stride, straddle_thresh\n    )\n    return anchor_generator\n\n\ndef make_anchor_generator_retinanet(config):\n    anchor_sizes = config.RETINANET.ANCHOR_SIZES\n    aspect_ratios = config.RETINANET.ASPECT_RATIOS\n    anchor_strides = config.RETINANET.ANCHOR_STRIDES\n    straddle_thresh = config.RETINANET.STRADDLE_THRESH\n    octave = config.RETINANET.OCTAVE\n    scales_per_octave = config.RETINANET.SCALES_PER_OCTAVE\n\n    assert len(anchor_strides) == len(anchor_sizes), ""Only support FPN now""\n    new_anchor_sizes = []\n    for size in anchor_sizes:\n        per_layer_anchor_sizes = []\n        for scale_per_octave in range(scales_per_octave):\n            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))\n            per_layer_anchor_sizes.append(octave_scale * size)\n        new_anchor_sizes.append(tuple(per_layer_anchor_sizes))\n\n    anchor_generator = AnchorGenerator(\n        tuple(new_anchor_sizes), aspect_ratios, anchor_strides, straddle_thresh\n    )\n    return anchor_generator\n\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#        [-175.,  -87.,  192.,  104.],\n#        [-359., -183.,  376.,  200.],\n#        [ -55.,  -55.,   72.,   72.],\n#        [-119., -119.,  136.,  136.],\n#        [-247., -247.,  264.,  264.],\n#        [ -35.,  -79.,   52.,   96.],\n#        [ -79., -167.,   96.,  184.],\n#        [-167., -343.,  184.,  360.]])\n\n\ndef generate_anchors(\n    stride=16, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)\n):\n    """"""Generates a matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors\n    are centered on stride / 2, have (approximate) sqrt areas of the specified\n    sizes, and aspect ratios as given.\n    """"""\n    return _generate_anchors(\n        stride,\n        np.array(sizes, dtype=np.float) / stride,\n        np.array(aspect_ratios, dtype=np.float),\n    )\n\n\ndef _generate_anchors(base_size, scales, aspect_ratios):\n    """"""Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, base_size - 1, base_size - 1) window.\n    """"""\n    anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1\n    anchors = _ratio_enum(anchor, aspect_ratios)\n    anchors = np.vstack(\n        [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]\n    )\n    return torch.from_numpy(anchors)\n\n\ndef _whctrs(anchor):\n    """"""Return width, height, x center, and y center for an anchor (window).""""""\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack(\n        (\n            x_ctr - 0.5 * (ws - 1),\n            y_ctr - 0.5 * (hs - 1),\n            x_ctr + 0.5 * (ws - 1),\n            y_ctr + 0.5 * (hs - 1),\n        )\n    )\n    return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""Enumerate a set of anchors for each aspect ratio wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""Enumerate a set of anchors for each scale wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n'"
maskrcnn_benchmark/modeling/rpn/free_anchor_loss.py,22,"b'""""""\nThis file contains specific functions for computing losses on the RetinaNet\nfile\n""""""\n\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\n\nfrom ..utils import cat\n\nfrom maskrcnn_benchmark.modeling.matcher import Matcher\nfrom maskrcnn_benchmark.structures.boxlist_ops import BoxList\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\n\n\nclass Clip(Function):\n    @staticmethod\n    def forward(ctx, x, a, b):\n        return x.clamp(a, b)\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        return grad_output, None, None\n\n\nclip = Clip.apply\n\n\nclass FreeAnchorLossComputation(object):\n    """"""\n    This class computes the RetinaNet loss.\n    """"""\n\n    def __init__(self, cfg, box_coder):\n        """"""\n        Arguments:\n            proposal_matcher (Matcher)\n            box_coder (BoxCoder)\n        """"""\n        self.box_coder = box_coder\n        self.num_classes = cfg.RETINANET.NUM_CLASSES - 1\n        self.pre_anchor_topk = cfg.FREEANCHOR.PRE_ANCHOR_TOPK\n        self.smooth_l1_loss_param = (cfg.FREEANCHOR.BBOX_REG_WEIGHT, cfg.FREEANCHOR.BBOX_REG_BETA)\n        self.bbox_threshold = cfg.FREEANCHOR.BBOX_THRESHOLD\n        self.focal_loss_alpha = cfg.FREEANCHOR.FOCAL_LOSS_ALPHA\n        self.focal_loss_gamma = cfg.FREEANCHOR.FOCAL_LOSS_GAMMA\n\n        self.positive_bag_loss_func = positive_bag_loss\n        self.negative_bag_loss_func = focal_loss\n\n    def __call__(self, anchors, box_cls, box_regression, targets):\n        """"""\n        Arguments:\n            anchors (list[BoxList])\n            objectness (list[Tensor])\n            box_regression (list[Tensor])\n            targets (list[BoxList])\n\n        Returns:\n            objectness_loss (Tensor)\n            box_loss (Tensor\n        """"""\n        anchors = [cat_boxlist(anchors_per_image) for anchors_per_image in anchors]\n        box_cls_flattened = []\n        box_regression_flattened = []\n        # for each feature level, permute the outputs to make them be in the\n        # same format as the labels. Note that the labels are computed for\n        # all feature levels concatenated, so we keep the same representation\n        # for the objectness and the box_regression\n        for box_cls_per_level, box_regression_per_level in zip(\n            box_cls, box_regression\n        ):\n            N, A, H, W = box_cls_per_level.shape\n            C = self.num_classes\n            box_cls_per_level = box_cls_per_level.view(N, -1, C, H, W)\n            box_cls_per_level = box_cls_per_level.permute(0, 3, 4, 1, 2)\n            box_cls_per_level = box_cls_per_level.reshape(N, -1, C)\n            box_regression_per_level = box_regression_per_level.view(N, -1, 4, H, W)\n            box_regression_per_level = box_regression_per_level.permute(0, 3, 4, 1, 2)\n            box_regression_per_level = box_regression_per_level.reshape(N, -1, 4)\n            box_cls_flattened.append(box_cls_per_level)\n            box_regression_flattened.append(box_regression_per_level)\n        # concatenate on the first dimension (representing the feature levels), to\n        # take into account the way the labels were generated (with all feature maps\n        # being concatenated as well)\n        box_cls = cat(box_cls_flattened, dim=1)\n        box_regression = cat(box_regression_flattened, dim=1)\n\n        cls_prob = torch.sigmoid(box_cls)\n        box_prob = []\n        positive_numels = 0\n        positive_losses = []\n        for img, (anchors_, targets_, cls_prob_, box_regression_) in enumerate(\n                zip(anchors, targets, cls_prob, box_regression)\n        ):\n            labels_ = targets_.get_field(""labels"") - 1\n\n            with torch.set_grad_enabled(False):\n                # box_localization: a_{j}^{loc}, shape: [j, 4]\n                box_localization = self.box_coder.decode(box_regression_, anchors_.bbox)\n\n                # object_box_iou: IoU_{ij}^{loc}, shape: [i, j]\n                object_box_iou = boxlist_iou(\n                    targets_,\n                    BoxList(box_localization, anchors_.size, mode=\'xyxy\')\n                )\n\n                t1 = self.bbox_threshold\n                t2 = object_box_iou.max(dim=1, keepdim=True).values.clamp(min=t1 + 1e-12)\n\n                # object_box_prob: P{a_{j} -> b_{i}}, shape: [i, j]\n                object_box_prob = (\n                    (object_box_iou - t1) / (t2 - t1)\n                ).clamp(min=0, max=1)\n\n                indices = torch.stack([torch.arange(len(labels_)).type_as(labels_), labels_], dim=0)\n\n                # object_cls_box_prob: P{a_{j} -> b_{i}}, shape: [i, c, j]\n                object_cls_box_prob = torch.sparse_coo_tensor(indices, object_box_prob)\n\n                # image_box_prob: P{a_{j} \\in A_{+}}, shape: [j, c]\n                """"""\n                from ""start"" to ""end"" implement:\n                \n                image_box_prob = torch.sparse.max(object_cls_box_prob, dim=0).t()\n                \n                """"""\n                # start\n                indices = torch.nonzero(torch.sparse.sum(\n                    object_cls_box_prob, dim=0\n                ).to_dense()).t_()\n\n                if indices.numel() == 0:\n                    image_box_prob = torch.zeros(anchors_.bbox.size(0), self.num_classes).type_as(object_box_prob)\n                else:\n                    nonzero_box_prob = torch.where(\n                        (labels_.unsqueeze(dim=-1) == indices[0]),\n                        object_box_prob[:, indices[1]],\n                        torch.tensor([0]).type_as(object_box_prob)\n                    ).max(dim=0).values\n\n                    image_box_prob = torch.sparse_coo_tensor(\n                        indices.flip([0]), nonzero_box_prob,\n                        size=(anchors_.bbox.size(0), self.num_classes)\n                    ).to_dense()\n                # end\n\n                box_prob.append(image_box_prob)\n\n            # construct bags for objects\n            match_quality_matrix = boxlist_iou(targets_, anchors_)\n            _, matched = torch.topk(match_quality_matrix, self.pre_anchor_topk, dim=1, sorted=False)\n            del match_quality_matrix\n\n            # matched_cls_prob: P_{ij}^{cls}\n            matched_cls_prob = torch.gather(\n                cls_prob_[matched], 2, labels_.view(-1, 1, 1).repeat(1, self.pre_anchor_topk, 1)\n            ).squeeze(2)\n\n            # matched_box_prob: P_{ij}^{loc}\n            matched_object_targets = self.box_coder.encode(targets_.bbox.unsqueeze(dim=1), anchors_.bbox[matched])\n            retinanet_regression_loss = smooth_l1_loss(\n                box_regression_[matched], matched_object_targets, *self.smooth_l1_loss_param\n            )\n            matched_box_prob = torch.exp(-retinanet_regression_loss)\n\n            # positive_losses: { -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) }\n            positive_numels += len(targets_)\n            positive_losses.append(self.positive_bag_loss_func(matched_cls_prob * matched_box_prob, dim=1))\n\n        # positive_loss: \\sum_{i}{ -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) } / ||B||\n        positive_loss = torch.cat(positive_losses).sum() / max(1, positive_numels)\n\n        # box_prob: P{a_{j} \\in A_{+}}\n        box_prob = torch.stack(box_prob, dim=0)\n\n        # negative_loss: \\sum_{j}{ FL( (1 - P{a_{j} \\in A_{+}}) * (1 - P_{j}^{bg}) ) } / n||B||\n        negative_loss = self.negative_bag_loss_func(\n            cls_prob * (1 - box_prob), self.focal_loss_gamma\n        ) / max(1, positive_numels * self.pre_anchor_topk)\n\n        losses = {\n            ""loss_retina_positive"": positive_loss * self.focal_loss_alpha,\n            ""loss_retina_negative"": negative_loss * (1 - self.focal_loss_alpha),\n        }\n        return losses\n\n\ndef smooth_l1_loss(pred, target, weight, beta):\n    val = target - pred\n    abs_val = val.abs()\n    smooth_mask = abs_val < beta\n    return weight * torch.where(smooth_mask, 0.5 / beta * val ** 2, (abs_val - 0.5 * beta)).sum(dim=-1)\n\n\ndef positive_bag_loss(logits, *args, **kwargs):\n    # bag_prob = Mean-max(logits)\n    weight = 1 / clip(1 - logits, 1e-12, None)\n    weight /= weight.sum(*args, **kwargs).unsqueeze(dim=-1)\n    bag_prob = (weight * logits).sum(*args, **kwargs)\n    # positive_bag_loss = -log(bag_prob)\n    return F.binary_cross_entropy(bag_prob, torch.ones_like(bag_prob), reduction=\'none\')\n\n\ndef focal_loss(logits, gamma):\n    return torch.sum(\n        logits ** gamma * F.binary_cross_entropy(logits, torch.zeros_like(logits), reduction=\'none\')\n    )\n\n\ndef make_free_anchor_loss_evaluator(cfg, box_coder):\n    return FreeAnchorLossComputation(cfg, box_coder)\n'"
maskrcnn_benchmark/modeling/rpn/inference.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_nms\nfrom maskrcnn_benchmark.structures.boxlist_ops import remove_small_boxes\n\nfrom ..utils import cat\n\n\nclass RPNPostProcessor(torch.nn.Module):\n    """"""\n    Performs post-processing on the outputs of the RPN boxes, before feeding the\n    proposals to the heads\n    """"""\n\n    def __init__(\n        self,\n        pre_nms_top_n,\n        post_nms_top_n,\n        nms_thresh,\n        min_size,\n        box_coder=None,\n        fpn_post_nms_top_n=None,\n    ):\n        """"""\n        Arguments:\n            pre_nms_top_n (int)\n            post_nms_top_n (int)\n            nms_thresh (float)\n            min_size (int)\n            box_coder (BoxCoder)\n            fpn_post_nms_top_n (int)\n        """"""\n        super(RPNPostProcessor, self).__init__()\n        self.pre_nms_top_n = pre_nms_top_n\n        self.post_nms_top_n = post_nms_top_n\n        self.nms_thresh = nms_thresh\n        self.min_size = min_size\n\n        if box_coder is None:\n            box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n        self.box_coder = box_coder\n\n        if fpn_post_nms_top_n is None:\n            fpn_post_nms_top_n = post_nms_top_n\n        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n\n    def add_gt_proposals(self, proposals, targets):\n        """"""\n        Arguments:\n            proposals: list[BoxList]\n            targets: list[BoxList]\n        """"""\n        # Get the device we\'re operating on\n        device = proposals[0].bbox.device\n\n        gt_boxes = [target.copy_with_fields([]) for target in targets]\n\n        # later cat of bbox requires all fields to be present for all bbox\n        # so we need to add a dummy for objectness that\'s missing\n        for gt_box in gt_boxes:\n            gt_box.add_field(""objectness"", torch.ones(len(gt_box), device=device))\n\n        proposals = [\n            cat_boxlist((proposal, gt_box))\n            for proposal, gt_box in zip(proposals, gt_boxes)\n        ]\n\n        return proposals\n\n    def forward_for_single_feature_map(self, anchors, objectness, box_regression):\n        """"""\n        Arguments:\n            anchors: list[BoxList]\n            objectness: tensor of size N, A, H, W\n            box_regression: tensor of size N, A * 4, H, W\n        """"""\n        device = objectness.device\n        N, A, H, W = objectness.shape\n\n        # put in the same format as anchors\n        objectness = objectness.permute(0, 2, 3, 1).reshape(N, -1)\n        objectness = objectness.sigmoid()\n        box_regression = box_regression.view(N, -1, 4, H, W).permute(0, 3, 4, 1, 2)\n        box_regression = box_regression.reshape(N, -1, 4)\n\n        num_anchors = A * H * W\n\n        pre_nms_top_n = min(self.pre_nms_top_n, num_anchors)\n        objectness, topk_idx = objectness.topk(pre_nms_top_n, dim=1, sorted=True)\n\n        batch_idx = torch.arange(N, device=device)[:, None]\n        box_regression = box_regression[batch_idx, topk_idx]\n\n        image_shapes = [box.size for box in anchors]\n        concat_anchors = torch.cat([a.bbox for a in anchors], dim=0)\n        concat_anchors = concat_anchors.reshape(N, -1, 4)[batch_idx, topk_idx]\n\n        proposals = self.box_coder.decode(\n            box_regression.view(-1, 4), concat_anchors.view(-1, 4)\n        )\n\n        proposals = proposals.view(N, -1, 4)\n\n        result = []\n        for proposal, score, im_shape in zip(proposals, objectness, image_shapes):\n            boxlist = BoxList(proposal, im_shape, mode=""xyxy"")\n            boxlist.add_field(""objectness"", score)\n            boxlist = boxlist.clip_to_image(remove_empty=False)\n            boxlist = remove_small_boxes(boxlist, self.min_size)\n            boxlist = boxlist_nms(\n                boxlist,\n                self.nms_thresh,\n                max_proposals=self.post_nms_top_n,\n                score_field=""objectness"",\n            )\n            result.append(boxlist)\n        return result\n\n    def forward(self, anchors, objectness, box_regression, targets=None):\n        """"""\n        Arguments:\n            anchors: list[list[BoxList]]\n            objectness: list[tensor]\n            box_regression: list[tensor]\n\n        Returns:\n            boxlists (list[BoxList]): the post-processed anchors, after\n                applying box decoding and NMS\n        """"""\n        sampled_boxes = []\n        num_levels = len(objectness)\n        anchors = list(zip(*anchors))\n        for a, o, b in zip(anchors, objectness, box_regression):\n            sampled_boxes.append(self.forward_for_single_feature_map(a, o, b))\n\n        boxlists = list(zip(*sampled_boxes))\n        boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]\n\n        if num_levels > 1:\n            boxlists = self.select_over_all_levels(boxlists)\n\n        # append ground-truth bboxes to proposals\n        if self.training and targets is not None:\n            boxlists = self.add_gt_proposals(boxlists, targets)\n\n        return boxlists\n\n    def select_over_all_levels(self, boxlists):\n        num_images = len(boxlists)\n        # different behavior during training and during testing:\n        # during training, post_nms_top_n is over *all* the proposals combined, while\n        # during testing, it is over the proposals for each image\n        # TODO resolve this difference and make it consistent. It should be per image,\n        # and not per batch\n        if self.training:\n            objectness = torch.cat(\n                [boxlist.get_field(""objectness"") for boxlist in boxlists], dim=0\n            )\n            box_sizes = [len(boxlist) for boxlist in boxlists]\n            post_nms_top_n = min(self.fpn_post_nms_top_n, len(objectness))\n            _, inds_sorted = torch.topk(objectness, post_nms_top_n, dim=0, sorted=True)\n            inds_mask = torch.zeros_like(objectness, dtype=torch.uint8)\n            inds_mask[inds_sorted] = 1\n            inds_mask = inds_mask.split(box_sizes)\n            for i in range(num_images):\n                boxlists[i] = boxlists[i][inds_mask[i]]\n        else:\n            for i in range(num_images):\n                objectness = boxlists[i].get_field(""objectness"")\n                post_nms_top_n = min(self.fpn_post_nms_top_n, len(objectness))\n                _, inds_sorted = torch.topk(\n                    objectness, post_nms_top_n, dim=0, sorted=True\n                )\n                boxlists[i] = boxlists[i][inds_sorted]\n        return boxlists\n\n\ndef make_rpn_postprocessor(config, rpn_box_coder, is_train):\n    fpn_post_nms_top_n = config.MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN\n    if not is_train:\n        fpn_post_nms_top_n = config.MODEL.RPN.FPN_POST_NMS_TOP_N_TEST\n\n    pre_nms_top_n = config.MODEL.RPN.PRE_NMS_TOP_N_TRAIN\n    post_nms_top_n = config.MODEL.RPN.POST_NMS_TOP_N_TRAIN\n    if not is_train:\n        pre_nms_top_n = config.MODEL.RPN.PRE_NMS_TOP_N_TEST\n        post_nms_top_n = config.MODEL.RPN.POST_NMS_TOP_N_TEST\n    nms_thresh = config.MODEL.RPN.NMS_THRESH\n    min_size = config.MODEL.RPN.MIN_SIZE\n    box_selector = RPNPostProcessor(\n        pre_nms_top_n=pre_nms_top_n,\n        post_nms_top_n=post_nms_top_n,\n        nms_thresh=nms_thresh,\n        min_size=min_size,\n        box_coder=rpn_box_coder,\n        fpn_post_nms_top_n=fpn_post_nms_top_n,\n    )\n    return box_selector\n'"
maskrcnn_benchmark/modeling/rpn/loss.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nThis file contains specific functions for computing losses on the RPN\nfile\n""""""\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom ..balanced_positive_negative_sampler import BalancedPositiveNegativeSampler\nfrom ..utils import cat\n\nfrom maskrcnn_benchmark.layers import smooth_l1_loss\nfrom maskrcnn_benchmark.modeling.matcher import Matcher\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\n\n\nclass RPNLossComputation(object):\n    """"""\n    This class computes the RPN loss.\n    """"""\n\n    def __init__(self, proposal_matcher, fg_bg_sampler, box_coder):\n        """"""\n        Arguments:\n            proposal_matcher (Matcher)\n            fg_bg_sampler (BalancedPositiveNegativeSampler)\n            box_coder (BoxCoder)\n        """"""\n        # self.target_preparator = target_preparator\n        self.proposal_matcher = proposal_matcher\n        self.fg_bg_sampler = fg_bg_sampler\n        self.box_coder = box_coder\n\n    def match_targets_to_anchors(self, anchor, target):\n        match_quality_matrix = boxlist_iou(target, anchor)\n        matched_idxs = self.proposal_matcher(match_quality_matrix)\n        # RPN doesn\'t need any fields from target\n        # for creating the labels, so clear them all\n        target = target.copy_with_fields([])\n        # get the targets corresponding GT for each anchor\n        # NB: need to clamp the indices because we can have a single\n        # GT in the image, and matched_idxs can be -2, which goes\n        # out of bounds\n        matched_targets = target[matched_idxs.clamp(min=0)]\n        matched_targets.add_field(""matched_idxs"", matched_idxs)\n        return matched_targets\n\n    def prepare_targets(self, anchors, targets):\n        labels = []\n        regression_targets = []\n        for anchors_per_image, targets_per_image in zip(anchors, targets):\n            matched_targets = self.match_targets_to_anchors(\n                anchors_per_image, targets_per_image\n            )\n\n            matched_idxs = matched_targets.get_field(""matched_idxs"")\n            labels_per_image = matched_idxs >= 0\n            labels_per_image = labels_per_image.to(dtype=torch.float32)\n            # discard anchors that go out of the boundaries of the image\n            labels_per_image[~anchors_per_image.get_field(""visibility"")] = -1\n\n            # discard indices that are between thresholds\n            inds_to_discard = matched_idxs == Matcher.BETWEEN_THRESHOLDS\n            labels_per_image[inds_to_discard] = -1\n\n            # compute regression targets\n            regression_targets_per_image = self.box_coder.encode(\n                matched_targets.bbox, anchors_per_image.bbox\n            )\n\n            labels.append(labels_per_image)\n            regression_targets.append(regression_targets_per_image)\n\n        return labels, regression_targets\n\n    def __call__(self, anchors, objectness, box_regression, targets):\n        """"""\n        Arguments:\n            anchors (list[BoxList])\n            objectness (list[Tensor])\n            box_regression (list[Tensor])\n            targets (list[BoxList])\n\n        Returns:\n            objectness_loss (Tensor)\n            box_loss (Tensor\n        """"""\n        anchors = [cat_boxlist(anchors_per_image) for anchors_per_image in anchors]\n        labels, regression_targets = self.prepare_targets(anchors, targets)\n        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n        sampled_pos_inds = torch.nonzero(torch.cat(sampled_pos_inds, dim=0)).squeeze(1)\n        sampled_neg_inds = torch.nonzero(torch.cat(sampled_neg_inds, dim=0)).squeeze(1)\n\n        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n\n        objectness_flattened = []\n        box_regression_flattened = []\n        # for each feature level, permute the outputs to make them be in the\n        # same format as the labels. Note that the labels are computed for\n        # all feature levels concatenated, so we keep the same representation\n        # for the objectness and the box_regression\n        for objectness_per_level, box_regression_per_level in zip(\n            objectness, box_regression\n        ):\n            N, A, H, W = objectness_per_level.shape\n            objectness_per_level = objectness_per_level.permute(0, 2, 3, 1).reshape(\n                N, -1\n            )\n            box_regression_per_level = box_regression_per_level.view(N, -1, 4, H, W)\n            box_regression_per_level = box_regression_per_level.permute(0, 3, 4, 1, 2)\n            box_regression_per_level = box_regression_per_level.reshape(N, -1, 4)\n            objectness_flattened.append(objectness_per_level)\n            box_regression_flattened.append(box_regression_per_level)\n        # concatenate on the first dimension (representing the feature levels), to\n        # take into account the way the labels were generated (with all feature maps\n        # being concatenated as well)\n        objectness = cat(objectness_flattened, dim=1).reshape(-1)\n        box_regression = cat(box_regression_flattened, dim=1).reshape(-1, 4)\n\n        labels = torch.cat(labels, dim=0)\n        regression_targets = torch.cat(regression_targets, dim=0)\n\n        box_loss = smooth_l1_loss(\n            box_regression[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            beta=1.0 / 9,\n            size_average=False,\n        ) / (sampled_inds.numel())\n\n        objectness_loss = F.binary_cross_entropy_with_logits(\n            objectness[sampled_inds], labels[sampled_inds]\n        )\n\n        return objectness_loss, box_loss\n\n\ndef make_rpn_loss_evaluator(cfg, box_coder):\n    matcher = Matcher(\n        cfg.MODEL.RPN.FG_IOU_THRESHOLD,\n        cfg.MODEL.RPN.BG_IOU_THRESHOLD,\n        allow_low_quality_matches=True,\n    )\n\n    fg_bg_sampler = BalancedPositiveNegativeSampler(\n        cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE, cfg.MODEL.RPN.POSITIVE_FRACTION\n    )\n\n    loss_evaluator = RPNLossComputation(matcher, fg_bg_sampler, box_coder)\n    return loss_evaluator\n'"
maskrcnn_benchmark/modeling/rpn/retinanet.py,7,"b'import numpy as np\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom .retinanet_loss import make_retinanet_loss_evaluator\nfrom .free_anchor_loss import make_free_anchor_loss_evaluator\nfrom .anchor_generator import make_anchor_generator_retinanet\nfrom .retinanet_infer import make_retinanet_postprocessor\n\n\nclass RetinaNetHead(torch.nn.Module):\n    """"""\n    Adds a RetinNet head with classification and regression heads\n    """"""\n\n    def __init__(self, cfg):\n        """"""\n        Arguments:\n            in_channels (int): number of channels of the input feature\n            num_anchors (int): number of anchors to be predicted\n        """"""\n        super(RetinaNetHead, self).__init__()\n        # TODO: Implement the sigmoid version first.\n        num_classes = cfg.RETINANET.NUM_CLASSES - 1\n        in_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS\n        num_anchors = len(cfg.RETINANET.ASPECT_RATIOS) \\\n                        * cfg.RETINANET.SCALES_PER_OCTAVE\n\n        cls_tower = []\n        bbox_tower = []\n        for i in range(cfg.RETINANET.NUM_CONVS):\n            cls_tower.append(\n                nn.Conv2d(\n                    in_channels,\n                    in_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1\n                )\n            )\n            if cfg.MODEL.USE_GN:\n                cls_tower.append(nn.GroupNorm(32, in_channels))\n\n            cls_tower.append(nn.ReLU())\n            bbox_tower.append(\n                nn.Conv2d(\n                    in_channels,\n                    in_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1\n                )\n            )\n            if cfg.MODEL.USE_GN:\n                bbox_tower.append(nn.GroupNorm(32, in_channels))\n\n            bbox_tower.append(nn.ReLU())\n\n        self.add_module(\'cls_tower\', nn.Sequential(*cls_tower))\n        self.add_module(\'bbox_tower\', nn.Sequential(*bbox_tower))\n        self.cls_logits = nn.Conv2d(\n            in_channels, num_anchors * num_classes, kernel_size=3, stride=1,\n            padding=1\n        )\n        self.bbox_pred = nn.Conv2d(\n            in_channels,  num_anchors * 4, kernel_size=3, stride=1,\n            padding=1\n        )\n\n        # Initialization\n        for modules in [self.cls_tower, self.bbox_tower, self.cls_logits,\n                  self.bbox_pred]:\n            for l in modules.modules():\n                if isinstance(l, nn.Conv2d):\n                    torch.nn.init.normal_(l.weight, std=0.01)\n                    torch.nn.init.constant_(l.bias, 0)\n                if isinstance(l, nn.GroupNorm):\n                    torch.nn.init.constant_(l.weight, 1.0)\n                    torch.nn.init.constant_(l.bias, 0)\n\n        # retinanet_bias_init\n        prior_prob = cfg.RETINANET.PRIOR_PROB\n        bias_value = -np.log((1 - prior_prob) / prior_prob)\n        torch.nn.init.constant_(self.cls_logits.bias, bias_value)\n\n    def forward(self, x):\n        logits = []\n        bbox_reg = []\n        for feature in x:\n            logits.append(self.cls_logits(self.cls_tower(feature)))\n            bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))\n        return logits, bbox_reg\n\n\nclass RetinaNetModule(torch.nn.Module):\n    """"""\n    Module for RetinaNet computation. Takes feature maps from the backbone and RPN\n    proposals and losses.\n    """"""\n\n    def __init__(self, cfg):\n        super(RetinaNetModule, self).__init__()\n\n        self.cfg = cfg.clone()\n\n        anchor_generator = make_anchor_generator_retinanet(cfg)\n        head = RetinaNetHead(cfg)\n        box_coder = BoxCoder(weights=(10., 10., 5., 5.))\n\n        if self.cfg.MODEL.SPARSE_MASK_ON or self.cfg.MODEL.SPARSE_MASK_ON:\n            raise NotImplementedError\n        else:\n            box_selector_test = make_retinanet_postprocessor(\n                cfg, 100, box_coder)\n        box_selector_train = None\n\n        loss_evaluator = make_free_anchor_loss_evaluator(cfg, box_coder) if cfg.FREEANCHOR.FREEANCHOR_ON \\\n            else make_retinanet_loss_evaluator(cfg, box_coder)\n\n        self.anchor_generator = anchor_generator\n        self.head = head\n        self.box_selector_test = box_selector_test\n        self.box_selector_train = box_selector_train\n        self.loss_evaluator = loss_evaluator\n\n    def forward(self, images, features, targets=None):\n        """"""\n        Arguments:\n            images (ImageList): images for which we want to compute the predictions\n            features (list[Tensor]): features computed from the images that are\n                used for computing the predictions. Each tensor in the list\n                correspond to different feature levels\n            targets (list[BoxList): ground-truth boxes present in the image (optional)\n\n        Returns:\n            boxes (list[BoxList]): the predicted boxes from the RPN, one BoxList per\n                image.\n            losses (dict[Tensor]): the losses for the model during training. During\n                testing, it is an empty dict.\n        """"""\n        box_cls, box_regression = self.head(features)\n        anchors = self.anchor_generator(images, features)\n \n        if self.training:\n            return self._forward_train(anchors, box_cls, box_regression, targets)\n        else:\n            return self._forward_test(anchors, box_cls, box_regression)\n\n    def _forward_train(self, anchors, box_cls, box_regression, targets):\n\n        losses = self.loss_evaluator(\n            anchors, box_cls, box_regression, targets\n        )\n        detections = None\n\n        return (anchors, detections), losses\n\n    def _forward_test(self, anchors, box_cls, box_regression):\n        boxes = self.box_selector_test(anchors, box_cls, box_regression)\n        return (anchors, boxes), {}\n\n\ndef build_retinanet(cfg):\n    return RetinaNetModule(cfg)\n'"
maskrcnn_benchmark/modeling/rpn/retinanet_infer.py,11,"b'import torch\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_nms\nfrom maskrcnn_benchmark.structures.boxlist_ops import remove_small_boxes\n\nfrom ..utils import cat\n\n\nclass RetinaNetPostProcessor(torch.nn.Module):\n    """"""\n    Performs post-processing on the outputs of the RetinaNet boxes.\n    This is only used in the testing.\n    """"""\n    def __init__(\n        self,\n        pre_nms_thresh,\n        pre_nms_top_n,\n        nms_thresh,\n        fpn_post_nms_top_n,\n        min_size,\n        box_coder=None,\n    ):\n        """"""\n        Arguments:\n            pre_nms_thresh (float)\n            pre_nms_top_n (int)\n            nms_thresh (float)\n            fpn_post_nms_top_n (int)\n            min_size (int)\n            box_coder (BoxCoder)\n        """"""\n        super(RetinaNetPostProcessor, self).__init__()\n        self.pre_nms_thresh = pre_nms_thresh\n        self.pre_nms_top_n = pre_nms_top_n\n        self.nms_thresh = nms_thresh\n        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n        self.min_size = min_size\n\n        if box_coder is None:\n            box_coder = BoxCoder(weights=(10., 10., 5., 5.))\n        self.box_coder = box_coder\n\n    def forward_for_single_feature_map(self, anchors, box_cls, box_regression,\n                                      pre_nms_thresh):\n        """"""\n        Arguments:\n            anchors: list[BoxList]\n            box_cls: tensor of size N, A * C, H, W\n            box_regression: tensor of size N, A * 4, H, W\n        """"""\n        device = box_cls.device\n        N, _ , H, W = box_cls.shape\n        A = int(box_regression.size(1) / 4)\n        C = int(box_cls.size(1) / A)\n\n        # put in the same format as anchors\n        box_cls = box_cls.view(N, -1, C, H, W).permute(0, 3, 4, 1, 2)\n        box_cls = box_cls.reshape(N, -1, C)\n        box_cls = box_cls.sigmoid()\n\n        box_regression = box_regression.view(N, -1, 4, H, W)\n        box_regression = box_regression.permute(0, 3, 4, 1, 2)\n        box_regression = box_regression.reshape(N, -1, 4)\n\n        num_anchors = A * H * W\n\n        results = [[] for _ in range(N)]\n        candidate_inds = box_cls > pre_nms_thresh\n        if candidate_inds.sum().item() == 0:\n            empty_boxlists = []\n            for a in anchors:\n                empty_boxlist = BoxList(torch.Tensor(0, 4).to(device), a.size)\n                empty_boxlist.add_field(\n                    ""labels"", torch.LongTensor([]).to(device))\n                empty_boxlist.add_field(\n                    ""scores"", torch.Tensor([]).to(device))\n                empty_boxlists.append(empty_boxlist)\n            return empty_boxlists\n\n        pre_nms_top_n = candidate_inds.view(N, -1).sum(1)\n        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n\n        for batch_idx, (per_box_cls, per_box_regression, per_pre_nms_top_n, \\\n        per_candidate_inds, per_anchors) in enumerate(zip(\n            box_cls,\n            box_regression,\n            pre_nms_top_n,\n            candidate_inds,\n            anchors)):\n\n            # Sort and select TopN\n            per_box_cls = per_box_cls[per_candidate_inds]\n            per_candidate_nonzeros = per_candidate_inds.nonzero()\n            per_box_loc = per_candidate_nonzeros[:, 0]\n            per_class = per_candidate_nonzeros[:, 1]\n            per_class += 1\n            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n                per_box_cls, top_k_indices = \\\n                        per_box_cls.topk(per_pre_nms_top_n, sorted=False)\n                per_box_loc = per_box_loc[top_k_indices]\n                per_class = per_class[top_k_indices]\n\n            detections = self.box_coder.decode(\n                per_box_regression[per_box_loc, :].view(-1, 4),\n                per_anchors.bbox[per_box_loc, :].view(-1, 4)\n            )\n\n            boxlist = BoxList(detections, per_anchors.size, mode=""xyxy"")\n            boxlist.add_field(""labels"", per_class)\n            boxlist.add_field(""scores"", per_box_cls)\n            boxlist = boxlist.clip_to_image(remove_empty=False)\n            boxlist = remove_small_boxes(boxlist, self.min_size)\n            results[batch_idx] = boxlist\n\n        return results\n\n    def forward(self, anchors, box_cls, box_regression, targets=None):\n        """"""\n        Arguments:\n            anchors: list[list[BoxList]]\n            box_cls: list[tensor]\n            box_regression: list[tensor]\n\n        Returns:\n            boxlists (list[BoxList]): the post-processed anchors, after\n                applying box decoding and NMS\n        """"""\n        sampled_boxes = []\n        num_levels = len(box_cls)\n        anchors = list(zip(*anchors))\n        for layer, (a, o, b) in enumerate(\n            zip(anchors, box_cls, box_regression)):\n            sampled_boxes.append(\n                self.forward_for_single_feature_map(\n                    a, o, b,\n                    self.pre_nms_thresh\n                )\n            )\n\n        sampled_boxes = list(zip(*sampled_boxes))\n        boxlists = []\n        for boxlist in sampled_boxes:\n            resized_list = []\n            size = boxlist[-1].size\n            for box in boxlist:\n                if box.size == size:\n                    resized_list.append(box)\n                else:\n                    resized_list.append(box.resize(size))\n            boxlists.append(cat_boxlist(resized_list))\n\n        boxlists = self.select_over_all_levels(boxlists)\n\n        return boxlists\n\n    def select_over_all_levels(self, boxlists):\n        num_images = len(boxlists)\n        results = []\n        for i in range(num_images):\n            scores = boxlists[i].get_field(""scores"")\n            labels = boxlists[i].get_field(""labels"")\n            boxes = boxlists[i].bbox\n            boxlist = boxlists[i]\n            result = []\n            # skip the background\n            for j in range(1, 81):\n                inds = (labels == j).nonzero().view(-1)\n                if len(inds) == 0:\n                    continue\n\n                scores_j = scores[inds]\n                boxes_j = boxes[inds, :].view(-1, 4)\n                boxlist_for_class = BoxList(boxes_j, boxlist.size, mode=""xyxy"")\n                boxlist_for_class.add_field(""scores"", scores_j)\n                boxlist_for_class = boxlist_nms(\n                    boxlist_for_class, self.nms_thresh,\n                    score_field=""scores""\n                )\n                num_labels = len(boxlist_for_class)\n                boxlist_for_class.add_field(\n                    ""labels"", torch.full((num_labels,), j,\n                                         dtype=torch.int64,\n                                         device=scores.device)\n                )\n                result.append(boxlist_for_class)\n\n            if len(result) > 0:\n                result = cat_boxlist(result)\n                number_of_detections = len(result)\n\n                # Limit to max_per_image detections **over all classes**\n                if number_of_detections > self.fpn_post_nms_top_n > 0:\n                    cls_scores = result.get_field(""scores"")\n                    image_thresh, _ = torch.kthvalue(\n                        cls_scores.cpu(),\n                        number_of_detections - self.fpn_post_nms_top_n + 1\n                    )\n                    keep = cls_scores >= image_thresh.item()\n                    keep = torch.nonzero(keep).squeeze(1)\n                    result = result[keep]\n                results.append(result)\n            else:\n                empty_boxlist = BoxList(torch.zeros(1, 4).to(\'cuda\'), boxlist.size)\n                empty_boxlist.add_field(\n                    ""labels"", torch.LongTensor([1]).to(\'cuda\'))\n                empty_boxlist.add_field(\n                    ""scores"", torch.Tensor([0.01]).to(\'cuda\'))\n                results.append(empty_boxlist)\n        return results\n\n\ndef make_retinanet_postprocessor(\n    config, fpn_post_nms_top_n, rpn_box_coder):\n\n    pre_nms_thresh = 0.05\n    pre_nms_top_n = config.RETINANET.PRE_NMS_TOP_N\n    fpn_post_nms_top_n = fpn_post_nms_top_n\n    nms_thresh = config.MODEL.RPN.NMS_THRESH\n    min_size = config.MODEL.RPN.MIN_SIZE\n    box_selector = RetinaNetPostProcessor(\n        pre_nms_thresh=pre_nms_thresh,\n        pre_nms_top_n=pre_nms_top_n,\n        nms_thresh=nms_thresh,\n        fpn_post_nms_top_n=fpn_post_nms_top_n,\n        box_coder=rpn_box_coder,\n        min_size=min_size\n    )\n    return box_selector\n'"
maskrcnn_benchmark/modeling/rpn/retinanet_loss.py,6,"b'""""""\nThis file contains specific functions for computing losses on the RetinaNet\nfile\n""""""\n\nimport torch\nfrom ..utils import cat\n\nfrom maskrcnn_benchmark.layers import SmoothL1Loss\nfrom maskrcnn_benchmark.layers import SigmoidFocalLoss\nfrom maskrcnn_benchmark.modeling.matcher import Matcher\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\n\n\nclass RetinaNetLossComputation(object):\n    """"""\n    This class computes the RetinaNet loss.\n    """"""\n\n    def __init__(self, cfg, proposal_matcher, box_coder):\n        """"""\n        Arguments:\n            proposal_matcher (Matcher)\n            box_coder (BoxCoder)\n        """"""\n        # self.target_preparator = target_preparator\n        self.proposal_matcher = proposal_matcher\n        self.box_coder = box_coder\n        self.num_classes = cfg.RETINANET.NUM_CLASSES -1\n        self.box_cls_loss_func = SigmoidFocalLoss(\n            self.num_classes,\n            cfg.RETINANET.LOSS_GAMMA,\n            cfg.RETINANET.LOSS_ALPHA\n        )\n        self.regression_loss = SmoothL1Loss(\n            beta=cfg.RETINANET.BBOX_REG_BETA\n        )\n\n    def match_targets_to_anchors(self, anchor, target):\n        match_quality_matrix = boxlist_iou(target, anchor)\n        matched_idxs = self.proposal_matcher(match_quality_matrix)\n        # RPN doesn\'t need any fields from target\n        # for creating the labels, so clear them all\n        target = target.copy_with_fields([\'labels\'])\n        # get the targets corresponding GT for each anchor\n        # NB: need to clamp the indices because we can have a single\n        # GT in the image, and matched_idxs can be -2, which goes\n        # out of bounds\n        matched_targets = target[matched_idxs.clamp(min=0)]\n        matched_targets.add_field(""matched_idxs"", matched_idxs)\n        return matched_targets\n\n    def prepare_targets(self, anchors, targets):\n        labels = []\n        regression_targets = []\n        for anchors_per_image, targets_per_image in zip(anchors, targets):\n            matched_targets = self.match_targets_to_anchors(\n                anchors_per_image, targets_per_image\n            )\n\n            matched_idxs = matched_targets.get_field(""matched_idxs"")\n            labels_per_image = matched_targets.get_field(""labels"").clone()\n\n            # Background (negative examples)\n            bg_indices = matched_idxs == Matcher.BELOW_LOW_THRESHOLD\n            labels_per_image[bg_indices] = 0\n\n            # discard indices that are between thresholds \n            # -1 will be ignored in SigmoidFocalLoss\n            inds_to_discard = matched_idxs == Matcher.BETWEEN_THRESHOLDS\n            labels_per_image[inds_to_discard] = -1\n\n            labels_per_image = labels_per_image.to(dtype=torch.float32)\n            # compute regression targets\n            regression_targets_per_image = self.box_coder.encode(\n                matched_targets.bbox, anchors_per_image.bbox\n            )\n\n            labels.append(labels_per_image)\n            regression_targets.append(regression_targets_per_image)\n\n        return labels, regression_targets\n\n    def __call__(self, anchors, box_cls, box_regression, targets):\n        """"""\n        Arguments:\n            anchors (list[BoxList])\n            objectness (list[Tensor])\n            box_regression (list[Tensor])\n            targets (list[BoxList])\n\n        Returns:\n            objectness_loss (Tensor)\n            box_loss (Tensor\n        """"""\n        anchors = [cat_boxlist(anchors_per_image) for anchors_per_image in anchors]\n        labels, regression_targets = self.prepare_targets(anchors, targets)\n\n        # sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n        # sampled_pos_inds = torch.nonzero(torch.cat(sampled_pos_inds, dim=0)).squeeze(1)\n        # sampled_neg_inds = torch.nonzero(torch.cat(sampled_neg_inds, dim=0)).squeeze(1)\n\n        # sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n        num_layers = len(box_cls)\n        box_cls_flattened = []\n        box_regression_flattened = []\n        # for each feature level, permute the outputs to make them be in the\n        # same format as the labels. Note that the labels are computed for\n        # all feature levels concatenated, so we keep the same representation\n        # for the objectness and the box_regression\n        for box_cls_per_level, box_regression_per_level in zip(\n            box_cls, box_regression\n        ):\n            N, A, H, W = box_cls_per_level.shape\n            C = self.num_classes\n            box_cls_per_level = box_cls_per_level.view(N, -1, C, H, W)\n            box_cls_per_level = box_cls_per_level.permute(0, 3, 4, 1, 2)\n            box_cls_per_level = box_cls_per_level.reshape(N, -1, C)\n            box_regression_per_level = box_regression_per_level.view(N, -1, 4, H, W)\n            box_regression_per_level = box_regression_per_level.permute(0, 3, 4, 1, 2)\n            box_regression_per_level = box_regression_per_level.reshape(N, -1, 4)\n            box_cls_flattened.append(box_cls_per_level)\n            box_regression_flattened.append(box_regression_per_level)\n        # concatenate on the first dimension (representing the feature levels), to\n        # take into account the way the labels were generated (with all feature maps\n        # being concatenated as well)\n        box_cls = cat(box_cls_flattened, dim=1).reshape(-1, C)\n        box_regression = cat(box_regression_flattened, dim=1).reshape(-1, 4)\n\n        labels = torch.cat(labels, dim=0)\n        regression_targets = torch.cat(regression_targets, dim=0)\n        pos_inds = labels > 0\n\n        retinanet_regression_loss = self.regression_loss(\n            box_regression[pos_inds],\n            regression_targets[pos_inds],\n            size_average=False,\n        ) / (pos_inds.sum() * 4)\n        labels = labels.int()\n\n        retinanet_cls_loss =self.box_cls_loss_func(\n            box_cls,\n            labels\n        ) / ((labels > 0).sum() + N)\n\n        losses = {\n            ""loss_retina_cls"": retinanet_cls_loss,\n            ""loss_retina_reg"": retinanet_regression_loss,\n        }\n\n        return losses\n\n\ndef make_retinanet_loss_evaluator(cfg, box_coder):\n    matcher = Matcher(\n        cfg.MODEL.RPN.FG_IOU_THRESHOLD,\n        cfg.MODEL.RPN.BG_IOU_THRESHOLD,\n        allow_low_quality_matches=cfg.RETINANET.LOW_QUALITY_MATCHES,\n        low_quality_threshold=cfg.RETINANET.LOW_QUALITY_THRESHOLD\n    )\n\n    loss_evaluator = RetinaNetLossComputation(\n        cfg, matcher, box_coder\n    )\n    return loss_evaluator\n'"
maskrcnn_benchmark/modeling/rpn/rpn.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom .loss import make_rpn_loss_evaluator\nfrom .anchor_generator import make_anchor_generator\nfrom .inference import make_rpn_postprocessor\n\n\nclass RPNHead(nn.Module):\n    """"""\n    Adds a simple RPN Head with classification and regression heads\n    """"""\n\n    def __init__(self, in_channels, num_anchors):\n        """"""\n        Arguments:\n            in_channels (int): number of channels of the input feature\n            num_anchors (int): number of anchors to be predicted\n        """"""\n        super(RPNHead, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n        self.bbox_pred = nn.Conv2d(\n            in_channels, num_anchors * 4, kernel_size=1, stride=1\n        )\n\n        for l in [self.conv, self.cls_logits, self.bbox_pred]:\n            torch.nn.init.normal_(l.weight, std=0.01)\n            torch.nn.init.constant_(l.bias, 0)\n\n    def forward(self, x):\n        logits = []\n        bbox_reg = []\n        for feature in x:\n            t = F.relu(self.conv(feature))\n            logits.append(self.cls_logits(t))\n            bbox_reg.append(self.bbox_pred(t))\n        return logits, bbox_reg\n\n\nclass RPNModule(torch.nn.Module):\n    """"""\n    Module for RPN computation. Takes feature maps from the backbone and RPN\n    proposals and losses. Works for both FPN and non-FPN.\n    """"""\n\n    def __init__(self, cfg):\n        super(RPNModule, self).__init__()\n\n        self.cfg = cfg.clone()\n\n        anchor_generator = make_anchor_generator(cfg)\n\n        in_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS\n        head = RPNHead(in_channels, anchor_generator.num_anchors_per_location()[0])\n\n        rpn_box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n\n        box_selector_train = make_rpn_postprocessor(cfg, rpn_box_coder, is_train=True)\n        box_selector_test = make_rpn_postprocessor(cfg, rpn_box_coder, is_train=False)\n\n        loss_evaluator = make_rpn_loss_evaluator(cfg, rpn_box_coder)\n\n        self.anchor_generator = anchor_generator\n        self.head = head\n        self.box_selector_train = box_selector_train\n        self.box_selector_test = box_selector_test\n        self.loss_evaluator = loss_evaluator\n\n    def forward(self, images, features, targets=None):\n        """"""\n        Arguments:\n            images (ImageList): images for which we want to compute the predictions\n            features (list[Tensor]): features computed from the images that are\n                used for computing the predictions. Each tensor in the list\n                correspond to different feature levels\n            targets (list[BoxList): ground-truth boxes present in the image (optional)\n\n        Returns:\n            boxes (list[BoxList]): the predicted boxes from the RPN, one BoxList per\n                image.\n            losses (dict[Tensor]): the losses for the model during training. During\n                testing, it is an empty dict.\n        """"""\n        objectness, rpn_box_regression = self.head(features)\n        anchors = self.anchor_generator(images, features)\n\n        if self.training:\n            return self._forward_train(anchors, objectness, rpn_box_regression, targets)\n        else:\n            return self._forward_test(anchors, objectness, rpn_box_regression)\n\n    def _forward_train(self, anchors, objectness, rpn_box_regression, targets):\n        if self.cfg.MODEL.RPN_ONLY:\n            # When training an RPN-only model, the loss is determined by the\n            # predicted objectness and rpn_box_regression values and there is\n            # no need to transform the anchors into predicted boxes; this is an\n            # optimization that avoids the unnecessary transformation.\n            boxes = anchors\n        else:\n            # For end-to-end models, anchors must be transformed into boxes and\n            # sampled into a training batch.\n            with torch.no_grad():\n                boxes = self.box_selector_train(\n                    anchors, objectness, rpn_box_regression, targets\n                )\n        loss_objectness, loss_rpn_box_reg = self.loss_evaluator(\n            anchors, objectness, rpn_box_regression, targets\n        )\n        losses = {\n            ""loss_objectness"": loss_objectness,\n            ""loss_rpn_box_reg"": loss_rpn_box_reg,\n        }\n        return boxes, losses\n\n    def _forward_test(self, anchors, objectness, rpn_box_regression):\n        boxes = self.box_selector_test(anchors, objectness, rpn_box_regression)\n        if self.cfg.MODEL.RPN_ONLY:\n            # For end-to-end models, the RPN proposals are an intermediate state\n            # and don\'t bother to sort them in decreasing score order. For RPN-only\n            # models, the proposals are the final output and we return them in\n            # high-to-low confidence order.\n            inds = [\n                box.get_field(""objectness"").sort(descending=True)[1] for box in boxes\n            ]\n            boxes = [box[ind] for box, ind in zip(boxes, inds)]\n        return boxes, {}\n\n\ndef build_rpn(cfg):\n    """"""\n    This gives the gist of it. Not super important because it doesn\'t change as much\n    """"""\n    return RPNModule(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/box_head/box_head.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\n\nfrom .roi_box_feature_extractors import make_roi_box_feature_extractor\nfrom .roi_box_predictors import make_roi_box_predictor\nfrom .inference import make_roi_box_post_processor\nfrom .loss import make_roi_box_loss_evaluator\n\nclass ROIBoxHead(torch.nn.Module):\n    """"""\n    Generic Box Head class.\n    """"""\n\n    def __init__(self, cfg):\n        super(ROIBoxHead, self).__init__()\n        self.feature_extractor = make_roi_box_feature_extractor(cfg)\n        self.predictor = make_roi_box_predictor(cfg)\n        self.post_processor = make_roi_box_post_processor(cfg)\n        self.loss_evaluator = make_roi_box_loss_evaluator(cfg)\n\n    def forward(self, features, proposals, targets=None):\n        """"""\n        Arguments:\n            features (list[Tensor]): feature-maps from possibly several levels\n            proposals (list[BoxList]): proposal boxes\n            targets (list[BoxList], optional): the ground-truth targets.\n\n        Returns:\n            x (Tensor): the result of the feature extractor\n            proposals (list[BoxList]): during training, the subsampled proposals\n                are returned. During testing, the predicted boxlists are returned\n            losses (dict[Tensor]): During training, returns the losses for the\n                head. During testing, returns an empty dict.\n        """"""\n\n        if self.training:\n            # Faster R-CNN subsamples during training the proposals with a fixed\n            # positive / negative ratio\n            with torch.no_grad():\n                proposals = self.loss_evaluator.subsample(proposals, targets)\n\n        # extract features that will be fed to the final classifier. The\n        # feature_extractor generally corresponds to the pooler + heads\n        x = self.feature_extractor(features, proposals)\n        # final classifier that converts the features into predictions\n        class_logits, box_regression = self.predictor(x)\n\n        if not self.training:\n            result = self.post_processor((class_logits, box_regression), proposals)\n            return x, result, {}\n\n        loss_dict = self.loss_evaluator(\n            [class_logits], [box_regression]\n        )\n        return (\n            x,\n            proposals,\n            loss_dict,\n        )\n\n\ndef build_roi_box_head(cfg):\n    """"""\n    Constructs a new box head.\n    By default, uses ROIBoxHead, but if it turns out not to be enough, just register a new class\n    and make it a parameter in the config\n    """"""\n    return ROIBoxHead(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_nms\nfrom maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\n\n\nclass PostProcessor(nn.Module):\n    """"""\n    From a set of classification scores, box regression and proposals,\n    computes the post-processed boxes, and applies NMS to obtain the\n    final results\n    """"""\n\n    def __init__(\n        self, score_thresh=0.05, nms=0.5, detections_per_img=100, box_coder=None, free_anchor=False\n    ):\n        """"""\n        Arguments:\n            score_thresh (float)\n            nms (float)\n            detections_per_img (int)\n            box_coder (BoxCoder)\n        """"""\n        super(PostProcessor, self).__init__()\n        self.score_thresh = score_thresh\n        self.nms = nms\n        self.detections_per_img = detections_per_img\n        if box_coder is None:\n            box_coder = BoxCoder(weights=(10., 10., 5., 5.))\n        self.box_coder = box_coder\n        self.free_anchor = free_anchor\n\n    def forward(self, x, boxes):\n        """"""\n        Arguments:\n            x (tuple[tensor, tensor]): x contains the class logits\n                and the box_regression from the model.\n            boxes (list[BoxList]): bounding boxes that are used as\n                reference, one for ech image\n\n        Returns:\n            results (list[BoxList]): one BoxList for each image, containing\n                the extra fields labels and scores\n        """"""\n        class_logits, box_regression = x\n\n        class_prob = torch.sigmoid(class_logits) if self.free_anchor else F.softmax(class_logits, -1)\n\n        # TODO think about a representation of batch of boxes\n        image_shapes = [box.size for box in boxes]\n        boxes_per_image = [len(box) for box in boxes]\n        concat_boxes = torch.cat([a.bbox for a in boxes], dim=0)\n\n        proposals = self.box_coder.decode(\n            box_regression.view(sum(boxes_per_image), -1), concat_boxes\n        )\n\n        num_classes = class_prob.shape[1]\n\n        proposals = proposals.split(boxes_per_image, dim=0)\n        class_prob = class_prob.split(boxes_per_image, dim=0)\n\n        results = []\n        for prob, boxes_per_img, image_shape in zip(\n            class_prob, proposals, image_shapes\n        ):\n            boxlist = self.prepare_boxlist(boxes_per_img, prob, image_shape)\n            boxlist = boxlist.clip_to_image(remove_empty=False)\n            boxlist = self.filter_results(boxlist, num_classes)\n            results.append(boxlist)\n        return results\n\n    def prepare_boxlist(self, boxes, scores, image_shape):\n        """"""\n        Returns BoxList from `boxes` and adds probability scores information\n        as an extra field\n        `boxes` has shape (#detections, 4 * #classes), where each row represents\n        a list of predicted bounding boxes for each of the object classes in the\n        dataset (including the background class). The detections in each row\n        originate from the same object proposal.\n        `scores` has shape (#detection, #classes), where each row represents a list\n        of object detection confidence scores for each of the object classes in the\n        dataset (including the background class). `scores[i, j]`` corresponds to the\n        box at `boxes[i, j * 4:(j + 1) * 4]`.\n        """"""\n        boxes = boxes.reshape(-1, 4)\n        scores = scores.reshape(-1)\n        boxlist = BoxList(boxes, image_shape, mode=""xyxy"")\n        boxlist.add_field(""scores"", scores)\n        return boxlist\n\n    def filter_results(self, boxlist, num_classes):\n        """"""Returns bounding-box detection results by thresholding on scores and\n        applying non-maximum suppression (NMS).\n        """"""\n        # unwrap the boxlist to avoid additional overhead.\n        # if we had multi-class NMS, we could perform this directly on the boxlist\n        boxes = boxlist.bbox.reshape(-1, num_classes * 4)\n        scores = boxlist.get_field(""scores"").reshape(-1, num_classes)\n\n        device = scores.device\n        result = []\n        # Apply threshold on detection probabilities and apply NMS\n        # Skip j = 0, because it\'s the background class\n        inds_all = scores > self.score_thresh\n        no_background = int(self.free_anchor)\n        for j in range(1 - no_background, num_classes - no_background):\n            inds = inds_all[:, j].nonzero().squeeze(1)\n            scores_j = scores[inds, j]\n            boxes_j = boxes[inds, j * 4 : (j + 1) * 4]\n            boxlist_for_class = BoxList(boxes_j, boxlist.size, mode=""xyxy"")\n            boxlist_for_class.add_field(""scores"", scores_j)\n            boxlist_for_class = boxlist_nms(\n                boxlist_for_class, self.nms, score_field=""scores""\n            )\n            num_labels = len(boxlist_for_class)\n            boxlist_for_class.add_field(\n                ""labels"", torch.full((num_labels,), j + no_background, dtype=torch.int64, device=device)\n            )\n            result.append(boxlist_for_class)\n\n        result = cat_boxlist(result)\n        number_of_detections = len(result)\n\n        # Limit to max_per_image detections **over all classes**\n        if number_of_detections > self.detections_per_img > 0:\n            cls_scores = result.get_field(""scores"")\n            image_thresh, _ = torch.kthvalue(\n                cls_scores.cpu(), number_of_detections - self.detections_per_img + 1\n            )\n            keep = cls_scores >= image_thresh.item()\n            keep = torch.nonzero(keep).squeeze(1)\n            result = result[keep]\n        return result\n\n\ndef make_roi_box_post_processor(cfg):\n    free_anchor = cfg.FREEANCHOR.FREEANCHOR_ON\n\n    bbox_reg_weights = cfg.MODEL.ROI_HEADS.BBOX_REG_WEIGHTS\n    box_coder = BoxCoder(weights=bbox_reg_weights)\n\n    score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH\n    nms_thresh = cfg.MODEL.ROI_HEADS.NMS\n    detections_per_img = cfg.MODEL.ROI_HEADS.DETECTIONS_PER_IMG\n\n    postprocessor = PostProcessor(\n        score_thresh, nms_thresh, detections_per_img, box_coder, free_anchor\n    )\n    return postprocessor\n'"
maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch.nn import functional as F\n\nfrom maskrcnn_benchmark.layers import smooth_l1_loss\nfrom maskrcnn_benchmark.modeling.box_coder import BoxCoder\nfrom maskrcnn_benchmark.modeling.matcher import Matcher\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.modeling.balanced_positive_negative_sampler import (\n    BalancedPositiveNegativeSampler\n)\nfrom maskrcnn_benchmark.modeling.utils import cat\n\n\nclass FastRCNNLossComputation(object):\n    """"""\n    Computes the loss for Faster R-CNN.\n    Also supports FPN\n    """"""\n\n    def __init__(self, proposal_matcher, fg_bg_sampler, box_coder):\n        """"""\n        Arguments:\n            proposal_matcher (Matcher)\n            fg_bg_sampler (BalancedPositiveNegativeSampler)\n            box_coder (BoxCoder)\n        """"""\n        self.proposal_matcher = proposal_matcher\n        self.fg_bg_sampler = fg_bg_sampler\n        self.box_coder = box_coder\n\n    def match_targets_to_proposals(self, proposal, target):\n        match_quality_matrix = boxlist_iou(target, proposal)\n        matched_idxs = self.proposal_matcher(match_quality_matrix)\n        # Fast RCNN only need ""labels"" field for selecting the targets\n        target = target.copy_with_fields(""labels"")\n        # get the targets corresponding GT for each proposal\n        # NB: need to clamp the indices because we can have a single\n        # GT in the image, and matched_idxs can be -2, which goes\n        # out of bounds\n        matched_targets = target[matched_idxs.clamp(min=0)]\n        matched_targets.add_field(""matched_idxs"", matched_idxs)\n        return matched_targets\n\n    def prepare_targets(self, proposals, targets):\n        labels = []\n        regression_targets = []\n        for proposals_per_image, targets_per_image in zip(proposals, targets):\n            matched_targets = self.match_targets_to_proposals(\n                proposals_per_image, targets_per_image\n            )\n            matched_idxs = matched_targets.get_field(""matched_idxs"")\n\n            labels_per_image = matched_targets.get_field(""labels"")\n            labels_per_image = labels_per_image.to(dtype=torch.int64)\n\n            # Label background (below the low threshold)\n            bg_inds = matched_idxs == Matcher.BELOW_LOW_THRESHOLD\n            labels_per_image[bg_inds] = 0\n\n            # Label ignore proposals (between low and high thresholds)\n            ignore_inds = matched_idxs == Matcher.BETWEEN_THRESHOLDS\n            labels_per_image[ignore_inds] = -1  # -1 is ignored by sampler\n\n            # compute regression targets\n            regression_targets_per_image = self.box_coder.encode(\n                matched_targets.bbox, proposals_per_image.bbox\n            )\n\n            labels.append(labels_per_image)\n            regression_targets.append(regression_targets_per_image)\n\n        return labels, regression_targets\n\n    def subsample(self, proposals, targets):\n        """"""\n        This method performs the positive/negative sampling, and return\n        the sampled proposals.\n        Note: this function keeps a state.\n\n        Arguments:\n            proposals (list[BoxList])\n            targets (list[BoxList])\n        """"""\n\n        labels, regression_targets = self.prepare_targets(proposals, targets)\n        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n\n        proposals = list(proposals)\n        # add corresponding label and regression_targets information to the bounding boxes\n        for labels_per_image, regression_targets_per_image, proposals_per_image in zip(\n            labels, regression_targets, proposals\n        ):\n            proposals_per_image.add_field(""labels"", labels_per_image)\n            proposals_per_image.add_field(\n                ""regression_targets"", regression_targets_per_image\n            )\n\n        # distributed sampled proposals, that were obtained on all feature maps\n        # concatenated via the fg_bg_sampler, into individual feature map levels\n        for img_idx, (pos_inds_img, neg_inds_img) in enumerate(\n            zip(sampled_pos_inds, sampled_neg_inds)\n        ):\n            img_sampled_inds = torch.nonzero(pos_inds_img | neg_inds_img).squeeze(1)\n            proposals_per_image = proposals[img_idx][img_sampled_inds]\n            proposals[img_idx] = proposals_per_image\n\n        self._proposals = proposals\n        return proposals\n\n    def __call__(self, class_logits, box_regression):\n        """"""\n        Computes the loss for Faster R-CNN.\n        This requires that the subsample method has been called beforehand.\n\n        Arguments:\n            class_logits (list[Tensor])\n            box_regression (list[Tensor])\n\n        Returns:\n            classification_loss (Tensor)\n            box_loss (Tensor)\n        """"""\n\n        class_logits = cat(class_logits, dim=0)\n        box_regression = cat(box_regression, dim=0)\n        device = class_logits.device\n\n        if not hasattr(self, ""_proposals""):\n            raise RuntimeError(""subsample needs to be called before"")\n\n        proposals = self._proposals\n\n        labels = cat([proposal.get_field(""labels"") for proposal in proposals], dim=0)\n        regression_targets = cat(\n            [proposal.get_field(""regression_targets"") for proposal in proposals], dim=0\n        )\n\n        classification_loss = F.cross_entropy(class_logits, labels)\n\n        # get indices that correspond to the regression targets for\n        # the corresponding ground truth labels, to be used with\n        # advanced indexing\n        sampled_pos_inds_subset = torch.nonzero(labels > 0).squeeze(1)\n        labels_pos = labels[sampled_pos_inds_subset]\n        map_inds = 4 * labels_pos[:, None] + torch.tensor([0, 1, 2, 3], device=device)\n\n        box_loss = smooth_l1_loss(\n            box_regression[sampled_pos_inds_subset[:, None], map_inds],\n            regression_targets[sampled_pos_inds_subset],\n            size_average=False,\n            beta=1,\n        )\n        box_loss = box_loss / labels.numel()\n\n        return dict(loss_classifier=classification_loss, loss_box_reg=box_loss)\n\n\ndef make_roi_box_loss_evaluator(cfg):\n    matcher = Matcher(\n        cfg.MODEL.ROI_HEADS.FG_IOU_THRESHOLD,\n        cfg.MODEL.ROI_HEADS.BG_IOU_THRESHOLD,\n        allow_low_quality_matches=False,\n    )\n\n    bbox_reg_weights = cfg.MODEL.ROI_HEADS.BBOX_REG_WEIGHTS\n    box_coder = BoxCoder(weights=bbox_reg_weights)\n\n    fg_bg_sampler = BalancedPositiveNegativeSampler(\n        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE, cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION\n    )\n\n    loss_evaluator = FastRCNNLossComputation(matcher, fg_bg_sampler, box_coder)\n\n    return loss_evaluator\n'"
maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_feature_extractors.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom maskrcnn_benchmark.modeling.backbone import resnet\nfrom maskrcnn_benchmark.modeling.poolers import Pooler\n\n\nclass ResNet50Conv5ROIFeatureExtractor(nn.Module):\n    def __init__(self, config):\n        super(ResNet50Conv5ROIFeatureExtractor, self).__init__()\n\n        resolution = config.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n        scales = config.MODEL.ROI_BOX_HEAD.POOLER_SCALES\n        sampling_ratio = config.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n        pooler = Pooler(\n            output_size=(resolution, resolution),\n            scales=scales,\n            sampling_ratio=sampling_ratio,\n        )\n\n        stage = resnet.StageSpec(index=4, block_count=3, return_features=False)\n        head = resnet.ResNetHead(\n            block_module=config.MODEL.RESNETS.TRANS_FUNC,\n            stages=(stage,),\n            num_groups=config.MODEL.RESNETS.NUM_GROUPS,\n            width_per_group=config.MODEL.RESNETS.WIDTH_PER_GROUP,\n            stride_in_1x1=config.MODEL.RESNETS.STRIDE_IN_1X1,\n            stride_init=None,\n            res2_out_channels=config.MODEL.RESNETS.RES2_OUT_CHANNELS,\n        )\n\n        self.pooler = pooler\n        self.head = head\n\n    def forward(self, x, proposals):\n        x = self.pooler(x, proposals)\n        x = self.head(x)\n        return x\n\n\nclass FPN2MLPFeatureExtractor(nn.Module):\n    """"""\n    Heads for FPN for classification\n    """"""\n\n    def __init__(self, cfg):\n        super(FPN2MLPFeatureExtractor, self).__init__()\n\n        resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n        scales = cfg.MODEL.ROI_BOX_HEAD.POOLER_SCALES\n        sampling_ratio = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n        pooler = Pooler(\n            output_size=(resolution, resolution),\n            scales=scales,\n            sampling_ratio=sampling_ratio,\n        )\n        input_size = cfg.MODEL.BACKBONE.OUT_CHANNELS * resolution ** 2\n        representation_size = cfg.MODEL.ROI_BOX_HEAD.MLP_HEAD_DIM\n        self.pooler = pooler\n        self.fc6 = nn.Linear(input_size, representation_size)\n        self.fc7 = nn.Linear(representation_size, representation_size)\n\n        for l in [self.fc6, self.fc7]:\n            # Caffe2 implementation uses XavierFill, which in fact\n            # corresponds to kaiming_uniform_ in PyTorch\n            nn.init.kaiming_uniform_(l.weight, a=1)\n            nn.init.constant_(l.bias, 0)\n\n    def forward(self, x, proposals):\n        x = self.pooler(x, proposals)\n        x = x.view(x.size(0), -1)\n\n        x = F.relu(self.fc6(x))\n        x = F.relu(self.fc7(x))\n\n        return x\n\n\n_ROI_BOX_FEATURE_EXTRACTORS = {\n    ""ResNet50Conv5ROIFeatureExtractor"": ResNet50Conv5ROIFeatureExtractor,\n    ""FPN2MLPFeatureExtractor"": FPN2MLPFeatureExtractor,\n}\n\n\ndef make_roi_box_feature_extractor(cfg):\n    func = _ROI_BOX_FEATURE_EXTRACTORS[cfg.MODEL.ROI_BOX_HEAD.FEATURE_EXTRACTOR]\n    return func(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_predictors.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom torch import nn\nimport math\n\n\nclass FastRCNNPredictor(nn.Module):\n    def __init__(self, cfg, pretrained=None):\n        super(FastRCNNPredictor, self).__init__()\n\n        stage_index = 4\n        stage2_relative_factor = 2 ** (stage_index - 1)\n        res2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n        num_inputs = res2_out_channels * stage2_relative_factor\n\n        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=7)\n\n        if cfg.FREEANCHOR.FREEANCHOR_ON:\n            num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES - 1\n            prior_prob = cfg.RETINANET.PRIOR_PROB\n            bias_value = -math.log((1 - prior_prob) / prior_prob)\n        else:\n            num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES\n            bias_value = 0\n        self.cls_score = nn.Linear(num_inputs, num_classes)\n        self.bbox_pred = nn.Linear(num_inputs, num_classes * 4)\n\n        nn.init.normal_(self.cls_score.weight, mean=0, std=0.01)\n        nn.init.constant_(self.cls_score.bias, bias_value)\n\n        nn.init.normal_(self.bbox_pred.weight, mean=0, std=0.001)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n\n    def forward(self, x):\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        cls_logit = self.cls_score(x)\n        bbox_pred = self.bbox_pred(x)\n        return cls_logit, bbox_pred\n\n\nclass FPNPredictor(nn.Module):\n    def __init__(self, cfg):\n        super(FPNPredictor, self).__init__()\n        representation_size = cfg.MODEL.ROI_BOX_HEAD.MLP_HEAD_DIM\n\n        if cfg.FREEANCHOR.FREEANCHOR_ON:\n            num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES - 1\n            prior_prob = cfg.RETINANET.PRIOR_PROB\n            bias_value = -math.log((1 - prior_prob) / prior_prob)\n        else:\n            num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES\n            bias_value = 0\n\n        self.cls_score = nn.Linear(representation_size, num_classes)\n        self.bbox_pred = nn.Linear(representation_size, num_classes * 4)\n\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.constant_(self.cls_score.bias, bias_value)\n\n        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n\n    def forward(self, x):\n        scores = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n\n        return scores, bbox_deltas\n\n\n_ROI_BOX_PREDICTOR = {\n    ""FastRCNNPredictor"": FastRCNNPredictor,\n    ""FPNPredictor"": FPNPredictor,\n}\n\n\ndef make_roi_box_predictor(cfg):\n    func = _ROI_BOX_PREDICTOR[cfg.MODEL.ROI_BOX_HEAD.PREDICTOR]\n    return func(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/mask_head/inference.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\n\n# TODO check if want to return a single BoxList or a composite\n# object\nclass MaskPostProcessor(nn.Module):\n    """"""\n    From the results of the CNN, post process the masks\n    by taking the mask corresponding to the class with max\n    probability (which are of fixed size and directly output\n    by the CNN) and return the masks in the mask field of the BoxList.\n\n    If a masker object is passed, it will additionally\n    project the masks in the image according to the locations in boxes,\n    """"""\n\n    def __init__(self, masker=None):\n        super(MaskPostProcessor, self).__init__()\n        self.masker = masker\n\n    def forward(self, x, boxes):\n        """"""\n        Arguments:\n            x (Tensor): the mask logits\n            boxes (list[BoxList]): bounding boxes that are used as\n                reference, one for ech image\n\n        Returns:\n            results (list[BoxList]): one BoxList for each image, containing\n                the extra field mask\n        """"""\n        mask_prob = x.sigmoid()\n\n        # select masks coresponding to the predicted classes\n        num_masks = x.shape[0]\n        labels = [bbox.get_field(""labels"") for bbox in boxes]\n        labels = torch.cat(labels)\n        index = torch.arange(num_masks, device=labels.device)\n        mask_prob = mask_prob[index, labels][:, None]\n\n        if self.masker:\n            mask_prob = self.masker(mask_prob, boxes)\n\n        boxes_per_image = [len(box) for box in boxes]\n        mask_prob = mask_prob.split(boxes_per_image, dim=0)\n\n        results = []\n        for prob, box in zip(mask_prob, boxes):\n            bbox = BoxList(box.bbox, box.size, mode=""xyxy"")\n            for field in box.fields():\n                bbox.add_field(field, box.get_field(field))\n            bbox.add_field(""mask"", prob)\n            results.append(bbox)\n\n        return results\n\n\nclass MaskPostProcessorCOCOFormat(MaskPostProcessor):\n    """"""\n    From the results of the CNN, post process the results\n    so that the masks are pasted in the image, and\n    additionally convert the results to COCO format.\n    """"""\n\n    def forward(self, x, boxes):\n        import pycocotools.mask as mask_util\n        import numpy as np\n\n        results = super(MaskPostProcessorCOCOFormat, self).forward(x, boxes)\n        for result in results:\n            masks = result.get_field(""mask"").cpu()\n            rles = [\n                mask_util.encode(np.array(mask[0, :, :, np.newaxis], order=""F""))[0]\n                for mask in masks\n            ]\n            for rle in rles:\n                rle[""counts""] = rle[""counts""].decode(""utf-8"")\n            result.add_field(""mask"", rles)\n        return results\n\n\n# the next two functions should be merged inside Masker\n# but are kept here for the moment while we need them\n# temporarily gor paste_mask_in_image\ndef expand_boxes(boxes, scale):\n    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_exp = torch.zeros_like(boxes)\n    boxes_exp[:, 0] = x_c - w_half\n    boxes_exp[:, 2] = x_c + w_half\n    boxes_exp[:, 1] = y_c - h_half\n    boxes_exp[:, 3] = y_c + h_half\n    return boxes_exp\n\n\ndef expand_masks(mask, padding):\n    N = mask.shape[0]\n    M = mask.shape[-1]\n    pad2 = 2 * padding\n    scale = float(M + pad2) / M\n    padded_mask = mask.new_zeros((N, 1, M + pad2, M + pad2))\n    padded_mask[:, :, padding:-padding, padding:-padding] = mask\n    return padded_mask, scale\n\n\ndef paste_mask_in_image(mask, box, im_h, im_w, thresh=0.5, padding=1):\n    padded_mask, scale = expand_masks(mask[None], padding=padding)\n    mask = padded_mask[0, 0]\n    box = expand_boxes(box[None], scale)[0]\n    box = box.numpy().astype(np.int32)\n\n    TO_REMOVE = 1\n    w = box[2] - box[0] + TO_REMOVE\n    h = box[3] - box[1] + TO_REMOVE\n    w = max(w, 1)\n    h = max(h, 1)\n\n    mask = Image.fromarray(mask.cpu().numpy())\n    mask = mask.resize((w, h), resample=Image.BILINEAR)\n    mask = np.array(mask, copy=False)\n\n    if thresh >= 0:\n        mask = np.array(mask > thresh, dtype=np.uint8)\n        mask = torch.from_numpy(mask)\n    else:\n        # for visualization and debugging, we also\n        # allow it to return an unmodified mask\n        mask = torch.from_numpy(mask * 255).to(torch.uint8)\n\n    im_mask = torch.zeros((im_h, im_w), dtype=torch.uint8)\n    x_0 = max(box[0], 0)\n    x_1 = min(box[2] + 1, im_w)\n    y_0 = max(box[1], 0)\n    y_1 = min(box[3] + 1, im_h)\n\n    im_mask[y_0:y_1, x_0:x_1] = mask[\n        (y_0 - box[1]) : (y_1 - box[1]), (x_0 - box[0]) : (x_1 - box[0])\n    ]\n    return im_mask\n\n\nclass Masker(object):\n    """"""\n    Projects a set of masks in an image on the locations\n    specified by the bounding boxes\n    """"""\n\n    def __init__(self, threshold=0.5, padding=1):\n        self.threshold = threshold\n        self.padding = padding\n\n    def forward_single_image(self, masks, boxes):\n        boxes = boxes.convert(""xyxy"")\n        im_w, im_h = boxes.size\n        res = [\n            paste_mask_in_image(mask[0], box, im_h, im_w, self.threshold, self.padding)\n            for mask, box in zip(masks, boxes.bbox)\n        ]\n        if len(res) > 0:\n            res = torch.stack(res, dim=0)[:, None]\n        else:\n            res = masks.new_empty((0, 1, masks.shape[-2], masks.shape[-1]))\n        return res\n\n    def __call__(self, masks, boxes):\n        # TODO do this properly\n        if isinstance(boxes, BoxList):\n            boxes = [boxes]\n        assert len(boxes) == 1, ""Only single image batch supported""\n        result = self.forward_single_image(masks, boxes[0])\n        return result\n\n\ndef make_roi_mask_post_processor(cfg):\n    masker = None\n    mask_post_processor = MaskPostProcessor(masker)\n    return mask_post_processor\n'"
maskrcnn_benchmark/modeling/roi_heads/mask_head/loss.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch.nn import functional as F\n\nfrom maskrcnn_benchmark.layers import smooth_l1_loss\nfrom maskrcnn_benchmark.modeling.matcher import Matcher\nfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou\nfrom maskrcnn_benchmark.modeling.utils import cat\n\n\ndef project_masks_on_boxes(segmentation_masks, proposals, discretization_size):\n    """"""\n    Given segmentation masks and the bounding boxes corresponding\n    to the location of the masks in the image, this function\n    crops and resizes the masks in the position defined by the\n    boxes. This prepares the masks for them to be fed to the\n    loss computation as the targets.\n\n    Arguments:\n        segmentation_masks: an instance of SegmentationMask\n        proposals: an instance of BoxList\n    """"""\n    masks = []\n    M = discretization_size\n    device = proposals.bbox.device\n    proposals = proposals.convert(""xyxy"")\n    assert segmentation_masks.size == proposals.size, ""{}, {}"".format(\n        segmentation_masks, proposals\n    )\n    # TODO put the proposals on the CPU, as the representation for the\n    # masks is not efficient GPU-wise (possibly several small tensors for\n    # representing a single instance mask)\n    proposals = proposals.bbox.to(torch.device(""cpu""))\n    for segmentation_mask, proposal in zip(segmentation_masks, proposals):\n        # crop the masks, resize them to the desired resolution and\n        # then convert them to the tensor representation,\n        # instead of the list representation that was used\n        cropped_mask = segmentation_mask.crop(proposal)\n        scaled_mask = cropped_mask.resize((M, M))\n        mask = scaled_mask.convert(mode=""mask"")\n        masks.append(mask)\n    if len(masks) == 0:\n        return torch.empty(0, dtype=torch.float32, device=device)\n    return torch.stack(masks, dim=0).to(device, dtype=torch.float32)\n\n\nclass MaskRCNNLossComputation(object):\n    def __init__(self, proposal_matcher, discretization_size):\n        """"""\n        Arguments:\n            proposal_matcher (Matcher)\n            discretization_size (int)\n        """"""\n        self.proposal_matcher = proposal_matcher\n        self.discretization_size = discretization_size\n\n    def match_targets_to_proposals(self, proposal, target):\n        match_quality_matrix = boxlist_iou(target, proposal)\n        matched_idxs = self.proposal_matcher(match_quality_matrix)\n        # Mask RCNN needs ""labels"" and ""masks ""fields for creating the targets\n        target = target.copy_with_fields([""labels"", ""masks""])\n        # get the targets corresponding GT for each proposal\n        # NB: need to clamp the indices because we can have a single\n        # GT in the image, and matched_idxs can be -2, which goes\n        # out of bounds\n        matched_targets = target[matched_idxs.clamp(min=0)]\n        matched_targets.add_field(""matched_idxs"", matched_idxs)\n        return matched_targets\n\n    def prepare_targets(self, proposals, targets):\n        labels = []\n        masks = []\n        for proposals_per_image, targets_per_image in zip(proposals, targets):\n            matched_targets = self.match_targets_to_proposals(\n                proposals_per_image, targets_per_image\n            )\n            matched_idxs = matched_targets.get_field(""matched_idxs"")\n\n            labels_per_image = matched_targets.get_field(""labels"")\n            labels_per_image = labels_per_image.to(dtype=torch.int64)\n\n            # this can probably be removed, but is left here for clarity\n            # and completeness\n            neg_inds = matched_idxs == Matcher.BELOW_LOW_THRESHOLD\n            labels_per_image[neg_inds] = 0\n\n            # mask scores are only computed on positive samples\n            positive_inds = torch.nonzero(labels_per_image > 0).squeeze(1)\n\n            segmentation_masks = matched_targets.get_field(""masks"")\n            segmentation_masks = segmentation_masks[positive_inds]\n\n            positive_proposals = proposals_per_image[positive_inds]\n\n            masks_per_image = project_masks_on_boxes(\n                segmentation_masks, positive_proposals, self.discretization_size\n            )\n\n            labels.append(labels_per_image)\n            masks.append(masks_per_image)\n\n        return labels, masks\n\n    def __call__(self, proposals, mask_logits, targets):\n        """"""\n        Arguments:\n            proposals (list[BoxList])\n            mask_logits (Tensor)\n            targets (list[BoxList])\n\n        Return:\n            mask_loss (Tensor): scalar tensor containing the loss\n        """"""\n        labels, mask_targets = self.prepare_targets(proposals, targets)\n\n        labels = cat(labels, dim=0)\n        mask_targets = cat(mask_targets, dim=0)\n\n        positive_inds = torch.nonzero(labels > 0).squeeze(1)\n        labels_pos = labels[positive_inds]\n\n        # torch.mean (in binary_cross_entropy_with_logits) doesn\'t\n        # accept empty tensors, so handle it separately\n        if mask_targets.numel() == 0:\n            return mask_logits.sum() * 0\n\n        mask_loss = F.binary_cross_entropy_with_logits(\n            mask_logits[positive_inds, labels_pos], mask_targets\n        )\n        return mask_loss\n\n\ndef make_roi_mask_loss_evaluator(cfg):\n    matcher = Matcher(\n        cfg.MODEL.ROI_HEADS.FG_IOU_THRESHOLD,\n        cfg.MODEL.ROI_HEADS.BG_IOU_THRESHOLD,\n        allow_low_quality_matches=False,\n    )\n\n    loss_evaluator = MaskRCNNLossComputation(\n        matcher, cfg.MODEL.ROI_MASK_HEAD.RESOLUTION\n    )\n\n    return loss_evaluator\n'"
maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\nfrom .roi_mask_feature_extractors import make_roi_mask_feature_extractor\nfrom .roi_mask_predictors import make_roi_mask_predictor\nfrom .inference import make_roi_mask_post_processor\nfrom .loss import make_roi_mask_loss_evaluator\n\n\ndef keep_only_positive_boxes(boxes):\n    """"""\n    Given a set of BoxList containing the `labels` field,\n    return a set of BoxList for which `labels > 0`.\n\n    Arguments:\n        boxes (list of BoxList)\n    """"""\n    assert isinstance(boxes, (list, tuple))\n    assert isinstance(boxes[0], BoxList)\n    assert boxes[0].has_field(""labels"")\n    positive_boxes = []\n    positive_inds = []\n    num_boxes = 0\n    for boxes_per_image in boxes:\n        labels = boxes_per_image.get_field(""labels"")\n        inds_mask = labels > 0\n        inds = inds_mask.nonzero().squeeze(1)\n        positive_boxes.append(boxes_per_image[inds])\n        positive_inds.append(inds_mask)\n    return positive_boxes, positive_inds\n\n\nclass ROIMaskHead(torch.nn.Module):\n    def __init__(self, cfg):\n        super(ROIMaskHead, self).__init__()\n        self.cfg = cfg.clone()\n        self.feature_extractor = make_roi_mask_feature_extractor(cfg)\n        self.predictor = make_roi_mask_predictor(cfg)\n        self.post_processor = make_roi_mask_post_processor(cfg)\n        self.loss_evaluator = make_roi_mask_loss_evaluator(cfg)\n\n    def forward(self, features, proposals, targets=None):\n        """"""\n        Arguments:\n            features (list[Tensor]): feature-maps from possibly several levels\n            proposals (list[BoxList]): proposal boxes\n            targets (list[BoxList], optional): the ground-truth targets.\n\n        Returns:\n            x (Tensor): the result of the feature extractor\n            proposals (list[BoxList]): during training, the original proposals\n                are returned. During testing, the predicted boxlists are returned\n                with the `mask` field set\n            losses (dict[Tensor]): During training, returns the losses for the\n                head. During testing, returns an empty dict.\n        """"""\n\n        if self.training:\n            # during training, only focus on positive boxes\n            all_proposals = proposals\n            proposals, positive_inds = keep_only_positive_boxes(proposals)\n        if self.training and self.cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:\n            x = features\n            x = x[torch.cat(positive_inds, dim=0)]\n        else:\n            x = self.feature_extractor(features, proposals)\n        mask_logits = self.predictor(x)\n\n        if not self.training:\n            result = self.post_processor(mask_logits, proposals)\n            return x, result, {}\n\n        loss_mask = self.loss_evaluator(proposals, mask_logits, targets)\n\n        return x, all_proposals, dict(loss_mask=loss_mask)\n\n\ndef build_roi_mask_head(cfg):\n    return ROIMaskHead(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_feature_extractors.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..box_head.roi_box_feature_extractors import ResNet50Conv5ROIFeatureExtractor\nfrom maskrcnn_benchmark.modeling.poolers import Pooler\nfrom maskrcnn_benchmark.layers import Conv2d\n\n\nclass MaskRCNNFPNFeatureExtractor(nn.Module):\n    """"""\n    Heads for FPN for classification\n    """"""\n\n    def __init__(self, cfg):\n        """"""\n        Arguments:\n            num_classes (int): number of output classes\n            input_size (int): number of channels of the input once it\'s flattened\n            representation_size (int): size of the intermediate representation\n        """"""\n        super(MaskRCNNFPNFeatureExtractor, self).__init__()\n\n        resolution = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\n        scales = cfg.MODEL.ROI_MASK_HEAD.POOLER_SCALES\n        sampling_ratio = cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO\n        pooler = Pooler(\n            output_size=(resolution, resolution),\n            scales=scales,\n            sampling_ratio=sampling_ratio,\n            canonical_level=cfg.MODEL.ROI_MASK_HEAD.CANONICAL_LEVEL,\n        )\n        input_size = cfg.MODEL.BACKBONE.OUT_CHANNELS\n        self.pooler = pooler\n\n        layers = cfg.MODEL.ROI_MASK_HEAD.CONV_LAYERS\n\n        next_feature = input_size\n        self.blocks = []\n        for layer_idx, layer_features in enumerate(layers, 1):\n            layer_name = ""mask_fcn{}"".format(layer_idx)\n            module = Conv2d(next_feature, layer_features, 3, stride=1, padding=1)\n            # Caffe2 implementation uses MSRAFill, which in fact\n            # corresponds to kaiming_normal_ in PyTorch\n            nn.init.kaiming_normal_(module.weight, mode=""fan_out"", nonlinearity=""relu"")\n            nn.init.constant_(module.bias, 0)\n            self.add_module(layer_name, module)\n            next_feature = layer_features\n            self.blocks.append(layer_name)\n\n    def forward(self, x, proposals):\n        x = self.pooler(x, proposals)\n\n        for layer_name in self.blocks:\n            x = F.relu(getattr(self, layer_name)(x))\n\n        return x\n\n\n_ROI_MASK_FEATURE_EXTRACTORS = {\n    ""ResNet50Conv5ROIFeatureExtractor"": ResNet50Conv5ROIFeatureExtractor,\n    ""MaskRCNNFPNFeatureExtractor"": MaskRCNNFPNFeatureExtractor,\n}\n\n\ndef make_roi_mask_feature_extractor(cfg):\n    func = _ROI_MASK_FEATURE_EXTRACTORS[cfg.MODEL.ROI_MASK_HEAD.FEATURE_EXTRACTOR]\n    return func(cfg)\n'"
maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_predictors.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom maskrcnn_benchmark.layers import Conv2d\nfrom maskrcnn_benchmark.layers import ConvTranspose2d\n\n\nclass MaskRCNNC4Predictor(nn.Module):\n    def __init__(self, cfg):\n        super(MaskRCNNC4Predictor, self).__init__()\n        num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES\n        dim_reduced = cfg.MODEL.ROI_MASK_HEAD.CONV_LAYERS[-1]\n\n        if cfg.MODEL.ROI_HEADS.USE_FPN:\n            num_inputs = dim_reduced\n        else:\n            stage_index = 4\n            stage2_relative_factor = 2 ** (stage_index - 1)\n            res2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n            num_inputs = res2_out_channels * stage2_relative_factor\n\n        self.conv5_mask = ConvTranspose2d(num_inputs, dim_reduced, 2, 2, 0)\n        self.mask_fcn_logits = Conv2d(dim_reduced, num_classes, 1, 1, 0)\n\n        for name, param in self.named_parameters():\n            if ""bias"" in name:\n                nn.init.constant_(param, 0)\n            elif ""weight"" in name:\n                # Caffe2 implementation uses MSRAFill, which in fact\n                # corresponds to kaiming_normal_ in PyTorch\n                nn.init.kaiming_normal_(param, mode=""fan_out"", nonlinearity=""relu"")\n\n    def forward(self, x):\n        x = F.relu(self.conv5_mask(x))\n        return self.mask_fcn_logits(x)\n\n\n_ROI_MASK_PREDICTOR = {""MaskRCNNC4Predictor"": MaskRCNNC4Predictor}\n\n\ndef make_roi_mask_predictor(cfg):\n    func = _ROI_MASK_PREDICTOR[cfg.MODEL.ROI_MASK_HEAD.PREDICTOR]\n    return func(cfg)\n'"
