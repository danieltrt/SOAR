file_path,api_count,code
gan.py,25,"b'from PIL import Image\nimport numpy as np\nimport cv2\nimport torchvision.transforms as transforms\nimport torch\nimport io\nimport os\nimport functools\n\nclass DataLoader():\n\n\tdef __init__(self, opt, cv_img):\n\t\tsuper(DataLoader, self).__init__()\n\n\t\tself.dataset = Dataset()\n\t\tself.dataset.initialize(opt, cv_img)\n\n\t\tself.dataloader = torch.utils.data.DataLoader(\n\t\t\tself.dataset,\n\t\t\tbatch_size=opt.batchSize,\n\t\t\tshuffle=not opt.serial_batches,\n\t\t\tnum_workers=int(opt.nThreads))\n\n\tdef load_data(self):\n\t\treturn self.dataloader\n\n\tdef __len__(self):\n\t\treturn 1\n\nclass Dataset(torch.utils.data.Dataset):\n\tdef __init__(self):\n\t\tsuper(Dataset, self).__init__()\n\n\tdef initialize(self, opt, cv_img):\n\t\tself.opt = opt\n\t\tself.root = opt.dataroot\n\n\t\tself.A = Image.fromarray(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))\n\t\tself.dataset_size = 1\n\t\n\tdef __getitem__(self, index):        \n\n\t\ttransform_A = get_transform(self.opt)\n\t\tA_tensor = transform_A(self.A.convert(\'RGB\'))\n\n\t\tB_tensor = inst_tensor = feat_tensor = 0\n\n\t\tinput_dict = {\'label\': A_tensor, \'inst\': inst_tensor, \'image\': B_tensor, \n\t\t\t\t\t  \'feat\': feat_tensor, \'path\': """"}\n\n\t\treturn input_dict\n\n\tdef __len__(self):\n\t\treturn 1    \n\nclass DeepModel(torch.nn.Module):\n\n\tdef initialize(self, opt):\n\n\t\ttorch.cuda.empty_cache()\n\n\t\tself.opt = opt\n\n\t\tself.gpu_ids = [0] #FIX CPU\n\n\t\tself.netG = self.__define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, \n\t\t\t\t\t\t\t\t\t  opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n\t\t\t\t\t\t\t\t\t  opt.n_blocks_local, opt.norm, self.gpu_ids)        \n\n\t\t# load networks\n\t\tself.__load_network(self.netG)\n\n\t\t\n\t\n\tdef inference(self, label, inst):\n\t\t\n\t\t# Encode Inputs        \n\t\tinput_label, inst_map, _, _ = self.__encode_input(label, inst, infer=True)\n\n\t\t# Fake Generation\n\t\tinput_concat = input_label        \n\t\t\n\t\twith torch.no_grad():\n\t\t\tfake_image = self.netG.forward(input_concat)\n\n\t\treturn fake_image\n\t\n\t# helper loading function that can be used by subclasses\n\tdef __load_network(self, network):\n\n\t\tsave_path = os.path.join(self.opt.checkpoints_dir)\n\n\t\tnetwork.load_state_dict(torch.load(save_path))\n\n\tdef __encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, infer=False):             \n\t\tif (len(self.gpu_ids) > 0): \n\t\t\tinput_label = label_map.data.cuda() #GPU\n\t\telse: \n\t\t\tinput_label = label_map.data #CPU\n\t\t\t\n\t\treturn input_label, inst_map, real_image, feat_map\n\n\tdef __weights_init(self, m):\n\t\tclassname = m.__class__.__name__\n\t\tif classname.find(\'Conv\') != -1:\n\t\t\tm.weight.data.normal_(0.0, 0.02)\n\t\telif classname.find(\'BatchNorm2d\') != -1:\n\t\t\tm.weight.data.normal_(1.0, 0.02)\n\t\t\tm.bias.data.fill_(0)\n\n\tdef __define_G(self, input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1, \n\t\t\t\t n_blocks_local=3, norm=\'instance\', gpu_ids=[]):    \n\t\tnorm_layer = self.__get_norm_layer(norm_type=norm)         \n\t\tnetG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)\n\t\t\n\t\tif len(gpu_ids) > 0:\n\t\t\tnetG.cuda(gpu_ids[0])\n\t\tnetG.apply(self.__weights_init)\n\t\treturn netG\n\n\tdef __get_norm_layer(self, norm_type=\'instance\'):\n\t\tnorm_layer = functools.partial(torch.nn.InstanceNorm2d, affine=False)\n\t\treturn norm_layer\n\n##############################################################################\n# Generator\n##############################################################################\nclass GlobalGenerator(torch.nn.Module):\n\tdef __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=torch.nn.BatchNorm2d, \n\t\t\t\t padding_type=\'reflect\'):\n\t\tassert(n_blocks >= 0)\n\t\tsuper(GlobalGenerator, self).__init__()        \n\t\tactivation = torch.nn.ReLU(True)        \n\n\t\tmodel = [torch.nn.ReflectionPad2d(3), torch.nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n\t\t### downsample\n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**i\n\t\t\tmodel += [torch.nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\t  norm_layer(ngf * mult * 2), activation]\n\n\t\t### resnet blocks\n\t\tmult = 2**n_downsampling\n\t\tfor i in range(n_blocks):\n\t\t\tmodel += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n\t\t\n\t\t### upsample         \n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**(n_downsampling - i)\n\t\t\tmodel += [torch.nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n\t\t\t\t\t   norm_layer(int(ngf * mult / 2)), activation]\n\t\tmodel += [torch.nn.ReflectionPad2d(3), torch.nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), torch.nn.Tanh()]        \n\t\tself.model = torch.nn.Sequential(*model)\n\t\t\t\n\tdef forward(self, input):\n\t\treturn self.model(input)             \n\t\t\n# Define a resnet block\nclass ResnetBlock(torch.nn.Module):\n\tdef __init__(self, dim, padding_type, norm_layer, activation=torch.nn.ReLU(True), use_dropout=False):\n\t\tsuper(ResnetBlock, self).__init__()\n\t\tself.conv_block = self.__build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n\n\tdef __build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n\t\tconv_block = []\n\t\tp = 0\n\t\tif padding_type == \'reflect\':\n\t\t\tconv_block += [torch.nn.ReflectionPad2d(1)]\n\t\telif padding_type == \'replicate\':\n\t\t\tconv_block += [torch.nn.ReplicationPad2d(1)]\n\t\telif padding_type == \'zero\':\n\t\t\tp = 1\n\t\telse:\n\t\t\traise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n\t\tconv_block += [torch.nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n\t\t\t\t\t   norm_layer(dim),\n\t\t\t\t\t   activation]\n\t\tif use_dropout:\n\t\t\tconv_block += [torch.nn.Dropout(0.5)]\n\n\t\tp = 0\n\t\tif padding_type == \'reflect\':\n\t\t\tconv_block += [torch.nn.ReflectionPad2d(1)]\n\t\telif padding_type == \'replicate\':\n\t\t\tconv_block += [torch.nn.ReplicationPad2d(1)]\n\t\telif padding_type == \'zero\':\n\t\t\tp = 1\n\t\telse:\n\t\t\traise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\t\tconv_block += [torch.nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n\t\t\t\t\t   norm_layer(dim)]\n\n\t\treturn torch.nn.Sequential(*conv_block)\n\n\tdef forward(self, x):\n\t\tout = x + self.conv_block(x)\n\t\treturn out\n\n# Data utils:\ndef get_transform(opt, method=Image.BICUBIC, normalize=True):\n\ttransform_list = []\n\n\tbase = float(2 ** opt.n_downsample_global)\n\tif opt.netG == \'local\':\n\t\tbase *= (2 ** opt.n_local_enhancers)\n\ttransform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n\n\ttransform_list += [transforms.ToTensor()]\n\n\tif normalize:\n\t\ttransform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n\t\t\t\t\t\t\t\t\t\t\t\t(0.5, 0.5, 0.5))]\n\treturn transforms.Compose(transform_list)\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n\tow, oh = img.size        \n\th = int(round(oh / base) * base)\n\tw = int(round(ow / base) * base)\n\tif (h == oh) and (w == ow):\n\t\treturn img\n\treturn img.resize((w, h), method)\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True):\n\tif isinstance(image_tensor, list):\n\t\timage_numpy = []\n\t\tfor i in range(len(image_tensor)):\n\t\t\timage_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n\t\treturn image_numpy\n\timage_numpy = image_tensor.cpu().float().numpy()\n\tif normalize:\n\t\timage_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n\telse:\n\t\timage_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0      \n\timage_numpy = np.clip(image_numpy, 0, 255)\n\tif image_numpy.shape[2] == 1 or image_numpy.shape[2] > 3:        \n\t\timage_numpy = image_numpy[:,:,0]\n\treturn image_numpy.astype(imtype)'"
main.py,0,"b'# coding=utf-8\nimport sys\nimport cv2\nimport argparse\nimport os\nimport sys\nimport subprocess\n\nfrom run import process\n\n""""""\nmain.py\n\n How to run:\n python main.py\n\n""""""\n\ndef open_file(filepath):\n    if sys.platform == ""win32"":\n        os.startfile(filepath)\n    else:\n        opener =""open"" if sys.platform == ""darwin"" else ""xdg-open""\n        subprocess.call([opener, filepath])\n\ndef main(inputpath, outpath, show):\n\tautomatic_show_image = show.lower() == ""true""\n\tif isinstance(inputpath, list):\n\t\tfor item in inputpath:\n\t\t\twatermark = deep_nude_process(item)\n\t\t\tcv2.imwrite(""output_""+item, watermark)\n\t\t\tif automatic_show_image:\n\t\t\t\topen_file(""output_""+item)\n\telse:\n\t\twatermark = deep_nude_process(inputpath)\n\t\tcv2.imwrite(outputpath, watermark)\n\t\tif automatic_show_image:\n\t\t\topen_file(outputpath)\n\ndef deep_nude_process(item):\n    print(\'Processing {}\'.format(item))\n    dress = cv2.imread(item)\n    h = dress.shape[0]\n    w = dress.shape[1]\n    dress = cv2.resize(dress, (512,512), interpolation=cv2.INTER_CUBIC)\n    watermark = process(dress)\n    watermark = cv2.resize(watermark, (w,h), interpolation=cv2.INTER_CUBIC)\n    return watermark\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser(description=""simple deep nude script tool"")\n\tparser.add_argument(""-i"", ""--input"", action=""store"", nargs = ""*"", default=""input.png"", help = ""Use to enter input one or more files\'s name"")\n\tparser.add_argument(""-o"", ""--output"", action=""store"", default=""output.png"", help = ""Use to enter output file name"")\n\tparser.add_argument(""-s"", ""--show"", action=""store"", default=""true"", help = ""Use to automatically display or not display generated images"")\n\tinputpath, outputpath, show = parser.parse_args().input, parser.parse_args().output, parser.parse_args().show\n\tmain(inputpath, outputpath, show)\n'"
run.py,0,"b'import cv2\n\n#Import Neural Network Model\nfrom gan import DataLoader, DeepModel, tensor2im\n\n#OpenCv Transform:\nfrom opencv_transform.mask_to_maskref import create_maskref\nfrom opencv_transform.maskdet_to_maskfin import create_maskfin\nfrom opencv_transform.dress_to_correct import create_correct\nfrom opencv_transform.nude_to_watermark import create_watermark\n\n""""""\nrun.py\n\nThis script manage the entire transormation.\n\nTransformation happens in 6 phases:\n\t0: dress -> correct [opencv] dress_to_correct\n\t1: correct -> mask:  [GAN] correct_to_mask\n\t2: mask -> maskref [opencv] mask_to_maskref\n\t3: maskref -> maskdet [GAN] maskref_to_maskdet\n\t4: maskdet -> maskfin [opencv] maskdet_to_maskfin\n\t5: maskfin -> nude [GAN] maskfin_to_nude\n\t6: nude -> watermark [opencv] nude_to_watermark\n\n""""""\n\nphases = [""dress_to_correct"", ""correct_to_mask"", ""mask_to_maskref"", ""maskref_to_maskdet"", ""maskdet_to_maskfin"", ""maskfin_to_nude"", ""nude_to_watermark""]\n\nclass Options():\n\n\t#Init options with default values\n\tdef __init__(self):\n\t\n\t\t# experiment specifics\n\t\tself.norm = \'batch\' #instance normalization or batch normalization\n\t\tself.use_dropout = False #use dropout for the generator\n\t\tself.data_type = 32 #Supported data type i.e. 8, 16, 32 bit\n\n\t\t# input/output sizes       \n\t\tself.batchSize = 1 #input batch size\n\t\tself.input_nc = 3 # of input image channels\n\t\tself.output_nc = 3 # of output image channels\n\n\t\t# for setting inputs\n\t\tself.serial_batches = True #if true, takes images in order to make batches, otherwise takes them randomly\n\t\tself.nThreads = 1 ## threads for loading data (???)\n\t\tself.max_dataset_size = 1 #Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\n\t\t\n\t\t# for generator\n\t\tself.netG = \'global\' #selects model to use for netG\n\t\tself.ngf = 64 ## of gen filters in first conv layer\n\t\tself.n_downsample_global = 4 #number of downsampling layers in netG\n\t\tself.n_blocks_global = 9 #number of residual blocks in the global generator network\n\t\tself.n_blocks_local = 0 #number of residual blocks in the local enhancer network\n\t\tself.n_local_enhancers = 0 #number of local enhancers to use\n\t\tself.niter_fix_global = 0 #number of epochs that we only train the outmost local enhancer\n\n\t\t#Phase specific options\n\t\tself.checkpoints_dir = """"\n\t\tself.dataroot = """"\n\n\t#Changes options accordlying to actual phase\n\tdef updateOptions(self, phase):\n\n\t\tif phase == ""correct_to_mask"":\n\t\t\tself.checkpoints_dir = ""checkpoints/cm.lib""\n\n\t\telif phase == ""maskref_to_maskdet"":\n\t\t\tself.checkpoints_dir = ""checkpoints/mm.lib""\n\n\t\telif phase == ""maskfin_to_nude"":\n\t\t\tself.checkpoints_dir = ""checkpoints/mn.lib""\n\n# process(cv_img, mode)\n# return:\n# \twatermark image\ndef process(cv_img):\n\n\t#InMemory cv2 images:\n\tdress = cv_img\n\tcorrect = None\n\tmask = None\n\tmaskref = None\n\tmaskfin = None\n\tmaskdet = None\n\tnude = None\n\twatermark = None\n\n\tfor index, phase in enumerate(phases):\n\n\t\tprint(""Executing phase: "" + phase) \n\t\t\t\n\t\t#GAN phases:\n\t\tif (phase == ""correct_to_mask"") or (phase == ""maskref_to_maskdet"") or (phase == ""maskfin_to_nude""):\n\n\t\t\t#Load global option\n\t\t\topt = Options()\n\n\t\t\t#Load custom phase options:\n\t\t\topt.updateOptions(phase)\n\n\t\t\t#Load Data\n\t\t\tif (phase == ""correct_to_mask""):\n\t\t\t\tdata_loader = DataLoader(opt, correct)\n\t\t\telif (phase == ""maskref_to_maskdet""):\n\t\t\t\tdata_loader = DataLoader(opt, maskref)\n\t\t\telif (phase == ""maskfin_to_nude""):\n\t\t\t\tdata_loader = DataLoader(opt, maskfin)\n\t\t\t\n\t\t\tdataset = data_loader.load_data()\n\t\t\t\n\t\t\t#Create Model\n\t\t\tmodel = DeepModel()\n\t\t\tmodel.initialize(opt)\n\n\t\t\t#Run for every image:\n\t\t\tfor i, data in enumerate(dataset):\n\n\t\t\t\tgenerated = model.inference(data[\'label\'], data[\'inst\'])\n\n\t\t\t\tim = tensor2im(generated.data[0])\n\n\t\t\t\t#Save Data\n\t\t\t\tif (phase == ""correct_to_mask""):\n\t\t\t\t\tmask = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n\n\t\t\t\telif (phase == ""maskref_to_maskdet""):\n\t\t\t\t\tmaskdet = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n\n\t\t\t\telif (phase == ""maskfin_to_nude""):\n\t\t\t\t\tnude = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n\n\t\t#Correcting:\n\t\telif (phase == \'dress_to_correct\'):\n\t\t\tcorrect = create_correct(dress)\n\n\t\t#mask_ref phase (opencv)\n\t\telif (phase == ""mask_to_maskref""):\n\t\t\tmaskref = create_maskref(mask, correct)\n\n\t\t#mask_fin phase (opencv)\n\t\telif (phase == ""maskdet_to_maskfin""):\n\t\t\tmaskfin = create_maskfin(maskref, maskdet)\n\n\t\t#nude_to_watermark phase (opencv)\n\t\telif (phase == ""nude_to_watermark""):\n\t\t\twatermark = create_watermark(nude)\n\n\treturn watermark'"
opencv_transform/__init__.py,0,b''
opencv_transform/annotation.py,0,"b'\n#Object annotation class:\nclass BodyPart:\n\t\n\tdef __init__(self, name, xmin, ymin, xmax, ymax, x, y, w, h):\n\t\tself.name = name\n\t\t#Bounding Box:\n\t\tself.xmin = xmin\n\t\tself.ymin = ymin\n\t\tself.xmax = xmax\n\t\tself.ymax = ymax\n\t\t#Center:\n\t\tself.x = x\n\t\tself.y = y\n\t\t#Dimensione:\n\t\tself.w = w\n\t\tself.h = h'"
opencv_transform/dress_to_correct.py,0,"b'import cv2\nimport math\nimport numpy as np\nimport os\n\n# create_correct ===============================================================\n# return:\n#\t(<Boolean> True/False), depending on the transformation process\ndef create_correct(cv_dress):\n\n\t#Production dir:\n\treturn correct_color(cv_dress, 5)\n\n# correct_color ==============================================================================\n# return:\n# <RGB> image corrected\ndef correct_color(img, percent):\n\n\tassert img.shape[2] == 3\n\tassert percent > 0 and percent < 100\n\n\thalf_percent = percent / 200.0\n\n\tchannels = cv2.split(img)\n\n\tout_channels = []\n\tfor channel in channels:\n\t\tassert len(channel.shape) == 2\n\t\t# find the low and high precentile values (based on the input percentile)\n\t\theight, width = channel.shape\n\t\tvec_size = width * height\n\t\tflat = channel.reshape(vec_size)\n\n\t\tassert len(flat.shape) == 1\n\n\t\tflat = np.sort(flat)\n\n\t\tn_cols = flat.shape[0]\n\n\t\tlow_val  = flat[math.floor(n_cols * half_percent)]\n\t\thigh_val = flat[math.ceil( n_cols * (1.0 - half_percent))]\n\n\t\t# saturate below the low percentile and above the high percentile\n\t\tthresholded = apply_threshold(channel, low_val, high_val)\n\t\t# scale the channel\n\t\tnormalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n\t\tout_channels.append(normalized)\n\n\treturn cv2.merge(out_channels)\n\n#Color correction utils\ndef apply_threshold(matrix, low_value, high_value):\n\tlow_mask = matrix < low_value\n\tmatrix = apply_mask(matrix, low_mask, low_value)\n\n\thigh_mask = matrix > high_value\n\tmatrix = apply_mask(matrix, high_mask, high_value)\n\n\treturn matrix\n\n#Color correction utils\ndef apply_mask(matrix, mask, fill_value):\n\tmasked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n\treturn masked.filled()\n'"
opencv_transform/mask_to_maskref.py,0,"b'import numpy as np\nimport cv2\nimport os\n\n###\n#\n#\tmaskdet_to_maskfin \n#\t\n#\n###\n\n# create_maskref ===============================================================\n# return:\n#\tmaskref image\ndef create_maskref(cv_mask, cv_correct):\n\n\t#Create a total green image\n\tgreen = np.zeros((512,512,3), np.uint8)\n\tgreen[:,:,:] = (0,255,0)      # (B, G, R)\n\n\t#Define the green color filter\n\tf1 = np.asarray([0, 250, 0])   # green color filter\n\tf2 = np.asarray([10, 255, 10])\n\t\n\t#From mask, extrapolate only the green mask\t\t\n\tgreen_mask = cv2.inRange(cv_mask, f1, f2) #green is 0\n\n\t# (OPTIONAL) Apply dilate and open to mask\n\tkernel = np.ones((5,5),np.uint8) #Try change it?\n\tgreen_mask = cv2.dilate(green_mask, kernel, iterations = 1)\n\t#green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)\n\n\t# Create an inverted mask\n\tgreen_mask_inv = cv2.bitwise_not(green_mask)\n\n\t# Cut correct and green image, using the green_mask & green_mask_inv\n\tres1 = cv2.bitwise_and(cv_correct, cv_correct, mask = green_mask_inv)\n\tres2 = cv2.bitwise_and(green, green, mask = green_mask)\n\n\t# Compone:\n\treturn cv2.add(res1, res2)'"
opencv_transform/maskdet_to_maskfin.py,0,"b'import numpy as np\nimport cv2\nimport os\nimport random\n\n#My library:\nfrom opencv_transform.annotation import BodyPart\n\n###\n#\n#\tmaskdet_to_maskfin \n#\t\n#\tsteps:\n#\t\t1. Extract annotation\n#\t\t\t1.a: Filter by color\n#\t\t\t1.b: Find ellipses\n#\t\t\t1.c: Filter out ellipses by max size, and max total numbers\n#\t\t\t1.d: Detect Problems\n#\t\t\t1.e: Resolve the problems, or discard the transformation\n#\t\t2. With the body list, draw maskfin, using maskref\n#\n###\n\n# create_maskfin ==============================================================================\n# return:\n#\t(<Boolean> True/False), depending on the transformation process\ndef create_maskfin(maskref, maskdet):\n\t\n\t#Create a total green image, in which draw details ellipses\n\tdetails = np.zeros((512,512,3), np.uint8)\n\tdetails[:,:,:] = (0,255,0)      # (B, G, R)\n\n\t#Extract body part features:\n\tbodypart_list = extractAnnotations(maskdet);\n\n\t#Check if the list is not empty:\n\tif bodypart_list:\n\t\t\n\t\t#Draw body part in details image:\n\t\tfor obj in bodypart_list:\n\n\t\t\tif obj.w < obj.h:\n\t\t\t\taMax = int(obj.h/2) #asse maggiore\n\t\t\t\taMin = int(obj.w/2) #asse minore\n\t\t\t\tangle = 0 #angle\n\t\t\telse:\n\t\t\t\taMax = int(obj.w/2)\n\t\t\t\taMin = int(obj.h/2)\n\t\t\t\tangle = 90\n\n\t\t\tx = int(obj.x)\n\t\t\ty = int(obj.y)\n\n\t\t\t#Draw ellipse\n\t\t\tif obj.name == ""tit"":\n\t\t\t\tcv2.ellipse(details,(x,y),(aMax,aMin),angle,0,360,(0,205,0),-1) #(0,0,0,50)\n\t\t\telif obj.name == ""aur"":\n\t\t\t\tcv2.ellipse(details,(x,y),(aMax,aMin),angle,0,360,(0,0,255),-1) #red\n\t\t\telif obj.name == ""nip"":\n\t\t\t\tcv2.ellipse(details,(x,y),(aMax,aMin),angle,0,360,(255,255,255),-1) #white\n\t\t\telif obj.name == ""belly"":\n\t\t\t\tcv2.ellipse(details,(x,y),(aMax,aMin),angle,0,360,(255,0,255),-1) #purple\n\t\t\telif obj.name == ""vag"":\n\t\t\t\tcv2.ellipse(details,(x,y),(aMax,aMin),angle,0,360,(255,0,0),-1) #blue\n\t\t\telif obj.name == ""hair"":\n\t\t\t\txmin = x - int(obj.w/2)\n\t\t\t\tymin = y - int(obj.h/2)\n\t\t\t\txmax = x + int(obj.w/2)\n\t\t\t\tymax = y + int(obj.h/2)\n\t\t\t\tcv2.rectangle(details,(xmin,ymin),(xmax,ymax),(100,100,100),-1)\n\n\t\t#Define the green color filter\n\t\tf1 = np.asarray([0, 250, 0])   # green color filter\n\t\tf2 = np.asarray([10, 255, 10])\n\t\t\n\t\t#From maskref, extrapolate only the green mask\t\t\n\t\tgreen_mask = cv2.bitwise_not(cv2.inRange(maskref, f1, f2)) #green is 0\n\n\t\t# Create an inverted mask\n\t\tgreen_mask_inv = cv2.bitwise_not(green_mask)\n\n\t\t# Cut maskref and detail image, using the green_mask & green_mask_inv\n\t\tres1 = cv2.bitwise_and(maskref, maskref, mask = green_mask)\n\t\tres2 = cv2.bitwise_and(details, details, mask = green_mask_inv)\n\n\t\t# Compone:\n\t\tmaskfin = cv2.add(res1, res2)\n\t\treturn maskfin\n\t\n# extractAnnotations ==============================================================================\n# input parameter:\n# \t(<string> maskdet_img): relative path of the single maskdet image (es: testimg1/maskdet/1.png)\n# return:\n#\t(<BodyPart []> bodypart_list) - for failure/error, return an empty list []\ndef extractAnnotations(maskdet):\n\n\t#Load the image\n\t#image = cv2.imread(maskdet_img)\n\n\t#Find body part\n\ttits_list = findBodyPart(maskdet, ""tit"")\n\taur_list = findBodyPart(maskdet, ""aur"")\n\tvag_list = findBodyPart(maskdet, ""vag"")\n\tbelly_list = findBodyPart(maskdet, ""belly"")\n\n\t#Filter out parts basing on dimension (area and aspect ratio):\n\taur_list = filterDimParts(aur_list, 100, 1000, 0.5, 3);\n\ttits_list = filterDimParts(tits_list, 1000, 60000, 0.2, 3);\n\tvag_list = filterDimParts(vag_list, 10, 1000, 0.2, 3);\n\tbelly_list = filterDimParts(belly_list, 10, 1000, 0.2, 3);\n\n\t#Filter couple (if parts are > 2, choose only 2)\n\taur_list = filterCouple(aur_list);\n\ttits_list = filterCouple(tits_list);\n\n\t#Detect a missing problem:\n\tmissing_problem = detectTitAurMissingProblem(tits_list, aur_list) #return a Number (code of the problem)\n\n\t#Check if problem is SOLVEABLE:\n\tif (missing_problem in [3,6,7,8]):\n\t\tresolveTitAurMissingProblems(tits_list, aur_list, missing_problem)\n\t\n\t#Infer the nips:\n\tnip_list = inferNip(aur_list)\n\n\t#Infer the hair:\n\thair_list = inferHair(vag_list)\n\n\t#Return a combined list:\n\treturn tits_list + aur_list + nip_list + vag_list + hair_list + belly_list\n\n# findBodyPart ==============================================================================\n# input parameters:\n# \t(<RGB>image, <string>part_name)\n# return\n#\t(<BodyPart[]>list)\ndef findBodyPart(image, part_name):\n\n\tbodypart_list = [] #empty BodyPart list\n\n\t#Get the correct color filter:\n\tif part_name == ""tit"":\n\t\t#Use combined color filter \n\t\tf1 = np.asarray([0, 0, 0])   # tit color filter\n\t\tf2 = np.asarray([10, 10, 10])\n\t\tf3 = np.asarray([0, 0, 250])   # aur color filter\n\t\tf4 = np.asarray([0, 0, 255])\n\t\tcolor_mask1 = cv2.inRange(image, f1, f2)\n\t\tcolor_mask2 = cv2.inRange(image, f3, f4)\n\t\tcolor_mask = cv2.bitwise_or(color_mask1, color_mask2) #combine\n\t\n\telif part_name == ""aur"":\n\t\tf1 = np.asarray([0, 0, 250])   # aur color filter\n\t\tf2 = np.asarray([0, 0, 255])\n\t\tcolor_mask = cv2.inRange(image, f1, f2)\n\t\n\telif part_name == ""vag"":\n\t\tf1 = np.asarray([250, 0, 0])   # vag filter\n\t\tf2 = np.asarray([255, 0, 0])\n\t\tcolor_mask = cv2.inRange(image, f1, f2)\n\n\telif part_name == ""belly"":\n\t\tf1 = np.asarray([250, 0, 250])   # belly filter\n\t\tf2 = np.asarray([255, 0, 255])\n\t\tcolor_mask = cv2.inRange(image, f1, f2)\n\n\t#find contours:\n\tcontours, hierarchy = cv2.findContours(color_mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n\n\t#for every contour:\n\tfor cnt in contours:\n\n\t\tif len(cnt)>5: #at least 5 points to fit ellipse\n\n\t\t\t#(x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n\t\t\tellipse = cv2.fitEllipse(cnt)\n\n\t\t\t#Fit Result:\n\t\t\tx = ellipse[0][0] #center x\n\t\t\ty = ellipse[0][1] #center y\n\t\t\tangle = ellipse[2] #angle\n\t\t\taMin = ellipse[1][0]; #asse minore\n\t\t\taMax = ellipse[1][1]; #asse maggiore\n\n\t\t\t#Detect direction:\n\t\t\tif angle == 0:\n\t\t\t\th = aMax\n\t\t\t\tw = aMin\n\t\t\telse:\n\t\t\t\th = aMin\n\t\t\t\tw = aMax\n\t\t\t\n\t\t\t#Normalize the belly size:\n\t\t\tif part_name == ""belly"":\n\t\t\t\tif w<15:\n\t\t\t\t\tw *= 2\n\t\t\t\tif h<15:\n\t\t\t\t\th *= 2\n\n\t\t\t#Normalize the vag size:\n\t\t\tif part_name == ""vag"":\n\t\t\t\tif w<15:\n\t\t\t\t\tw *= 2\n\t\t\t\tif h<15:\n\t\t\t\t\th *= 2\n\n\t\t\t#Calculate Bounding Box:\n\t\t\txmin = int(x - (w/2))\n\t\t\txmax = int(x + (w/2))\n\t\t\tymin = int(y - (h/2))\n\t\t\tymax = int(y + (h/2))\n\n\t\t\tbodypart_list.append(BodyPart(part_name, xmin, ymin, xmax, ymax, x, y, w, h ))\n\n\treturn bodypart_list\n\n# filterDimParts ==============================================================================\n# input parameters:\n# \t(<BodyPart[]>list, <num> minimum area of part,  <num> max area, <num> min aspect ratio, <num> max aspect ratio)\ndef filterDimParts(bp_list, min_area, max_area, min_ar, max_ar):\n\n\tb_filt = []\n\n\tfor obj in bp_list:\n\n\t\ta = obj.w*obj.h #Object AREA\n\t\t\n\t\tif ((a > min_area)and(a < max_area)):\n\n\t\t\tar = obj.w/obj.h #Object ASPECT RATIO\n\n\t\t\tif ((ar>min_ar)and(ar<max_ar)):\n\n\t\t\t\tb_filt.append(obj)\n\n\treturn b_filt\n\n# filterCouple ==============================================================================\n# input parameters:\n# \t(<BodyPart[]>list)\ndef filterCouple(bp_list):\n\n\t#Remove exceed parts\n\tif (len(bp_list)>2):\n\n\t\t#trovare coppia (a,b) che minimizza bp_list[a].y-bp_list[b].y\n\t\tmin_a = 0\n\t\tmin_b = 1\n\t\tmin_diff = abs(bp_list[min_a].y-bp_list[min_b].y)\n\t\t\n\t\tfor a in range(0,len(bp_list)):\n\t\t\tfor b in range(0,len(bp_list)):\n\t\t\t\t#TODO: avoid repetition (1,0) (0,1)\n\t\t\t\tif a != b:\n\t\t\t\t\tdiff = abs(bp_list[a].y-bp_list[b].y)\n\t\t\t\t\tif diff<min_diff:\n\t\t\t\t\t\tmin_diff = diff\n\t\t\t\t\t\tmin_a = a\n\t\t\t\t\t\tmin_b = b\n\t\tb_filt = []\n\n\t\tb_filt.append(bp_list[min_a])\n\t\tb_filt.append(bp_list[min_b])\n\n\t\treturn b_filt\n\telse:\n\t\t#No change\n\t\treturn bp_list\n\n\n\n# detectTitAurMissingProblem ==============================================================================\n# input parameters:\n# \t(<BodyPart[]> tits list, <BodyPart[]> aur list)\n# return\n#\t(<num> problem code)\n#   TIT  |  AUR  |  code |  SOLVE?  |\n#    0   |   0   |   1   |    NO    |\n#    0   |   1   |   2   |    NO    |\n#    0   |   2   |   3   |    YES   |\n#    1   |   0   |   4   |    NO    |\n#    1   |   1   |   5   |    NO    |\n#    1   |   2   |   6   |    YES   |\n#    2   |   0   |   7   |    YES   |\n#    2   |   1   |   8   |    YES   |\ndef detectTitAurMissingProblem(tits_list, aur_list):\n\n\tt_len = len(tits_list)\n\ta_len = len(aur_list)\n\n\tif (t_len == 0):\n\t\tif (a_len == 0):\n\t\t\treturn 1\n\t\telif (a_len == 1):\n\t\t\treturn 2\n\t\telif (a_len == 2):\n\t\t\treturn 3\n\t\telse:\n\t\t\treturn -1\n\telif (t_len == 1):\n\t\tif (a_len == 0):\n\t\t\treturn 4\n\t\telif (a_len == 1):\n\t\t\treturn 5\n\t\telif (a_len == 2):\n\t\t\treturn 6\n\t\telse:\n\t\t\treturn -1\n\telif (t_len == 2):\n\t\tif (a_len == 0):\n\t\t\treturn 7\n\t\telif (a_len == 1):\n\t\t\treturn 8\n\t\telse:\n\t\t\treturn -1\n\telse:\n\t\treturn -1\n\n# resolveTitAurMissingProblems ==============================================================================\n# input parameters:\n# \t(<BodyPart[]> tits list, <BodyPart[]> aur list, problem code)\n# return\n#\tnone\ndef resolveTitAurMissingProblems(tits_list, aur_list, problem_code):\n\n\tif problem_code == 3:\n\n\t\trandom_tit_factor = random.randint(2, 5) #TOTEST\n\n\t\t#Add the first tit:\n\t\tnew_w = aur_list[0].w * random_tit_factor #TOTEST\n\t\tnew_x = aur_list[0].x\n\t\tnew_y = aur_list[0].y\n\n\t\txmin = int(new_x - (new_w/2))\n\t\txmax = int(new_x + (new_w/2))\n\t\tymin = int(new_y - (new_w/2))\n\t\tymax = int(new_y + (new_w/2))\n\n\t\ttits_list.append(BodyPart(""tit"", xmin, ymin, xmax, ymax, new_x, new_y, new_w, new_w ))\n\n\t\t#Add the second tit:\n\t\tnew_w = aur_list[1].w * random_tit_factor #TOTEST\n\t\tnew_x = aur_list[1].x\n\t\tnew_y = aur_list[1].y\n\n\t\txmin = int(new_x - (new_w/2))\n\t\txmax = int(new_x + (new_w/2))\n\t\tymin = int(new_y - (new_w/2))\n\t\tymax = int(new_y + (new_w/2))\n\n\t\ttits_list.append(BodyPart(""tit"", xmin, ymin, xmax, ymax, new_x, new_y, new_w, new_w ))\n\n\telif problem_code == 6:\n\n\t\t#Find wich aur is full:\n\t\td1 = abs(tits_list[0].x - aur_list[0].x)\n\t\td2 = abs(tits_list[0].x - aur_list[1].x)\n\n\t\tif d1 > d2:\n\t\t\t#aur[0] is empty\n\t\t\tnew_x = aur_list[0].x\n\t\t\tnew_y = aur_list[0].y\n\t\telse:\n\t\t\t#aur[1] is empty\n\t\t\tnew_x = aur_list[1].x\n\t\t\tnew_y = aur_list[1].y\n\n\t\t#Calculate Bounding Box:\n\t\txmin = int(new_x - (tits_list[0].w/2))\n\t\txmax = int(new_x + (tits_list[0].w/2))\n\t\tymin = int(new_y - (tits_list[0].w/2))\n\t\tymax = int(new_y + (tits_list[0].w/2))\n\n\t\ttits_list.append(BodyPart(""tit"", xmin, ymin, xmax, ymax, new_x, new_y, tits_list[0].w, tits_list[0].w ))\n\n\telif problem_code == 7:\n\n\t\t#Add the first aur:\n\t\tnew_w = tits_list[0].w * random.uniform(0.03, 0.1) #TOTEST\n\t\tnew_x = tits_list[0].x\n\t\tnew_y = tits_list[0].y\n\n\t\txmin = int(new_x - (new_w/2))\n\t\txmax = int(new_x + (new_w/2))\n\t\tymin = int(new_y - (new_w/2))\n\t\tymax = int(new_y + (new_w/2))\n\n\t\taur_list.append(BodyPart(""aur"", xmin, ymin, xmax, ymax, new_x, new_y, new_w, new_w ))\n\n\t\t#Add the second aur:\n\t\tnew_w = tits_list[1].w * random.uniform(0.03, 0.1) #TOTEST\n\t\tnew_x = tits_list[1].x\n\t\tnew_y = tits_list[1].y\n\n\t\txmin = int(new_x - (new_w/2))\n\t\txmax = int(new_x + (new_w/2))\n\t\tymin = int(new_y - (new_w/2))\n\t\tymax = int(new_y + (new_w/2))\n\t\t\n\t\taur_list.append(BodyPart(""aur"", xmin, ymin, xmax, ymax, new_x, new_y, new_w, new_w ))\n\n\telif problem_code == 8:\n\n\t\t#Find wich tit is full:\n\t\td1 = abs(aur_list[0].x - tits_list[0].x)\n\t\td2 = abs(aur_list[0].x - tits_list[1].x)\n\n\t\tif d1 > d2:\n\t\t\t#tit[0] is empty\n\t\t\tnew_x = tits_list[0].x\n\t\t\tnew_y = tits_list[0].y\n\t\telse:\n\t\t\t#tit[1] is empty\n\t\t\tnew_x = tits_list[1].x\n\t\t\tnew_y = tits_list[1].y\n\n\t\t#Calculate Bounding Box:\n\t\txmin = int(new_x - (aur_list[0].w/2))\n\t\txmax = int(new_x + (aur_list[0].w/2))\n\t\tymin = int(new_y - (aur_list[0].w/2))\n\t\tymax = int(new_y + (aur_list[0].w/2))\n\t\taur_list.append(BodyPart(""aur"", xmin, ymin, xmax, ymax, new_x, new_y, aur_list[0].w, aur_list[0].w ))\n\n# detectTitAurPositionProblem ==============================================================================\n# input parameters:\n# \t(<BodyPart[]> tits list, <BodyPart[]> aur list)\n# return\n#\t(<Boolean> True/False)\ndef detectTitAurPositionProblem(tits_list, aur_list):\n\n\tdiffTitsX = abs(tits_list[0].x - tits_list[1].x)\n\tif diffTitsX < 40:\n\t\tprint(""diffTitsX"")\n\t\t#Tits too narrow (orizontally)\n\t\treturn True\n\n\tdiffTitsY = abs(tits_list[0].y - tits_list[1].y)\n\tif diffTitsY > 120:\n\t\t#Tits too distanced (vertically)\n\t\tprint(""diffTitsY"")\n\t\treturn True\n\n\tdiffTitsW = abs(tits_list[0].w - tits_list[1].w)\n\tif ((diffTitsW < 0.1)or(diffTitsW>60)):\n\t\tprint(""diffTitsW"")\n\t\t#Tits too equals, or too different (width)\n\t\treturn True\n\n\t#Check if body position is too low (face not covered by watermark)\n\tif aur_list[0].y > 350: #tits too low\n\t\t#Calculate the ratio between y and aurs distance\n\t\trapp = aur_list[0].y/(abs(aur_list[0].x - aur_list[1].x))\n\t\tif rapp > 2.8:\n\t\t\tprint(""aurDown"")\n\t\t\treturn True\n\n\treturn False\n\n# inferNip ==============================================================================\n# input parameters:\n# \t(<BodyPart[]> aur list)\n# return\n#\t(<BodyPart[]> nip list)\ndef inferNip(aur_list):\n\tnip_list = []\n\n\tfor aur in aur_list:\n\n\t\t#Nip rules:\n\t\t# - circle (w == h)\n\t\t# - min dim: 5\n\t\t# - bigger if aur is bigger\n\t\tnip_dim = int(5 + aur.w*random.uniform(0.03, 0.09))\n\n\t\t#center:\n\t\tx = aur.x\n\t\ty = aur.y\n\n\t\t#Calculate Bounding Box:\n\t\txmin = int(x - (nip_dim/2))\n\t\txmax = int(x + (nip_dim/2))\n\t\tymin = int(y - (nip_dim/2))\n\t\tymax = int(y + (nip_dim/2))\n\n\t\tnip_list.append(BodyPart(""nip"", xmin, ymin, xmax, ymax, x, y, nip_dim, nip_dim ))\n\n\treturn nip_list\n\n# inferHair (TOTEST) ==============================================================================\n# input parameters:\n# \t(<BodyPart[]> vag list)\n# return\n#\t(<BodyPart[]> hair list)\ndef inferHair(vag_list):\n\thair_list = []\n\n\t#70% of chanche to add hair\n\tif random.uniform(0.0, 1.0) > 0.3:\n\n\t\tfor vag in vag_list:\n\n\t\t\t#Hair rules:\n\t\t\thair_w = vag.w*random.uniform(0.4, 1.5)\n\t\t\thair_h = vag.h*random.uniform(0.4, 1.5) \n\n\t\t\t#center:\n\t\t\tx = vag.x\n\t\t\ty = vag.y - (hair_h/2) - (vag.h/2)\n\n\t\t\t#Calculate Bounding Box:\n\t\t\txmin = int(x - (hair_w/2))\n\t\t\txmax = int(x + (hair_w/2))\n\t\t\tymin = int(y - (hair_h/2))\n\t\t\tymax = int(y + (hair_h/2))\n\n\t\t\thair_list.append(BodyPart(""hair"", xmin, ymin, xmax, ymax, x, y, hair_w, hair_h ))\n\n\treturn hair_list\n'"
opencv_transform/nude_to_watermark.py,0,"b'import cv2\nimport numpy as np\nimport os\n\n# create_watermark ===============================================================\n# return:\n#\t(<Boolean> True/False), depending on the transformation process\ndef create_watermark(nude):\n\n\t# Add alpha channel if missing\n\t# if nude.shape[2] < 4:\n\t# \tnude = np.dstack([nude, np.ones((512, 512), dtype=""uint8"") * 255])\n\n\t# watermark = cv2.imread(""fake.png"", cv2.IMREAD_UNCHANGED)\n\t\n\t# f1 = np.asarray([0, 0, 0, 250])   # red color filter\n\t# f2 = np.asarray([255, 255, 255, 255])\n\t# mask = cv2.bitwise_not(cv2.inRange(watermark, f1, f2))\n\t# mask_inv = cv2.bitwise_not(mask)\n\n\t# res1 = cv2.bitwise_and(nude, nude, mask = mask)\n\t# # res2 = cv2.bitwise_and(nude, nude, mask = mask)\n\t# # res2 = cv2.bitwise_and(watermark, watermark, mask = mask_inv)\n\t# res = res1\n\n\t# alpha = 0.6\n\t# return cv2.addWeighted(res, alpha, nude, 1 - alpha, 0) \n\treturn nude'"
