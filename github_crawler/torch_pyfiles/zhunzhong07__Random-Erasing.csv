file_path,api_count,code
cifar.py,13,"b'from __future__ import print_function\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport transforms\nimport torchvision.datasets as datasets\nimport models.cifar as models\n\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch CIFAR10 and 100 Training\')\n# Datasets\nparser.add_argument(\'-d\', \'--dataset\', default=\'cifar10\', type=str)\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\n# Optimization options\nparser.add_argument(\'--epochs\', default=300, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--train-batch\', default=128, type=int, metavar=\'N\',\n                    help=\'train batchsize\')\nparser.add_argument(\'--test-batch\', default=100, type=int, metavar=\'N\',\n                    help=\'test batchsize\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--drop\', \'--dropout\', default=0, type=float,\n                    metavar=\'Dropout\', help=\'Dropout ratio\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                        help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n# Checkpoints\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoint)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n# Architecture\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet20)\')\nparser.add_argument(\'--depth\', type=int, default=20, help=\'Model depth.\')\nparser.add_argument(\'--widen-factor\', type=int, default=10, help=\'Widen factor. 10\')\nparser.add_argument(\'--growthRate\', type=int, default=12, help=\'Growth rate for DenseNet.\')\nparser.add_argument(\'--compressionRate\', type=int, default=2, help=\'Compression Rate (theta) for DenseNet.\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\n\n# Random Erasing\nparser.add_argument(\'--p\', default=0, type=float, help=\'Random Erasing probability\')\nparser.add_argument(\'--sh\', default=0.4, type=float, help=\'max erasing area\')\nparser.add_argument(\'--r1\', default=0.3, type=float, help=\'aspect of erasing area\')\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\n# Validate dataset\nassert args.dataset == \'cifar10\' or args.dataset == \'cifar100\', \'Dataset can only be cifar10 or cifar100.\'\n\n# Use CUDA\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\nif use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef main():\n    global best_acc\n    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n\n    if not os.path.isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n\n\n    # Data\n    print(\'==> Preparing dataset %s\' % args.dataset)\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        transforms.RandomErasing(probability = args.p, sh = args.sh, r1 = args.r1, ),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    if args.dataset == \'cifar10\':\n        dataloader = datasets.CIFAR10\n        num_classes = 10\n    else:\n        dataloader = datasets.CIFAR100\n        num_classes = 100\n\n\n    trainset = dataloader(root=\'./data\', train=True, download=True, transform=transform_train)\n    trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n\n    testset = dataloader(root=\'./data\', train=False, download=False, transform=transform_test)\n    testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n\n    # Model   \n    print(""==> creating model \'{}\'"".format(args.arch))\n    if args.arch.startswith(\'wrn\'):\n        model = models.__dict__[args.arch](\n                    num_classes=num_classes,\n                    depth=args.depth,\n                    widen_factor=args.widen_factor,\n                    dropRate=args.drop,\n                )\n    elif args.arch.endswith(\'resnet\'):\n        model = models.__dict__[args.arch](\n                    num_classes=num_classes,\n                    depth=args.depth,\n                )\n\n    model = torch.nn.DataParallel(model).cuda()\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    # Resume\n    title = \'cifar-10-\' + args.arch\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\')\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.checkpoint = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume)\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n    else:\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        test_loss, test_acc = test(testloader, model, criterion, start_epoch, use_cuda)\n        print(\' Test Loss:  %.8f, Test Acc:  %.2f\' % (test_loss, test_acc))\n        return\n\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        print(\'\\nEpoch: [%d | %d] LR: %f\' % (epoch + 1, args.epochs, state[\'lr\']))\n\n        train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n        test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n\n        # append logger file\n        logger.append([state[\'lr\'], train_loss, test_loss, train_acc, test_acc])\n\n        # save model\n        is_best = test_acc > best_acc\n        best_acc = max(test_acc, best_acc)\n        save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': model.state_dict(),\n                \'acc\': test_acc,\n                \'best_acc\': best_acc,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n    logger.plot()\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n\n    print(\'Best acc:\')\n    print(best_acc)\n\ndef train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n    # switch to train mode\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    bar = Bar(\'Processing\', max=len(trainloader))\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda(async=True)\n        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec1.item(), inputs.size(0))\n        top5.update(prec5.item(), inputs.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef test(testloader, model, criterion, epoch, use_cuda):\n    global best_acc\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    bar = Bar(\'Processing\', max=len(testloader))\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec1.item(), inputs.size(0))\n        top5.update(prec5.item(), inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(testloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\ndef adjust_learning_rate(optimizer, epoch):\n    global state\n    if epoch in args.schedule:\n        state[\'lr\'] *= args.gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'lr\']\n\nif __name__ == \'__main__\':\n    main()\n'"
fashionmnist.py,13,"b'\'\'\'\nTraining script for Fashion-MNIST\n\'\'\'\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.datasets as datasets\nimport models.fashion as models\nimport transforms\nimport numpy as np\n\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch Fashion-MNIST Training\')\n# Datasets\nparser.add_argument(\'-d\', \'--dataset\', default=\'fashionmnist\', type=str)\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\n# Optimization options\nparser.add_argument(\'--epochs\', default=300, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--train-batch\', default=128, type=int, metavar=\'N\',\n                    help=\'train batchsize\')\nparser.add_argument(\'--test-batch\', default=100, type=int, metavar=\'N\',\n                    help=\'test batchsize\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--drop\', \'--dropout\', default=0, type=float,\n                    metavar=\'Dropout\', help=\'Dropout ratio\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                        help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n# Checkpoints\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoint)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n# Architecture\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet20)\')\nparser.add_argument(\'--depth\', type=int, default=20, help=\'Model depth.\')\nparser.add_argument(\'--widen-factor\', type=int, default=10, help=\'Widen factor. 10\')\nparser.add_argument(\'--growthRate\', type=int, default=12, help=\'Growth rate for DenseNet.\')\nparser.add_argument(\'--compressionRate\', type=int, default=2, help=\'Compression Rate (theta) for DenseNet.\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\n\n\n# Random Erasing\nparser.add_argument(\'--p\', default=0, type=float, help=\'Random Erasing probability\')\nparser.add_argument(\'--sh\', default=0.4, type=float, help=\'max erasing area\')\nparser.add_argument(\'--r1\', default=0.3, type=float, help=\'aspect of erasing area\')\n\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\n# Validate dataset\nassert args.dataset == \'fashionmnist\'\n\n# Use CUDA\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\nif use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef main():\n    global best_acc\n    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n\n    if not os.path.isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # Data\n    print(\'==> Preparing dataset %s\' % args.dataset)\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(28, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        transforms.RandomErasing(probability = args.p, sh = args.sh, r1 = args.r1, mean = [0.4914]),\n    ])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n    ])\n\n    if args.dataset == \'fashionmnist\':\n        dataloader = datasets.FashionMNIST\n        num_classes = 10\n    trainset = dataloader(root=\'./data\', train=True, download=True, transform=transform_train)\n    trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n\n    testset = dataloader(root=\'./data\', train=False, download=False, transform=transform_test)\n    testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n\n    # Model   \n    print(""==> creating model \'{}\'"".format(args.arch))\n    if args.arch.startswith(\'wrn\'):\n        model = models.__dict__[args.arch](\n                    num_classes=num_classes,\n                    depth=args.depth,\n                    widen_factor=args.widen_factor,\n                    dropRate=args.drop,\n                )\n    elif args.arch.endswith(\'resnet\'):\n        model = models.__dict__[args.arch](\n                    num_classes=num_classes,\n                    depth=args.depth,\n                )\n\n    model = torch.nn.DataParallel(model).cuda()\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    # Resume\n    title = \'fashionmnist-\' + args.arch\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\')\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.checkpoint = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume)\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n    else:\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        test_loss, test_acc = test(testloader, model, criterion, start_epoch, use_cuda)\n        print(\' Test Loss:  %.8f, Test Acc:  %.2f\' % (test_loss, test_acc))\n        return\n\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        print(\'\\nEpoch: [%d | %d] LR: %f\' % (epoch + 1, args.epochs, state[\'lr\']))\n\n        train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n        test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n\n        # append logger file\n        logger.append([state[\'lr\'], train_loss, test_loss, train_acc, test_acc])\n\n        # save model\n        is_best = test_acc > best_acc\n        best_acc = max(test_acc, best_acc)\n        save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': model.state_dict(),\n                \'acc\': test_acc,\n                \'best_acc\': best_acc,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n    logger.plot()\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n\n    print(\'Best acc:\')\n    print(best_acc)\n\ndef train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n    # switch to train mode\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    bar = Bar(\'Processing\', max=len(trainloader))\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda(async=True)\n        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec1.item(), inputs.size(0))\n        top5.update(prec5.item(), inputs.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef test(testloader, model, criterion, epoch, use_cuda):\n    global best_acc\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    bar = Bar(\'Processing\', max=len(testloader))\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec1.item(), inputs.size(0))\n        top5.update(prec5.item(), inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(testloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\ndef adjust_learning_rate(optimizer, epoch):\n    global state\n    if epoch in args.schedule:\n        state[\'lr\'] *= args.gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'lr\']\n\nif __name__ == \'__main__\':\n    main()\n'"
transforms.py,0,"b""from __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\nfrom PIL import Image\nimport random\nimport math\nimport numpy as np\nimport torch\n\nclass RandomErasing(object):\n    '''\n    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n    -------------------------------------------------------------------------------------\n    probability: The probability that the operation will be performed.\n    sl: min erasing area\n    sh: max erasing area\n    r1: min aspect ratio\n    mean: erasing value\n    -------------------------------------------------------------------------------------\n    '''\n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img\n\n"""
models/__init__.py,0,b''
utils/__init__.py,0,"b'""""""Useful utils\n""""""\nfrom .misc import *\nfrom .logger import *\nfrom .visualize import *\nfrom .eval import *\n\n# progress bar\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar'"
utils/eval.py,0,"b'from __future__ import print_function, absolute_import\n\n__all__ = [\'accuracy\']\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res'"
utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport numpy as np\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        if name in [\'Train Acc.\', \'Valid Acc.\']:\n            plt.plot(x, 100-np.asarray(numbers[name], dtype=\'float\'))\n        else:\n            plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.plot()\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        legend_text = [\'WRN-28-10+Ours (error 17.65%)\', \'WRN-28-10 (error 18.68%)\']\n        plt.legend(legend_text, loc=0)\n        plt.ylabel(\'test error (%)\')\n        plt.xlabel(\'epoch\')\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'WRN-28-10\':\'checkpoint/cifar100/log_erasing_erasing_0_wrn_28.txt\', \n    \'WRN-28-10+Ours\':\'checkpoint/cifar100/log_erasing_erasing_1_wrn_28.txt\', \n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.pdf\')\n'"
utils/misc.py,6,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n__all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\']\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
utils/transforms.py,2,"b'from __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\nimport numpy as np\nimport torch\n\nclass RandomErasing(object):\n    def __init__(self, EPSILON = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.EPSILON = EPSILON\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.EPSILON:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    #img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    #img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    #img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                    #img[:, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(3, h, w))\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[1]\n                    # img[0, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(1, h, w))\n                return img\n\n        return img\n\n'"
utils/visualize.py,6,"b""import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom .misc import *   \n\n__all__ = ['make_image', 'show_batch', 'show_mask', 'show_mask_single']\n\n# functions to show an image\ndef make_image(img, mean=(0,0,0), std=(1,1,1)):\n    for i in range(0, 3):\n        img[i] = img[i] * std[i] + mean[i]    # unnormalize\n    npimg = img.numpy()\n    return np.transpose(npimg, (1, 2, 0))\n\ndef gauss(x,a,b,c):\n    return torch.exp(-torch.pow(torch.add(x,-b),2).div(2*c*c)).mul(a)\n\ndef colorize(x):\n    ''' Converts a one-channel grayscale image to a color heatmap image '''\n    if x.dim() == 2:\n        torch.unsqueeze(x, 0, out=x)\n    if x.dim() == 3:\n        cl = torch.zeros([3, x.size(1), x.size(2)])\n        cl[0] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[1] = gauss(x,1,.5,.3)\n        cl[2] = gauss(x,1,.2,.3)\n        cl[cl.gt(1)] = 1\n    elif x.dim() == 4:\n        cl = torch.zeros([x.size(0), 3, x.size(2), x.size(3)])\n        cl[:,0,:,:] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[:,1,:,:] = gauss(x,1,.5,.3)\n        cl[:,2,:,:] = gauss(x,1,.2,.3)\n    return cl\n\ndef show_batch(images, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.imshow(images)\n    plt.show()\n\n\ndef show_mask_single(images, mask, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(2, 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    # for b in range(mask.size(0)):\n    #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n    mask_size = mask.size(2)\n    # print('Max %f Min %f' % (mask.max(), mask.min()))\n    mask = (upsampling(mask, scale_factor=im_size/mask_size))\n    # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n    # for c in range(3):\n    #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n    # print(mask.size())\n    mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n    # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n    plt.subplot(2, 1, 2)\n    plt.imshow(mask)\n    plt.axis('off')\n\ndef show_mask(images, masklist, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(1+len(masklist), 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    for i in range(len(masklist)):\n        mask = masklist[i].data.cpu()\n        # for b in range(mask.size(0)):\n        #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n        mask_size = mask.size(2)\n        # print('Max %f Min %f' % (mask.max(), mask.min()))\n        mask = (upsampling(mask, scale_factor=im_size/mask_size))\n        # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n        # for c in range(3):\n        #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n        # print(mask.size())\n        mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n        # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n        plt.subplot(1+len(masklist), 1, i+2)\n        plt.imshow(mask)\n        plt.axis('off')\n\n\n\n# x = torch.zeros(1, 3, 3)\n# out = colorize(x)\n# out_im = make_image(out)\n# plt.imshow(out_im)\n# plt.show()"""
models/cifar/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .resnet import *\nfrom .wrn import *\n'
models/cifar/resnet.py,1,"b'from __future__ import absolute_import\n\n\'\'\'Resnet for cifar dataset. \nPorted form \nhttps://github.com/facebook/fb.resnet.torch\nand\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n(c) YANG, Wei \n\'\'\'\nimport torch.nn as nn\nimport math\n\n\n__all__ = [\'resnet\']\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, depth, num_classes=1000):\n        super(ResNet, self).__init__()\n        # Model type specifies number of layers for CIFAR-10 model\n        assert (depth - 2) % 6 == 0, \'depth should be 6n+2\'\n        n = (depth - 2) // 6\n\n        block = Bottleneck if depth >=44 else BasicBlock\n\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, n)\n        self.layer2 = self._make_layer(block, 32, n, stride=2)\n        self.layer3 = self._make_layer(block, 64, n, stride=2)\n        self.avgpool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)    # 32x32\n\n        x = self.layer1(x)  # 32x32\n        x = self.layer2(x)  # 16x16\n        x = self.layer3(x)  # 8x8\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet(**kwargs):\n    """"""\n    Constructs a ResNet model.\n    """"""\n    return ResNet(**kwargs)\n'"
models/cifar/wrn.py,3,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'wrn\']\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                               padding=0, bias=False) or None\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.layer(x)\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n        assert (depth - 4) % 6 == 0, \'depth should be 6n+4\'\n        n = (depth - 4) / 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n                \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n\ndef wrn(**kwargs):\n    """"""\n    Constructs a Wide Residual Networks.\n    """"""\n    model = WideResNet(**kwargs)\n    return model'"
models/fashion/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .resnet import *\nfrom .wrn import *\n'
models/fashion/resnet.py,1,"b'from __future__ import absolute_import\n\n\'\'\'Resnet for cifar dataset. \nPorted form \nhttps://github.com/facebook/fb.resnet.torch\nand\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n(c) YANG, Wei \n\'\'\'\nimport torch.nn as nn\nimport math\n\n\n__all__ = [\'resnet\']\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, depth, num_classes=1000):\n        super(ResNet, self).__init__()\n        # Model type specifies number of layers for CIFAR-10 model\n        assert (depth - 2) % 6 == 0, \'depth should be 6n+2\'\n        n = (depth - 2) / 6\n\n        block = Bottleneck if depth >=44 else BasicBlock\n\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, n)\n        self.layer2 = self._make_layer(block, 32, n, stride=2)\n        self.layer3 = self._make_layer(block, 64, n, stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(64 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)    # 32x32\n\n        x = self.layer1(x)  # 32x32\n        x = self.layer2(x)  # 16x16\n        x = self.layer3(x)  # 8x8\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet(**kwargs):\n    """"""\n    Constructs a ResNet model.\n    """"""\n    return ResNet(**kwargs)\n'"
models/fashion/wrn.py,3,"b'from __future__ import division\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'wrn\']\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                               padding=0, bias=False) or None\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.layer(x)\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n        assert (depth - 4) % 6 == 0, \'depth should be 6n+4\'\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n                \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 7)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n\ndef wrn(**kwargs):\n    """"""\n    Constructs a Wide Residual Networks.\n    """"""\n    model = WideResNet(**kwargs)\n    return model\n'"
utils/progress/setup.py,0,"b""#!/usr/bin/env python\n\nfrom setuptools import setup\n\nimport progress\n\n\nsetup(\n    name='progress',\n    version=progress.__version__,\n    description='Easy to use progress bars',\n    long_description=open('README.rst').read(),\n    author='Giorgos Verigakis',\n    author_email='verigak@gmail.com',\n    url='http://github.com/verigak/progress/',\n    license='ISC',\n    packages=['progress'],\n    classifiers=[\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: ISC License (ISCL)',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n    ]\n)\n"""
utils/progress/test_progress.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport random\nimport time\n\nfrom progress.bar import (Bar, ChargingBar, FillingSquaresBar,\n                          FillingCirclesBar, IncrementalBar, PixelBar,\n                          ShadyBar)\nfrom progress.spinner import (Spinner, PieSpinner, MoonSpinner, LineSpinner,\n                              PixelSpinner)\nfrom progress.counter import Counter, Countdown, Stack, Pie\n\n\ndef sleep():\n    t = 0.01\n    t += t * random.uniform(-0.1, 0.1)  # Add some variance\n    time.sleep(t)\n\n\nfor bar_cls in (Bar, ChargingBar, FillingSquaresBar, FillingCirclesBar):\n    suffix = '%(index)d/%(max)d [%(elapsed)d / %(eta)d / %(eta_td)s]'\n    bar = bar_cls(bar_cls.__name__, suffix=suffix)\n    for i in bar.iter(range(200)):\n        sleep()\n\nfor bar_cls in (IncrementalBar, PixelBar, ShadyBar):\n    suffix = '%(percent)d%% [%(elapsed_td)s / %(eta)d / %(eta_td)s]'\n    bar = bar_cls(bar_cls.__name__, suffix=suffix)\n    for i in bar.iter(range(200)):\n        sleep()\n\nfor spin in (Spinner, PieSpinner, MoonSpinner, LineSpinner, PixelSpinner):\n    for i in spin(spin.__name__ + ' ').iter(range(100)):\n        sleep()\n    print()\n\nfor singleton in (Counter, Countdown, Stack, Pie):\n    for i in singleton(singleton.__name__ + ' ').iter(range(100)):\n        sleep()\n    print()\n\nbar = IncrementalBar('Random', suffix='%(index)d')\nfor i in range(100):\n    bar.goto(random.randint(0, 100))\n    sleep()\nbar.finish()\n"""
utils/progress/progress/__init__.py,0,"b'# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import division\n\nfrom collections import deque\nfrom datetime import timedelta\nfrom math import ceil\nfrom sys import stderr\nfrom time import time\n\n\n__version__ = \'1.3\'\n\n\nclass Infinite(object):\n    file = stderr\n    sma_window = 10         # Simple Moving Average window\n\n    def __init__(self, *args, **kwargs):\n        self.index = 0\n        self.start_ts = time()\n        self.avg = 0\n        self._ts = self.start_ts\n        self._xput = deque(maxlen=self.sma_window)\n        for key, val in kwargs.items():\n            setattr(self, key, val)\n\n    def __getitem__(self, key):\n        if key.startswith(\'_\'):\n            return None\n        return getattr(self, key, None)\n\n    @property\n    def elapsed(self):\n        return int(time() - self.start_ts)\n\n    @property\n    def elapsed_td(self):\n        return timedelta(seconds=self.elapsed)\n\n    def update_avg(self, n, dt):\n        if n > 0:\n            self._xput.append(dt / n)\n            self.avg = sum(self._xput) / len(self._xput)\n\n    def update(self):\n        pass\n\n    def start(self):\n        pass\n\n    def finish(self):\n        pass\n\n    def next(self, n=1):\n        now = time()\n        dt = now - self._ts\n        self.update_avg(n, dt)\n        self._ts = now\n        self.index = self.index + n\n        self.update()\n\n    def iter(self, it):\n        try:\n            for x in it:\n                yield x\n                self.next()\n        finally:\n            self.finish()\n\n\nclass Progress(Infinite):\n    def __init__(self, *args, **kwargs):\n        super(Progress, self).__init__(*args, **kwargs)\n        self.max = kwargs.get(\'max\', 100)\n\n    @property\n    def eta(self):\n        return int(ceil(self.avg * self.remaining))\n\n    @property\n    def eta_td(self):\n        return timedelta(seconds=self.eta)\n\n    @property\n    def percent(self):\n        return self.progress * 100\n\n    @property\n    def progress(self):\n        return min(1, self.index / self.max)\n\n    @property\n    def remaining(self):\n        return max(self.max - self.index, 0)\n\n    def start(self):\n        self.update()\n\n    def goto(self, index):\n        incr = index - self.index\n        self.next(incr)\n\n    def iter(self, it):\n        try:\n            self.max = len(it)\n        except TypeError:\n            pass\n\n        try:\n            for x in it:\n                yield x\n                self.next()\n        finally:\n            self.finish()\n'"
utils/progress/progress/bar.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Progress\nfrom .helpers import WritelnMixin\n\n\nclass Bar(WritelnMixin, Progress):\n    width = 32\n    message = \'\'\n    suffix = \'%(index)d/%(max)d\'\n    bar_prefix = \' |\'\n    bar_suffix = \'| \'\n    empty_fill = \' \'\n    fill = \'#\'\n    hide_cursor = True\n\n    def update(self):\n        filled_length = int(self.width * self.progress)\n        empty_length = self.width - filled_length\n\n        message = self.message % self\n        bar = self.fill * filled_length\n        empty = self.empty_fill * empty_length\n        suffix = self.suffix % self\n        line = \'\'.join([message, self.bar_prefix, bar, empty, self.bar_suffix,\n                        suffix])\n        self.writeln(line)\n\n\nclass ChargingBar(Bar):\n    suffix = \'%(percent)d%%\'\n    bar_prefix = \' \'\n    bar_suffix = \' \'\n    empty_fill = \'\xe2\x88\x99\'\n    fill = \'\xe2\x96\x88\'\n\n\nclass FillingSquaresBar(ChargingBar):\n    empty_fill = \'\xe2\x96\xa2\'\n    fill = \'\xe2\x96\xa3\'\n\n\nclass FillingCirclesBar(ChargingBar):\n    empty_fill = \'\xe2\x97\xaf\'\n    fill = \'\xe2\x97\x89\'\n\n\nclass IncrementalBar(Bar):\n    phases = (\' \', \'\xe2\x96\x8f\', \'\xe2\x96\x8e\', \'\xe2\x96\x8d\', \'\xe2\x96\x8c\', \'\xe2\x96\x8b\', \'\xe2\x96\x8a\', \'\xe2\x96\x89\', \'\xe2\x96\x88\')\n\n    def update(self):\n        nphases = len(self.phases)\n        filled_len = self.width * self.progress\n        nfull = int(filled_len)                      # Number of full chars\n        phase = int((filled_len - nfull) * nphases)  # Phase of last char\n        nempty = self.width - nfull                  # Number of empty chars\n\n        message = self.message % self\n        bar = self.phases[-1] * nfull\n        current = self.phases[phase] if phase > 0 else \'\'\n        empty = self.empty_fill * max(0, nempty - len(current))\n        suffix = self.suffix % self\n        line = \'\'.join([message, self.bar_prefix, bar, current, empty,\n                        self.bar_suffix, suffix])\n        self.writeln(line)\n\n\nclass PixelBar(IncrementalBar):\n    phases = (\'\xe2\xa1\x80\', \'\xe2\xa1\x84\', \'\xe2\xa1\x86\', \'\xe2\xa1\x87\', \'\xe2\xa3\x87\', \'\xe2\xa3\xa7\', \'\xe2\xa3\xb7\', \'\xe2\xa3\xbf\')\n\n\nclass ShadyBar(IncrementalBar):\n    phases = (\' \', \'\xe2\x96\x91\', \'\xe2\x96\x92\', \'\xe2\x96\x93\', \'\xe2\x96\x88\')\n'"
utils/progress/progress/counter.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Infinite, Progress\nfrom .helpers import WriteMixin\n\n\nclass Counter(WriteMixin, Infinite):\n    message = \'\'\n    hide_cursor = True\n\n    def update(self):\n        self.write(str(self.index))\n\n\nclass Countdown(WriteMixin, Progress):\n    hide_cursor = True\n\n    def update(self):\n        self.write(str(self.remaining))\n\n\nclass Stack(WriteMixin, Progress):\n    phases = (\' \', \'\xe2\x96\x81\', \'\xe2\x96\x82\', \'\xe2\x96\x83\', \'\xe2\x96\x84\', \'\xe2\x96\x85\', \'\xe2\x96\x86\', \'\xe2\x96\x87\', \'\xe2\x96\x88\')\n    hide_cursor = True\n\n    def update(self):\n        nphases = len(self.phases)\n        i = min(nphases - 1, int(self.progress * nphases))\n        self.write(self.phases[i])\n\n\nclass Pie(Stack):\n    phases = (\'\xe2\x97\x8b\', \'\xe2\x97\x94\', \'\xe2\x97\x91\', \'\xe2\x97\x95\', \'\xe2\x97\x8f\')\n'"
utils/progress/progress/helpers.py,0,"b'# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import print_function\n\n\nHIDE_CURSOR = \'\\x1b[?25l\'\nSHOW_CURSOR = \'\\x1b[?25h\'\n\n\nclass WriteMixin(object):\n    hide_cursor = False\n\n    def __init__(self, message=None, **kwargs):\n        super(WriteMixin, self).__init__(**kwargs)\n        self._width = 0\n        if message:\n            self.message = message\n\n        if self.file.isatty():\n            if self.hide_cursor:\n                print(HIDE_CURSOR, end=\'\', file=self.file)\n            print(self.message, end=\'\', file=self.file)\n            self.file.flush()\n\n    def write(self, s):\n        if self.file.isatty():\n            b = \'\\b\' * self._width\n            c = s.ljust(self._width)\n            print(b + c, end=\'\', file=self.file)\n            self._width = max(self._width, len(s))\n            self.file.flush()\n\n    def finish(self):\n        if self.file.isatty() and self.hide_cursor:\n            print(SHOW_CURSOR, end=\'\', file=self.file)\n\n\nclass WritelnMixin(object):\n    hide_cursor = False\n\n    def __init__(self, message=None, **kwargs):\n        super(WritelnMixin, self).__init__(**kwargs)\n        if message:\n            self.message = message\n\n        if self.file.isatty() and self.hide_cursor:\n            print(HIDE_CURSOR, end=\'\', file=self.file)\n\n    def clearln(self):\n        if self.file.isatty():\n            print(\'\\r\\x1b[K\', end=\'\', file=self.file)\n\n    def writeln(self, line):\n        if self.file.isatty():\n            self.clearln()\n            print(line, end=\'\', file=self.file)\n            self.file.flush()\n\n    def finish(self):\n        if self.file.isatty():\n            print(file=self.file)\n            if self.hide_cursor:\n                print(SHOW_CURSOR, end=\'\', file=self.file)\n\n\nfrom signal import signal, SIGINT\nfrom sys import exit\n\n\nclass SigIntMixin(object):\n    """"""Registers a signal handler that calls finish on SIGINT""""""\n\n    def __init__(self, *args, **kwargs):\n        super(SigIntMixin, self).__init__(*args, **kwargs)\n        signal(SIGINT, self._sigint_handler)\n\n    def _sigint_handler(self, signum, frame):\n        self.finish()\n        exit(0)\n'"
utils/progress/progress/spinner.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Infinite\nfrom .helpers import WriteMixin\n\n\nclass Spinner(WriteMixin, Infinite):\n    message = \'\'\n    phases = (\'-\', \'\\\\\', \'|\', \'/\')\n    hide_cursor = True\n\n    def update(self):\n        i = self.index % len(self.phases)\n        self.write(self.phases[i])\n\n\nclass PieSpinner(Spinner):\n    phases = [\'\xe2\x97\xb7\', \'\xe2\x97\xb6\', \'\xe2\x97\xb5\', \'\xe2\x97\xb4\']\n\n\nclass MoonSpinner(Spinner):\n    phases = [\'\xe2\x97\x91\', \'\xe2\x97\x92\', \'\xe2\x97\x90\', \'\xe2\x97\x93\']\n\n\nclass LineSpinner(Spinner):\n    phases = [\'\xe2\x8e\xba\', \'\xe2\x8e\xbb\', \'\xe2\x8e\xbc\', \'\xe2\x8e\xbd\', \'\xe2\x8e\xbc\', \'\xe2\x8e\xbb\']\n\nclass PixelSpinner(Spinner):\n    phases = [\'\xe2\xa3\xbe\',\'\xe2\xa3\xb7\', \'\xe2\xa3\xaf\', \'\xe2\xa3\x9f\', \'\xe2\xa1\xbf\', \'\xe2\xa2\xbf\', \'\xe2\xa3\xbb\', \'\xe2\xa3\xbd\']\n'"
