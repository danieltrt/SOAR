file_path,api_count,code
datasets/__init__.py,0,b'from . import cityscapes\nfrom . import voc\n'
datasets/cityscapes.py,3,"b""import os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\n\nnum_classes = 19\nignore_label = 255\nroot = '/media/b3-542/LIBRARY/Datasets/cityscapes'\n\npalette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(palette)\n\n    return new_mask\n\n\ndef make_dataset(quality, mode):\n    assert (quality == 'fine' and mode in ['train', 'val']) or \\\n           (quality == 'coarse' and mode in ['train', 'train_extra', 'val'])\n\n    if quality == 'coarse':\n        img_dir_name = 'leftImg8bit_trainextra' if mode == 'train_extra' else 'leftImg8bit_trainvaltest'\n        mask_path = os.path.join(root, 'gtCoarse', 'gtCoarse', mode)\n        mask_postfix = '_gtCoarse_labelIds.png'\n    else:\n        img_dir_name = 'leftImg8bit_trainvaltest'\n        mask_path = os.path.join(root, 'gtFine_trainvaltest', 'gtFine', mode)\n        mask_postfix = '_gtFine_labelIds.png'\n    img_path = os.path.join(root, img_dir_name, 'leftImg8bit', mode)\n    assert os.listdir(img_path) == os.listdir(mask_path)\n    items = []\n    categories = os.listdir(img_path)\n    for c in categories:\n        c_items = [name.split('_leftImg8bit.png')[0] for name in os.listdir(os.path.join(img_path, c))]\n        for it in c_items:\n            item = (os.path.join(img_path, c, it + '_leftImg8bit.png'), os.path.join(mask_path, c, it + mask_postfix))\n            items.append(item)\n    return items\n\n\nclass CityScapes(data.Dataset):\n    def __init__(self, quality, mode, joint_transform=None, sliding_crop=None, transform=None, target_transform=None):\n        self.imgs = make_dataset(quality, mode)\n        if len(self.imgs) == 0:\n            raise RuntimeError('Found 0 images, please check the data set')\n        self.quality = quality\n        self.mode = mode\n        self.joint_transform = joint_transform\n        self.sliding_crop = sliding_crop\n        self.transform = transform\n        self.target_transform = target_transform\n        self.id_to_trainid = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n                              3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n                              7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n                              14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n                              18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n                              28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n\n    def __getitem__(self, index):\n        img_path, mask_path = self.imgs[index]\n        img, mask = Image.open(img_path).convert('RGB'), Image.open(mask_path)\n\n        mask = np.array(mask)\n        mask_copy = mask.copy()\n        for k, v in self.id_to_trainid.items():\n            mask_copy[mask == k] = v\n        mask = Image.fromarray(mask_copy.astype(np.uint8))\n\n        if self.joint_transform is not None:\n            img, mask = self.joint_transform(img, mask)\n        if self.sliding_crop is not None:\n            img_slices, mask_slices, slices_info = self.sliding_crop(img, mask)\n            if self.transform is not None:\n                img_slices = [self.transform(e) for e in img_slices]\n            if self.target_transform is not None:\n                mask_slices = [self.target_transform(e) for e in mask_slices]\n            img, mask = torch.stack(img_slices, 0), torch.stack(mask_slices, 0)\n            return img, mask, torch.LongTensor(slices_info)\n        else:\n            if self.transform is not None:\n                img = self.transform(img)\n            if self.target_transform is not None:\n                mask = self.target_transform(mask)\n            return img, mask\n\n    def __len__(self):\n        return len(self.imgs)\n"""
datasets/voc.py,3,"b""import os\n\nimport numpy as np\nimport scipy.io as sio\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\n\nnum_classes = 21\nignore_label = 255\nroot = '/media/b3-542/LIBRARY/Datasets/VOC'\n\n'''\ncolor map\n0=background, 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle # 6=bus, 7=car, 8=cat, 9=chair, 10=cow, 11=diningtable,\n12=dog, 13=horse, 14=motorbike, 15=person # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n'''\npalette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]\n\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(palette)\n\n    return new_mask\n\n\ndef make_dataset(mode):\n    assert mode in ['train', 'val', 'test']\n    items = []\n    if mode == 'train':\n        img_path = os.path.join(root, 'benchmark_RELEASE', 'dataset', 'img')\n        mask_path = os.path.join(root, 'benchmark_RELEASE', 'dataset', 'cls')\n        data_list = [l.strip('\\n') for l in open(os.path.join(\n            root, 'benchmark_RELEASE', 'dataset', 'train.txt')).readlines()]\n        for it in data_list:\n            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.mat'))\n            items.append(item)\n    elif mode == 'val':\n        img_path = os.path.join(root, 'VOCdevkit', 'VOC2012', 'JPEGImages')\n        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2012', 'SegmentationClass')\n        data_list = [l.strip('\\n') for l in open(os.path.join(\n            root, 'VOCdevkit', 'VOC2012', 'ImageSets', 'Segmentation', 'seg11valid.txt')).readlines()]\n        for it in data_list:\n            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n            items.append(item)\n    else:\n        img_path = os.path.join(root, 'VOCdevkit (test)', 'VOC2012', 'JPEGImages')\n        data_list = [l.strip('\\n') for l in open(os.path.join(\n            root, 'VOCdevkit (test)', 'VOC2012', 'ImageSets', 'Segmentation', 'test.txt')).readlines()]\n        for it in data_list:\n            items.append((img_path, it))\n    return items\n\n\nclass VOC(data.Dataset):\n    def __init__(self, mode, joint_transform=None, sliding_crop=None, transform=None, target_transform=None):\n        self.imgs = make_dataset(mode)\n        if len(self.imgs) == 0:\n            raise RuntimeError('Found 0 images, please check the data set')\n        self.mode = mode\n        self.joint_transform = joint_transform\n        self.sliding_crop = sliding_crop\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        if self.mode == 'test':\n            img_path, img_name = self.imgs[index]\n            img = Image.open(os.path.join(img_path, img_name + '.jpg')).convert('RGB')\n            if self.transform is not None:\n                img = self.transform(img)\n            return img_name, img\n\n        img_path, mask_path = self.imgs[index]\n        img = Image.open(img_path).convert('RGB')\n        if self.mode == 'train':\n            mask = sio.loadmat(mask_path)['GTcls']['Segmentation'][0][0]\n            mask = Image.fromarray(mask.astype(np.uint8))\n        else:\n            mask = Image.open(mask_path)\n\n        if self.joint_transform is not None:\n            img, mask = self.joint_transform(img, mask)\n\n        if self.sliding_crop is not None:\n            img_slices, mask_slices, slices_info = self.sliding_crop(img, mask)\n            if self.transform is not None:\n                img_slices = [self.transform(e) for e in img_slices]\n            if self.target_transform is not None:\n                mask_slices = [self.target_transform(e) for e in mask_slices]\n            img, mask = torch.stack(img_slices, 0), torch.stack(mask_slices, 0)\n            return img, mask, torch.LongTensor(slices_info)\n        else:\n            if self.transform is not None:\n                img = self.transform(img)\n            if self.target_transform is not None:\n                mask = self.target_transform(mask)\n            return img, mask\n\n    def __len__(self):\n        return len(self.imgs)\n"""
eval/eval_voc.py,4,"b""import os\n\nimport torchvision.transforms as standard_transforms\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\n\nfrom datasets import voc\nfrom models import *\nfrom utils import check_mkdir\n\ncudnn.benchmark = True\n\nckpt_path = './ckpt'\n\nargs = {\n    'exp_name': 'voc-psp_net',\n    'snapshot': 'epoch_33_loss_0.31766_acc_0.92188_acc-cls_0.81110_mean-iu_0.70271_fwavacc_0.86757_lr_0.0023769346.pth'\n}\n\n\ndef main():\n    net = PSPNet(num_classes=voc.num_classes).cuda()\n    print('load model ' + args['snapshot'])\n    net.load_state_dict(torch.load(os.path.join(ckpt_path, args['exp_name'], args['snapshot'])))\n    net.eval()\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    val_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n\n    test_set = voc.VOC('test', transform=val_input_transform)\n    test_loader = DataLoader(test_set, batch_size=1, num_workers=8, shuffle=False)\n\n    check_mkdir(os.path.join(ckpt_path, args['exp_name'], 'test'))\n\n    for vi, data in enumerate(test_loader):\n        img_name, img = data\n        img_name = img_name[0]\n\n        img = Variable(img, volatile=True).cuda()\n        output = net(img)\n\n        prediction = output.data.max(1)[1].squeeze_(1).squeeze_(0).cpu().numpy()\n        prediction = voc.colorize_mask(prediction)\n        prediction.save(os.path.join(ckpt_path, args['exp_name'], 'test', img_name + '.png'))\n\n        print('%d / %d' % (vi + 1, len(test_loader)))\n\n\nif __name__ == '__main__':\n    main()\n"""
models/__init__.py,0,b'from .duc_hdc import *\nfrom .fcn16s import *\nfrom .fcn32s import *\nfrom .fcn8s import *\nfrom .gcn import *\nfrom .psp_net import *\nfrom .seg_net import *\nfrom .u_net import *\n'
models/config.py,0,"b""import os\n\n# here (https://github.com/pytorch/vision/tree/master/torchvision/models) to find the download link of pretrained models\n\nroot = '/media/b3-542/LIBRARY/ZijunDeng/PyTorch Pretrained'\nres101_path = os.path.join(root, 'ResNet', 'resnet101-5d3b4d8f.pth')\nres152_path = os.path.join(root, 'ResNet', 'resnet152-b121ed2d.pth')\ninception_v3_path = os.path.join(root, 'Inception', 'inception_v3_google-1a9a5a14.pth')\nvgg19_bn_path = os.path.join(root, 'VggNet', 'vgg19_bn-c79401a0.pth')\nvgg16_path = os.path.join(root, 'VggNet', 'vgg16-397923af.pth')\ndense201_path = os.path.join(root, 'DenseNet', 'densenet201-4c113574.pth')\n\n'''\nvgg16 trained using caffe\nvisit this (https://github.com/jcjohnson/pytorch-vgg) to download the converted vgg16\n'''\nvgg16_caffe_path = os.path.join(root, 'VggNet', 'vgg16-caffe.pth')\n"""
models/duc_hdc.py,2,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .config import res152_path\n\n\nclass _DenseUpsamplingConvModule(nn.Module):\n    def __init__(self, down_factor, in_dim, num_classes):\n        super(_DenseUpsamplingConvModule, self).__init__()\n        upsample_dim = (down_factor ** 2) * num_classes\n        self.conv = nn.Conv2d(in_dim, upsample_dim, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm2d(upsample_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(down_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n\n\nclass ResNetDUC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True):\n        super(ResNetDUC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation = (2, 2)\n                m.padding = (2, 2)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation = (4, 4)\n                m.padding = (4, 4)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n\n\nclass ResNetDUCHDC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True):\n        super(ResNetDUCHDC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        layer3_group_config = [1, 2, 5, 9]\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2.dilation = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n            self.layer3[idx].conv2.padding = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n        layer4_group_config = [5, 9, 17]\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2.dilation = (layer4_group_config[idx], layer4_group_config[idx])\n            self.layer4[idx].conv2.padding = (layer4_group_config[idx], layer4_group_config[idx])\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n"""
models/fcn16s.py,1,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n\nclass FCN16VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(FCN16VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features4 = nn.Sequential(*features[: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore16 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=32, stride=16, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore16.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 32))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool4 = self.features4(x)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore16 = self.upscore16(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                   + upscore2)\n        return upscore16[:, :, 27: (27 + x_size[2]), 27: (27 + x_size[3])].contiguous()\n"""
models/fcn32s.py,1,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n\nclass FCN32VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(FCN32VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features5 = nn.Sequential(*features)\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n        self.upscore.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 64))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool5 = self.features5(x)\n        score_fr = self.score_fr(pool5)\n        upscore = self.upscore(score_fr)\n        return upscore[:, :, 19: (19 + x_size[2]), 19: (19 + x_size[3])].contiguous()\n"""
models/fcn8s.py,2,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import get_upsampling_weight\nfrom .config import vgg16_path, vgg16_caffe_path\n\n\n# This is implemented in full accordance with the original one (https://github.com/shelhamer/fcn.berkeleyvision.org)\nclass FCN8s(nn.Module):\n    def __init__(self, num_classes, pretrained=True, caffe=False):\n        super(FCN8s, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            if caffe:\n                # load the pretrained vgg16 used by the paper's author\n                vgg.load_state_dict(torch.load(vgg16_caffe_path))\n            else:\n                vgg.load_state_dict(torch.load(vgg16_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        '''\n        100 padding for 2 reasons:\n            1) support very small input size\n            2) allow cropping in order to match size of different layers' feature maps\n        Note that the cropped part corresponds to a part of the 100 padding\n        Spatial information of different layers' feature maps cannot be align exactly because of cropping, which is bad\n        '''\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features3 = nn.Sequential(*features[: 17])\n        self.features4 = nn.Sequential(*features[17: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1)\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool3.weight.data.zero_()\n        self.score_pool3.bias.data.zero_()\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore_pool4 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore8 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=16, stride=8, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore_pool4.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore8.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 16))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool3 = self.features3(x)\n        pool4 = self.features4(pool3)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore_pool4 = self.upscore_pool4(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                           + upscore2)\n\n        score_pool3 = self.score_pool3(0.0001 * pool3)\n        upscore8 = self.upscore8(score_pool3[:, :, 9: (9 + upscore_pool4.size()[2]), 9: (9 + upscore_pool4.size()[3])]\n                                 + upscore_pool4)\n        return upscore8[:, :, 31: (31 + x_size[2]), 31: (31 + x_size[3])].contiguous()\n"""
models/gcn.py,2,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import initialize_weights\nfrom .config import res152_path\n\n\n# many are borrowed from https://github.com/ycszen/pytorch-ss/blob/master/gcn.py\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n        pad0 = (kernel_size[0] - 1) / 2\n        pad1 = (kernel_size[1] - 1) / 2\n        # kernel size had better be odd number so as to avoid alignment error\n        super(_GlobalConvModule, self).__init__()\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1),\n                                 padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]),\n                                 padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]),\n                                 padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1),\n                                 padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_classes, input_size, pretrained=True):\n        super(GCN, self).__init__()\n        self.input_size = input_size\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n        self.layer1 = nn.Sequential(resnet.maxpool, resnet.layer1)\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes, (7, 7))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (7, 7))\n        self.gcm3 = _GlobalConvModule(512, num_classes, (7, 7))\n        self.gcm4 = _GlobalConvModule(256, num_classes, (7, 7))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3,\n                           self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9)\n\n    def forward(self, x):\n        # if x: 512\n        fm0 = self.layer0(x)  # 256\n        fm1 = self.layer1(fm0)  # 128\n        fm2 = self.layer2(fm1)  # 64\n        fm3 = self.layer3(fm2)  # 32\n        fm4 = self.layer4(fm3)  # 16\n\n        gcfm1 = self.brm1(self.gcm1(fm4))  # 16\n        gcfm2 = self.brm2(self.gcm2(fm3))  # 32\n        gcfm3 = self.brm3(self.gcm3(fm2))  # 64\n        gcfm4 = self.brm4(self.gcm4(fm1))  # 128\n\n        fs1 = self.brm5(F.upsample_bilinear(gcfm1, fm3.size()[2:]) + gcfm2)  # 32\n        fs2 = self.brm6(F.upsample_bilinear(fs1, fm2.size()[2:]) + gcfm3)  # 64\n        fs3 = self.brm7(F.upsample_bilinear(fs2, fm1.size()[2:]) + gcfm4)  # 128\n        fs4 = self.brm8(F.upsample_bilinear(fs3, fm0.size()[2:]))  # 256\n        out = self.brm9(F.upsample_bilinear(fs4, self.input_size))  # 512\n\n        return out\n'"
models/psp_net.py,4,"b""import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import initialize_weights\nfrom ..utils.misc import Conv2dDeformable\nfrom .config import res101_path\n\n\nclass _PyramidPoolingModule(nn.Module):\n    def __init__(self, in_dim, reduction_dim, setting):\n        super(_PyramidPoolingModule, self).__init__()\n        self.features = []\n        for s in setting:\n            self.features.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(s),\n                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n                nn.BatchNorm2d(reduction_dim, momentum=.95),\n                nn.ReLU(inplace=True)\n            ))\n        self.features = nn.ModuleList(self.features)\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for f in self.features:\n            out.append(F.upsample(f(x), x_size[2:], mode='bilinear'))\n        out = torch.cat(out, 1)\n        return out\n\n\nclass PSPNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, use_aux=True):\n        super(PSPNet, self).__init__()\n        self.use_aux = use_aux\n        resnet = models.resnet101()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res101_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n        self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n        self.final = nn.Sequential(\n            nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512, momentum=.95),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1)\n        )\n\n        if use_aux:\n            self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n            initialize_weights(self.aux_logits)\n\n        initialize_weights(self.ppm, self.final)\n\n    def forward(self, x):\n        x_size = x.size()\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if self.training and self.use_aux:\n            aux = self.aux_logits(x)\n        x = self.layer4(x)\n        x = self.ppm(x)\n        x = self.final(x)\n        if self.training and self.use_aux:\n            return F.upsample(x, x_size[2:], mode='bilinear'), F.upsample(aux, x_size[2:], mode='bilinear')\n        return F.upsample(x, x_size[2:], mode='bilinear')\n\n\n# just a try, not recommend to use\nclass PSPNetDeform(nn.Module):\n    def __init__(self, num_classes, input_size, pretrained=True, use_aux=True):\n        super(PSPNetDeform, self).__init__()\n        self.input_size = input_size\n        self.use_aux = use_aux\n        resnet = models.resnet101()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res101_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.padding = (1, 1)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.padding = (1, 1)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2 = Conv2dDeformable(self.layer3[idx].conv2)\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2 = Conv2dDeformable(self.layer4[idx].conv2)\n\n        self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n        self.final = nn.Sequential(\n            nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512, momentum=.95),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1)\n        )\n\n        if use_aux:\n            self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n            initialize_weights(self.aux_logits)\n\n        initialize_weights(self.ppm, self.final)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if self.training and self.use_aux:\n            aux = self.aux_logits(x)\n        x = self.layer4(x)\n        x = self.ppm(x)\n        x = self.final(x)\n        if self.training and self.use_aux:\n            return F.upsample(x, self.input_size, mode='bilinear'), F.upsample(aux, self.input_size, mode='bilinear')\n        return F.upsample(x, self.input_size, mode='bilinear')\n"""
models/seg_net.py,5,"b'import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom ..utils import initialize_weights\nfrom .config import vgg19_bn_path\n\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conv_layers):\n        super(_DecoderBlock, self).__init__()\n        middle_channels = in_channels / 2\n        layers = [\n            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True)\n        ]\n        layers += [\n                      nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n                      nn.BatchNorm2d(middle_channels),\n                      nn.ReLU(inplace=True),\n                  ] * (num_conv_layers - 2)\n        layers += [\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        self.decode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass SegNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(SegNet, self).__init__()\n        vgg = models.vgg19_bn()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg19_bn_path))\n        features = list(vgg.features.children())\n        self.enc1 = nn.Sequential(*features[0:7])\n        self.enc2 = nn.Sequential(*features[7:14])\n        self.enc3 = nn.Sequential(*features[14:27])\n        self.enc4 = nn.Sequential(*features[27:40])\n        self.enc5 = nn.Sequential(*features[40:])\n\n        self.dec5 = nn.Sequential(\n            *([nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)] +\n              [nn.Conv2d(512, 512, kernel_size=3, padding=1),\n               nn.BatchNorm2d(512),\n               nn.ReLU(inplace=True)] * 4)\n        )\n        self.dec4 = _DecoderBlock(1024, 256, 4)\n        self.dec3 = _DecoderBlock(512, 128, 4)\n        self.dec2 = _DecoderBlock(256, 64, 2)\n        self.dec1 = _DecoderBlock(128, num_classes, 2)\n        initialize_weights(self.dec5, self.dec4, self.dec3, self.dec2, self.dec1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.enc5(enc4)\n\n        dec5 = self.dec5(enc5)\n        dec4 = self.dec4(torch.cat([enc4, dec5], 1))\n        dec3 = self.dec3(torch.cat([enc3, dec4], 1))\n        dec2 = self.dec2(torch.cat([enc2, dec3], 1))\n        dec1 = self.dec1(torch.cat([enc1, dec2], 1))\n        return dec1\n'"
models/u_net.py,5,"b""import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ..utils import initialize_weights\n\n\nclass _EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super(_EncoderBlock, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super(_DecoderBlock, self).__init__()\n        self.decode = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n        )\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.enc1 = _EncoderBlock(3, 64)\n        self.enc2 = _EncoderBlock(64, 128)\n        self.enc3 = _EncoderBlock(128, 256)\n        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n        self.center = _DecoderBlock(512, 1024, 512)\n        self.dec4 = _DecoderBlock(1024, 512, 256)\n        self.dec3 = _DecoderBlock(512, 256, 128)\n        self.dec2 = _DecoderBlock(256, 128, 64)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n        initialize_weights(self)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        center = self.center(enc4)\n        dec4 = self.dec4(torch.cat([center, F.upsample(enc4, center.size()[2:], mode='bilinear')], 1))\n        dec3 = self.dec3(torch.cat([dec4, F.upsample(enc3, dec4.size()[2:], mode='bilinear')], 1))\n        dec2 = self.dec2(torch.cat([dec3, F.upsample(enc2, dec3.size()[2:], mode='bilinear')], 1))\n        dec1 = self.dec1(torch.cat([dec2, F.upsample(enc1, dec2.size()[2:], mode='bilinear')], 1))\n        final = self.final(dec1)\n        return F.upsample(final, x.size()[2:], mode='bilinear')\n"""
utils/__init__.py,0,b'from .misc import *\nfrom .joint_transforms import *\nfrom .transforms import *\n'
utils/joint_transforms.py,0,"b""import math\nimport numbers\nimport random\n\nfrom PIL import Image, ImageOps\nimport numpy as np\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        if w == tw and h == th:\n            return img, mask\n        if w < tw or h < th:\n            return img.resize((tw, th), Image.BILINEAR), mask.resize((tw, th), Image.NEAREST)\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, img, mask):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n        return img, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        return img.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w >= h and w == self.size) or (h >= w and h == self.size):\n            return img, mask\n        if w > h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                return img.resize((self.size, self.size), Image.BILINEAR), mask.resize((self.size, self.size),\n                                                                                       Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(img, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, img, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        return img.rotate(rotate_degree, Image.BILINEAR), mask.rotate(rotate_degree, Image.NEAREST)\n\n\nclass RandomSized(object):\n    def __init__(self, size):\n        self.size = size\n        self.scale = Scale(self.size)\n        self.crop = RandomCrop(self.size)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w = int(random.uniform(0.5, 2) * img.size[0])\n        h = int(random.uniform(0.5, 2) * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return self.crop(*self.scale(img, mask))\n\n\nclass SlidingCropOld(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), 'constant')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), 'constant', constant_values=self.ignore_label)\n        return img, mask\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_sublist, mask_sublist = [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub = self._pad(img_sub, mask_sub)\n                    img_sublist.append(Image.fromarray(img_sub.astype(np.uint8)).convert('RGB'))\n                    mask_sublist.append(Image.fromarray(mask_sub.astype(np.uint8)).convert('P'))\n            return img_sublist, mask_sublist\n        else:\n            img, mask = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n            return img, mask\n\n\nclass SlidingCrop(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), 'constant')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), 'constant', constant_values=self.ignore_label)\n        return img, mask, h, w\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_slices, mask_slices, slices_info = [], [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub, sub_h, sub_w = self._pad(img_sub, mask_sub)\n                    img_slices.append(Image.fromarray(img_sub.astype(np.uint8)).convert('RGB'))\n                    mask_slices.append(Image.fromarray(mask_sub.astype(np.uint8)).convert('P'))\n                    slices_info.append([sy, ey, sx, ex, sub_h, sub_w])\n            return img_slices, mask_slices, slices_info\n        else:\n            img, mask, sub_h, sub_w = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n            return [img], [mask], [[0, sub_h, 0, sub_w, sub_h, sub_w]]\n"""
utils/misc.py,15,"b""import os\nfrom math import ceil\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\ndef check_mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n\n\ndef get_upsampling_weight(in_channels, out_channels, kernel_size):\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)\n    weight[list(range(in_channels)), list(range(out_channels)), :, :] = filt\n    return torch.from_numpy(weight).float()\n\n\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True, ignore_index=255):\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\n\nclass FocalLoss2d(nn.Module):\n    def __init__(self, gamma=2, weight=None, size_average=True, ignore_index=255):\n        super(FocalLoss2d, self).__init__()\n        self.gamma = gamma\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss((1 - F.softmax(inputs)) ** self.gamma * F.log_softmax(inputs), targets)\n\n\ndef _fast_hist(label_pred, label_true, num_classes):\n    mask = (label_true >= 0) & (label_true < num_classes)\n    hist = np.bincount(\n        num_classes * label_true[mask].astype(int) +\n        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n    return hist\n\n\ndef evaluate(predictions, gts, num_classes):\n    hist = np.zeros((num_classes, num_classes))\n    for lp, lt in zip(predictions, gts):\n        hist += _fast_hist(lp.flatten(), lt.flatten(), num_classes)\n    # axis 0: gt, axis 1: prediction\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass PolyLR(object):\n    def __init__(self, optimizer, curr_iter, max_iter, lr_decay):\n        self.max_iter = float(max_iter)\n        self.init_lr_groups = []\n        for p in optimizer.param_groups:\n            self.init_lr_groups.append(p['lr'])\n        self.param_groups = optimizer.param_groups\n        self.curr_iter = curr_iter\n        self.lr_decay = lr_decay\n\n    def step(self):\n        for idx, p in enumerate(self.param_groups):\n            p['lr'] = self.init_lr_groups[idx] * (1 - self.curr_iter / self.max_iter) ** self.lr_decay\n\n\n# just a try, not recommend to use\nclass Conv2dDeformable(nn.Module):\n    def __init__(self, regular_filter, cuda=True):\n        super(Conv2dDeformable, self).__init__()\n        assert isinstance(regular_filter, nn.Conv2d)\n        self.regular_filter = regular_filter\n        self.offset_filter = nn.Conv2d(regular_filter.in_channels, 2 * regular_filter.in_channels, kernel_size=3,\n                                       padding=1, bias=False)\n        self.offset_filter.weight.data.normal_(0, 0.0005)\n        self.input_shape = None\n        self.grid_w = None\n        self.grid_h = None\n        self.cuda = cuda\n\n    def forward(self, x):\n        x_shape = x.size()  # (b, c, h, w)\n        offset = self.offset_filter(x)  # (b, 2*c, h, w)\n        offset_w, offset_h = torch.split(offset, self.regular_filter.in_channels, 1)  # (b, c, h, w)\n        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        if not self.input_shape or self.input_shape != x_shape:\n            self.input_shape = x_shape\n            grid_w, grid_h = np.meshgrid(np.linspace(-1, 1, x_shape[3]), np.linspace(-1, 1, x_shape[2]))  # (h, w)\n            grid_w = torch.Tensor(grid_w)\n            grid_h = torch.Tensor(grid_h)\n            if self.cuda:\n                grid_w = grid_w.cuda()\n                grid_h = grid_h.cuda()\n            self.grid_w = nn.Parameter(grid_w)\n            self.grid_h = nn.Parameter(grid_h)\n        offset_w = offset_w + self.grid_w  # (b*c, h, w)\n        offset_h = offset_h + self.grid_h  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3])).unsqueeze(1)  # (b*c, 1, h, w)\n        x = F.grid_sample(x, torch.stack((offset_h, offset_w), 3))  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))  # (b, c, h, w)\n        x = self.regular_filter(x)\n        return x\n\n\ndef sliced_forward(single_forward):\n    def _pad(x, crop_size):\n        h, w = x.size()[2:]\n        pad_h = max(crop_size - h, 0)\n        pad_w = max(crop_size - w, 0)\n        x = F.pad(x, (0, pad_w, 0, pad_h))\n        return x, pad_h, pad_w\n\n    def wrapper(self, x):\n        batch_size, _, ori_h, ori_w = x.size()\n        if self.training and self.use_aux:\n            outputs_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            aux_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.upsample(x, size=new_size, mode='bilinear')\n                scaled_x = Variable(scaled_x).cuda()\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n                print(scaled_x.size())\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    aux_outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n                            print(x_sub.size())\n                            outputs_sub, aux_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n                                aux_sub = aux_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n                                aux_sub = aux_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n                            aux_outputs[:, :, sy: ey, sx: ex] = aux_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = Variable(count).cuda()\n                    outputs = (outputs / count)\n                    aux_outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs, aux_outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                    aux_outputs = aux_outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n                aux_all_scales += aux_outputs\n            return outputs_all_scales / len(self.scales), aux_all_scales\n        else:\n            outputs_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.upsample(x, size=new_size, mode='bilinear')\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n\n                            outputs_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = Variable(count).cuda()\n                    outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n            return outputs_all_scales\n\n    return wrapper\n"""
utils/transforms.py,1,"b'import random\n\nimport numpy as np\nfrom skimage.filters import gaussian\nimport torch\nfrom PIL import Image, ImageFilter\n\n\nclass RandomVerticalFlip(object):\n    def __call__(self, img):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_TOP_BOTTOM)\n        return img\n\n\nclass DeNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n\n\nclass MaskToTensor(object):\n    def __call__(self, img):\n        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n\n\nclass FreeScale(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = tuple(reversed(size))  # size: (h, w)\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return img.resize(self.size, self.interpolation)\n\n\nclass FlipChannels(object):\n    def __call__(self, img):\n        img = np.array(img)[:, :, ::-1]\n        return Image.fromarray(img.astype(np.uint8))\n\n\nclass RandomGaussianBlur(object):\n    def __call__(self, img):\n        sigma = 0.15 + random.random() * 1.15\n        blurred_img = gaussian(np.array(img), sigma=sigma, multichannel=True)\n        blurred_img *= 255\n        return Image.fromarray(blurred_img.astype(np.uint8))\n'"
train/cityscapes-fcn (caffe vgg)/train.py,9,"b""import datetime\nimport os\nimport random\n\nimport numpy as np\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nimport utils.joint_transforms as joint_transforms\nimport utils.transforms as extended_transforms\nfrom datasets import cityscapes\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\ncudnn.benchmark = True\n\nckpt_path = '../../ckpt'\nexp_name = 'cityscapes-fcn8s (caffe vgg)'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'train_batch_size': 12,\n    'epoch_num': 500,\n    'lr': 1e-10,\n    'weight_decay': 5e-4,\n    'input_size': (256, 512),\n    'momentum': 0.99,\n    'lr_patience': 100,  # large patience denotes fixed lr\n    'snapshot': '',  # empty string denotes no snapshot\n    'print_freq': 20,\n    'val_batch_size': 16,\n    'val_save_to_img_file': False,\n    'val_img_sample_rate': 0.05  # randomly sample some validation results to display\n}\n\n\ndef main():\n    net = FCN8s(num_classes=cityscapes.num_classes, caffe=True).cuda()\n\n    if len(args['snapshot']) == 0:\n        curr_epoch = 1\n        args['best_record'] = {'epoch': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n    else:\n        print('training resumes from ' + args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'])))\n        split_snapshot = args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        args['best_record'] = {'epoch': int(split_snapshot[1]), 'val_loss': float(split_snapshot[3]),\n                               'acc': float(split_snapshot[5]), 'acc_cls': float(split_snapshot[7]),\n                               'mean_iu': float(split_snapshot[9]), 'fwavacc': float(split_snapshot[11])}\n\n    net.train()\n\n    mean_std = ([103.939, 116.779, 123.68], [1.0, 1.0, 1.0])\n\n    short_size = int(min(args['input_size']) / 0.875)\n    train_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(short_size),\n        joint_transforms.RandomCrop(args['input_size']),\n        joint_transforms.RandomHorizontallyFlip()\n    ])\n    val_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(short_size),\n        joint_transforms.CenterCrop(args['input_size'])\n    ])\n    input_transform = standard_transforms.Compose([\n        extended_transforms.FlipChannels(),\n        standard_transforms.ToTensor(),\n        standard_transforms.Lambda(lambda x: x.mul_(255)),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    restore_transform = standard_transforms.Compose([\n        extended_transforms.DeNormalize(*mean_std),\n        standard_transforms.Lambda(lambda x: x.div_(255)),\n        standard_transforms.ToPILImage(),\n        extended_transforms.FlipChannels()\n    ])\n    visualize = standard_transforms.ToTensor()\n\n    train_set = cityscapes.CityScapes('fine', 'train', joint_transform=train_joint_transform,\n                                      transform=input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=8, shuffle=True)\n    val_set = cityscapes.CityScapes('fine', 'val', joint_transform=val_joint_transform, transform=input_transform,\n                                    target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=args['val_batch_size'], num_workers=8, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=False, ignore_index=cityscapes.ignore_label).cuda()\n\n    optimizer = optim.Adam([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': args['lr'], 'weight_decay': args['weight_decay']}\n    ], betas=(args['momentum'], 0.999))\n\n    if len(args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * args['lr']\n        optimizer.param_groups[1]['lr'] = args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(args) + '\\n\\n')\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=args['lr_patience'], min_lr=1e-10, verbose=True)\n    for epoch in range(curr_epoch, args['epoch_num'] + 1):\n        train(train_loader, net, criterion, optimizer, epoch, args)\n        val_loss = validate(val_loader, net, criterion, optimizer, epoch, args, restore_transform, visualize)\n        scheduler.step(val_loss)\n\n\ndef train(train_loader, net, criterion, optimizer, epoch, train_args):\n    train_loss = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        assert inputs.size()[2:] == labels.size()[1:]\n        N = inputs.size(0)\n        inputs = Variable(inputs).cuda()\n        labels = Variable(labels).cuda()\n\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        assert outputs.size()[2:] == labels.size()[1:]\n        assert outputs.size()[1] == cityscapes.num_classes\n\n        loss = criterion(outputs, labels) / N\n        loss.backward()\n        optimizer.step()\n\n        train_loss.update(loss.data[0], N)\n\n        curr_iter += 1\n        writer.add_scalar('train_loss', train_loss.avg, curr_iter)\n\n        if (i + 1) % train_args['print_freq'] == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg))\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, train_args, restore, visualize):\n    net.eval()\n\n    val_loss = AverageMeter()\n    inputs_all, gts_all, predictions_all = [], [], []\n\n    for vi, data in enumerate(val_loader):\n        inputs, gts = data\n        N = inputs.size(0)\n        inputs = Variable(inputs, volatile=True).cuda()\n        gts = Variable(gts, volatile=True).cuda()\n\n        outputs = net(inputs)\n        predictions = outputs.data.max(1)[1].squeeze_(1).cpu().numpy()\n\n        val_loss.update(criterion(outputs, gts).data[0] / N, N)\n\n        for i in inputs:\n            if random.random() > train_args['val_img_sample_rate']:\n                inputs_all.append(None)\n            else:\n                inputs_all.append(i.data.cpu())\n        gts_all.append(gts.data.cpu().numpy())\n        predictions_all.append(predictions)\n\n    gts_all = np.concatenate(gts_all)\n    predictions_all = np.concatenate(predictions_all)\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, cityscapes.num_classes)\n\n    if mean_iu > train_args['best_record']['mean_iu']:\n        train_args['best_record']['val_loss'] = val_loss.avg\n        train_args['best_record']['epoch'] = epoch\n        train_args['best_record']['acc'] = acc\n        train_args['best_record']['acc_cls'] = acc_cls\n        train_args['best_record']['mean_iu'] = mean_iu\n        train_args['best_record']['fwavacc'] = fwavacc\n        snapshot_name = 'epoch_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n            epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr']\n        )\n        torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n        torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n        if train_args['val_save_to_img_file']:\n            to_save_dir = os.path.join(ckpt_path, exp_name, str(epoch))\n            check_mkdir(to_save_dir)\n\n        val_visual = []\n        for idx, data in enumerate(zip(inputs_all, gts_all, predictions_all)):\n            if data[0] is None:\n                continue\n            input_pil = restore(data[0])\n            gt_pil = cityscapes.colorize_mask(data[1])\n            predictions_pil = cityscapes.colorize_mask(data[2])\n            if train_args['val_save_to_img_file']:\n                input_pil.save(os.path.join(to_save_dir, '%d_input.png' % idx))\n                predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n                gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n            val_visual.extend([visualize(input_pil.convert('RGB')), visualize(gt_pil.convert('RGB')),\n                               visualize(predictions_pil.convert('RGB'))])\n        val_visual = torch.stack(val_visual, 0)\n        val_visual = vutils.make_grid(val_visual, nrow=3, padding=5)\n        writer.add_image(snapshot_name, val_visual)\n\n        print('-----------------------------------------------------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d]' % (\n        train_args['best_record']['val_loss'], train_args['best_record']['acc'], train_args['best_record']['acc_cls'],\n        train_args['best_record']['mean_iu'], train_args['best_record']['fwavacc'], train_args['best_record']['epoch']))\n\n    print('-----------------------------------------------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n    writer.add_scalar('lr', optimizer.param_groups[1]['lr'], epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main()\n"""
train/cityscapes-fcn/train.py,9,"b""import datetime\nimport os\nimport random\n\nimport numpy as np\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nimport utils.joint_transforms as joint_transforms\nimport utils.transforms as extended_transforms\nfrom datasets import cityscapes\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\ncudnn.benchmark = True\n\nckpt_path = '../../ckpt'\nexp_name = 'cityscapes-fcn8s'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'train_batch_size': 16,\n    'epoch_num': 500,\n    'lr': 1e-10,\n    'weight_decay': 5e-4,\n    'input_size': (256, 512),\n    'momentum': 0.95,\n    'lr_patience': 100,  # large patience denotes fixed lr\n    'snapshot': '',  # empty string denotes no snapshot\n    'print_freq': 20,\n    'val_batch_size': 16,\n    'val_save_to_img_file': False,\n    'val_img_sample_rate': 0.05  # randomly sample some validation results to display\n}\n\n\ndef main():\n    net = FCN8s(num_classes=cityscapes.num_classes).cuda()\n\n    if len(args['snapshot']) == 0:\n        curr_epoch = 1\n        args['best_record'] = {'epoch': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n    else:\n        print('training resumes from ' + args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'])))\n        split_snapshot = args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        args['best_record'] = {'epoch': int(split_snapshot[1]), 'val_loss': float(split_snapshot[3]),\n                               'acc': float(split_snapshot[5]), 'acc_cls': float(split_snapshot[7]),\n                               'mean_iu': float(split_snapshot[9]), 'fwavacc': float(split_snapshot[11])}\n    net.train()\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    short_size = int(min(args['input_size']) / 0.875)\n    train_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(short_size),\n        joint_transforms.RandomCrop(args['input_size']),\n        joint_transforms.RandomHorizontallyFlip()\n    ])\n    val_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(short_size),\n        joint_transforms.CenterCrop(args['input_size'])\n    ])\n    input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    restore_transform = standard_transforms.Compose([\n        extended_transforms.DeNormalize(*mean_std),\n        standard_transforms.ToPILImage()\n    ])\n    visualize = standard_transforms.ToTensor()\n\n    train_set = cityscapes.CityScapes('fine', 'train', joint_transform=train_joint_transform,\n                                      transform=input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=8, shuffle=True)\n    val_set = cityscapes.CityScapes('fine', 'val', joint_transform=val_joint_transform, transform=input_transform,\n                                    target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=args['val_batch_size'], num_workers=8, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=False, ignore_index=cityscapes.ignore_label).cuda()\n\n    optimizer = optim.SGD([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': args['lr'], 'weight_decay': args['weight_decay']}\n    ], momentum=args['momentum'])\n\n    if len(args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * args['lr']\n        optimizer.param_groups[1]['lr'] = args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(args) + '\\n\\n')\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=args['lr_patience'], min_lr=1e-10)\n    for epoch in range(curr_epoch, args['epoch_num'] + 1):\n        train(train_loader, net, criterion, optimizer, epoch, args)\n        val_loss = validate(val_loader, net, criterion, optimizer, epoch, args, restore_transform, visualize)\n        scheduler.step(val_loss)\n\n\ndef train(train_loader, net, criterion, optimizer, epoch, train_args):\n    train_loss = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        assert inputs.size()[2:] == labels.size()[1:]\n        N = inputs.size(0)\n        inputs = Variable(inputs).cuda()\n        labels = Variable(labels).cuda()\n\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        assert outputs.size()[2:] == labels.size()[1:]\n        assert outputs.size()[1] == cityscapes.num_classes\n\n        loss = criterion(outputs, labels) / N\n        loss.backward()\n        optimizer.step()\n\n        train_loss.update(loss.data[0], N)\n\n        curr_iter += 1\n        writer.add_scalar('train_loss', train_loss.avg, curr_iter)\n\n        if (i + 1) % train_args['print_freq'] == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg))\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, train_args, restore, visualize):\n    net.eval()\n\n    val_loss = AverageMeter()\n    inputs_all, gts_all, predictions_all = [], [], []\n\n    for vi, data in enumerate(val_loader):\n        inputs, gts = data\n        N = inputs.size(0)\n        inputs = Variable(inputs, volatile=True).cuda()\n        gts = Variable(gts, volatile=True).cuda()\n\n        outputs = net(inputs)\n        predictions = outputs.data.max(1)[1].squeeze_(1).cpu().numpy()\n\n        val_loss.update(criterion(outputs, gts).data[0] / N, N)\n\n        for i in inputs:\n            if random.random() > train_args['val_img_sample_rate']:\n                inputs_all.append(None)\n            else:\n                inputs_all.append(i.data.cpu())\n        gts_all.append(gts.data.cpu().numpy())\n        predictions_all.append(predictions)\n\n    gts_all = np.concatenate(gts_all)\n    predictions_all = np.concatenate(predictions_all)\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, cityscapes.num_classes)\n\n    if mean_iu > train_args['best_record']['mean_iu']:\n        train_args['best_record']['val_loss'] = val_loss.avg\n        train_args['best_record']['epoch'] = epoch\n        train_args['best_record']['acc'] = acc\n        train_args['best_record']['acc_cls'] = acc_cls\n        train_args['best_record']['mean_iu'] = mean_iu\n        train_args['best_record']['fwavacc'] = fwavacc\n        snapshot_name = 'epoch_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n            epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr']\n        )\n        torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n        torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n        if train_args['val_save_to_img_file']:\n            to_save_dir = os.path.join(ckpt_path, exp_name, str(epoch))\n            check_mkdir(to_save_dir)\n\n        val_visual = []\n        for idx, data in enumerate(zip(inputs_all, gts_all, predictions_all)):\n            if data[0] is None:\n                continue\n            input_pil = restore(data[0])\n            gt_pil = cityscapes.colorize_mask(data[1])\n            predictions_pil = cityscapes.colorize_mask(data[2])\n            if train_args['val_save_to_img_file']:\n                input_pil.save(os.path.join(to_save_dir, '%d_input.png' % idx))\n                predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n                gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n            val_visual.extend([visualize(input_pil.convert('RGB')), visualize(gt_pil.convert('RGB')),\n                               visualize(predictions_pil.convert('RGB'))])\n        val_visual = torch.stack(val_visual, 0)\n        val_visual = vutils.make_grid(val_visual, nrow=3, padding=5)\n        writer.add_image(snapshot_name, val_visual)\n\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d]' % (\n        train_args['best_record']['val_loss'], train_args['best_record']['acc'], train_args['best_record']['acc_cls'],\n        train_args['best_record']['mean_iu'], train_args['best_record']['fwavacc'], train_args['best_record']['epoch']))\n\n    print('-----------------------------------------------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n    writer.add_scalar('lr', optimizer.param_groups[1]['lr'], epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main()\n"""
train/cityscapes-psp_net/train.py,10,"b""import datetime\nimport os\nfrom math import sqrt\n\nimport numpy as np\nimport torchvision.transforms as standard_transforms\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport utils.joint_transforms as joint_transforms\nimport utils.transforms as extended_transforms\nfrom datasets import cityscapes\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\nckpt_path = '../../ckpt'\nexp_name = 'cityscapes (fine)-psp_net'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'train_batch_size': 2,\n    'lr': 1e-2 / sqrt(16 / 2),\n    'lr_decay': 0.9,\n    'max_iter': 9e4,\n    'longer_size': 2048,\n    'crop_size': 713,\n    'stride_rate': 2 / 3.,\n    'weight_decay': 1e-4,\n    'momentum': 0.9,\n    'snapshot': '',\n    'print_freq': 10,\n    'val_save_to_img_file': False,\n    'val_img_sample_rate': 0.01,  # randomly sample some validation results to display,\n    'val_img_display_size': 384,\n    'val_freq': 400\n}\n\n\ndef main():\n    net = PSPNet(num_classes=cityscapes.num_classes)\n\n    if len(args['snapshot']) == 0:\n        # net.load_state_dict(torch.load(os.path.join(ckpt_path, 'cityscapes (coarse)-psp_net', 'xx.pth')))\n        curr_epoch = 1\n        args['best_record'] = {'epoch': 0, 'iter': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0,\n                               'fwavacc': 0}\n    else:\n        print('training resumes from ' + args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'])))\n        split_snapshot = args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        args['best_record'] = {'epoch': int(split_snapshot[1]), 'iter': int(split_snapshot[3]),\n                               'val_loss': float(split_snapshot[5]), 'acc': float(split_snapshot[7]),\n                               'acc_cls': float(split_snapshot[9]),'mean_iu': float(split_snapshot[11]),\n                               'fwavacc': float(split_snapshot[13])}\n    net.cuda().train()\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    train_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(args['longer_size']),\n        joint_transforms.RandomRotate(10),\n        joint_transforms.RandomHorizontallyFlip()\n    ])\n    sliding_crop = joint_transforms.SlidingCrop(args['crop_size'], args['stride_rate'], cityscapes.ignore_label)\n    train_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    val_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    visualize = standard_transforms.Compose([\n        standard_transforms.Scale(args['val_img_display_size']),\n        standard_transforms.ToTensor()\n    ])\n\n    train_set = cityscapes.CityScapes('fine', 'train', joint_transform=train_joint_transform, sliding_crop=sliding_crop,\n                                      transform=train_input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=8, shuffle=True)\n    val_set = cityscapes.CityScapes('fine', 'val', transform=val_input_transform, sliding_crop=sliding_crop,\n                                    target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=1, num_workers=8, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=True, ignore_index=cityscapes.ignore_label).cuda()\n\n    optimizer = optim.SGD([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': args['lr'], 'weight_decay': args['weight_decay']}\n    ], momentum=args['momentum'], nesterov=True)\n\n    if len(args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * args['lr']\n        optimizer.param_groups[1]['lr'] = args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(args) + '\\n\\n')\n\n    train(train_loader, net, criterion, optimizer, curr_epoch, args, val_loader, visualize)\n\n\ndef train(train_loader, net, criterion, optimizer, curr_epoch, train_args, val_loader, visualize):\n    while True:\n        train_main_loss = AverageMeter()\n        train_aux_loss = AverageMeter()\n        curr_iter = (curr_epoch - 1) * len(train_loader)\n        for i, data in enumerate(train_loader):\n            optimizer.param_groups[0]['lr'] = 2 * train_args['lr'] * (1 - float(curr_iter) / train_args['max_iter']\n                                                                      ) ** train_args['lr_decay']\n            optimizer.param_groups[1]['lr'] = train_args['lr'] * (1 - float(curr_iter) / train_args['max_iter']\n                                                                  ) ** train_args['lr_decay']\n\n            inputs, gts, _ = data\n            assert len(inputs.size()) == 5 and len(gts.size()) == 4\n            inputs.transpose_(0, 1)\n            gts.transpose_(0, 1)\n\n            assert inputs.size()[3:] == gts.size()[2:]\n            slice_batch_pixel_size = inputs.size(1) * inputs.size(3) * inputs.size(4)\n\n            for inputs_slice, gts_slice in zip(inputs, gts):\n                inputs_slice = Variable(inputs_slice).cuda()\n                gts_slice = Variable(gts_slice).cuda()\n\n                optimizer.zero_grad()\n                outputs, aux = net(inputs_slice)\n                assert outputs.size()[2:] == gts_slice.size()[1:]\n                assert outputs.size()[1] == cityscapes.num_classes\n\n                main_loss = criterion(outputs, gts_slice)\n                aux_loss = criterion(aux, gts_slice)\n                loss = main_loss + 0.4 * aux_loss\n                loss.backward()\n                optimizer.step()\n\n                train_main_loss.update(main_loss.data[0], slice_batch_pixel_size)\n                train_aux_loss.update(aux_loss.data[0], slice_batch_pixel_size)\n\n            curr_iter += 1\n            writer.add_scalar('train_main_loss', train_main_loss.avg, curr_iter)\n            writer.add_scalar('train_aux_loss', train_aux_loss.avg, curr_iter)\n            writer.add_scalar('lr', optimizer.param_groups[1]['lr'], curr_iter)\n\n            if (i + 1) % train_args['print_freq'] == 0:\n                print('[epoch %d], [iter %d / %d], [train main loss %.5f], [train aux loss %.5f]. [lr %.10f]' % (\n                    curr_epoch, i + 1, len(train_loader), train_main_loss.avg, train_aux_loss.avg,\n                    optimizer.param_groups[1]['lr']))\n            if curr_iter >= train_args['max_iter']:\n                return\n            if curr_iter % train_args['val_freq'] == 0:\n                validate(val_loader, net, criterion, optimizer, curr_epoch, i + 1, train_args, visualize)\n        curr_epoch += 1\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, iter_num, train_args, visualize):\n    # the following code is written assuming that batch size is 1\n    net.eval()\n\n    val_loss = AverageMeter()\n\n    gts_all = np.zeros((len(val_loader), args['longer_size'] / 2, args['longer_size']), dtype=int)\n    predictions_all = np.zeros((len(val_loader), args['longer_size'] / 2, args['longer_size']), dtype=int)\n    for vi, data in enumerate(val_loader):\n        input, gt, slices_info = data\n        assert len(input.size()) == 5 and len(gt.size()) == 4 and len(slices_info.size()) == 3\n        input.transpose_(0, 1)\n        gt.transpose_(0, 1)\n        slices_info.squeeze_(0)\n        assert input.size()[3:] == gt.size()[2:]\n\n        count = torch.zeros(args['longer_size'] / 2, args['longer_size']).cuda()\n        output = torch.zeros(cityscapes.num_classes, args['longer_size'] / 2, args['longer_size']).cuda()\n\n        slice_batch_pixel_size = input.size(1) * input.size(3) * input.size(4)\n\n        for input_slice, gt_slice, info in zip(input, gt, slices_info):\n            input_slice = Variable(input_slice).cuda()\n            gt_slice = Variable(gt_slice).cuda()\n\n            output_slice = net(input_slice)\n            assert output_slice.size()[2:] == gt_slice.size()[1:]\n            assert output_slice.size()[1] == cityscapes.num_classes\n            output[:, info[0]: info[1], info[2]: info[3]] += output_slice[0, :, :info[4], :info[5]].data\n            gts_all[vi, info[0]: info[1], info[2]: info[3]] += gt_slice[0, :info[4], :info[5]].data.cpu().numpy()\n\n            count[info[0]: info[1], info[2]: info[3]] += 1\n\n            val_loss.update(criterion(output_slice, gt_slice).data[0], slice_batch_pixel_size)\n\n        output /= count\n        gts_all[vi, :, :] /= count.cpu().numpy().astype(int)\n        predictions_all[vi, :, :] = output.max(0)[1].squeeze_(0).cpu().numpy()\n\n        print('validating: %d / %d' % (vi + 1, len(val_loader)))\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, cityscapes.num_classes)\n    if val_loss.avg < train_args['best_record']['val_loss']:\n        train_args['best_record']['val_loss'] = val_loss.avg\n        train_args['best_record']['epoch'] = epoch\n        train_args['best_record']['iter'] = iter_num\n        train_args['best_record']['acc'] = acc\n        train_args['best_record']['acc_cls'] = acc_cls\n        train_args['best_record']['mean_iu'] = mean_iu\n        train_args['best_record']['fwavacc'] = fwavacc\n    snapshot_name = 'epoch_%d_iter_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n        epoch, iter_num, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr'])\n    torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n    torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n    if train_args['val_save_to_img_file']:\n        to_save_dir = os.path.join(ckpt_path, exp_name, '%d_%d' % (epoch, iter_num))\n        check_mkdir(to_save_dir)\n\n    val_visual = []\n    for idx, data in enumerate(zip(gts_all, predictions_all)):\n        gt_pil = cityscapes.colorize_mask(data[0])\n        predictions_pil = cityscapes.colorize_mask(data[1])\n        if train_args['val_save_to_img_file']:\n            predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n            gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n            val_visual.extend([visualize(gt_pil.convert('RGB')),\n                               visualize(predictions_pil.convert('RGB'))])\n    val_visual = torch.stack(val_visual, 0)\n    val_visual = vutils.make_grid(val_visual, nrow=2, padding=5)\n    writer.add_image(snapshot_name, val_visual)\n\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('[epoch %d], [iter %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, iter_num, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d], '\n          '[iter %d]' % (train_args['best_record']['val_loss'], train_args['best_record']['acc'],\n                         train_args['best_record']['acc_cls'], train_args['best_record']['mean_iu'],\n                         train_args['best_record']['fwavacc'], train_args['best_record']['epoch'],\n                         train_args['best_record']['iter']))\n\n    print('-----------------------------------------------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main()\n"""
train/voc-fcn (caffe vgg)/train.py,9,"b""import datetime\nimport os\nimport random\n\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nimport utils.transforms as extended_transforms\nfrom datasets import voc\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\ncudnn.benchmark = True\n\nckpt_path = '../../ckpt'\nexp_name = 'voc-fcn8s (caffe vgg)'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'epoch_num': 30,\n    'lr': 1e-10,\n    'weight_decay': 5e-4,\n    'momentum': 0.99,\n    'lr_patience': 100,  # large patience denotes fixed lr\n    'snapshot': '',  # empty string denotes learning from scratch\n    'print_freq': 20,\n    'val_save_to_img_file': False,\n    'val_img_sample_rate': 0.1  # randomly sample some validation results to display\n}\n\n\ndef main(train_args):\n    net = FCN8s(num_classes=voc.num_classes, caffe=True).cuda()\n\n    if len(train_args['snapshot']) == 0:\n        curr_epoch = 1\n        train_args['best_record'] = {'epoch': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n    else:\n        print('training resumes from ' + train_args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, train_args['snapshot'])))\n        split_snapshot = train_args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        train_args['best_record'] = {'epoch': int(split_snapshot[1]), 'val_loss': float(split_snapshot[3]),\n                                     'acc': float(split_snapshot[5]), 'acc_cls': float(split_snapshot[7]),\n                                     'mean_iu': float(split_snapshot[9]), 'fwavacc': float(split_snapshot[11])}\n\n    net.train()\n\n    mean_std = ([103.939, 116.779, 123.68], [1.0, 1.0, 1.0])\n\n    input_transform = standard_transforms.Compose([\n        extended_transforms.FlipChannels(),\n        standard_transforms.ToTensor(),\n        standard_transforms.Lambda(lambda x: x.mul_(255)),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    restore_transform = standard_transforms.Compose([\n        extended_transforms.DeNormalize(*mean_std),\n        standard_transforms.Lambda(lambda x: x.div_(255)),\n        standard_transforms.ToPILImage(),\n        extended_transforms.FlipChannels()\n    ])\n    visualize = standard_transforms.Compose([\n        standard_transforms.Scale(400),\n        standard_transforms.CenterCrop(400),\n        standard_transforms.ToTensor()\n    ])\n\n    train_set = voc.VOC('train', transform=input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=1, num_workers=4, shuffle=True)\n    val_set = voc.VOC('val', transform=input_transform, target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=1, num_workers=4, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=False, ignore_index=voc.ignore_label).cuda()\n\n    optimizer = optim.SGD([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * train_args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': train_args['lr'], 'weight_decay': train_args['weight_decay']}\n    ], momentum=train_args['momentum'])\n\n    if len(train_args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + train_args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * train_args['lr']\n        optimizer.param_groups[1]['lr'] = train_args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(train_args) + '\\n\\n')\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=train_args['lr_patience'], min_lr=1e-10, verbose=True)\n    for epoch in range(curr_epoch, train_args['epoch_num'] + 1):\n        train(train_loader, net, criterion, optimizer, epoch, train_args)\n        val_loss = validate(val_loader, net, criterion, optimizer, epoch, train_args, restore_transform, visualize)\n        scheduler.step(val_loss)\n\n\ndef train(train_loader, net, criterion, optimizer, epoch, train_args):\n    train_loss = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        assert inputs.size()[2:] == labels.size()[1:]\n        N = inputs.size(0)\n        inputs = Variable(inputs).cuda()\n        labels = Variable(labels).cuda()\n\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        assert outputs.size()[2:] == labels.size()[1:]\n        assert outputs.size()[1] == voc.num_classes\n\n        loss = criterion(outputs, labels) / N\n        loss.backward()\n        optimizer.step()\n\n        train_loss.update(loss.data[0], N)\n\n        curr_iter += 1\n        writer.add_scalar('train_loss', train_loss.avg, curr_iter)\n\n        if (i + 1) % train_args['print_freq'] == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg\n            ))\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, train_args, restore, visualize):\n    net.eval()\n\n    val_loss = AverageMeter()\n    inputs_all, gts_all, predictions_all = [], [], []\n\n    for vi, data in enumerate(val_loader):\n        inputs, gts = data\n        N = inputs.size(0)\n        inputs = Variable(inputs, volatile=True).cuda()\n        gts = Variable(gts, volatile=True).cuda()\n\n        outputs = net(inputs)\n        predictions = outputs.data.max(1)[1].squeeze_(1).squeeze_(0).cpu().numpy()\n\n        val_loss.update(criterion(outputs, gts).data[0] / N, N)\n\n        if random.random() > train_args['val_img_sample_rate']:\n            inputs_all.append(None)\n        else:\n            inputs_all.append(inputs.data.squeeze_(0).cpu())\n        gts_all.append(gts.data.squeeze_(0).cpu().numpy())\n        predictions_all.append(predictions)\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, voc.num_classes)\n\n    if mean_iu > train_args['best_record']['mean_iu']:\n        train_args['best_record']['val_loss'] = val_loss.avg\n        train_args['best_record']['epoch'] = epoch\n        train_args['best_record']['acc'] = acc\n        train_args['best_record']['acc_cls'] = acc_cls\n        train_args['best_record']['mean_iu'] = mean_iu\n        train_args['best_record']['fwavacc'] = fwavacc\n        snapshot_name = 'epoch_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n            epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr']\n        )\n        torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n        torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n        if train_args['val_save_to_img_file']:\n            to_save_dir = os.path.join(ckpt_path, exp_name, str(epoch))\n            check_mkdir(to_save_dir)\n\n        val_visual = []\n        for idx, data in enumerate(zip(inputs_all, gts_all, predictions_all)):\n            if data[0] is None:\n                continue\n            input_pil = restore(data[0])\n            gt_pil = voc.colorize_mask(data[1])\n            predictions_pil = voc.colorize_mask(data[2])\n            if train_args['val_save_to_img_file']:\n                input_pil.save(os.path.join(to_save_dir, '%d_input.png' % idx))\n                predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n                gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n            val_visual.extend([visualize(input_pil.convert('RGB')), visualize(gt_pil.convert('RGB')),\n                               visualize(predictions_pil.convert('RGB'))])\n        val_visual = torch.stack(val_visual, 0)\n        val_visual = vutils.make_grid(val_visual, nrow=3, padding=5)\n        writer.add_image(snapshot_name, val_visual)\n\n    print('--------------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d]' % (\n        train_args['best_record']['val_loss'], train_args['best_record']['acc'], train_args['best_record']['acc_cls'],\n        train_args['best_record']['mean_iu'], train_args['best_record']['fwavacc'], train_args['best_record']['epoch']))\n\n    print('--------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n    writer.add_scalar('lr', optimizer.param_groups[1]['lr'], epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main(args)\n"""
train/voc-fcn/train.py,9,"b""import datetime\nimport os\nimport random\n\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nimport utils.transforms as extended_transforms\nfrom datasets import voc\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\ncudnn.benchmark = True\n\nckpt_path = '../../ckpt'\nexp_name = 'voc-fcn8s'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'epoch_num': 300,\n    'lr': 1e-10,\n    'weight_decay': 1e-4,\n    'momentum': 0.95,\n    'lr_patience': 100,  # large patience denotes fixed lr\n    'snapshot': '',  # empty string denotes learning from scratch\n    'print_freq': 20,\n    'val_save_to_img_file': False,\n    'val_img_sample_rate': 0.1  # randomly sample some validation results to display\n}\n\n\ndef main(train_args):\n    net = FCN8s(num_classes=voc.num_classes).cuda()\n\n    if len(train_args['snapshot']) == 0:\n        curr_epoch = 1\n        train_args['best_record'] = {'epoch': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n    else:\n        print('training resumes from ' + train_args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, train_args['snapshot'])))\n        split_snapshot = train_args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        train_args['best_record'] = {'epoch': int(split_snapshot[1]), 'val_loss': float(split_snapshot[3]),\n                                     'acc': float(split_snapshot[5]), 'acc_cls': float(split_snapshot[7]),\n                                     'mean_iu': float(split_snapshot[9]), 'fwavacc': float(split_snapshot[11])}\n\n    net.train()\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    restore_transform = standard_transforms.Compose([\n        extended_transforms.DeNormalize(*mean_std),\n        standard_transforms.ToPILImage(),\n    ])\n    visualize = standard_transforms.Compose([\n        standard_transforms.Scale(400),\n        standard_transforms.CenterCrop(400),\n        standard_transforms.ToTensor()\n    ])\n\n    train_set = voc.VOC('train', transform=input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=1, num_workers=4, shuffle=True)\n    val_set = voc.VOC('val', transform=input_transform, target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=1, num_workers=4, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=False, ignore_index=voc.ignore_label).cuda()\n\n    optimizer = optim.Adam([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * train_args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': train_args['lr'], 'weight_decay': train_args['weight_decay']}\n    ], betas=(train_args['momentum'], 0.999))\n\n    if len(train_args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + train_args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * train_args['lr']\n        optimizer.param_groups[1]['lr'] = train_args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(train_args) + '\\n\\n')\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=train_args['lr_patience'], min_lr=1e-10, verbose=True)\n    for epoch in range(curr_epoch, train_args['epoch_num'] + 1):\n        train(train_loader, net, criterion, optimizer, epoch, train_args)\n        val_loss = validate(val_loader, net, criterion, optimizer, epoch, train_args, restore_transform, visualize)\n        scheduler.step(val_loss)\n\n\ndef train(train_loader, net, criterion, optimizer, epoch, train_args):\n    train_loss = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        assert inputs.size()[2:] == labels.size()[1:]\n        N = inputs.size(0)\n        inputs = Variable(inputs).cuda()\n        labels = Variable(labels).cuda()\n\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        assert outputs.size()[2:] == labels.size()[1:]\n        assert outputs.size()[1] == voc.num_classes\n\n        loss = criterion(outputs, labels) / N\n        loss.backward()\n        optimizer.step()\n\n        train_loss.update(loss.data[0], N)\n\n        curr_iter += 1\n        writer.add_scalar('train_loss', train_loss.avg, curr_iter)\n\n        if (i + 1) % train_args['print_freq'] == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg\n            ))\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, train_args, restore, visualize):\n    net.eval()\n\n    val_loss = AverageMeter()\n    inputs_all, gts_all, predictions_all = [], [], []\n\n    for vi, data in enumerate(val_loader):\n        inputs, gts = data\n        N = inputs.size(0)\n        inputs = Variable(inputs, volatile=True).cuda()\n        gts = Variable(gts, volatile=True).cuda()\n\n        outputs = net(inputs)\n        predictions = outputs.data.max(1)[1].squeeze_(1).squeeze_(0).cpu().numpy()\n\n        val_loss.update(criterion(outputs, gts).data[0] / N, N)\n\n        if random.random() > train_args['val_img_sample_rate']:\n            inputs_all.append(None)\n        else:\n            inputs_all.append(inputs.data.squeeze_(0).cpu())\n        gts_all.append(gts.data.squeeze_(0).cpu().numpy())\n        predictions_all.append(predictions)\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, voc.num_classes)\n\n    if mean_iu > train_args['best_record']['mean_iu']:\n        train_args['best_record']['val_loss'] = val_loss.avg\n        train_args['best_record']['epoch'] = epoch\n        train_args['best_record']['acc'] = acc\n        train_args['best_record']['acc_cls'] = acc_cls\n        train_args['best_record']['mean_iu'] = mean_iu\n        train_args['best_record']['fwavacc'] = fwavacc\n        snapshot_name = 'epoch_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n            epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr']\n        )\n        torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n        torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n        if train_args['val_save_to_img_file']:\n            to_save_dir = os.path.join(ckpt_path, exp_name, str(epoch))\n            check_mkdir(to_save_dir)\n\n        val_visual = []\n        for idx, data in enumerate(zip(inputs_all, gts_all, predictions_all)):\n            if data[0] is None:\n                continue\n            input_pil = restore(data[0])\n            gt_pil = voc.colorize_mask(data[1])\n            predictions_pil = voc.colorize_mask(data[2])\n            if train_args['val_save_to_img_file']:\n                input_pil.save(os.path.join(to_save_dir, '%d_input.png' % idx))\n                predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n                gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n            val_visual.extend([visualize(input_pil.convert('RGB')), visualize(gt_pil.convert('RGB')),\n                               visualize(predictions_pil.convert('RGB'))])\n        val_visual = torch.stack(val_visual, 0)\n        val_visual = vutils.make_grid(val_visual, nrow=3, padding=5)\n        writer.add_image(snapshot_name, val_visual)\n\n    print('--------------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d]' % (\n        train_args['best_record']['val_loss'], train_args['best_record']['acc'], train_args['best_record']['acc_cls'],\n        train_args['best_record']['mean_iu'], train_args['best_record']['fwavacc'], train_args['best_record']['epoch']))\n\n    print('--------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n    writer.add_scalar('lr', optimizer.param_groups[1]['lr'], epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main(args)\n"""
train/voc-psp_net/train.py,10,"b""import datetime\nimport os\nfrom math import sqrt\n\nimport numpy as np\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom tensorboard import SummaryWriter\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport utils.joint_transforms as joint_transforms\nimport utils.transforms as extended_transforms\nfrom datasets import voc\nfrom models import *\nfrom utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d\n\nckpt_path = '../../ckpt'\nexp_name = 'voc-psp_net'\nwriter = SummaryWriter(os.path.join(ckpt_path, 'exp', exp_name))\n\nargs = {\n    'train_batch_size': 1,\n    'lr': 1e-2 / sqrt(16 / 4),\n    'lr_decay': 0.9,\n    'max_iter': 3e4,\n    'longer_size': 512,\n    'crop_size': 473,\n    'stride_rate': 2 / 3.,\n    'weight_decay': 1e-4,\n    'momentum': 0.9,\n    'snapshot': '',\n    'print_freq': 10,\n    'val_save_to_img_file': True,\n    'val_img_sample_rate': 0.01,  # randomly sample some validation results to display,\n    'val_img_display_size': 384,\n}\n\n\ndef main():\n    net = PSPNet(num_classes=voc.num_classes).cuda()\n\n    if len(args['snapshot']) == 0:\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, 'cityscapes (coarse)-psp_net', 'xx.pth')))\n        curr_epoch = 1\n        args['best_record'] = {'epoch': 0, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n    else:\n        print('training resumes from ' + args['snapshot'])\n        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'])))\n        split_snapshot = args['snapshot'].split('_')\n        curr_epoch = int(split_snapshot[1]) + 1\n        args['best_record'] = {'epoch': int(split_snapshot[1]), 'val_loss': float(split_snapshot[3]),\n                               'acc': float(split_snapshot[5]), 'acc_cls': float(split_snapshot[7]),\n                               'mean_iu': float(split_snapshot[9]), 'fwavacc': float(split_snapshot[11])}\n    net.train()\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    train_joint_transform = joint_transforms.Compose([\n        joint_transforms.Scale(args['longer_size']),\n        joint_transforms.RandomRotate(10),\n        joint_transforms.RandomHorizontallyFlip()\n    ])\n    sliding_crop = joint_transforms.SlidingCrop(args['crop_size'], args['stride_rate'], voc.ignore_label)\n    train_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    val_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n    target_transform = extended_transforms.MaskToTensor()\n    visualize = standard_transforms.Compose([\n        standard_transforms.Scale(args['val_img_display_size']),\n        standard_transforms.ToTensor()\n    ])\n\n    train_set = voc.VOC('train', joint_transform=train_joint_transform, sliding_crop=sliding_crop,\n                                      transform=train_input_transform, target_transform=target_transform)\n    train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=8, shuffle=True)\n    val_set = voc.VOC('val', transform=val_input_transform, sliding_crop=sliding_crop,\n                                    target_transform=target_transform)\n    val_loader = DataLoader(val_set, batch_size=1, num_workers=8, shuffle=False)\n\n    criterion = CrossEntropyLoss2d(size_average=True, ignore_index=voc.ignore_label).cuda()\n\n    optimizer = optim.SGD([\n        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n         'lr': 2 * args['lr']},\n        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n         'lr': args['lr'], 'weight_decay': args['weight_decay']}\n    ], momentum=args['momentum'], nesterov=True)\n\n    if len(args['snapshot']) > 0:\n        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, 'opt_' + args['snapshot'])))\n        optimizer.param_groups[0]['lr'] = 2 * args['lr']\n        optimizer.param_groups[1]['lr'] = args['lr']\n\n    check_mkdir(ckpt_path)\n    check_mkdir(os.path.join(ckpt_path, exp_name))\n    open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(args) + '\\n\\n')\n\n    train(train_loader, net, criterion, optimizer, curr_epoch, args, val_loader, visualize)\n\n\ndef train(train_loader, net, criterion, optimizer, curr_epoch, train_args, val_loader, visualize):\n    while True:\n        train_main_loss = AverageMeter()\n        train_aux_loss = AverageMeter()\n        curr_iter = (curr_epoch - 1) * len(train_loader)\n        for i, data in enumerate(train_loader):\n            optimizer.param_groups[0]['lr'] = 2 * train_args['lr'] * (1 - float(curr_iter) / train_args['max_iter']\n                                                                      ) ** train_args['lr_decay']\n            optimizer.param_groups[1]['lr'] = train_args['lr'] * (1 - float(curr_iter) / train_args['max_iter']\n                                                                  ) ** train_args['lr_decay']\n\n            inputs, gts, _ = data\n            assert len(inputs.size()) == 5 and len(gts.size()) == 4\n            inputs.transpose_(0, 1)\n            gts.transpose_(0, 1)\n\n            assert inputs.size()[3:] == gts.size()[2:]\n            slice_batch_pixel_size = inputs.size(1) * inputs.size(3) * inputs.size(4)\n\n            for inputs_slice, gts_slice in zip(inputs, gts):\n                inputs_slice = Variable(inputs_slice).cuda()\n                gts_slice = Variable(gts_slice).cuda()\n\n                optimizer.zero_grad()\n                outputs, aux = net(inputs_slice)\n                assert outputs.size()[2:] == gts_slice.size()[1:]\n                assert outputs.size()[1] == voc.num_classes\n\n                main_loss = criterion(outputs, gts_slice)\n                aux_loss = criterion(aux, gts_slice)\n                loss = main_loss + 0.4 * aux_loss\n                loss.backward()\n                optimizer.step()\n\n                train_main_loss.update(main_loss.data[0], slice_batch_pixel_size)\n                train_aux_loss.update(aux_loss.data[0], slice_batch_pixel_size)\n\n            curr_iter += 1\n            writer.add_scalar('train_main_loss', train_main_loss.avg, curr_iter)\n            writer.add_scalar('train_aux_loss', train_aux_loss.avg, curr_iter)\n            writer.add_scalar('lr', optimizer.param_groups[1]['lr'], curr_iter)\n\n            if (i + 1) % train_args['print_freq'] == 0:\n                print('[epoch %d], [iter %d / %d], [train main loss %.5f], [train aux loss %.5f]. [lr %.10f]' % (\n                    curr_epoch, i + 1, len(train_loader), train_main_loss.avg, train_aux_loss.avg,\n                    optimizer.param_groups[1]['lr']))\n            if curr_iter >= train_args['max_iter']:\n                return\n        validate(val_loader, net, criterion, optimizer, curr_epoch, train_args, visualize)\n        curr_epoch += 1\n\n\ndef validate(val_loader, net, criterion, optimizer, epoch, train_args, visualize):\n    # the following code is written assuming that batch size is 1\n    net.eval()\n\n    val_loss = AverageMeter()\n\n    gts_all = np.zeros((len(val_loader), args['shorter_size'], 2 * args['shorter_size']), dtype=int)\n    predictions_all = np.zeros((len(val_loader), args['shorter_size'], 2 * args['shorter_size']), dtype=int)\n    for vi, data in enumerate(val_loader):\n        input, gt, slices_info = data\n        assert len(input.size()) == 5 and len(gt.size()) == 4 and len(slices_info.size()) == 3\n        input.transpose_(0, 1)\n        gt.transpose_(0, 1)\n        slices_info.squeeze_(0)\n        assert input.size()[3:] == gt.size()[2:]\n\n        count = torch.zeros(args['shorter_size'], 2 * args['shorter_size']).cuda()\n        output = torch.zeros(voc.num_classes, args['shorter_size'], 2 * args['shorter_size']).cuda()\n\n        slice_batch_pixel_size = input.size(1) * input.size(3) * input.size(4)\n\n        for input_slice, gt_slice, info in zip(input, gt, slices_info):\n            input_slice = Variable(input_slice).cuda()\n            gt_slice = Variable(gt_slice).cuda()\n\n            output_slice = net(input_slice)\n            assert output_slice.size()[2:] == gt_slice.size()[1:]\n            assert output_slice.size()[1] == voc.num_classes\n            output[:, info[0]: info[1], info[2]: info[3]] += output_slice[0, :, :info[4], :info[5]].data\n            gts_all[vi, info[0]: info[1], info[2]: info[3]] += gt_slice[0, :info[4], :info[5]].data.cpu().numpy()\n\n            count[info[0]: info[1], info[2]: info[3]] += 1\n\n            val_loss.update(criterion(output_slice, gt_slice).data[0], slice_batch_pixel_size)\n\n        output /= count\n        gts_all[vi, :, :] /= count.cpu().numpy().astype(int)\n        predictions_all[vi, :, :] = output.max(0)[1].squeeze_(0).cpu().numpy()\n\n        print('validating: %d / %d' % (vi + 1, len(val_loader)))\n\n    acc, acc_cls, mean_iu, fwavacc = evaluate(predictions_all, gts_all, voc.num_classes)\n\n    train_args['best_record']['val_loss'] = val_loss.avg\n    train_args['best_record']['epoch'] = epoch\n    train_args['best_record']['acc'] = acc\n    train_args['best_record']['acc_cls'] = acc_cls\n    train_args['best_record']['mean_iu'] = mean_iu\n    train_args['best_record']['fwavacc'] = fwavacc\n    snapshot_name = 'epoch_%d_loss_%.5f_acc_%.5f_acc-cls_%.5f_mean-iu_%.5f_fwavacc_%.5f_lr_%.10f' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc, optimizer.param_groups[1]['lr'])\n    torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, snapshot_name + '.pth'))\n    torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + snapshot_name + '.pth'))\n\n    if train_args['val_save_to_img_file']:\n        to_save_dir = os.path.join(ckpt_path, exp_name, str(epoch))\n        check_mkdir(to_save_dir)\n\n    val_visual = []\n    for idx, data in enumerate(zip(gts_all, predictions_all)):\n        gt_pil = voc.colorize_mask(data[0])\n        predictions_pil = voc.colorize_mask(data[1])\n        if train_args['val_save_to_img_file']:\n            predictions_pil.save(os.path.join(to_save_dir, '%d_prediction.png' % idx))\n            gt_pil.save(os.path.join(to_save_dir, '%d_gt.png' % idx))\n        val_visual.extend([visualize(gt_pil.convert('RGB')),\n                           visualize(predictions_pil.convert('RGB'))])\n    val_visual = torch.stack(val_visual, 0)\n    val_visual = vutils.make_grid(val_visual, nrow=2, padding=5)\n    writer.add_image(snapshot_name, val_visual)\n\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f]' % (\n        epoch, val_loss.avg, acc, acc_cls, mean_iu, fwavacc))\n\n    print('best record: [val loss %.5f], [acc %.5f], [acc_cls %.5f], [mean_iu %.5f], [fwavacc %.5f], [epoch %d]' % (\n        train_args['best_record']['val_loss'], train_args['best_record']['acc'], train_args['best_record']['acc_cls'],\n        train_args['best_record']['mean_iu'], train_args['best_record']['fwavacc'], train_args['best_record']['epoch']))\n\n    print('-----------------------------------------------------------------------------------------------------------')\n\n    writer.add_scalar('val_loss', val_loss.avg, epoch)\n    writer.add_scalar('acc', acc, epoch)\n    writer.add_scalar('acc_cls', acc_cls, epoch)\n    writer.add_scalar('mean_iu', mean_iu, epoch)\n    writer.add_scalar('fwavacc', fwavacc, epoch)\n\n    net.train()\n    return val_loss.avg\n\n\nif __name__ == '__main__':\n    main()\n"""
