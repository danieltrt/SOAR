file_path,api_count,code
test.py,8,"b'from __future__ import print_function\nimport os\nimport argparse\nimport torch\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nfrom data import cfg\nfrom layers.functions.prior_box import PriorBox\nfrom utils.nms_wrapper import nms\n#from utils.nms.py_cpu_nms import py_cpu_nms\nimport cv2\nfrom models.faceboxes import FaceBoxes\nfrom utils.box_utils import decode\nfrom utils.timer import Timer\n\nparser = argparse.ArgumentParser(description=\'FaceBoxes\')\n\nparser.add_argument(\'-m\', \'--trained_model\', default=\'weights/FaceBoxes.pth\',\n                    type=str, help=\'Trained state_dict file path to open\')\nparser.add_argument(\'--save_folder\', default=\'eval/\', type=str, help=\'Dir to save results\')\nparser.add_argument(\'--cpu\', action=""store_true"", default=False, help=\'Use cpu inference\')\nparser.add_argument(\'--dataset\', default=\'PASCAL\', type=str, choices=[\'AFW\', \'PASCAL\', \'FDDB\'], help=\'dataset\')\nparser.add_argument(\'--confidence_threshold\', default=0.05, type=float, help=\'confidence_threshold\')\nparser.add_argument(\'--top_k\', default=5000, type=int, help=\'top_k\')\nparser.add_argument(\'--nms_threshold\', default=0.3, type=float, help=\'nms_threshold\')\nparser.add_argument(\'--keep_top_k\', default=750, type=int, help=\'keep_top_k\')\nparser.add_argument(\'-s\', \'--show_image\', action=""store_true"", default=False, help=\'show detection results\')\nparser.add_argument(\'--vis_thres\', default=0.5, type=float, help=\'visualization_threshold\')\nargs = parser.parse_args()\n\n\ndef check_keys(model, pretrained_state_dict):\n    ckpt_keys = set(pretrained_state_dict.keys())\n    model_keys = set(model.state_dict().keys())\n    used_pretrained_keys = model_keys & ckpt_keys\n    unused_pretrained_keys = ckpt_keys - model_keys\n    missing_keys = model_keys - ckpt_keys\n    print(\'Missing keys:{}\'.format(len(missing_keys)))\n    print(\'Unused checkpoint keys:{}\'.format(len(unused_pretrained_keys)))\n    print(\'Used keys:{}\'.format(len(used_pretrained_keys)))\n    assert len(used_pretrained_keys) > 0, \'load NONE from pretrained checkpoint\'\n    return True\n\n\ndef remove_prefix(state_dict, prefix):\n    \'\'\' Old style model is stored with all names of parameters sharing common prefix \'module.\' \'\'\'\n    print(\'remove prefix \\\'{}\\\'\'.format(prefix))\n    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n    return {f(key): value for key, value in state_dict.items()}\n\n\ndef load_model(model, pretrained_path, load_to_cpu):\n    print(\'Loading pretrained model from {}\'.format(pretrained_path))\n    if load_to_cpu:\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n    else:\n        device = torch.cuda.current_device()\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n    if ""state_dict"" in pretrained_dict.keys():\n        pretrained_dict = remove_prefix(pretrained_dict[\'state_dict\'], \'module.\')\n    else:\n        pretrained_dict = remove_prefix(pretrained_dict, \'module.\')\n    check_keys(model, pretrained_dict)\n    model.load_state_dict(pretrained_dict, strict=False)\n    return model\n\n\nif __name__ == \'__main__\':\n    torch.set_grad_enabled(False)\n    # net and model\n    net = FaceBoxes(phase=\'test\', size=None, num_classes=2)    # initialize detector\n    net = load_model(net, args.trained_model, args.cpu)\n    net.eval()\n    print(\'Finished loading model!\')\n    print(net)\n    cudnn.benchmark = True\n    device = torch.device(""cpu"" if args.cpu else ""cuda"")\n    net = net.to(device)\n\n\n    # save file\n    if not os.path.exists(args.save_folder):\n        os.makedirs(args.save_folder)\n    fw = open(os.path.join(args.save_folder, args.dataset + \'_dets.txt\'), \'w\')\n\n    # testing dataset\n    testset_folder = os.path.join(\'data\', args.dataset, \'images/\')\n    testset_list = os.path.join(\'data\', args.dataset, \'img_list.txt\')\n    with open(testset_list, \'r\') as fr:\n        test_dataset = fr.read().split()\n    num_images = len(test_dataset)\n\n    # testing scale\n    if args.dataset == ""FDDB"":\n        resize = 3\n    elif args.dataset == ""PASCAL"":\n        resize = 2.5\n    elif args.dataset == ""AFW"":\n        resize = 1\n\n    _t = {\'forward_pass\': Timer(), \'misc\': Timer()}\n\n    # testing begin\n    for i, img_name in enumerate(test_dataset):\n        image_path = testset_folder + img_name + \'.jpg\'\n        img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        img = np.float32(img_raw)\n        if resize != 1:\n            img = cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n        im_height, im_width, _ = img.shape\n        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n        img -= (104, 117, 123)\n        img = img.transpose(2, 0, 1)\n        img = torch.from_numpy(img).unsqueeze(0)\n        img = img.to(device)\n        scale = scale.to(device)\n\n        _t[\'forward_pass\'].tic()\n        loc, conf = net(img)  # forward pass\n        _t[\'forward_pass\'].toc()\n        _t[\'misc\'].tic()\n        priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n        priors = priorbox.forward()\n        priors = priors.to(device)\n        prior_data = priors.data\n        boxes = decode(loc.data.squeeze(0), prior_data, cfg[\'variance\'])\n        boxes = boxes * scale / resize\n        boxes = boxes.cpu().numpy()\n        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n\n        # ignore low scores\n        inds = np.where(scores > args.confidence_threshold)[0]\n        boxes = boxes[inds]\n        scores = scores[inds]\n\n        # keep top-K before NMS\n        order = scores.argsort()[::-1][:args.top_k]\n        boxes = boxes[order]\n        scores = scores[order]\n\n        # do NMS\n        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n        #keep = py_cpu_nms(dets, args.nms_threshold)\n        keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n        dets = dets[keep, :]\n\n        # keep top-K faster NMS\n        dets = dets[:args.keep_top_k, :]\n        _t[\'misc\'].toc()\n\n        # save dets\n        if args.dataset == ""FDDB"":\n            fw.write(\'{:s}\\n\'.format(img_name))\n            fw.write(\'{:.1f}\\n\'.format(dets.shape[0]))\n            for k in range(dets.shape[0]):\n                xmin = dets[k, 0]\n                ymin = dets[k, 1]\n                xmax = dets[k, 2]\n                ymax = dets[k, 3]\n                score = dets[k, 4]\n                w = xmax - xmin + 1\n                h = ymax - ymin + 1\n                fw.write(\'{:.3f} {:.3f} {:.3f} {:.3f} {:.10f}\\n\'.format(xmin, ymin, w, h, score))\n        else:\n            for k in range(dets.shape[0]):\n                xmin = dets[k, 0]\n                ymin = dets[k, 1]\n                xmax = dets[k, 2]\n                ymax = dets[k, 3]\n                ymin += 0.2 * (ymax - ymin + 1)\n                score = dets[k, 4]\n                fw.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.format(img_name, score, xmin, ymin, xmax, ymax))\n        print(\'im_detect: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s\'.format(i + 1, num_images, _t[\'forward_pass\'].average_time, _t[\'misc\'].average_time))\n\n        # show image\n        if args.show_image:\n            for b in dets:\n                if b[4] < args.vis_thres:\n                    continue\n                text = ""{:.4f}"".format(b[4])\n                b = list(map(int, b))\n                cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n                cx = b[0]\n                cy = b[1] + 12\n                cv2.putText(img_raw, text, (cx, cy),\n                            cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n            cv2.imshow(\'res\', img_raw)\n            cv2.waitKey(0)\n\n    fw.close()\n'"
train.py,9,"b'from __future__ import print_function\nimport os\nimport torch\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport argparse\nimport torch.utils.data as data\nfrom data import AnnotationTransform, VOCDetection, detection_collate, preproc, cfg\nfrom layers.modules import MultiBoxLoss\nfrom layers.functions.prior_box import PriorBox\nimport time\nimport datetime\nimport math\nfrom models.faceboxes import FaceBoxes\n\nparser = argparse.ArgumentParser(description=\'FaceBoxes Training\')\nparser.add_argument(\'--training_dataset\', default=\'./data/WIDER_FACE\', help=\'Training dataset directory\')\nparser.add_argument(\'-b\', \'--batch_size\', default=32, type=int, help=\'Batch size for training\')\nparser.add_argument(\'--num_workers\', default=8, type=int, help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--ngpu\', default=2, type=int, help=\'gpus\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\nparser.add_argument(\'--resume_net\', default=None, help=\'resume net for retraining\')\nparser.add_argument(\'--resume_epoch\', default=0, type=int, help=\'resume iter for retraining\')\nparser.add_argument(\'-max\', \'--max_epoch\', default=300, type=int, help=\'max epoch for retraining\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float, help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float, help=\'Gamma update for SGD\')\nparser.add_argument(\'--save_folder\', default=\'./weights/\', help=\'Location to save checkpoint models\')\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\nimg_dim = 1024 # only 1024 is supported\nrgb_mean = (104, 117, 123) # bgr order\nnum_classes = 2\nnum_gpu = args.ngpu\nnum_workers = args.num_workers\nbatch_size = args.batch_size\nmomentum = args.momentum\nweight_decay = args.weight_decay\ninitial_lr = args.lr\ngamma = args.gamma\nmax_epoch = args.max_epoch\ntraining_dataset = args.training_dataset\nsave_folder = args.save_folder\ngpu_train = cfg[\'gpu_train\']\n\nnet = FaceBoxes(\'train\', img_dim, num_classes)\nprint(""Printing net..."")\nprint(net)\n\nif args.resume_net is not None:\n    print(\'Loading resume network...\')\n    state_dict = torch.load(args.resume_net)\n    # create new OrderedDict that does not contain `module.`\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        head = k[:7]\n        if head == \'module.\':\n            name = k[7:] # remove `module.`\n        else:\n            name = k\n        new_state_dict[name] = v\n    net.load_state_dict(new_state_dict)\n\nif num_gpu > 1 and gpu_train:\n    net = torch.nn.DataParallel(net, device_ids=list(range(num_gpu)))\n\ndevice = torch.device(\'cuda:0\' if gpu_train else \'cpu\')\ncudnn.benchmark = True\nnet = net.to(device)\n\noptimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\ncriterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False)\n\npriorbox = PriorBox(cfg, image_size=(img_dim, img_dim))\nwith torch.no_grad():\n    priors = priorbox.forward()\n    priors = priors.to(device)\n\n\ndef train():\n    net.train()\n    epoch = 0 + args.resume_epoch\n    print(\'Loading Dataset...\')\n\n    dataset = VOCDetection(training_dataset, preproc(img_dim, rgb_mean), AnnotationTransform())\n\n    epoch_size = math.ceil(len(dataset) / batch_size)\n    max_iter = max_epoch * epoch_size\n\n    stepvalues = (200 * epoch_size, 250 * epoch_size)\n    step_index = 0\n\n    if args.resume_epoch > 0:\n        start_iter = args.resume_epoch * epoch_size\n    else:\n        start_iter = 0\n\n    for iteration in range(start_iter, max_iter):\n        if iteration % epoch_size == 0:\n            # create batch iterator\n            batch_iterator = iter(data.DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=detection_collate))\n            if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > 200):\n                torch.save(net.state_dict(), save_folder + \'FaceBoxes_epoch_\' + str(epoch) + \'.pth\')\n            epoch += 1\n\n        load_t0 = time.time()\n        if iteration in stepvalues:\n            step_index += 1\n        lr = adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size)\n\n        # load train data\n        images, targets = next(batch_iterator)\n        images = images.to(device)\n        targets = [anno.to(device) for anno in targets]\n\n        # forward\n        out = net(images)\n\n        # backprop\n        optimizer.zero_grad()\n        loss_l, loss_c = criterion(out, priors, targets)\n        loss = cfg[\'loc_weight\'] * loss_l + loss_c\n        loss.backward()\n        optimizer.step()\n        load_t1 = time.time()\n        batch_time = load_t1 - load_t0\n        eta = int(batch_time * (max_iter - iteration))\n        print(\'Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || L: {:.4f} C: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}\'.format(epoch, max_epoch, (iteration % epoch_size) + 1, epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), lr, batch_time, str(datetime.timedelta(seconds=eta))))\n\n    torch.save(net.state_dict(), save_folder + \'Final_FaceBoxes.pth\')\n\n\ndef adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):\n    """"""Sets the learning rate\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    warmup_epoch = -1\n    if epoch <= warmup_epoch:\n        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)\n    else:\n        lr = initial_lr * (gamma ** (step_index))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\nif __name__ == \'__main__\':\n    train()\n'"
data/__init__.py,0,"b'from .wider_voc import VOCDetection, AnnotationTransform, detection_collate\nfrom .data_augment import *\nfrom .config import *\n'"
data/config.py,0,"b""# config.py\n\ncfg = {\n    'name': 'FaceBoxes',\n    #'min_dim': 1024,\n    #'feature_maps': [[32, 32], [16, 16], [8, 8]],\n    # 'aspect_ratios': [[1], [1], [1]],\n    'min_sizes': [[32, 64, 128], [256], [512]],\n    'steps': [32, 64, 128],\n    'variance': [0.1, 0.2],\n    'clip': False,\n    'loc_weight': 2.0,\n    'gpu_train': True\n}\n"""
data/data_augment.py,0,"b'import cv2\nimport numpy as np\nimport random\nfrom utils.box_utils import matrix_iof\n\n\ndef _crop(image, boxes, labels, img_dim):\n    height, width, _ = image.shape\n    pad_image_flag = True\n\n    for _ in range(250):\n        if random.uniform(0, 1) <= 0.2:\n            scale = 1\n        else:\n            scale = random.uniform(0.3, 1.)\n        short_side = min(width, height)\n        w = int(scale * short_side)\n        h = w\n\n        if width == w:\n            l = 0\n        else:\n            l = random.randrange(width - w)\n        if height == h:\n            t = 0\n        else:\n            t = random.randrange(height - h)\n        roi = np.array((l, t, l + w, t + h))\n\n        value = matrix_iof(boxes, roi[np.newaxis])\n        flag = (value >= 1)\n        if not flag.any():\n            continue\n\n        centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n        mask_a = np.logical_and(roi[:2] < centers, centers < roi[2:]).all(axis=1)\n        boxes_t = boxes[mask_a].copy()\n        labels_t = labels[mask_a].copy()\n\n        if boxes_t.shape[0] == 0:\n            continue\n\n        image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n\n        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n        boxes_t[:, :2] -= roi[:2]\n        boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n        boxes_t[:, 2:] -= roi[:2]\n\n\t# make sure that the cropped image contains at least one face > 16 pixel at training image scale\n        b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\n        b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\n        mask_b = np.minimum(b_w_t, b_h_t) > 16.0\n        boxes_t = boxes_t[mask_b]\n        labels_t = labels_t[mask_b]\n\n        if boxes_t.shape[0] == 0:\n            continue\n\n        pad_image_flag = False\n\n        return image_t, boxes_t, labels_t, pad_image_flag\n    return image, boxes, labels, pad_image_flag\n\n\ndef _distort(image):\n\n    def _convert(image, alpha=1, beta=0):\n        tmp = image.astype(float) * alpha + beta\n        tmp[tmp < 0] = 0\n        tmp[tmp > 255] = 255\n        image[:] = tmp\n\n    image = image.copy()\n\n    if random.randrange(2):\n\n        #brightness distortion\n        if random.randrange(2):\n            _convert(image, beta=random.uniform(-32, 32))\n\n        #contrast distortion\n        if random.randrange(2):\n            _convert(image, alpha=random.uniform(0.5, 1.5))\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        #saturation distortion\n        if random.randrange(2):\n            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n\n        #hue distortion\n        if random.randrange(2):\n            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n            tmp %= 180\n            image[:, :, 0] = tmp\n\n        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n\n    else:\n\n        #brightness distortion\n        if random.randrange(2):\n            _convert(image, beta=random.uniform(-32, 32))\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        #saturation distortion\n        if random.randrange(2):\n            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n\n        #hue distortion\n        if random.randrange(2):\n            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n            tmp %= 180\n            image[:, :, 0] = tmp\n\n        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n\n        #contrast distortion\n        if random.randrange(2):\n            _convert(image, alpha=random.uniform(0.5, 1.5))\n\n    return image\n\n\ndef _expand(image, boxes, fill, p):\n    if random.randrange(2):\n        return image, boxes\n\n    height, width, depth = image.shape\n\n    scale = random.uniform(1, p)\n    w = int(scale * width)\n    h = int(scale * height)\n\n    left = random.randint(0, w - width)\n    top = random.randint(0, h - height)\n\n    boxes_t = boxes.copy()\n    boxes_t[:, :2] += (left, top)\n    boxes_t[:, 2:] += (left, top)\n    expand_image = np.empty(\n        (h, w, depth),\n        dtype=image.dtype)\n    expand_image[:, :] = fill\n    expand_image[top:top + height, left:left + width] = image\n    image = expand_image\n\n    return image, boxes_t\n\n\ndef _mirror(image, boxes):\n    _, width, _ = image.shape\n    if random.randrange(2):\n        image = image[:, ::-1]\n        boxes = boxes.copy()\n        boxes[:, 0::2] = width - boxes[:, 2::-2]\n    return image, boxes\n\n\ndef _pad_to_square(image, rgb_mean, pad_image_flag):\n    if not pad_image_flag:\n        return image\n    height, width, _ = image.shape\n    long_side = max(width, height)\n    image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\n    image_t[:, :] = rgb_mean\n    image_t[0:0 + height, 0:0 + width] = image\n    return image_t\n\n\ndef _resize_subtract_mean(image, insize, rgb_mean):\n    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n    interp_method = interp_methods[random.randrange(5)]\n    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n    image = image.astype(np.float32)\n    image -= rgb_mean\n    return image.transpose(2, 0, 1)\n\n\nclass preproc(object):\n\n    def __init__(self, img_dim, rgb_means):\n        self.img_dim = img_dim\n        self.rgb_means = rgb_means\n\n    def __call__(self, image, targets):\n        assert targets.shape[0] > 0, ""this image does not have gt""\n\n        boxes = targets[:, :-1].copy()\n        labels = targets[:, -1].copy()\n\n        #image_t = _distort(image)\n        #image_t, boxes_t = _expand(image_t, boxes, self.cfg[\'rgb_mean\'], self.cfg[\'max_expand_ratio\'])\n        #image_t, boxes_t, labels_t = _crop(image_t, boxes, labels, self.img_dim, self.rgb_means)\n        image_t, boxes_t, labels_t, pad_image_flag = _crop(image, boxes, labels, self.img_dim)\n        image_t = _distort(image_t)\n        image_t = _pad_to_square(image_t,self.rgb_means, pad_image_flag)\n        image_t, boxes_t = _mirror(image_t, boxes_t)\n        height, width, _ = image_t.shape\n        image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\n        boxes_t[:, 0::2] /= width\n        boxes_t[:, 1::2] /= height\n\n        labels_t = np.expand_dims(labels_t, 1)\n        targets_t = np.hstack((boxes_t, labels_t))\n\n        return image_t, targets_t\n'"
data/wider_voc.py,5,"b'import os\nimport os.path\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\nWIDER_CLASSES = ( \'__background__\', \'face\')\n\n\nclass AnnotationTransform(object):\n\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=True):\n        self.class_to_ind = class_to_ind or dict(\n            zip(WIDER_CLASSES, range(len(WIDER_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n        """"""\n        res = np.empty((0, 5))\n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text)\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res = np.vstack((res, bndbox))  # [xmin, ymin, xmax, ymax, label_ind]\n        return res\n\n\nclass VOCDetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to WIDER folder\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n    """"""\n\n    def __init__(self, root, preproc=None, target_transform=None):\n        self.root = root\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self._annopath = os.path.join(self.root, \'annotations\', \'%s\')\n        self._imgpath = os.path.join(self.root, \'images\', \'%s\')\n        self.ids = list()\n        with open(os.path.join(self.root, \'img_list.txt\'), \'r\') as f:\n          self.ids = [tuple(line.split()) for line in f]\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        target = ET.parse(self._annopath % img_id[1]).getroot()\n        img = cv2.imread(self._imgpath % img_id[0], cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n\n        return torch.from_numpy(img), target\n\n    def __len__(self):\n        return len(self.ids)\n\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on 0 dim\n    """"""\n    targets = []\n    imgs = []\n    for _, sample in enumerate(batch):\n        for _, tup in enumerate(sample):\n            if torch.is_tensor(tup):\n                imgs.append(tup)\n            elif isinstance(tup, type(np.empty(0))):\n                annos = torch.from_numpy(tup).float()\n                targets.append(annos)\n\n    return (torch.stack(imgs, 0), targets)\n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
models/__init__.py,0,b''
models/faceboxes.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n\nclass Inception(nn.Module):\n\n  def __init__(self):\n    super(Inception, self).__init__()\n    self.branch1x1 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n    self.branch1x1_2 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n    self.branch3x3_reduce = BasicConv2d(128, 24, kernel_size=1, padding=0)\n    self.branch3x3 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n    self.branch3x3_reduce_2 = BasicConv2d(128, 24, kernel_size=1, padding=0)\n    self.branch3x3_2 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n    self.branch3x3_3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n\n  def forward(self, x):\n    branch1x1 = self.branch1x1(x)\n\n    branch1x1_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n    branch1x1_2 = self.branch1x1_2(branch1x1_pool)\n\n    branch3x3_reduce = self.branch3x3_reduce(x)\n    branch3x3 = self.branch3x3(branch3x3_reduce)\n\n    branch3x3_reduce_2 = self.branch3x3_reduce_2(x)\n    branch3x3_2 = self.branch3x3_2(branch3x3_reduce_2)\n    branch3x3_3 = self.branch3x3_3(branch3x3_2)\n\n    outputs = [branch1x1, branch1x1_2, branch3x3, branch3x3_3]\n    return torch.cat(outputs, 1)\n\n\nclass CRelu(nn.Module):\n\n  def __init__(self, in_channels, out_channels, **kwargs):\n    super(CRelu, self).__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n    self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = self.bn(x)\n    x = torch.cat([x, -x], 1)\n    x = F.relu(x, inplace=True)\n    return x\n\n\nclass FaceBoxes(nn.Module):\n\n  def __init__(self, phase, size, num_classes):\n    super(FaceBoxes, self).__init__()\n    self.phase = phase\n    self.num_classes = num_classes\n    self.size = size\n\n    self.conv1 = CRelu(3, 24, kernel_size=7, stride=4, padding=3)\n    self.conv2 = CRelu(48, 64, kernel_size=5, stride=2, padding=2)\n\n    self.inception1 = Inception()\n    self.inception2 = Inception()\n    self.inception3 = Inception()\n\n    self.conv3_1 = BasicConv2d(128, 128, kernel_size=1, stride=1, padding=0)\n    self.conv3_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n\n    self.conv4_1 = BasicConv2d(256, 128, kernel_size=1, stride=1, padding=0)\n    self.conv4_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n\n    self.loc, self.conf = self.multibox(self.num_classes)\n\n    if self.phase == \'test\':\n        self.softmax = nn.Softmax(dim=-1)\n\n    if self.phase == \'train\':\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.bias is not None:\n                    nn.init.xavier_normal_(m.weight.data)\n                    m.bias.data.fill_(0.02)\n                else:\n                    m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n  def multibox(self, num_classes):\n    loc_layers = []\n    conf_layers = []\n    loc_layers += [nn.Conv2d(128, 21 * 4, kernel_size=3, padding=1)]\n    conf_layers += [nn.Conv2d(128, 21 * num_classes, kernel_size=3, padding=1)]\n    loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]\n    conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]\n    loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]\n    conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]\n    return nn.Sequential(*loc_layers), nn.Sequential(*conf_layers)\n\n  def forward(self, x):\n\n    detection_sources = list()\n    loc = list()\n    conf = list()\n\n    x = self.conv1(x)\n    x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n    x = self.conv2(x)\n    x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n    x = self.inception1(x)\n    x = self.inception2(x)\n    x = self.inception3(x)\n    detection_sources.append(x)\n\n    x = self.conv3_1(x)\n    x = self.conv3_2(x)\n    detection_sources.append(x)\n\n    x = self.conv4_1(x)\n    x = self.conv4_2(x)\n    detection_sources.append(x)\n\n    for (x, l, c) in zip(detection_sources, self.loc, self.conf):\n        loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n        conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n    loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n    conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n    if self.phase == ""test"":\n      output = (loc.view(loc.size(0), -1, 4),\n                self.softmax(conf.view(conf.size(0), -1, self.num_classes)))\n    else:\n      output = (loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes))\n\n    return output\n'"
utils/__init__.py,0,b''
utils/box_utils.py,24,"b'import torch\nimport numpy as np\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef matrix_iou(a, b):\n    """"""\n    return iou of a and b, numpy version for data augenmentation\n    """"""\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef matrix_iof(a, b):\n    """"""\n    return iof of a and b, numpy version for data augenmentation\n    """"""\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n\n    # ignore hard gt\n    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n    if best_prior_idx_filter.shape[0] <= 0:\n        loc_t[idx] = 0\n        conf_t[idx] = 0\n        return\n\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_idx_filter.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx]          # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n\n\n'"
utils/build.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                                   \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\': home, \'nvcc\': nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n              [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with gcc\n              # the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_52\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n]\n\nsetup(\n    name=\'mot_utils\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
utils/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .nms.cpu_nms import cpu_nms, cpu_soft_nms\nfrom .nms.gpu_nms import gpu_nms\n\n\n# def nms(dets, thresh, force_cpu=False):\n#     """"""Dispatch to either CPU or GPU NMS implementations.""""""\n#\n#     if dets.shape[0] == 0:\n#         return []\n#     if cfg.USE_GPU_NMS and not force_cpu:\n#         return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n#     else:\n#         return cpu_nms(dets, thresh)\n\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if force_cpu:\n        #return cpu_soft_nms(dets, thresh, method = 0)\n        return cpu_nms(dets, thresh)\n    return gpu_nms(dets, thresh)\n'"
utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
layers/functions/prior_box.py,1,"b""import torch\nfrom itertools import product as product\nimport numpy as np\nfrom math import ceil\n\n\nclass PriorBox(object):\n    def __init__(self, cfg, image_size=None, phase='train'):\n        super(PriorBox, self).__init__()\n        #self.aspect_ratios = cfg['aspect_ratios']\n        self.min_sizes = cfg['min_sizes']\n        self.steps = cfg['steps']\n        self.clip = cfg['clip']\n        self.image_size = image_size\n        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n\n    def forward(self):\n        anchors = []\n        for k, f in enumerate(self.feature_maps):\n            min_sizes = self.min_sizes[k]\n            for i, j in product(range(f[0]), range(f[1])):\n                for min_size in min_sizes:\n                    s_kx = min_size / self.image_size[1]\n                    s_ky = min_size / self.image_size[0]\n                    if min_size == 32:\n                        dense_cx = [x*self.steps[k]/self.image_size[1] for x in [j+0, j+0.25, j+0.5, j+0.75]]\n                        dense_cy = [y*self.steps[k]/self.image_size[0] for y in [i+0, i+0.25, i+0.5, i+0.75]]\n                        for cy, cx in product(dense_cy, dense_cx):\n                            anchors += [cx, cy, s_kx, s_ky]\n                    elif min_size == 64:\n                        dense_cx = [x*self.steps[k]/self.image_size[1] for x in [j+0, j+0.5]]\n                        dense_cy = [y*self.steps[k]/self.image_size[0] for y in [i+0, i+0.5]]\n                        for cy, cx in product(dense_cy, dense_cx):\n                            anchors += [cx, cy, s_kx, s_ky]\n                    else:\n                        cx = (j + 0.5) * self.steps[k] / self.image_size[1]\n                        cy = (i + 0.5) * self.steps[k] / self.image_size[0]\n                        anchors += [cx, cy, s_kx, s_ky]\n        # back to torch land\n        output = torch.Tensor(anchors).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n"""
layers/modules/__init__.py,0,"b""from .multibox_loss import MultiBoxLoss\n\n__all__ = ['MultiBoxLoss']\n"""
layers/modules/multibox_loss.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils.box_utils import match, log_sum_exp\nfrom data import cfg\nGPU = cfg[\'gpu_train\']\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target):\n        super(MultiBoxLoss, self).__init__()\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n        self.variance = [0.1, 0.2]\n\n    def forward(self, predictions, priors, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n\n        loc_data, conf_data = predictions\n        priors = priors\n        num = loc_data.size(0)\n        num_priors = (priors.size(0))\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:, :-1].data\n            labels = targets[idx][:, -1].data\n            defaults = priors.data\n            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n        if GPU:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n\n        pos = conf_t > 0\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\')\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n        # Hard Negative Mining\n        loss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1, keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'sum\')\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        N = max(num_pos.data.sum().float(), 1)\n        loss_l /= N\n        loss_c /= N\n\n        return loss_l, loss_c\n'"
utils/nms/__init__.py,0,b''
utils/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
