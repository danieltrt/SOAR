file_path,api_count,code
01-Python/Day11.py,0,b''
01-Python/err.py,0,"b""s = '0'\nn = int(s)\nprint(10 / n)\n"""
01-Python/spider/Day02.py,0,"b""# from urllib import request,error\n# try:\n#     response = request.urlopen('http://cuiqingcai.com/index.htm')\n# except error.URLError as e:\n#     print(e.reason)\n#\n# from urllib import request,error\n#\n# try:\n#     response = request.urlopen('http://cuiqingcai.com/index.htm')\n# except error.HTTPError as e:\n#     print(e.reason,e.code,e.headers,sep='\\n')\n\n# from urllib import request,error\n#\n# try:\n#     response = request.urlopen('http://cuiqingcai.com/index.htm')\n# except error.HTTPError as e:\n#     print(e.reason,e.code,e.headers,sep='\\n')\n# except error.URLError as e:\n#     print(e.reason)\n# else:\n#     print('Request Successfully')\n\nimport socket\nimport urllib.request\nimport urllib.error\n\ntry:\n    response = urllib.request.urlopen('https://www.zhihu.com',timeout=0.01)\nexcept urllib.error.URLError as e:\n    print(type(e.reason))\n    if isinstance(e.reason,socket.timeout):\n        print('TIME OUT')"""
01-Python/spider/Day03.py,0,"b""# from urllib.parse import urlparse\n#\n# result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')\n# print(type(result),result)\n\n# from urllib.parse import urlparse\n#\n# result = urlparse('www.baidu.com/index.html;user?id=5#comment',scheme='https')\n# print(result)\n\n# from urllib.parse import urlparse\n#\n# result = urlparse('http://www.baidu.com/index.html;user?id=5#comment',scheme='https')\n# print(result)\n\n# from urllib.parse import urlparse\n# result = urlparse('http://www.baidu.com/index.html;user?id=5#comment',allow_fragments=False)\n# print(result)\n\n# from urllib.parse import urlparse\n# result = urlparse('http://www.baidu.com/index.html#comment',allow_fragments=False)\n# print(result.scheme,result[0],result.netloc,result[1],sep='\\n')\n\n# from urllib.parse import urlunparse\n#\n# data = ['http','www.baidu.com','index.html','user','a=6','comment']\n# print(urlunparse(data))\n#\n# from urllib.parse import urlsplit\n#\n# result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')\n# print(result)\n\n# from urllib.parse import urlsplit\n#\n# result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')\n# print(result.scheme,result[0])\n\n# from urllib.parse import urlunsplit\n#\n# data = ['http','www.baidu.com','index.html','a=6','commment']\n# print(urlunsplit(data))\n\n# from urllib.parse import urljoin\n# print(urljoin('http://www.baidu.com','FAQ.html'))\n\n# from urllib.parse import urlencode\n# params = {\n#     'name':'sb',\n#     'age':22\n# }\n# base_url = 'http://www.baidu.com?'\n# url = base_url + urlencode(params)\n# print(url)\n\n# from urllib.parse import parse_qs\n# query = 'name=sb&age=22'\n# print(parse_qs(query))\n\n# from urllib.parse import parse_qsl\n# query = 'name=sb&age=22'\n# print(parse_qsl(query))\n\n# from urllib.parse import quote\n#\n# keyword='\xe6\xb5\x8b\xe8\xaf\x95'\n# url = 'https://www.baidu.com/s?wd='+quote(keyword)\n# print(url)\n\nfrom urllib.parse import unquote\n\nurl = 'https://www.baidu.com/s?wd=%E6%B5%8B%E8%AF%95'\nprint(unquote(url))"""
13-kaggle/Google-QUEST-Q&A-Labeling/EDA.py,0,"b""import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\n# color = sns.color_palette()\nimport plotly.offline as py\n# py.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\n# init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\n# offline.init_notebook_mode()\n#import cufflinks and offline mode\nimport cufflinks as cf\n# cf.go_offline()\n\n# Venn diagram\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\nimport os\n# print(os.path.abspath('.'))\nprint(os.listdir('../Google-QUEST-Q&A-Labeling/input'))\n\nprint('Reading data...')\ntrain_data = pd.read_csv('../Google-QUEST-Q&A-Labeling/input/train.csv')\ntest_data = pd.read_csv('../Google-QUEST-Q&A-Labeling/input/test.csv')\nsample_submission = pd.read_csv('../Google-QUEST-Q&A-Labeling/input/sample_submission.csv')\nprint('Reading data completed')\n\n\n# print('Size of train_data',train_data.shape)\n# print('Size of test_data',test_data.shape)\n# print('Size of sample_submission',sample_submission.shape)\n\n# print(train_data.head())\n# print(train_data.columns)\n# print(test_data.head())\n# print(test_data.columns)\n# print(sample_submission.head())\n# targets = list(sample_submission.columns[1:])\n# print(targets)\n\n# print(train_data[targets].describe())\n\n\n# total = train_data.isnull().sum().sort_values(ascending=False)\n# percent = (train_data.isnull().sum()/train_data.isnull().count()*100).sort_values(ascending=False)\n# missing_train_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n# print(missing_train_data.head())\n\n# total = test_data.isnull().sum().sort_values(ascending = False)\n# percent = (test_data.isnull().sum()/test_data.isnull().count()*100).sort_values(ascending = False)\n# missing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# print(missing_test_data.head())\n\ntemp = train_data['host'].value_counts()\ndf = pd.DataFrame({'labels':temp.index,'values':temp.values})\ndf.iplot(kind='pie',labels='labels',values='values',title='Distribution of hosts in Training data')"""
05-Machine-Learning-Code/sklearn/preprocessing/EncodingFeatures.py,0,"b""from sklearn import preprocessing\r\nimport numpy as np\r\n\r\nenc = preprocessing.OrdinalEncoder()\r\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\r\nenc.fit(X)\r\n\r\nprint(enc.transform([['female', 'from US', 'uses Safari']]))\r\n# [[0. 1. 1.]]\r\n\r\nenc = preprocessing.OneHotEncoder()\r\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\r\nenc.fit(X)\r\nprint(enc.transform([['female', 'from US', 'uses Safari'],\r\n               ['male', 'from Europe', 'uses Safari']]).toarray())\r\n\r\n# [[1. 0. 0. 1. 0. 1.]\r\n#  [0. 1. 1. 0. 0. 1.]]\r\n\r\nprint(enc.categories_)\r\n# [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\r\n\r\ngenders = ['female', 'male']\r\nlocations = ['from Africa', 'from Asia', 'from Europe', 'from US']\r\nbrowsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\r\nenc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\r\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\r\nprint(enc.fit(X))\r\n# OneHotEncoder(categorical_features=None,\r\n#               categories=[['female', 'male'],\r\n#                           ['from Africa', 'from Asia', 'from Europe',\r\n#                            'from US'],\r\n#                           ['uses Chrome', 'uses Firefox', 'uses IE',\r\n#                            'uses Safari']],\r\n#               drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error',\r\n#               n_values=None, sparse=True)\r\n\r\nprint(enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray())\r\n# [[1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]\r\n\r\n"""
05-Machine-Learning-Code/sklearn/preprocessing/MinMaxSaler.py,0,"b'from sklearn import preprocessing\r\nimport numpy as np\r\n\r\nX_train = np.array([[ 1., -1.,  2.],\r\n                    [ 2.,  0.,  0.],\r\n                    [ 0.,  1., -1.]])\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train_minmax = min_max_scaler.fit_transform(X_train)\r\nprint(X_train_minmax)\r\n# [[0.5        0.         1.        ]\r\n#  [1.         0.5        0.33333333]\r\n#  [0.         1.         0.        ]]\r\n\r\nX_test = np.array([[-3., -1.,  4.]])\r\nX_test_minmax = min_max_scaler.transform(X_test)\r\nprint(X_test_minmax)\r\n# [[0.5        0.         1.        ]\r\n#  [1.         0.5        0.33333333]\r\n#  [0.         1.         0.        ]]\r\n# [[-1.5         0.          1.66666667]]\r\n\r\nprint(min_max_scaler.scale_)\r\n# [0.5        0.5        0.33333333]\r\n\r\nprint(min_max_scaler.min_)\r\n# [0.         0.5        0.33333333]\r\n\r\n'"
05-Machine-Learning-Code/sklearn/preprocessing/Normaliztion.py,0,"b""from sklearn import preprocessing\r\nimport numpy as np\r\n\r\nX = [[ 1., -1.,  2.],\r\n     [ 2.,  0.,  0.],\r\n     [ 0.,  1., -1.]]\r\nX_normalized = preprocessing.normalize(X,norm='l2')\r\nprint(X_normalized)\r\n# [[ 0.40824829 -0.40824829  0.81649658]\r\n#  [ 1.          0.          0.        ]\r\n#  [ 0.          0.70710678 -0.70710678]]\r\n\r\n\r\nnormalizer = preprocessing.Normalizer().fit_transform(X)\r\nprint(normalizer)\r\n# [[ 0.40824829 -0.40824829  0.81649658]\r\n#  [ 1.          0.          0.        ]\r\n#  [ 0.          0.70710678 -0.70710678]]\r\n\r\n"""
05-Machine-Learning-Code/sklearn/preprocessing/Standardization.py,0,"b'from sklearn import preprocessing\r\nimport numpy as np\r\n\r\nX_train = np.array([[ 1., -1.,  2.],\r\n                    [ 2.,  0.,  0.],\r\n                    [ 0.,  1., -1.]])\r\nX_scaled = preprocessing.scale(X_train)\r\nprint(X_scaled)\r\n\r\n# [[ 0.         -1.22474487  1.33630621]\r\n#  [ 1.22474487  0.         -0.26726124]\r\n#  [-1.22474487  1.22474487 -1.06904497]]\r\n\r\nprint(X_scaled.mean(axis=0))\r\n#[0. 0. 0.]\r\n\r\nprint(X_scaled.std(axis=0))\r\n#[1. 1. 1.]\r\n\r\nscaler = preprocessing.StandardScaler().fit(X_train)\r\nprint(scaler)\r\n#StandardScaler(copy=True, with_mean=True, with_std=True)\r\n\r\nprint(scaler.mean_)\r\n#[1.         0.         0.33333333]\r\n\r\nprint(scaler.scale_)\r\n#[0.81649658 0.81649658 1.24721913]\r\n\r\na = scaler.transform(X_train)\r\nprint(a.mean(axis=0))\r\nprint(a.std(axis=0))\r\n# [0. 0. 0.]\r\n# [1. 1. 1.]\r\n\r\nX_test = [[-1., 1., 0.]]\r\nb = scaler.transform(X_test)\r\nprint(b.mean(axis=0))\r\n#[-2.44948974  1.22474487 -0.26726124]\r\n\r\n'"
