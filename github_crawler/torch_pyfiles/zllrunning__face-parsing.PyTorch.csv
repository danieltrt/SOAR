file_path,api_count,code
evaluate.py,7,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path=\'vis_results/parsing_map_on_im.jpg\'):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    # Save result or not\n    if save_im:\n        cv2.imwrite(save_path, vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n\n    # return vis_im\n\ndef evaluate(respth=\'./res/test_res\', dspth=\'./data\', cp=\'model_final_diss.pth\'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = osp.join(\'res/cp\', cp)\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        for image_path in os.listdir(dspth):\n            img = Image.open(osp.join(dspth, image_path))\n            image = img.resize((512, 512), Image.BILINEAR)\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.cuda()\n            out = net(img)[0]\n            parsing = out.squeeze(0).cpu().numpy().argmax(0)\n\n            vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=osp.join(respth, image_path))\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n    setup_logger(\'./res\')\n    evaluate()\n'"
face_dataset.py,1,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nimport os.path as osp\nimport os\nfrom PIL import Image\nimport numpy as np\nimport json\nimport cv2\n\nfrom transform import *\n\n\n\nclass FaceMask(Dataset):\n    def __init__(self, rootpth, cropsize=(640, 480), mode=\'train\', *args, **kwargs):\n        super(FaceMask, self).__init__(*args, **kwargs)\n        assert mode in (\'train\', \'val\', \'test\')\n        self.mode = mode\n        self.ignore_lb = 255\n        self.rootpth = rootpth\n\n        self.imgs = os.listdir(os.path.join(self.rootpth, \'CelebA-HQ-img\'))\n\n        #  pre-processing\n        self.to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ])\n        self.trans_train = Compose([\n            ColorJitter(\n                brightness=0.5,\n                contrast=0.5,\n                saturation=0.5),\n            HorizontalFlip(),\n            RandomScale((0.75, 1.0, 1.25, 1.5, 1.75, 2.0)),\n            RandomCrop(cropsize)\n            ])\n\n    def __getitem__(self, idx):\n        impth = self.imgs[idx]\n        img = Image.open(osp.join(self.rootpth, \'CelebA-HQ-img\', impth))\n        img = img.resize((512, 512), Image.BILINEAR)\n        label = Image.open(osp.join(self.rootpth, \'mask\', impth[:-3]+\'png\')).convert(\'P\')\n        # print(np.unique(np.array(label)))\n        if self.mode == \'train\':\n            im_lb = dict(im=img, lb=label)\n            im_lb = self.trans_train(im_lb)\n            img, label = im_lb[\'im\'], im_lb[\'lb\']\n        img = self.to_tensor(img)\n        label = np.array(label).astype(np.int64)[np.newaxis, :]\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nif __name__ == ""__main__"":\n    face_data = \'/home/zll/data/CelebAMask-HQ/CelebA-HQ-img\'\n    face_sep_mask = \'/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno\'\n    mask_path = \'/home/zll/data/CelebAMask-HQ/mask\'\n    counter = 0\n    total = 0\n    for i in range(15):\n        # files = os.listdir(osp.join(face_sep_mask, str(i)))\n\n        atts = [\'skin\', \'l_brow\', \'r_brow\', \'l_eye\', \'r_eye\', \'eye_g\', \'l_ear\', \'r_ear\', \'ear_r\',\n                \'nose\', \'mouth\', \'u_lip\', \'l_lip\', \'neck\', \'neck_l\', \'cloth\', \'hair\', \'hat\']\n\n        for j in range(i*2000, (i+1)*2000):\n\n            mask = np.zeros((512, 512))\n\n            for l, att in enumerate(atts, 1):\n                total += 1\n                file_name = \'\'.join([str(j).rjust(5, \'0\'), \'_\', att, \'.png\'])\n                path = osp.join(face_sep_mask, str(i), file_name)\n\n                if os.path.exists(path):\n                    counter += 1\n                    sep_mask = np.array(Image.open(path).convert(\'P\'))\n                    # print(np.unique(sep_mask))\n\n                    mask[sep_mask == 225] = l\n            cv2.imwrite(\'{}/{}.png\'.format(mask_path, j), mask)\n            print(j)\n\n    print(counter, total)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
logger.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport os.path as osp\nimport time\nimport sys\nimport logging\n\nimport torch.distributed as dist\n\n\ndef setup_logger(logpth):\n    logfile = 'BiSeNet-{}.log'.format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n    logfile = osp.join(logpth, logfile)\n    FORMAT = '%(levelname)s %(filename)s(%(lineno)d): %(message)s'\n    log_level = logging.INFO\n    if dist.is_initialized() and not dist.get_rank()==0:\n        log_level = logging.ERROR\n    logging.basicConfig(level=log_level, format=FORMAT, filename=logfile)\n    logging.root.addHandler(logging.StreamHandler())\n\n\n"""
loss.py,10,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nclass OhemCELoss(nn.Module):\n    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n        super(OhemCELoss, self).__init__()\n        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n        self.n_min = n_min\n        self.ignore_lb = ignore_lb\n        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n\n    def forward(self, logits, labels):\n        N, C, H, W = logits.size()\n        loss = self.criteria(logits, labels).view(-1)\n        loss, _ = torch.sort(loss, descending=True)\n        if loss[self.n_min] > self.thresh:\n            loss = loss[loss>self.thresh]\n        else:\n            loss = loss[:self.n_min]\n        return torch.mean(loss)\n\n\nclass SoftmaxFocalLoss(nn.Module):\n    def __init__(self, gamma, ignore_lb=255, *args, **kwargs):\n        super(SoftmaxFocalLoss, self).__init__()\n        self.gamma = gamma\n        self.nll = nn.NLLLoss(ignore_index=ignore_lb)\n\n    def forward(self, logits, labels):\n        scores = F.softmax(logits, dim=1)\n        factor = torch.pow(1.-scores, self.gamma)\n        log_score = F.log_softmax(logits, dim=1)\n        log_score = factor * log_score\n        loss = self.nll(log_score, labels)\n        return loss\n\n\nif __name__ == '__main__':\n    torch.manual_seed(15)\n    criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    net1 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net1.cuda()\n    net1.train()\n    net2 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net2.cuda()\n    net2.train()\n\n    with torch.no_grad():\n        inten = torch.randn(16, 3, 20, 20).cuda()\n        lbs = torch.randint(0, 19, [16, 20, 20]).cuda()\n        lbs[1, :, :] = 255\n\n    logits1 = net1(inten)\n    logits1 = F.interpolate(logits1, inten.size()[2:], mode='bilinear')\n    logits2 = net2(inten)\n    logits2 = F.interpolate(logits2, inten.size()[2:], mode='bilinear')\n\n    loss1 = criteria1(logits1, lbs)\n    loss2 = criteria2(logits2, lbs)\n    loss = loss1 + loss2\n    print(loss.detach().cpu())\n    loss.backward()\n"""
makeup.py,0,"b""import cv2\nimport os\nimport numpy as np\nfrom skimage.filters import gaussian\n\n\ndef sharpen(img):\n    img = img * 1.0\n    gauss_out = gaussian(img, sigma=5, multichannel=True)\n\n    alpha = 1.5\n    img_out = (img - gauss_out) * alpha + img\n\n    img_out = img_out / 255.0\n\n    mask_1 = img_out < 0\n    mask_2 = img_out > 1\n\n    img_out = img_out * (1 - mask_1)\n    img_out = img_out * (1 - mask_2) + mask_2\n    img_out = np.clip(img_out, 0, 1)\n    img_out = img_out * 255\n    return np.array(img_out, dtype=np.uint8)\n\n\ndef hair(image, parsing, part=17, color=[230, 50, 20]):\n    b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n    tar_color = np.zeros_like(image)\n    tar_color[:, :, 0] = b\n    tar_color[:, :, 1] = g\n    tar_color[:, :, 2] = r\n\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    tar_hsv = cv2.cvtColor(tar_color, cv2.COLOR_BGR2HSV)\n\n    if part == 12 or part == 13:\n        image_hsv[:, :, 0:2] = tar_hsv[:, :, 0:2]\n    else:\n        image_hsv[:, :, 0:1] = tar_hsv[:, :, 0:1]\n\n    changed = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2BGR)\n\n    if part == 17:\n        changed = sharpen(changed)\n\n    changed[parsing != part] = image[parsing != part]\n    # changed = cv2.resize(changed, (512, 512))\n    return changed\n\n#\n# def lip(image, parsing, part=17, color=[230, 50, 20]):\n#     b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n#     tar_color = np.zeros_like(image)\n#     tar_color[:, :, 0] = b\n#     tar_color[:, :, 1] = g\n#     tar_color[:, :, 2] = r\n#\n#     image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n#     il, ia, ib = cv2.split(image_lab)\n#\n#     tar_lab = cv2.cvtColor(tar_color, cv2.COLOR_BGR2Lab)\n#     tl, ta, tb = cv2.split(tar_lab)\n#\n#     image_lab[:, :, 0] = np.clip(il - np.mean(il) + tl, 0, 100)\n#     image_lab[:, :, 1] = np.clip(ia - np.mean(ia) + ta, -127, 128)\n#     image_lab[:, :, 2] = np.clip(ib - np.mean(ib) + tb, -127, 128)\n#\n#\n#     changed = cv2.cvtColor(image_lab, cv2.COLOR_Lab2BGR)\n#\n#     if part == 17:\n#         changed = sharpen(changed)\n#\n#     changed[parsing != part] = image[parsing != part]\n#     # changed = cv2.resize(changed, (512, 512))\n#     return changed\n\n\nif __name__ == '__main__':\n    # 1  face\n    # 10 nose\n    # 11 teeth\n    # 12 upper lip\n    # 13 lower lip\n    # 17 hair\n    num = 116\n    table = {\n        'hair': 17,\n        'upper_lip': 12,\n        'lower_lip': 13\n    }\n    image_path = '/home/zll/data/CelebAMask-HQ/test-img/{}.jpg'.format(num)\n    parsing_path = 'res/test_res/{}.png'.format(num)\n\n    image = cv2.imread(image_path)\n    ori = image.copy()\n    parsing = np.array(cv2.imread(parsing_path, 0))\n    parsing = cv2.resize(parsing, image.shape[0:2], interpolation=cv2.INTER_NEAREST)\n\n    parts = [table['hair'], table['upper_lip'], table['lower_lip']]\n    # colors = [[20, 20, 200], [100, 100, 230], [100, 100, 230]]\n    colors = [[100, 200, 100]]\n    for part, color in zip(parts, colors):\n        image = hair(image, parsing, part, color)\n    cv2.imwrite('res/makeup/116_ori.png', cv2.resize(ori, (512, 512)))\n    cv2.imwrite('res/makeup/116_2.png', cv2.resize(image, (512, 512)))\n\n    cv2.imshow('image', cv2.resize(ori, (512, 512)))\n    cv2.imshow('color', cv2.resize(image, (512, 512)))\n\n    # cv2.imshow('image', ori)\n    # cv2.imshow('color', image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
model.py,6,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom resnet import Resnet18\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode=\'nearest\')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode=\'nearest\')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode=\'nearest\')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\n### This is not used, since I replace this with the resnet feature with the same size\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        ## here self.sp is deleted\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode=\'bilinear\', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == ""__main__"":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n'"
optimizer.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport logging\n\nlogger = logging.getLogger()\n\nclass Optimizer(object):\n    def __init__(self,\n                model,\n                lr0,\n                momentum,\n                wd,\n                warmup_steps,\n                warmup_start_lr,\n                max_iter,\n                power,\n                *args, **kwargs):\n        self.warmup_steps = warmup_steps\n        self.warmup_start_lr = warmup_start_lr\n        self.lr0 = lr0\n        self.lr = self.lr0\n        self.max_iter = float(max_iter)\n        self.power = power\n        self.it = 0\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = model.get_params()\n        param_list = [\n                {'params': wd_params},\n                {'params': nowd_params, 'weight_decay': 0},\n                {'params': lr_mul_wd_params, 'lr_mul': True},\n                {'params': lr_mul_nowd_params, 'weight_decay': 0, 'lr_mul': True}]\n        self.optim = torch.optim.SGD(\n                param_list,\n                lr = lr0,\n                momentum = momentum,\n                weight_decay = wd)\n        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n\n\n    def get_lr(self):\n        if self.it <= self.warmup_steps:\n            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n        else:\n            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n            lr = self.lr0 * factor\n        return lr\n\n\n    def step(self):\n        self.lr = self.get_lr()\n        for pg in self.optim.param_groups:\n            if pg.get('lr_mul', False):\n                pg['lr'] = self.lr * 10\n            else:\n                pg['lr'] = self.lr\n        if self.optim.defaults.get('lr_mul', False):\n            self.optim.defaults['lr'] = self.lr * 10\n        else:\n            self.optim.defaults['lr'] = self.lr\n        self.it += 1\n        self.optim.step()\n        if self.it == self.warmup_steps+2:\n            logger.info('==> warmup done, start to implement poly lr strategy')\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n"""
prepropess_data.py,0,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport os.path as osp\nimport os\nimport cv2\nfrom transform import *\nfrom PIL import Image\n\nface_data = '/home/zll/data/CelebAMask-HQ/CelebA-HQ-img'\nface_sep_mask = '/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno'\nmask_path = '/home/zll/data/CelebAMask-HQ/mask'\ncounter = 0\ntotal = 0\nfor i in range(15):\n\n    atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',\n            'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']\n\n    for j in range(i * 2000, (i + 1) * 2000):\n\n        mask = np.zeros((512, 512))\n\n        for l, att in enumerate(atts, 1):\n            total += 1\n            file_name = ''.join([str(j).rjust(5, '0'), '_', att, '.png'])\n            path = osp.join(face_sep_mask, str(i), file_name)\n\n            if os.path.exists(path):\n                counter += 1\n                sep_mask = np.array(Image.open(path).convert('P'))\n                # print(np.unique(sep_mask))\n\n                mask[sep_mask == 225] = l\n        cv2.imwrite('{}/{}.png'.format(mask_path, j), mask)\n        print(j)\n\nprint(counter, total)"""
resnet.py,5,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\nresnet18_url = \'https://download.pytorch.org/models/resnet18-5c106cde.pth\'\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_chan),\n                )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = F.relu(self.bn1(residual))\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum-1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x) # 1/8\n        feat16 = self.layer3(feat8) # 1/16\n        feat32 = self.layer4(feat16) # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if \'fc\' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module,  nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nif __name__ == ""__main__"":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()\n'"
test.py,3,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\n\nimport torch\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path=\'vis_results/parsing_map_on_im.jpg\'):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    # Save result or not\n    if save_im:\n        cv2.imwrite(save_path[:-4] +\'.png\', vis_parsing_anno)\n        cv2.imwrite(save_path, vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n\n    # return vis_im\n\ndef evaluate(respth=\'./res/test_res\', dspth=\'./data\', cp=\'model_final_diss.pth\'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = osp.join(\'res/cp\', cp)\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        for image_path in os.listdir(dspth):\n            img = Image.open(osp.join(dspth, image_path))\n            image = img.resize((512, 512), Image.BILINEAR)\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.cuda()\n            out = net(img)[0]\n            parsing = out.squeeze(0).cpu().numpy().argmax(0)\n            # print(parsing)\n            print(np.unique(parsing))\n\n            vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=osp.join(respth, image_path))\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n    evaluate(dspth=\'/home/zll/data/CelebAMask-HQ/test-img\', cp=\'79999_iter.pth\')\n\n\n'"
train.py,10,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\nfrom loss import OhemCELoss\nfrom evaluate import evaluate\nfrom optimizer import Optimizer\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = \'./res\'\nif not osp.exists(respth):\n    os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            \'--local_rank\',\n            dest = \'local_rank\',\n            type = int,\n            default = -1,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = \'nccl\',\n                init_method = \'tcp://127.0.0.1:33241\',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    # dataset\n    n_classes = 19\n    n_img_per_gpu = 16\n    n_workers = 8\n    cropsize = [448, 448]\n    data_root = \'/home/zll/data/CelebAMask-HQ/\'\n\n    ds = FaceMask(data_root, cropsize=cropsize, mode=\'train\')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    # model\n    ignore_idx = -100\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    net.train()\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n    score_thres = 0.7\n    n_min = n_img_per_gpu * cropsize[0] * cropsize[1]//16\n    LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            model = net.module,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0] == n_img_per_gpu:\n                raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = LossP(out, lb)\n        loss2 = Loss2(out16, lb)\n        loss3 = Loss3(out32, lb)\n        loss = lossp + loss2 + loss3\n        loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n\n        #  print training log message\n        if (it+1) % msg_iter == 0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = \', \'.join([\n                    \'it: {it}/{max_it}\',\n                    \'lr: {lr:4f}\',\n                    \'loss: {loss:.4f}\',\n                    \'eta: {eta}\',\n                    \'time: {time:.4f}\',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n        if dist.get_rank() == 0:\n            if (it+1) % 5000 == 0:\n                state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n                if dist.get_rank() == 0:\n                    torch.save(state, \'./res/cp/{}_iter.pth\'.format(it))\n                evaluate(dspth=\'/home/zll/data/CelebAMask-HQ/test-img\', cp=\'{}_iter.pth\'.format(it))\n\n    #  dump the final model\n    save_pth = osp.join(respth, \'model_final_diss.pth\')\n    # net.cpu()\n    state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n    if dist.get_rank() == 0:\n        torch.save(state, save_pth)\n    logger.info(\'training done, model saved to: {}\'.format(save_pth))\n\n\nif __name__ == ""__main__"":\n    train()\n'"
transform.py,0,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nfrom PIL import Image\nimport PIL.ImageEnhance as ImageEnhance\nimport random\nimport numpy as np\n\nclass RandomCrop(object):\n    def __init__(self, size, *args, **kwargs):\n        self.size = size\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        assert im.size == lb.size\n        W, H = self.size\n        w, h = im.size\n\n        if (W, H) == (w, h): return dict(im=im, lb=lb)\n        if w < W or h < H:\n            scale = float(W) / w if w < h else float(H) / h\n            w, h = int(scale * w + 1), int(scale * h + 1)\n            im = im.resize((w, h), Image.BILINEAR)\n            lb = lb.resize((w, h), Image.NEAREST)\n        sw, sh = random.random() * (w - W), random.random() * (h - H)\n        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n        return dict(\n                im = im.crop(crop),\n                lb = lb.crop(crop)\n                    )\n\n\nclass HorizontalFlip(object):\n    def __init__(self, p=0.5, *args, **kwargs):\n        self.p = p\n\n    def __call__(self, im_lb):\n        if random.random() > self.p:\n            return im_lb\n        else:\n            im = im_lb['im']\n            lb = im_lb['lb']\n\n            # atts = [1 'skin', 2 'l_brow', 3 'r_brow', 4 'l_eye', 5 'r_eye', 6 'eye_g', 7 'l_ear', 8 'r_ear', 9 'ear_r',\n            #         10 'nose', 11 'mouth', 12 'u_lip', 13 'l_lip', 14 'neck', 15 'neck_l', 16 'cloth', 17 'hair', 18 'hat']\n\n            flip_lb = np.array(lb)\n            flip_lb[lb == 2] = 3\n            flip_lb[lb == 3] = 2\n            flip_lb[lb == 4] = 5\n            flip_lb[lb == 5] = 4\n            flip_lb[lb == 7] = 8\n            flip_lb[lb == 8] = 7\n            flip_lb = Image.fromarray(flip_lb)\n            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n                        lb = flip_lb.transpose(Image.FLIP_LEFT_RIGHT),\n                    )\n\n\nclass RandomScale(object):\n    def __init__(self, scales=(1, ), *args, **kwargs):\n        self.scales = scales\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        W, H = im.size\n        scale = random.choice(self.scales)\n        w, h = int(W * scale), int(H * scale)\n        return dict(im = im.resize((w, h), Image.BILINEAR),\n                    lb = lb.resize((w, h), Image.NEAREST),\n                )\n\n\nclass ColorJitter(object):\n    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n        if not brightness is None and brightness>0:\n            self.brightness = [max(1-brightness, 0), 1+brightness]\n        if not contrast is None and contrast>0:\n            self.contrast = [max(1-contrast, 0), 1+contrast]\n        if not saturation is None and saturation>0:\n            self.saturation = [max(1-saturation, 0), 1+saturation]\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n        im = ImageEnhance.Color(im).enhance(r_saturation)\n        return dict(im = im,\n                    lb = lb,\n                )\n\n\nclass MultiScale(object):\n    def __init__(self, scales):\n        self.scales = scales\n\n    def __call__(self, img):\n        W, H = img.size\n        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n        imgs = []\n        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n        return imgs\n\n\nclass Compose(object):\n    def __init__(self, do_list):\n        self.do_list = do_list\n\n    def __call__(self, im_lb):\n        for comp in self.do_list:\n            im_lb = comp(im_lb)\n        return im_lb\n\n\n\n\nif __name__ == '__main__':\n    flip = HorizontalFlip(p = 1)\n    crop = RandomCrop((321, 321))\n    rscales = RandomScale((0.75, 1.0, 1.5, 1.75, 2.0))\n    img = Image.open('data/img.jpg')\n    lb = Image.open('data/label.png')\n"""
modules/__init__.py,0,"b'from .bn import ABN, InPlaceABN, InPlaceABNSync\nfrom .functions import ACT_RELU, ACT_LEAKY_RELU, ACT_ELU, ACT_NONE\nfrom .misc import GlobalAvgPool2d, SingleGPU\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule\n'"
modules/bn.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom .functions import *\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n    """"""\n\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\n'"
modules/deeplab.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\nfrom models._util import try_index\nfrom .bn import ABN\n\n\nclass DeeplabV3(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=256,\n                 dilations=(12, 24, 36),\n                 norm_act=ABN,\n                 pooling_size=None):\n        super(DeeplabV3, self).__init__()\n        self.pooling_size = pooling_size\n\n        self.map_convs = nn.ModuleList([\n            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[0], padding=dilations[0]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[1], padding=dilations[1]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[2], padding=dilations[2])\n        ])\n        self.map_bn = norm_act(hidden_channels * 4)\n\n        self.global_pooling_conv = nn.Conv2d(in_channels, hidden_channels, 1, bias=False)\n        self.global_pooling_bn = norm_act(hidden_channels)\n\n        self.red_conv = nn.Conv2d(hidden_channels * 4, out_channels, 1, bias=False)\n        self.pool_red_conv = nn.Conv2d(hidden_channels, out_channels, 1, bias=False)\n        self.red_bn = norm_act(out_channels)\n\n        self.reset_parameters(self.map_bn.activation, self.map_bn.slope)\n\n    def reset_parameters(self, activation, slope):\n        gain = nn.init.calculate_gain(activation, slope)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight.data, gain)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, ABN):\n                if hasattr(m, ""weight"") and m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Map convolutions\n        out = torch.cat([m(x) for m in self.map_convs], dim=1)\n        out = self.map_bn(out)\n        out = self.red_conv(out)\n\n        # Global pooling\n        pool = self._global_pooling(x)\n        pool = self.global_pooling_conv(pool)\n        pool = self.global_pooling_bn(pool)\n        pool = self.pool_red_conv(pool)\n        if self.training or self.pooling_size is None:\n            pool = pool.repeat(1, 1, x.size(2), x.size(3))\n\n        out += pool\n        out = self.red_bn(out)\n        return out\n\n    def _global_pooling(self, x):\n        if self.training or self.pooling_size is None:\n            pool = x.view(x.size(0), x.size(1), -1).mean(dim=-1)\n            pool = pool.view(x.size(0), x.size(1), 1, 1)\n        else:\n            pooling_size = (min(try_index(self.pooling_size, 0), x.shape[2]),\n                            min(try_index(self.pooling_size, 1), x.shape[3]))\n            padding = (\n                (pooling_size[1] - 1) // 2,\n                (pooling_size[1] - 1) // 2 if pooling_size[1] % 2 == 1 else (pooling_size[1] - 1) // 2 + 1,\n                (pooling_size[0] - 1) // 2,\n                (pooling_size[0] - 1) // 2 if pooling_size[0] % 2 == 1 else (pooling_size[0] - 1) // 2 + 1\n            )\n\n            pool = functional.avg_pool2d(x, pooling_size, stride=1)\n            pool = functional.pad(pool, pad=padding, mode=""replicate"")\n        return pool\n'"
modules/dense.py,3,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(in_channels)),\n                (""conv"", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.growth * bottleneck_factor)),\n                (""conv"", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n'"
modules/functions.py,6,"b'from os import path\nimport torch \nimport torch.distributed as dist\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu"",\n                    ""inplace_abn_cuda_half.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_RELU = ""relu""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz * weight.sign() if ctx.affine else None\n        dbias = edz if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01, equal_batches=True):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        ctx.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        #count = _count_samples(x)\n        batch_size = x.new_tensor([x.shape[0]],dtype=torch.long)\n\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n            if ctx.world_size>1:\n                # get global batch size\n                if equal_batches:\n                    batch_size *= ctx.world_size\n                else:\n                    dist.all_reduce(batch_size, dist.ReduceOp.SUM)\n\n                ctx.factor = x.shape[0]/float(batch_size.item())\n\n                mean_all = mean.clone() * ctx.factor\n                dist.all_reduce(mean_all, dist.ReduceOp.SUM)\n\n                var_all = (var + (mean - mean_all) ** 2) * ctx.factor\n                dist.all_reduce(var_all, dist.ReduceOp.SUM)\n\n                mean = mean_all\n                var = var_all\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            count = batch_size.item() * x.view(x.shape[0],x.shape[1],-1).shape[-1]\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * (float(count) / (count - 1)))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n            edz_local = edz.clone()\n            eydz_local = eydz.clone()\n\n            if ctx.world_size>1:\n                edz *= ctx.factor\n                dist.all_reduce(edz, dist.ReduceOp.SUM)\n\n                eydz *= ctx.factor\n                dist.all_reduce(eydz, dist.ReduceOp.SUM)\n        else:\n            edz_local = edz = dz.new_zeros(dz.size(1))\n            eydz_local = eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz_local * weight.sign() if ctx.affine else None\n        dbias = edz_local if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_RELU"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
modules/misc.py,2,"b'import torch.nn as nn\nimport torch\nimport torch.distributed as dist\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n\nclass SingleGPU(nn.Module):\n    def __init__(self, module):\n        super(SingleGPU, self).__init__()\n        self.module=module\n\n    def forward(self, input):\n        return self.module(input.cuda(non_blocking=True))\n\n'"
modules/residual.py,1,"b'from collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
