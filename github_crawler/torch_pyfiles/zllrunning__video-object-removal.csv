file_path,api_count,code
convert_frames2video.py,0,"b""import glob\nimport cv2\nimport os\nimport numpy as np\nimport subprocess as sp\n\ndef createVideoClip(clip, folder, name, size=[256, 256]):\n\n    vf = clip.shape[0]\n    command = ['ffmpeg',\n               '-y',  # overwrite output file if it exists\n               '-f', 'rawvideo',\n               '-s', '%dx%d' % (size[1], size[0]),  # '256x256', # size of one frame\n               '-pix_fmt', 'rgb24',\n               '-r', '25',  # frames per second\n               '-an',  # Tells FFMPEG not to expect any audio\n               '-i', '-',  # The input comes from a pipe\n               '-vcodec', 'libx264',\n               '-b:v', '1500k',\n               '-vframes', str(vf),  # 5*25\n               '-s', '%dx%d' % (size[1], size[0]),  # '256x256', # size of one frame\n               folder + '/' + name]\n    # sfolder+'/'+name\n    pipe = sp.Popen(command, stdin=sp.PIPE, stderr=sp.PIPE)\n    out, err = pipe.communicate(clip.tostring())\n    pipe.wait()\n    pipe.terminate()\n    print(err)\n\n\nif __name__ == '__main__':\n    out_frames = []\n    video_name = 'surf'\n    for path in sorted(glob.glob(os.path.join('data', video_name, '*.jp*'))):\n        print(path)\n        out_frame = cv2.imread(path)\n        shape = out_frame.shape\n        out_frames.append(out_frame[:, :, ::-1])\n\n    final_clip = np.stack(out_frames)\n    video_path = 'results'\n    if not os.path.exists(video_path):\n        os.makedirs(video_path)\n    createVideoClip(final_clip, video_path, '%s.mp4' % (video_name), [shape[0], shape[1]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
demo.py,0,"b""import argparse\nfrom mask import mask\nfrom inpaint import inpaint\n\n\nparser = argparse.ArgumentParser(description='Demo')\n\nparser.add_argument('--resume', default='cp/SiamMask_DAVIS.pth', type=str,\n                    metavar='PATH', help='path to latest checkpoint (default: none)')\nparser.add_argument('--data', default='data/Human6', help='videos or image files')\nparser.add_argument('--mask-dilation', default=32, type=int, help='mask dilation when inpainting')\nargs = parser.parse_args()\n\nmask(args)\ninpaint(args)\n\n"""
inpaint.py,11,"b""import time\nimport subprocess as sp\nfrom torch.utils import data\nfrom inpainting.davis import DAVIS\nfrom inpainting.model import generate_model\nfrom inpainting.utils import *\n\n\nclass Object():\n    pass\n\n\ndef inpaint(args):\n    opt = Object()\n    opt.crop_size = 512\n    opt.double_size = True if opt.crop_size == 512 else False\n    ########## DAVIS\n    DAVIS_ROOT =os.path.join('results', args.data)\n    DTset = DAVIS(DAVIS_ROOT, mask_dilation=args.mask_dilation, size=(opt.crop_size, opt.crop_size))\n    DTloader = data.DataLoader(DTset, batch_size=1, shuffle=False, num_workers=1)\n\n    opt.search_range = 4  # fixed as 4: search range for flow subnetworks\n    opt.pretrain_path = 'cp/save_agg_rec_512.pth'\n    opt.result_path = 'results/inpainting'\n\n    opt.model = 'vinet_final'\n    opt.batch_norm = False\n    opt.no_cuda = False  # use GPU\n    opt.no_train = True\n    opt.test = True\n    opt.t_stride = 3\n    opt.loss_on_raw = False\n    opt.prev_warp = True\n    opt.save_image = False\n    opt.save_video = True\n\n    def createVideoClip(clip, folder, name, size=[256, 256]):\n\n        vf = clip.shape[0]\n        command = ['ffmpeg',\n                   '-y',  # overwrite output file if it exists\n                   '-f', 'rawvideo',\n                   '-s', '%dx%d' % (size[1], size[0]),  # '256x256', # size of one frame\n                   '-pix_fmt', 'rgb24',\n                   '-r', '25',  # frames per second\n                   '-an',  # Tells FFMPEG not to expect any audio\n                   '-i', '-',  # The input comes from a pipe\n                   '-vcodec', 'libx264',\n                   '-b:v', '1500k',\n                   '-vframes', str(vf),  # 5*25\n                   '-s', '%dx%d' % (size[1], size[0]),  # '256x256', # size of one frame\n                   folder + '/' + name]\n        # sfolder+'/'+name\n        pipe = sp.Popen(command, stdin=sp.PIPE, stderr=sp.PIPE)\n        out, err = pipe.communicate(clip.tostring())\n        pipe.wait()\n        pipe.terminate()\n        print(err)\n\n    def to_img(x):\n        tmp = (x[0, :, 0, :, :].cpu().data.numpy().transpose((1, 2, 0)) + 1) / 2\n        tmp = np.clip(tmp, 0, 1) * 255.\n        return tmp.astype(np.uint8)\n\n    model, _ = generate_model(opt)\n    print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\n    model.eval()\n    ts = opt.t_stride\n    # folder_name = 'davis_%d' % (int(opt.crop_size))\n    pre = 15\n\n    with torch.no_grad():\n        for seq, (inputs, masks, info) in enumerate(DTloader):\n\n            idx = torch.LongTensor([i for i in range(pre - 1, -1, -1)])\n            pre_inputs = inputs[:, :, :pre].index_select(2, idx)\n            pre_masks = masks[:, :, :pre].index_select(2, idx)\n            inputs = torch.cat((pre_inputs, inputs), 2)\n            masks = torch.cat((pre_masks, masks), 2)\n\n            bs = inputs.size(0)\n            num_frames = inputs.size(2)\n            seq_name = info['name'][0]\n\n            save_path = os.path.join(opt.result_path, seq_name)\n            if not os.path.exists(save_path) and opt.save_image:\n                os.makedirs(save_path)\n\n            inputs = 2. * inputs - 1\n            inverse_masks = 1 - masks\n            masked_inputs = inputs.clone() * inverse_masks\n\n            masks = to_var(masks)\n            masked_inputs = to_var(masked_inputs)\n            inputs = to_var(inputs)\n\n            total_time = 0.\n            in_frames = []\n            out_frames = []\n\n            lstm_state = None\n\n            for t in range(num_frames):\n                masked_inputs_ = []\n                masks_ = []\n\n                if t < 2 * ts:\n                    masked_inputs_.append(masked_inputs[0, :, abs(t - 2 * ts)])\n                    masked_inputs_.append(masked_inputs[0, :, abs(t - 1 * ts)])\n                    masked_inputs_.append(masked_inputs[0, :, t])\n                    masked_inputs_.append(masked_inputs[0, :, t + 1 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t + 2 * ts])\n                    masks_.append(masks[0, :, abs(t - 2 * ts)])\n                    masks_.append(masks[0, :, abs(t - 1 * ts)])\n                    masks_.append(masks[0, :, t])\n                    masks_.append(masks[0, :, t + 1 * ts])\n                    masks_.append(masks[0, :, t + 2 * ts])\n                elif t > num_frames - 2 * ts - 1:\n                    masked_inputs_.append(masked_inputs[0, :, t - 2 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t - 1 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t])\n                    masked_inputs_.append(masked_inputs[0, :, -1 - abs(num_frames - 1 - t - 1 * ts)])\n                    masked_inputs_.append(masked_inputs[0, :, -1 - abs(num_frames - 1 - t - 2 * ts)])\n                    masks_.append(masks[0, :, t - 2 * ts])\n                    masks_.append(masks[0, :, t - 1 * ts])\n                    masks_.append(masks[0, :, t])\n                    masks_.append(masks[0, :, -1 - abs(num_frames - 1 - t - 1 * ts)])\n                    masks_.append(masks[0, :, -1 - abs(num_frames - 1 - t - 2 * ts)])\n                else:\n                    masked_inputs_.append(masked_inputs[0, :, t - 2 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t - 1 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t])\n                    masked_inputs_.append(masked_inputs[0, :, t + 1 * ts])\n                    masked_inputs_.append(masked_inputs[0, :, t + 2 * ts])\n                    masks_.append(masks[0, :, t - 2 * ts])\n                    masks_.append(masks[0, :, t - 1 * ts])\n                    masks_.append(masks[0, :, t])\n                    masks_.append(masks[0, :, t + 1 * ts])\n                    masks_.append(masks[0, :, t + 2 * ts])\n\n                masked_inputs_ = torch.stack(masked_inputs_).permute(1, 0, 2, 3).unsqueeze(0)\n                masks_ = torch.stack(masks_).permute(1, 0, 2, 3).unsqueeze(0)\n\n                start = time.time()\n                if not opt.double_size:\n                    prev_mask_ = to_var(torch.zeros(masks_[:, :, 2].size()))  # rec given when 256\n                prev_mask = masks_[:, :, 2] if t == 0 else prev_mask_\n                prev_ones = to_var(torch.ones(prev_mask.size()))\n                prev_feed = torch.cat([masked_inputs_[:, :, 2, :, :], prev_ones, prev_ones * prev_mask],\n                                      dim=1) if t == 0 else torch.cat(\n                    [outputs.detach().squeeze(2), prev_ones, prev_ones * prev_mask], dim=1)\n\n                outputs, _, _, _, _ = model(masked_inputs_, masks_, lstm_state, prev_feed, t)\n                if opt.double_size:\n                    prev_mask_ = masks_[:, :, 2] * 0.5  # rec given whtn 512\n\n                lstm_state = None\n                end = time.time() - start\n                if lstm_state is not None:\n                    lstm_state = repackage_hidden(lstm_state)\n\n                total_time += end\n                if t > pre:\n                    print('{}th frame of {} is being processed'.format(t - pre, seq_name))\n                    out_frame = to_img(outputs)\n                    out_frame = cv2.resize(out_frame, (DTset.shape[1], DTset.shape[0]))\n                    cv2.imshow('Inpainting', out_frame)\n                    key = cv2.waitKey(1)\n                    if key > 0:\n                        break\n                    if opt.save_image:\n                        cv2.imwrite(os.path.join(save_path, '%05d.png' % (t - pre)), out_frame)\n                    out_frames.append(out_frame[:, :, ::-1])\n\n            if opt.save_video:\n                final_clip = np.stack(out_frames)\n                video_path = opt.result_path\n                if not os.path.exists(video_path):\n                    os.makedirs(video_path)\n\n                createVideoClip(final_clip, video_path, '%s.mp4' % (seq_name), [DTset.shape[0], DTset.shape[1]])\n                print('Predicted video clip saving')\n            cv2.destroyAllWindows()\n\n"""
mask.py,2,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport os\nimport glob\nfrom get_mask.test import *\nfrom get_mask.models.custom import Custom\n\n\ndef get_frames(video_name):\n    if not video_name:\n        cap = cv2.VideoCapture(0)\n        # warmup\n        for i in range(5):\n            cap.read()\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                yield frame\n            else:\n                break\n    elif video_name.endswith(\'avi\') or \\\n        video_name.endswith(\'mp4\'):\n        cap = cv2.VideoCapture(video_name)\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                yield frame\n            else:\n                break\n    else:\n        images = glob.glob(os.path.join(video_name, \'*.jp*\'))\n        images = sorted(images,\n                        key=lambda x: int(x.split(\'/\')[-1].split(\'.\')[0]))\n        for img in images:\n            frame = cv2.imread(img)\n            yield frame\n\n\ndef mask(args):\n    # Setup device\n    args.config = \'get_mask/experiments/siammask/config_davis.json\'\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    torch.backends.cudnn.benchmark = True\n\n    # Setup Model\n    cfg = load_config(args)\n    siammask = Custom(anchors=cfg[\'anchors\'])\n    if args.resume:\n        assert isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        siammask = load_pretrain(siammask, args.resume)\n\n    siammask.eval().to(device)\n\n    # Parse Image file\n    # img_files = sorted(glob.glob(join(args.base_path, \'*.jp*\')))\n    img_files = get_frames(args.data)\n    ims = [imf for imf in img_files]\n\n    # Select ROI\n    cv2.namedWindow(""Get_mask"", cv2.WND_PROP_FULLSCREEN)\n    # cv2.setWindowProperty(""SiamMask"", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n    try:\n        init_rect = cv2.selectROI(\'Get_mask\', ims[0], False, False)\n        x, y, w, h = init_rect\n    except:\n        exit()\n\n    toc = 0\n    counter = 0\n\n    if not os.path.exists(os.path.join(\'results\', \'{}_mask\'.format(args.data))):\n        os.makedirs(os.path.join(\'results\', \'{}_mask\'.format(args.data)))\n        os.makedirs(os.path.join(\'results\', \'{}_frame\'.format(args.data)))\n\n    for f, im in enumerate(ims):\n        tic = cv2.getTickCount()\n        if f == 0:  # init\n            target_pos = np.array([x + w / 2, y + h / 2])\n            target_sz = np.array([w, h])\n            state = siamese_init(im, target_pos, target_sz, siammask, cfg[\'hp\'])  # init tracker\n        elif f > 0:  # tracking\n            state = siamese_track(state, im, mask_enable=True, refine_enable=True)  # track\n            location = state[\'ploygon\'].flatten()\n            mask = state[\'mask\'] > state[\'p\'].seg_thr\n            mask = (mask * 255.).astype(np.uint8)\n            cv2.imwrite(\'results/{}_mask/{:05d}.png\'.format(args.data, counter), mask)\n            cv2.imwrite(\'results/{}_frame/{:05d}.jpg\'.format(args.data, counter), im)\n            counter += 1\n\n            im[:, :, 2] = (mask > 0) * 255 + (mask == 0) * im[:, :, 2]\n            cv2.polylines(im, [np.int0(location).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n            cv2.imshow(\'Get_mask\', im)\n            key = cv2.waitKey(1)\n            if key > 0:\n                break\n\n        toc += cv2.getTickCount() - tic\n    toc /= cv2.getTickFrequency()\n    fps = f / toc\n    print(\'SiamMask Time: {:02.1f}s Speed: {:3.1f}fps (with visulization!)\'.format(toc, fps))\n    cv2.destroyAllWindows()\n\n'"
get_mask/__init__.py,0,b''
get_mask/test.py,4,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nimport argparse\nimport logging\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom os import makedirs\nfrom os.path import join, isdir, isfile\n\nfrom get_mask.utils.log_helper import init_log, add_file_handler\nfrom get_mask.utils.load_helper import load_pretrain\nfrom get_mask.utils.bbox_helper import get_axis_aligned_bbox, cxy_wh_2_rect\nfrom get_mask.utils.benchmark_helper import load_dataset, dataset_zoo\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport get_mask.models as models\n\nfrom get_mask.utils.anchors import Anchors\nfrom get_mask.utils.tracker_config import TrackerConfig\n\nfrom get_mask.utils.config_helper import load_config\nfrom get_mask.utils.pyvotkit.region import vot_overlap, vot_float2str\n\nthrs = np.arange(0.3, 0.5, 0.05)\n\nmodel_zoo = sorted(name for name in models.__dict__\n            if not name.startswith(""__"")\n            and callable(models.__dict__[name]))\n\n\nparser = argparse.ArgumentParser(description=\'Test SiamMask\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'\', choices=model_zoo + [\'Custom\',],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'--config\', dest=\'config\', required=True, help=\'hyper-parameter for SiamMask\')\nparser.add_argument(\'--resume\', default=\'\', type=str, required=True,\n                    metavar=\'PATH\',help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--mask\', action=\'store_true\', help=\'whether use mask output\')\nparser.add_argument(\'--refine\', action=\'store_true\', help=\'whether use mask refine output\')\nparser.add_argument(\'--dataset\', dest=\'dataset\', default=\'VOT2018\', choices=dataset_zoo,\n                    help=\'datasets\')\nparser.add_argument(\'-l\', \'--log\', default=""log_test.txt"", type=str, help=\'log file\')\nparser.add_argument(\'-v\', \'--visualization\', dest=\'visualization\', action=\'store_true\',\n                    help=\'whether visualize result\')\nparser.add_argument(\'--save_mask\', action=\'store_true\', help=\'whether use save mask for davis\')\nparser.add_argument(\'--gt\', action=\'store_true\', help=\'whether use gt rect for davis (Oracle)\')\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef im_to_torch(img):\n    img = np.transpose(img, (2, 0, 1))  # C*H*W\n    img = to_torch(img).float()\n    return img\n\n\ndef get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans, out_mode=\'torch\'):\n    if isinstance(pos, float):\n        pos = [pos, pos]\n    sz = original_sz\n    im_sz = im.shape\n    c = (original_sz + 1) / 2\n    context_xmin = round(pos[0] - c)\n    context_xmax = context_xmin + sz - 1\n    context_ymin = round(pos[1] - c)\n    context_ymax = context_ymin + sz - 1\n    left_pad = int(max(0., -context_xmin))\n    top_pad = int(max(0., -context_ymin))\n    right_pad = int(max(0., context_xmax - im_sz[1] + 1))\n    bottom_pad = int(max(0., context_ymax - im_sz[0] + 1))\n\n    context_xmin = context_xmin + left_pad\n    context_xmax = context_xmax + left_pad\n    context_ymin = context_ymin + top_pad\n    context_ymax = context_ymax + top_pad\n\n    # zzp: a more easy speed version\n    r, c, k = im.shape\n    if any([top_pad, bottom_pad, left_pad, right_pad]):\n        te_im = np.zeros((r + top_pad + bottom_pad, c + left_pad + right_pad, k), np.uint8)\n        te_im[top_pad:top_pad + r, left_pad:left_pad + c, :] = im\n        if top_pad:\n            te_im[0:top_pad, left_pad:left_pad + c, :] = avg_chans\n        if bottom_pad:\n            te_im[r + top_pad:, left_pad:left_pad + c, :] = avg_chans\n        if left_pad:\n            te_im[:, 0:left_pad, :] = avg_chans\n        if right_pad:\n            te_im[:, c + left_pad:, :] = avg_chans\n        im_patch_original = te_im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n    else:\n        im_patch_original = im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n\n    if not np.array_equal(model_sz, original_sz):\n        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))\n    else:\n        im_patch = im_patch_original\n    # cv2.imshow(\'crop\', im_patch)\n    # cv2.waitKey(0)\n    return im_to_torch(im_patch) if out_mode in \'torch\' else im_patch\n\n\ndef generate_anchor(cfg, score_size):\n    anchors = Anchors(cfg)\n    anchor = anchors.anchors\n    x1, y1, x2, y2 = anchor[:, 0], anchor[:, 1], anchor[:, 2], anchor[:, 3]\n    anchor = np.stack([(x1+x2)*0.5, (y1+y2)*0.5, x2-x1, y2-y1], 1)\n\n    total_stride = anchors.stride\n    anchor_num = anchor.shape[0]\n\n    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n    ori = - (score_size // 2) * total_stride\n    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n                         [ori + total_stride * dy for dy in range(score_size)])\n    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n    return anchor\n\n\ndef siamese_init(im, target_pos, target_sz, model, hp=None):\n    state = dict()\n    state[\'im_h\'] = im.shape[0]\n    state[\'im_w\'] = im.shape[1]\n    p = TrackerConfig()\n    p.update(hp, model.anchors)\n\n    p.renew()\n\n    net = model\n    p.scales = model.anchors[\'scales\']\n    p.ratios = model.anchors[\'ratios\']\n    p.anchor_num = len(p.ratios) * len(p.scales)\n    p.anchor = generate_anchor(model.anchors, p.score_size)\n\n    avg_chans = np.mean(im, axis=(0, 1))\n\n    wc_z = target_sz[0] + p.context_amount * sum(target_sz)\n    hc_z = target_sz[1] + p.context_amount * sum(target_sz)\n    s_z = round(np.sqrt(wc_z * hc_z))\n    # initialize the exemplar\n    z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans)\n\n    z = Variable(z_crop.unsqueeze(0))\n    net.template(z.cuda())\n\n    if p.windowing == \'cosine\':\n        window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size))\n    elif p.windowing == \'uniform\':\n        window = np.ones((p.score_size, p.score_size))\n    window = np.tile(window.flatten(), p.anchor_num)\n\n    state[\'p\'] = p\n    state[\'net\'] = net\n    state[\'avg_chans\'] = avg_chans\n    state[\'window\'] = window\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n    return state\n\n\ndef siamese_track(state, im, mask_enable=False, refine_enable=False):\n    p = state[\'p\']\n    net = state[\'net\']\n    avg_chans = state[\'avg_chans\']\n    window = state[\'window\']\n    target_pos = state[\'target_pos\']\n    target_sz = state[\'target_sz\']\n\n    wc_x = target_sz[1] + p.context_amount * sum(target_sz)\n    hc_x = target_sz[0] + p.context_amount * sum(target_sz)\n    s_x = np.sqrt(wc_x * hc_x)\n    scale_x = p.exemplar_size / s_x\n    d_search = (p.instance_size - p.exemplar_size) / 2\n    pad = d_search / scale_x\n    s_x = s_x + 2 * pad\n    crop_box = [target_pos[0] - round(s_x) / 2, target_pos[1] - round(s_x) / 2, round(s_x), round(s_x)]\n\n    # extract scaled crops for search region x at previous target position\n    x_crop = Variable(get_subwindow_tracking(im, target_pos, p.instance_size, round(s_x), avg_chans).unsqueeze(0))\n\n    if mask_enable:\n        score, delta, mask = net.track_mask(x_crop.cuda())\n    else:\n        score, delta = net.track(x_crop.cuda())\n\n    delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n    score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1).permute(1, 0), dim=1).data[:,\n            1].cpu().numpy()\n\n    delta[0, :] = delta[0, :] * p.anchor[:, 2] + p.anchor[:, 0]\n    delta[1, :] = delta[1, :] * p.anchor[:, 3] + p.anchor[:, 1]\n    delta[2, :] = np.exp(delta[2, :]) * p.anchor[:, 2]\n    delta[3, :] = np.exp(delta[3, :]) * p.anchor[:, 3]\n\n    def change(r):\n        return np.maximum(r, 1. / r)\n\n    def sz(w, h):\n        pad = (w + h) * 0.5\n        sz2 = (w + pad) * (h + pad)\n        return np.sqrt(sz2)\n\n    def sz_wh(wh):\n        pad = (wh[0] + wh[1]) * 0.5\n        sz2 = (wh[0] + pad) * (wh[1] + pad)\n        return np.sqrt(sz2)\n\n    # size penalty\n    target_sz_in_crop = target_sz*scale_x\n    s_c = change(sz(delta[2, :], delta[3, :]) / (sz_wh(target_sz_in_crop)))  # scale penalty\n    r_c = change((target_sz_in_crop[0] / target_sz_in_crop[1]) / (delta[2, :] / delta[3, :]))  # ratio penalty\n\n    penalty = np.exp(-(r_c * s_c - 1) * p.penalty_k)\n    pscore = penalty * score\n\n    # cos window (motion model)\n    pscore = pscore * (1 - p.window_influence) + window * p.window_influence\n    best_pscore_id = np.argmax(pscore)\n\n    pred_in_crop = delta[:, best_pscore_id] / scale_x\n    lr = penalty[best_pscore_id] * score[best_pscore_id] * p.lr  # lr for OTB\n\n    res_x = pred_in_crop[0] + target_pos[0]\n    res_y = pred_in_crop[1] + target_pos[1]\n\n    res_w = target_sz[0] * (1 - lr) + pred_in_crop[2] * lr\n    res_h = target_sz[1] * (1 - lr) + pred_in_crop[3] * lr\n\n    target_pos = np.array([res_x, res_y])\n    target_sz = np.array([res_w, res_h])\n\n    # for Mask Branch\n    if mask_enable:\n        best_pscore_id_mask = np.unravel_index(best_pscore_id, (5, p.score_size, p.score_size))\n        delta_x, delta_y = best_pscore_id_mask[2], best_pscore_id_mask[1]\n\n        if refine_enable:\n            mask = net.track_refine((delta_y, delta_x)).cuda().sigmoid().squeeze().view(\n                p.out_size, p.out_size).cpu().data.numpy()\n        else:\n            mask = mask[0, :, delta_y, delta_x].sigmoid(). \\\n                squeeze().view(p.out_size, p.out_size).cpu().data.numpy()\n\n        def crop_back(image, bbox, out_sz, padding=-1):\n            a = (out_sz[0] - 1) / bbox[2]\n            b = (out_sz[1] - 1) / bbox[3]\n            c = -a * bbox[0]\n            d = -b * bbox[1]\n            mapping = np.array([[a, 0, c],\n                                [0, b, d]]).astype(np.float)\n            crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n                                  flags=cv2.INTER_LINEAR,\n                                  borderMode=cv2.BORDER_CONSTANT,\n                                  borderValue=padding)\n            return crop\n\n        s = crop_box[2] / p.instance_size\n        sub_box = [crop_box[0] + (delta_x - p.base_size / 2) * p.total_stride * s,\n                   crop_box[1] + (delta_y - p.base_size / 2) * p.total_stride * s,\n                   s * p.exemplar_size, s * p.exemplar_size]\n        s = p.out_size / sub_box[2]\n        back_box = [-sub_box[0] * s, -sub_box[1] * s, state[\'im_w\'] * s, state[\'im_h\'] * s]\n        mask_in_img = crop_back(mask, back_box, (state[\'im_w\'], state[\'im_h\']))\n\n        target_mask = (mask_in_img > p.seg_thr).astype(np.uint8)\n        if cv2.__version__[-5] == \'4\':\n            contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        else:\n            _, contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n        if len(contours) != 0 and np.max(cnt_area) > 100:\n            contour = contours[np.argmax(cnt_area)]  # use max area polygon\n            polygon = contour.reshape(-1, 2)\n            # pbox = cv2.boundingRect(polygon)  # Min Max Rectangle\n            prbox = cv2.boxPoints(cv2.minAreaRect(polygon))  # Rotated Rectangle\n\n            # box_in_img = pbox\n            rbox_in_img = prbox\n        else:  # empty mask\n            location = cxy_wh_2_rect(target_pos, target_sz)\n            rbox_in_img = np.array([[location[0], location[1]],\n                                    [location[0] + location[2], location[1]],\n                                    [location[0] + location[2], location[1] + location[3]],\n                                    [location[0], location[1] + location[3]]])\n\n    target_pos[0] = max(0, min(state[\'im_w\'], target_pos[0]))\n    target_pos[1] = max(0, min(state[\'im_h\'], target_pos[1]))\n    target_sz[0] = max(10, min(state[\'im_w\'], target_sz[0]))\n    target_sz[1] = max(10, min(state[\'im_h\'], target_sz[1]))\n\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n    state[\'score\'] = score\n    state[\'mask\'] = mask_in_img if mask_enable else []\n    state[\'ploygon\'] = rbox_in_img if mask_enable else []\n    return state\n\n\ndef track_vot(model, video, hp=None, mask_enable=False, refine_enable=False):\n    regions = []  # result and states[1 init / 2 lost / 0 skip]\n    image_files, gt = video[\'image_files\'], video[\'gt\']\n\n    start_frame, end_frame, lost_times, toc = 0, len(image_files), 0, 0\n\n    for f, image_file in enumerate(image_files):\n        im = cv2.imread(image_file)\n        tic = cv2.getTickCount()\n        if f == start_frame:  # init\n            cx, cy, w, h = get_axis_aligned_bbox(gt[f])\n            target_pos = np.array([cx, cy])\n            target_sz = np.array([w, h])\n            state = siamese_init(im, target_pos, target_sz, model, hp)  # init tracker\n            location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n            regions.append(1 if \'VOT\' in args.dataset else gt[f])\n        elif f > start_frame:  # tracking\n            state = siamese_track(state, im, mask_enable, refine_enable)  # track\n            if mask_enable:\n                location = state[\'ploygon\'].flatten()\n                mask = state[\'mask\']\n            else:\n                location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n                mask = []\n\n            if \'VOT\' in args.dataset:\n                gt_polygon = ((gt[f][0], gt[f][1]), (gt[f][2], gt[f][3]),\n                              (gt[f][4], gt[f][5]), (gt[f][6], gt[f][7]))\n                if mask_enable:\n                    pred_polygon = ((location[0], location[1]), (location[2], location[3]),\n                                    (location[4], location[5]), (location[6], location[7]))\n                else:\n                    pred_polygon = ((location[0], location[1]),\n                                    (location[0] + location[2], location[1]),\n                                    (location[0] + location[2], location[1] + location[3]),\n                                    (location[0], location[1] + location[3]))\n                b_overlap = vot_overlap(gt_polygon, pred_polygon, (im.shape[1], im.shape[0]))\n            else:\n                b_overlap = 1\n\n            if b_overlap:\n                regions.append(location)\n            else:  # lost\n                regions.append(2)\n                lost_times += 1\n                start_frame = f + 5  # skip 5 frames\n        else:  # skip\n            regions.append(0)\n        toc += cv2.getTickCount() - tic\n\n        if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n            im_show = im.copy()\n            if f == 0: cv2.destroyAllWindows()\n            if gt.shape[0] > f:\n                if len(gt[f]) == 8:\n                    cv2.polylines(im_show, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n                else:\n                    cv2.rectangle(im_show, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n            if len(location) == 8:\n                if mask_enable:\n                    mask = mask > state[\'p\'].seg_thr\n                    im_show[:, :, 2] = mask * 255 + (1 - mask) * im_show[:, :, 2]\n                location_int = np.int0(location)\n                cv2.polylines(im_show, [location_int.reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n            else:\n                location = [int(l) for l in location]\n                cv2.rectangle(im_show, (location[0], location[1]),\n                              (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n            cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n            cv2.putText(im_show, str(lost_times), (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n            cv2.imshow(video[\'name\'], im_show)\n            cv2.waitKey(1)\n    toc /= cv2.getTickFrequency()\n\n    # save result\n    name = args.arch.split(\'.\')[0] + \'_\' + (\'mask_\' if mask_enable else \'\') + (\'refine_\' if refine_enable else \'\') +\\\n           args.resume.split(\'/\')[-1].split(\'.\')[0]\n\n    if \'VOT\' in args.dataset:\n        video_path = join(\'test\', args.dataset, name,\n                          \'baseline\', video[\'name\'])\n        if not isdir(video_path): makedirs(video_path)\n        result_path = join(video_path, \'{:s}_001.txt\'.format(video[\'name\']))\n        with open(result_path, ""w"") as fin:\n            for x in regions:\n                fin.write(""{:d}\\n"".format(x)) if isinstance(x, int) else \\\n                        fin.write(\',\'.join([vot_float2str(""%.4f"", i) for i in x]) + \'\\n\')\n    else:  # OTB\n        video_path = join(\'test\', args.dataset, name)\n        if not isdir(video_path): makedirs(video_path)\n        result_path = join(video_path, \'{:s}.txt\'.format(video[\'name\']))\n        with open(result_path, ""w"") as fin:\n            for x in regions:\n                fin.write(\',\'.join([str(i) for i in x])+\'\\n\')\n\n    logger.info(\'({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps Lost: {:d}\'.format(\n        v_id, video[\'name\'], toc, f / toc, lost_times))\n\n    return lost_times, f / toc\n\n\ndef MultiBatchIouMeter(thrs, outputs, targets, start=None, end=None):\n    targets = np.array(targets)\n    outputs = np.array(outputs)\n\n    num_frame = targets.shape[0]\n    if start is None:\n        object_ids = np.array(list(range(outputs.shape[0]))) + 1\n    else:\n        object_ids = [int(id) for id in start]\n\n    num_object = len(object_ids)\n    res = np.zeros((num_object, len(thrs)), dtype=np.float32)\n\n    output_max_id = np.argmax(outputs, axis=0).astype(\'uint8\')+1\n    outputs_max = np.max(outputs, axis=0)\n    for k, thr in enumerate(thrs):\n        output_thr = outputs_max > thr\n        for j in range(num_object):\n            target_j = targets == object_ids[j]\n\n            if start is None:\n                start_frame, end_frame = 1, num_frame - 1\n            else:\n                start_frame, end_frame = start[str(object_ids[j])] + 1, end[str(object_ids[j])] - 1\n            iou = []\n            for i in range(start_frame, end_frame):\n                pred = (output_thr[i] * output_max_id[i]) == (j+1)\n                mask_sum = (pred == 1).astype(np.uint8) + (target_j[i] > 0).astype(np.uint8)\n                intxn = np.sum(mask_sum == 2)\n                union = np.sum(mask_sum > 0)\n                if union > 0:\n                    iou.append(intxn / union)\n                elif union == 0 and intxn == 0:\n                    iou.append(1)\n            res[j, k] = np.mean(iou)\n    return res\n\n\ndef track_vos(model, video, hp=None, mask_enable=False, refine_enable=False, mot_enable=False):\n    image_files = video[\'image_files\']\n\n    annos = [np.array(Image.open(x)) for x in video[\'anno_files\']]\n    if \'anno_init_files\' in video:\n        annos_init = [np.array(Image.open(x)) for x in video[\'anno_init_files\']]\n    else:\n        annos_init = [annos[0]]\n\n    if not mot_enable:\n        annos = [(anno > 0).astype(np.uint8) for anno in annos]\n        annos_init = [(anno_init > 0).astype(np.uint8) for anno_init in annos_init]\n\n    if \'start_frame\' in video:\n        object_ids = [int(id) for id in video[\'start_frame\']]\n    else:\n        object_ids = [o_id for o_id in np.unique(annos[0]) if o_id != 0]\n        if len(object_ids) != len(annos_init):\n            annos_init = annos_init*len(object_ids)\n    object_num = len(object_ids)\n    toc = 0\n    pred_masks = np.zeros((object_num, len(image_files), annos[0].shape[0], annos[0].shape[1]))-1\n    for obj_id, o_id in enumerate(object_ids):\n\n        if \'start_frame\' in video:\n            start_frame = video[\'start_frame\'][str(o_id)]\n            end_frame = video[\'end_frame\'][str(o_id)]\n        else:\n            start_frame, end_frame = 0, len(image_files)\n\n        for f, image_file in enumerate(image_files):\n            im = cv2.imread(image_file)\n            tic = cv2.getTickCount()\n            if f == start_frame:  # init\n                mask = annos_init[obj_id] == o_id\n                x, y, w, h = cv2.boundingRect((mask).astype(np.uint8))\n                cx, cy = x + w/2, y + h/2\n                target_pos = np.array([cx, cy])\n                target_sz = np.array([w, h])\n                state = siamese_init(im, target_pos, target_sz, model, hp)  # init tracker\n            elif end_frame >= f > start_frame:  # tracking\n                state = siamese_track(state, im, mask_enable, refine_enable)  # track\n                mask = state[\'mask\']\n            toc += cv2.getTickCount() - tic\n            if end_frame >= f >= start_frame:\n                pred_masks[obj_id, f, :, :] = mask\n    toc /= cv2.getTickFrequency()\n\n    if len(annos) == len(image_files):\n        multi_mean_iou = MultiBatchIouMeter(thrs, pred_masks, annos,\n                                            start=video[\'start_frame\'] if \'start_frame\' in video else None,\n                                            end=video[\'end_frame\'] if \'end_frame\' in video else None)\n        for i in range(object_num):\n            for j, thr in enumerate(thrs):\n                logger.info(\'Fusion Multi Object{:20s} IOU at {:.2f}: {:.4f}\'.format(video[\'name\'] + \'_\' + str(i + 1), thr,\n                                                                           multi_mean_iou[i, j]))\n    else:\n        multi_mean_iou = []\n\n    if args.save_mask:\n        video_path = join(\'test\', args.dataset, \'SiamMask\', video[\'name\'])\n        if not isdir(video_path): makedirs(video_path)\n        pred_mask_final = np.array(pred_masks)\n        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype(\'uint8\') + 1) * (\n                np.max(pred_mask_final, axis=0) > state[\'p\'].seg_thr).astype(\'uint8\')\n        for i in range(pred_mask_final.shape[0]):\n            cv2.imwrite(join(video_path, image_files[i].split(\'/\')[-1].split(\'.\')[0] + \'.png\'), pred_mask_final[i].astype(np.uint8))\n\n    if args.visualization:\n        pred_mask_final = np.array(pred_masks)\n        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype(\'uint8\') + 1) * (\n                np.max(pred_mask_final, axis=0) > state[\'p\'].seg_thr).astype(\'uint8\')\n        COLORS = np.random.randint(128, 255, size=(object_num, 3), dtype=""uint8"")\n        COLORS = np.vstack([[0, 0, 0], COLORS]).astype(""uint8"")\n        mask = COLORS[pred_mask_final]\n        for f, image_file in enumerate(image_files):\n            output = ((0.4 * cv2.imread(image_file)) + (0.6 * mask[f,:,:,:])).astype(""uint8"")\n            cv2.imshow(""mask"", output)\n            cv2.waitKey(1)\n\n    logger.info(\'({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps\'.format(\n        v_id, video[\'name\'], toc, f*len(object_ids) / toc))\n\n    return multi_mean_iou, f*len(object_ids) / toc\n\n\ndef main():\n    global args, logger, v_id\n    args = parser.parse_args()\n    cfg = load_config(args)\n\n    init_log(\'global\', logging.INFO)\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    logger = logging.getLogger(\'global\')\n    logger.info(args)\n\n    # setup model\n    if args.arch == \'Custom\':\n        from get_mask.models.custom import Custom\n        model = Custom(anchors=cfg[\'anchors\'])\n    else:\n        model = models.__dict__[args.arch](anchors=cfg[\'anchors\'])\n\n    if args.resume:\n        assert isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model = load_pretrain(model, args.resume)\n    model.eval()\n    model = model.cuda()\n\n    # setup dataset\n    dataset = load_dataset(args.dataset)\n\n    # VOS or VOT?\n    if args.dataset in [\'DAVIS2016\', \'DAVIS2017\', \'ytb_vos\'] and args.mask:\n        vos_enable = True  # enable Mask output\n    else:\n        vos_enable = False\n\n    total_lost = 0  # VOT\n    iou_lists = []  # VOS\n    speed_list = []\n\n    for v_id, video in enumerate(dataset.keys(), start=1):\n        if vos_enable:\n            iou_list, speed = track_vos(model, dataset[video], cfg[\'hp\'] if \'hp\' in cfg.keys() else None,\n                                 args.mask, args.refine, args.dataset in [\'DAVIS2017\', \'ytb_vos\'])\n            iou_lists.append(iou_list)\n        else:\n            lost, speed = track_vot(model, dataset[video], cfg[\'hp\'] if \'hp\' in cfg.keys() else None,\n                             args.mask, args.refine)\n            total_lost += lost\n        speed_list.append(speed)\n\n    # report final result\n    if vos_enable:\n        for thr, iou in zip(thrs, np.mean(np.concatenate(iou_lists), axis=0)):\n            logger.info(\'Segmentation Threshold {:.2f} mIoU: {:.3f}\'.format(thr, iou))\n    else:\n        logger.info(\'Total Lost: {:d}\'.format(total_lost))\n\n    logger.info(\'Mean Speed: {:.2f} FPS\'.format(np.mean(speed_list)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
inpainting/__init__.py,0,b''
inpainting/davis.py,6,"b'from __future__ import division\nimport torch\nfrom torch.utils import data\n\n# general libs\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport math\nimport time\nimport tqdm\nimport os\nimport random\nimport argparse\nimport glob\nimport json\n\nfrom scipy import ndimage, signal\nimport pdb\n\ndef temporal_transform(frame_indices, sample_range):\n    tmp = np.random.randint(0,len(frame_indices)-sample_range)\n    return frame_indices[tmp:tmp+sample_range]\n\nDAVIS_2016 = [\'bear\'\n,\'bmx-bumps\',\'boat\',\'breakdance-flare\',\'bus\',\'car-turn\',\'dance-jump\',\'dog-agility\',\'drift-turn\',\'elephant\',\'flamingo\',\'hike\',\'hockey\',\'horsejump-low\',\'kite-walk\',\'lucia\',\'mallard-fly\',\'mallard-water\',\'motocross-bumps\',\'motorbike\',\'paragliding\',\'rhino\',\'rollerblade\',\'scooter-gray\',\'soccerball\',\'stroller\',\'surf\',\'swing\',\'tennis\',\'train\',\'blackswan\',\'bmx-trees\',\'breakdance\',\'camel\',\'car-roundabout\',\'car-shadow\',\'cows\',\'dance-twirl\',\'dog\',\'drift-chicane\',\'drift-straight\',\'goat\',\'horsejump-high\',\'kite-surf\',\'libby\',\'motocross-jump\',\'paragliding-launch\',\'parkour\',\'scooter-black\',\'soapbox\']\n\nclass DAVIS(data.Dataset):\n    def __init__(self, root, mask_dilation, resolution=\'480p\', size=(256, 256), sample_duration=0):\n        self.mask_dilation = mask_dilation\n        self.sample_duration = sample_duration\n        self.root = root\n        self.mask_dir = root + \'_mask\'\n        self.image_dir = root + \'_frame\'\n\n        self.size = size\n\n        if \'.\' in root.split(\'/\')[-1]:\n            self.videos = (root.split(\'/\')[-1].split(\'.\')[0])\n        else:\n            self.videos = (root.split(\'/\')[-1])\n\n        self.num_frames = len(glob.glob(os.path.join(self.image_dir, \'*.jpg\')))\n        _mask = np.array(Image.open(os.path.join(self.mask_dir, \'00000.png\')).convert(""P""))\n        self.num_objects = np.max(_mask)\n        self.shape = np.shape(_mask)\n\n    def __len__(self):\n        return 1\n\n\n    def __getitem__(self, index):\n        video = self.videos\n        info = {}\n        info[\'name\'] = video\n        info[\'num_frames\'] = self.num_frames\n        num_objects = 1\n        info[\'num_objects\'] = num_objects        \n        \n        images = []\n        masks = []\n        struct = ndimage.generate_binary_structure(2, 2)\n\n        f_list = list(range(self.num_frames))\n        if self.sample_duration > 0:\n            f_list = temporal_transform(f_list, self.sample_duration)\n\n        for f in f_list:\n                            \n            img_file = os.path.join(self.image_dir, \'{:05d}.jpg\'.format(f))\n            image_ = cv2.resize(cv2.imread(img_file), self.size, cv2.INTER_CUBIC)\n            image_ = np.float32(image_)/255.0\n            images.append(torch.from_numpy(image_))\n\n            try:\n                mask_file = os.path.join(self.mask_dir, \'{:05d}.png\'.format(f))\n                mask_ = np.array(Image.open(mask_file).convert(\'P\'), np.uint8)\n                mask_ = cv2.resize(mask_,self.size, cv2.INTER_NEAREST)\n            except:\n                mask_file = os.path.join(self.mask_dir, \'00000.png\')\n                mask_ = np.array(Image.open(mask_file).convert(\'P\'), np.uint8)\n                mask_ = cv2.resize(mask_, self.size, cv2.INTER_NEAREST)\n\n            if video in DAVIS_2016:\n                mask_ = (mask_ != 0)\n            else:\n                select_mask = max(1, mask_.max())\n                mask_ = (mask_ == select_mask).astype(np.float)\n            \n            w_k = np.ones((10, 6))\n            mask2 = signal.convolve2d(mask_.astype(np.float), w_k, \'same\')\n            mask2 = 1 - (mask2 == 0)\n            mask_ = np.float32(mask2)\n            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (self.mask_dilation, self.mask_dilation))\n            mask_ = cv2.dilate(mask_, kernel)\n            masks.append(torch.from_numpy(mask_))\n\n        masks = torch.stack(masks)\n        masks = (masks == 1).type(torch.FloatTensor).unsqueeze(0)\n        images = torch.stack(images).permute(3,0,1,2)\n\n        return images, masks, info\n'"
inpainting/model.py,1,"b""import torch\nfrom torch import nn\nfrom inpainting.models import vinet\nimport pdb\n\ndef generate_model(opt):\n\n    try: \n        assert(opt.model == 'vinet_final')\n        model = vinet.VINet_final(opt=opt)\n    except:\n        print('Model name should be: vinet_final')\n\n    assert(opt.no_cuda is False)\n    model = model.cuda()\n    model = nn.DataParallel(model)\n    loaded, empty = 0,0\n    if opt.pretrain_path:\n        print('Loading pretrained model {}'.format(opt.pretrain_path))\n        pretrain = torch.load(opt.pretrain_path)\n\n        child_dict = model.state_dict()\n        parent_list = pretrain['state_dict'].keys()\n        parent_dict = {}\n        for chi,_ in child_dict.items():\n            if chi in parent_list:\n                parent_dict[chi] = pretrain['state_dict'][chi]\n                #print('Loaded: ',chi)\n                loaded += 1\n            else:\n                #print('Empty:',chi)\n                empty += 1\n        print('Loaded: %d/%d params'%(loaded, loaded+empty))\n        child_dict.update(parent_dict)\n        model.load_state_dict(child_dict)   \n\n    return model, model.parameters()\n"""
inpainting/utils.py,15,"b'import csv\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom random import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport pdb\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass Logger(object):\n\n    def __init__(self, path, header):\n        self.log_file = open(path, \'w\')\n        self.logger = csv.writer(self.log_file, delimiter=\'\\t\')\n\n        self.logger.writerow(header)\n        self.header = header\n\n    def __del(self):\n        self.log_file.close()\n\n    def log(self, values):\n        write_values = []\n        for col in self.header:\n            assert col in values\n            write_values.append(values[col])\n\n        self.logger.writerow(write_values)\n        self.log_file.flush()\n\n\ndef load_value_file(file_path):\n    with open(file_path, \'r\') as input_file:\n        value = float(input_file.read().rstrip(\'\\n\\r\'))\n\n    return value\n\n\ndef calculate_accuracy(outputs, targets):\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1))\n    n_correct_elems = correct.float().sum().data[0]\n\n    return n_correct_elems / batch_size\n\n\ndef tensor2img(x,opt):\n    if opt.no_mean_norm:\n        x = x.copy() * 255.\n        if len(np.shape(x)) == 3:\n            return x.transpose((1,2,0)).astype(np.uint8)\n        else:\n            return x.astype(np.uint8)\n\n    x[0] = x[0] + opt.mean[0]\n    x[1] = x[1] + opt.mean[1]\n    x[2] = x[2] + opt.mean[2]\n    x = x.transpose((1,2,0)).astype(np.uint8)\n    return x\n\ndef cvimg2tensor(src):\n    out = src.copy()\n    out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n    out = out.transpose((2,0,1)).astype(np.float64)\n    # out = out / 255\n    return out\n\ndef DxDy(x):\n    # shift one pixel and get difference (for both x and y direction)\n    #return x[:,:,:,:,:-1] - x[:,:,:,:,1:], x[:,:,:,:-1,:]-x[:,:,:,1:,:]\n    return x[:,:,:,:-1] - x[:,:,:,1:], x[:,:,:-1,:]-x[:,:,1:,:]\n\n\n######################################################################################\n##  Image utility\n######################################################################################\n\n\ndef rotate_image(img, degree, interp=cv2.INTER_LINEAR):\n\n    height, width = img.shape[:2]\n    image_center = (width/2, height/2)\n\n    rotation_mat = cv2.getRotationMatrix2D(image_center, degree, 1.)\n\n    abs_cos = abs(rotation_mat[0,0])\n    abs_sin = abs(rotation_mat[0,1])\n\n    bound_w = int(height * abs_sin + width * abs_cos)\n    bound_h = int(height * abs_cos + width * abs_sin)\n\n    rotation_mat[0, 2] += bound_w/2 - image_center[0]\n    rotation_mat[1, 2] += bound_h/2 - image_center[1]\n\n    img_out = cv2.warpAffine(img, rotation_mat, (bound_w, bound_h), flags=interp+cv2.WARP_FILL_OUTLIERS)\n  \n    return img_out\n\n\ndef numpy_to_PIL(img_np):\n\n    ## input image is numpy array in [0, 1]\n    ## convert to PIL image in [0, 255]\n\n    img_PIL = np.uint8(img_np * 255)\n    img_PIL = Image.fromarray(img_PIL)\n\n    return img_PIL\n\ndef PIL_to_numpy(img_PIL):\n\n    img_np = np.asarray(img_PIL)\n    img_np = np.float32(img_np) / 255.0\n\n    return img_np\n\n\ndef read_img(filename):\n    ## read image and convert to RGB in [0, 1]\n    img = cv2.imread(filename)\n    if img is None:\n        raise Exception(""Image %s does not exist"" %filename)\n    img = img[:, :, ::-1] ## BGR to RGB    \n    img = np.float32(img) / 255.0\n    return img\n\ndef save_img(img, filename):\n\n    print(""Save %s"" %filename)\n    ## clip to [0, 1]\n    img = np.clip(img, 0, 1)\n    ## quantize to [0, 255]\n    img = np.uint8(img * 255.0)\n    cv2.imwrite(filename, img, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\ndef var_to_numpy(obj, for_vis=True):\n    if for_vis:\n        obj = obj.permute(0,2,3,1)\n        obj = (obj+1) / 2\n    return obj.data.cpu().numpy()\n\n\ndef to_var(x, volatile=False):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, volatile=volatile)\n\n\n\n\n\n### python lib\nimport os, sys, random, math, cv2, pickle, subprocess\nimport numpy as np\nfrom PIL import Image\n\n### torch lib\nimport torch\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data import DataLoader\n\n### custom lib\nfrom inpainting.lib.resample2d_package.modules.resample2d import Resample2d\n\nFLO_TAG = 202021.25\nEPS = 1e-12\n\nUNKNOWN_FLOW_THRESH = 1e7\nSMALLFLOW = 0.0\nLARGEFLOW = 1e8\n\n\n######################################################################################\n##  Training utility\n######################################################################################\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\ndef normalize_ImageNet_stats(batch):\n\n    mean = torch.zeros_like(batch)\n    std = torch.zeros_like(batch)\n    mean[:, 0, :, :] = 0.485\n    mean[:, 1, :, :] = 0.456\n    mean[:, 2, :, :] = 0.406\n    std[:, 0, :, :] = 0.229\n    std[:, 1, :, :] = 0.224\n    std[:, 2, :, :] = 0.225\n\n    batch_out = (batch - mean) / std\n\n    return batch_out\n\n\ndef img2tensor(img):\n\n    img_t = np.expand_dims(img.transpose(2, 0, 1), axis=0)\n    img_t = torch.from_numpy(img_t.astype(np.float32))\n\n    return img_t\n\ndef tensor2img(img_t):\n\n    img = img_t[0].detach().to(""cpu"").numpy()\n    img = np.transpose(img, (1, 2, 0))\n\n    return img\n\n\ndef save_model(model, optimizer, opts):\n\n    # save opts\n    opts_filename = os.path.join(opts.model_dir, ""opts.pth"")\n    print(""Save %s"" %opts_filename)\n    with open(opts_filename, \'wb\') as f:\n        pickle.dump(opts, f)\n\n    # serialize model and optimizer to dict\n    state_dict = {\n        \'model\': model.state_dict(),\n        \'optimizer\' : optimizer.state_dict(),\n    }\n\n    model_filename = os.path.join(opts.model_dir, ""model_epoch_%d.pth"" %model.epoch)\n    print(""Save %s"" %model_filename)\n    torch.save(state_dict, model_filename)\n\n\ndef load_model(model, optimizer, opts, epoch):\n\n    # load model\n    model_filename = os.path.join(opts.model_dir, ""model_epoch_%d.pth"" %epoch)\n    print(""Load %s"" %model_filename)\n    state_dict = torch.load(model_filename)\n    \n    model.load_state_dict(state_dict[\'model\'])\n    optimizer.load_state_dict(state_dict[\'optimizer\'])\n\n    ### move optimizer state to GPU\n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if torch.is_tensor(v):\n                state[k] = v.cuda()\n\n    model.epoch = epoch ## reset model epoch\n\n    return model, optimizer\n\n\n\nclass SubsetSequentialSampler(Sampler):\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in range(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\ndef create_data_loader(data_set, opts, mode):\n\n    ### generate random index\n    if mode == \'train\':\n        total_samples = opts.train_epoch_size * opts.batch_size\n    else:\n        total_samples = opts.valid_epoch_size * opts.batch_size\n\n    num_epochs = int(math.ceil(float(total_samples) / len(data_set)))\n\n    indices = np.random.permutation(len(data_set))\n    indices = np.tile(indices, num_epochs)\n    indices = indices[:total_samples]\n\n    ### generate data sampler and loader\n    sampler = SubsetSequentialSampler(indices)\n    data_loader = DataLoader(dataset=data_set, num_workers=opts.threads, batch_size=opts.batch_size, sampler=sampler, pin_memory=True)\n\n    return data_loader\n\n\ndef learning_rate_decay(opts, epoch):\n    \n    ###             1 ~ offset              : lr_init\n    ###        offset ~ offset + step       : lr_init * drop^1\n    ### offset + step ~ offset + step * 2   : lr_init * drop^2\n    ###              ...\n    \n    if opts.lr_drop == 0: # constant learning rate\n        decay = 0\n    else:\n        assert(opts.lr_step > 0)\n        decay = math.floor( float(epoch) / opts.lr_step )\n        decay = max(decay, 0) ## decay = 1 for the first lr_offset iterations\n\n    lr = opts.lr_init * math.pow(opts.lr_drop, decay)\n    lr = max(lr, opts.lr_init * opts.lr_min)\n\n    return lr\n\n\ndef count_network_parameters(model):\n\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    N = sum([np.prod(p.size()) for p in parameters])\n\n    return N\n\n\n\n######################################################################################\n##  Image utility\n######################################################################################\n\n\ndef rotate_image(img, degree, interp=cv2.INTER_LINEAR):\n\n    height, width = img.shape[:2]\n    image_center = (width/2, height/2)\n\n    rotation_mat = cv2.getRotationMatrix2D(image_center, degree, 1.)\n\n    abs_cos = abs(rotation_mat[0,0])\n    abs_sin = abs(rotation_mat[0,1])\n\n    bound_w = int(height * abs_sin + width * abs_cos)\n    bound_h = int(height * abs_cos + width * abs_sin)\n\n    rotation_mat[0, 2] += bound_w/2 - image_center[0]\n    rotation_mat[1, 2] += bound_h/2 - image_center[1]\n\n    img_out = cv2.warpAffine(img, rotation_mat, (bound_w, bound_h), flags=interp+cv2.WARP_FILL_OUTLIERS)\n  \n    return img_out\n\n\ndef numpy_to_PIL(img_np):\n\n    ## input image is numpy array in [0, 1]\n    ## convert to PIL image in [0, 255]\n\n    img_PIL = np.uint8(img_np * 255)\n    img_PIL = Image.fromarray(img_PIL)\n\n    return img_PIL\n\ndef PIL_to_numpy(img_PIL):\n\n    img_np = np.asarray(img_PIL)\n    img_np = np.float32(img_np) / 255.0\n\n    return img_np\n\n\ndef read_img(filename, grayscale=0):\n\n    ## read image and convert to RGB in [0, 1]\n\n    if grayscale:\n        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n\n        if img is None:\n            raise Exception(""Image %s does not exist"" %filename)\n\n        img = np.expand_dims(img, axis=2)\n    else:\n        img = cv2.imread(filename)\n\n        if img is None:\n            raise Exception(""Image %s does not exist"" %filename)\n\n        img = img[:, :, ::-1] ## BGR to RGB\n    \n    img = np.float32(img) / 255.0\n\n    return img\n\ndef save_img(img, filename):\n\n    print(""Save %s"" %filename)\n\n    if img.ndim == 3:\n        img = img[:, :, ::-1] ### RGB to BGR\n    \n    ## clip to [0, 1]\n    img = np.clip(img, 0, 1)\n\n    ## quantize to [0, 255]\n    img = np.uint8(img * 255.0)\n\n    cv2.imwrite(filename, img, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\n\n######################################################################################\n##  Flow utility\n######################################################################################\n\ndef read_flo(filename):\n\n    with open(filename, \'rb\') as f:\n        tag = np.fromfile(f, np.float32, count=1)\n        \n        if tag != FLO_TAG:\n            sys.exit(\'Wrong tag. Invalid .flo file %s\' %filename)\n        else:\n            w = int(np.fromfile(f, np.int32, count=1))\n            h = int(np.fromfile(f, np.int32, count=1))\n            #print \'Reading %d x %d flo file\' % (w, h)\n                \n            data = np.fromfile(f, np.float32, count=2*w*h)\n\n            # Reshape data into 3D array (columns, rows, bands)\n            flow = np.resize(data, (h, w, 2))\n\n    return flow\n\ndef save_flo(flow, filename):\n\n    with open(filename, \'wb\') as f:\n\n        tag = np.array([FLO_TAG], dtype=np.float32)\n\n        (height, width) = flow.shape[0:2]\n        w = np.array([width], dtype=np.int32)\n        h = np.array([height], dtype=np.int32)\n        tag.tofile(f)\n        w.tofile(f)\n        h.tofile(f)\n        flow.tofile(f)\n    \ndef resize_flow(flow, W_out=0, H_out=0, scale=0):\n\n    if W_out == 0 and H_out == 0 and scale == 0:\n        raise Exception(""(W_out, H_out) or scale should be non-zero"")\n\n    H_in = flow.shape[0]\n    W_in = flow.shape[1]\n\n    if scale == 0:\n        y_scale = float(H_out) / H_in\n        x_scale = float(W_out) / W_in\n    else:\n        y_scale = scale\n        x_scale = scale\n\n    flow_out = cv2.resize(flow, None, fx=x_scale, fy=y_scale, interpolation=cv2.INTER_LINEAR)\n\n    flow_out[:, :, 0] = flow_out[:, :, 0] * x_scale\n    flow_out[:, :, 1] = flow_out[:, :, 1] * y_scale\n\n    return flow_out\n\n\ndef rotate_flow(flow, degree, interp=cv2.INTER_LINEAR):\n    \n    ## angle in radian\n    angle = math.radians(degree)\n\n    H = flow.shape[0]\n    W = flow.shape[1]\n\n    #rotation_matrix = cv2.getRotationMatrix2D((W/2, H/2), math.degrees(angle), 1)\n    #flow_out = cv2.warpAffine(flow, rotation_matrix, (W, H))\n    flow_out = rotate_image(flow, degree, interp)\n    \n    fu = flow_out[:, :, 0] * math.cos(-angle) - flow_out[:, :, 1] * math.sin(-angle)\n    fv = flow_out[:, :, 0] * math.sin(-angle) + flow_out[:, :, 1] * math.cos(-angle)\n\n    flow_out[:, :, 0] = fu\n    flow_out[:, :, 1] = fv\n\n    return flow_out\n\ndef hflip_flow(flow):\n\n    flow_out = cv2.flip(flow, flipCode=0)\n    flow_out[:, :, 0] = flow_out[:, :, 0] * (-1)\n\n    return flow_out\n\ndef vflip_flow(flow):\n\n    flow_out = cv2.flip(flow, flipCode=1)\n    flow_out[:, :, 1] = flow_out[:, :, 1] * (-1)\n\n    return flow_out\n\ndef flow_to_rgb(flow):\n    """"""\n    Convert flow into middlebury color code image\n    :param flow: optical flow map\n    :return: optical flow image in middlebury color\n    """"""\n    u = flow[:, :, 0]\n    v = flow[:, :, 1]\n\n    maxu = -999.\n    maxv = -999.\n    minu = 999.\n    minv = 999.\n\n    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n    u[idxUnknow] = 0\n    v[idxUnknow] = 0\n\n    maxu = max(maxu, np.max(u))\n    minu = min(minu, np.min(u))\n\n    maxv = max(maxv, np.max(v))\n    minv = min(minv, np.min(v))\n\n    rad = np.sqrt(u ** 2 + v ** 2)\n    maxrad = max(-1, np.max(rad))\n\n    #print ""max flow: %.4f\\nflow range:\\nu = %.3f .. %.3f\\nv = %.3f .. %.3f"" % (maxrad, minu,maxu, minv, maxv)\n\n    u = u/(maxrad + np.finfo(float).eps)\n    v = v/(maxrad + np.finfo(float).eps)\n\n    img = compute_color(u, v)\n\n    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n    img[idx] = 0\n\n    return np.float32(img) / 255.0\n\n\ndef compute_color(u, v):\n    """"""\n    compute optical flow color map\n    :param u: optical flow horizontal map\n    :param v: optical flow vertical map\n    :return: optical flow in color code\n    """"""\n    [h, w] = u.shape\n    img = np.zeros([h, w, 3])\n    nanIdx = np.isnan(u) | np.isnan(v)\n    u[nanIdx] = 0\n    v[nanIdx] = 0\n\n    colorwheel = make_color_wheel()\n    ncols = np.size(colorwheel, 0)\n\n    rad = np.sqrt(u**2+v**2)\n\n    a = np.arctan2(-v, -u) / np.pi\n\n    fk = (a+1) / 2 * (ncols - 1) + 1\n\n    k0 = np.floor(fk).astype(int)\n\n    k1 = k0 + 1\n    k1[k1 == ncols+1] = 1\n    f = fk - k0\n\n    for i in range(0, np.size(colorwheel,1)):\n        tmp = colorwheel[:, i]\n        col0 = tmp[k0-1] / 255\n        col1 = tmp[k1-1] / 255\n        col = (1-f) * col0 + f * col1\n\n        idx = rad <= 1\n        col[idx] = 1-rad[idx]*(1-col[idx])\n        notidx = np.logical_not(idx)\n\n        col[notidx] *= 0.75\n        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n\n    return img\n\n\ndef make_color_wheel():\n    """"""\n    Generate color wheel according Middlebury color code\n    :return: Color wheel\n    """"""\n    RY = 15\n    YG = 6\n    GC = 4\n    CB = 11\n    BM = 13\n    MR = 6\n\n    ncols = RY + YG + GC + CB + BM + MR\n\n    colorwheel = np.zeros([ncols, 3])\n\n    col = 0\n\n    # RY\n    colorwheel[0:RY, 0] = 255\n    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n    col += RY\n\n    # YG\n    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n    colorwheel[col:col+YG, 1] = 255\n    col += YG\n\n    # GC\n    colorwheel[col:col+GC, 1] = 255\n    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n    col += GC\n\n    # CB\n    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n    colorwheel[col:col+CB, 2] = 255\n    col += CB\n\n    # BM\n    colorwheel[col:col+BM, 2] = 255\n    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n    col += + BM\n\n    # MR\n    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n    colorwheel[col:col+MR, 0] = 255\n\n    return colorwheel\n\n\ndef compute_flow_magnitude(flow):\n\n    flow_mag = flow[:, :, 0] ** 2 + flow[:, :, 1] ** 2\n\n    return flow_mag\n\ndef compute_flow_gradients(flow):\n\n    H = flow.shape[0]\n    W = flow.shape[1]\n\n    flow_x_du = np.zeros((H, W))\n    flow_x_dv = np.zeros((H, W))\n    flow_y_du = np.zeros((H, W))\n    flow_y_dv = np.zeros((H, W))\n    \n    flow_x = flow[:, :, 0]\n    flow_y = flow[:, :, 1]\n\n    flow_x_du[:, :-1] = flow_x[:, :-1] - flow_x[:, 1:]\n    flow_x_dv[:-1, :] = flow_x[:-1, :] - flow_x[1:, :]\n    flow_y_du[:, :-1] = flow_y[:, :-1] - flow_y[:, 1:]\n    flow_y_dv[:-1, :] = flow_y[:-1, :] - flow_y[1:, :]\n\n    return flow_x_du, flow_x_dv, flow_y_du, flow_y_dv\n\n\ndef detect_occlusion(fw_flow, bw_flow):\n    \n    ## fw-flow: img1 => img2\n    ## bw-flow: img2 => img1\n\n    \n    with torch.no_grad():\n\n        ## convert to tensor\n        fw_flow_t = img2tensor(fw_flow).cuda()\n        bw_flow_t = img2tensor(bw_flow).cuda()\n\n        ## warp fw-flow to img2\n        flow_warping = Resample2d().cuda()\n        fw_flow_w = flow_warping(fw_flow_t, bw_flow_t)\n    \n        ## convert to numpy array\n        fw_flow_w = tensor2img(fw_flow_w)\n\n\n    ## occlusion\n    fb_flow_sum = fw_flow_w + bw_flow\n    fb_flow_mag = compute_flow_magnitude(fb_flow_sum)\n    fw_flow_w_mag = compute_flow_magnitude(fw_flow_w)\n    bw_flow_mag = compute_flow_magnitude(bw_flow)\n\n    mask1 = fb_flow_mag > 0.01 * (fw_flow_w_mag + bw_flow_mag) + 0.5\n    \n    ## motion boundary\n    fx_du, fx_dv, fy_du, fy_dv = compute_flow_gradients(bw_flow)\n    fx_mag = fx_du ** 2 + fx_dv ** 2\n    fy_mag = fy_du ** 2 + fy_dv ** 2\n    \n    mask2 = (fx_mag + fy_mag) > 0.01 * bw_flow_mag + 0.002\n\n    ## combine mask\n    mask = np.logical_or(mask1, mask2)\n    occlusion = np.zeros((fw_flow.shape[0], fw_flow.shape[1]))\n    occlusion[mask == 1] = 1\n\n    return occlusion\n\n######################################################################################\n##  Other utility\n######################################################################################\n\ndef save_vector_to_txt(matrix, filename):\n\n    with open(filename, \'w\') as f:\n\n        print(""Save %s"" %filename)\n        \n        for i in range(matrix.size):\n            line = ""%f"" %matrix[i]\n            f.write(""%s\\n""%line)\n\ndef run_cmd(cmd):\n    print(cmd)\n    subprocess.call(cmd, shell=True)\n\ndef make_video(input_dir, img_fmt, video_filename, fps=24):\n\n    cmd = ""ffmpeg -y -loglevel error -framerate %s -i %s/%s -vcodec libx264 -pix_fmt yuv420p -vf \\""scale=trunc(iw/2)*2:trunc(ih/2)*2\\"" %s"" \\\n            %(fps, input_dir, img_fmt, video_filename)\n\n    run_cmd(cmd)\n'"
get_mask/data/create_json.py,0,"b'import json\nimport os\nimport re\nimport numpy as np\nimport cv2\n\nfrom glob import glob\nfrom fire import Fire\n\ndef process(dataset_name):\n    with open(os.path.join(dataset_name, \'list.txt\'), \'r\') as f:\n        lines = f.readlines()\n    videos = [x.strip() for x in lines]\n\n    # if dataset_name == \'VOT2016\':\n    meta_data = {}\n    tags = []\n    for video in videos:\n        with open(os.path.join(dataset_name, video, ""groundtruth.txt""),\'r\') as f:\n            gt_traj = [list(map(float, x.strip().split(\',\'))) for x in f.readlines()]\n\n        img_names = sorted(glob(os.path.join(dataset_name, video, \'color\', \'*.jpg\')))\n        if len(img_names) == 0:\n            img_names = sorted(glob(os.path.join(dataset_name, video, \'*.jpg\')))\n        im = cv2.imread(img_names[0])\n        img_names = [x.split(\'/\', 1)[1] for x in img_names]\n        # tag\n        if dataset_name in [\'VOT2018\', \'VOT2019\']:\n            tag_file = os.path.join(dataset_name, video, \'camera_motion.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    camera_motion = [int(x.strip()) for x in f.readlines()]\n                    camera_motion += [0] * (len(gt_traj) - len(camera_motion))\n            else:\n                print(""File not exists: "", tag_file)\n                camera_motion = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'illum_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    illum_change = [int(x.strip()) for x in f.readlines()]\n                    illum_change += [0] * (len(gt_traj) - len(illum_change))\n            else:\n                print(""File not exists: "", tag_file)\n                illum_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'motion_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    motion_change = [int(x.strip()) for x in f.readlines()]\n                    motion_change  += [0] * (len(gt_traj) - len(motion_change))\n            else:\n                print(""File not exists: "", tag_file)\n                motion_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'size_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    size_change = [int(x.strip()) for x in f.readlines()]\n                    size_change  += [0] * (len(gt_traj) - len(size_change))\n            else:\n                print(""File not exists: "", tag_file)\n                size_change  = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'occlusion.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    occlusion = [int(x.strip()) for x in f.readlines()]\n                    occlusion  += [0] * (len(gt_traj) - len(occlusion))\n            else:\n                print(""File not exists: "", tag_file)\n                occlusion  = [] # [0] * len(gt_traj)\n            img_files = os.path.join(\'VOT2019\', )\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0],\n                                \'gt_rect\': gt_traj,\n                                \'camera_motion\': camera_motion,\n                                \'illum_change\': illum_change,\n                                \'motion_change\': motion_change,\n                                \'size_change\': size_change,\n                                \'occlusion\': occlusion}\n        elif \'VOT2016\' == dataset_name:\n            tag_file = os.path.join(dataset_name, video, \'camera_motion.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    camera_motion = [int(x.strip()) for x in f.readlines()]\n                    camera_motion += [0] * (len(gt_traj) - len(camera_motion))\n            else:\n                print(""File not exists: "", tag_file)\n                camera_motion = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'illum_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    illum_change = [int(x.strip()) for x in f.readlines()]\n                    illum_change += [0] * (len(gt_traj) - len(illum_change))\n            else:\n                print(""File not exists: "", tag_file)\n                illum_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'motion_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    motion_change = [int(x.strip()) for x in f.readlines()]\n                    motion_change  += [0] * (len(gt_traj) - len(motion_change))\n            else:\n                print(""File not exists: "", tag_file)\n                motion_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'size_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    size_change = [int(x.strip()) for x in f.readlines()]\n                    size_change  += [0] * (len(gt_traj) - len(size_change))\n            else:\n                print(""File not exists: "", tag_file)\n                size_change  = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'occlusion.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    occlusion = [int(x.strip()) for x in f.readlines()]\n                    occlusion  += [0] * (len(gt_traj) - len(occlusion))\n            else:\n                print(""File not exists: "", tag_file)\n                occlusion  = [] # [0] * len(gt_traj)\n\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'gt_rect\': gt_traj,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0],\n                                \'camera_motion\': camera_motion,\n                                \'illum_change\': illum_change,\n                                \'motion_change\': motion_change,\n                                \'size_change\': size_change,\n                                \'occlusion\': occlusion}\n        else:\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'gt_rect\': gt_traj,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0]}\n\n\n    json.dump(meta_data, open(dataset_name+\'.json\', \'w\'))\n\nif __name__ == \'__main__\':\n    Fire(process)\n\n'"
get_mask/models/__init__.py,0,b''
get_mask/models/custom.py,5,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom get_mask.models.siammask import SiamMask\nfrom get_mask.models.features import Features\nfrom get_mask.models.rpn import RPN, DepthCorr\nfrom get_mask.models.mask import Mask\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom get_mask.utils.load_helper import load_pretrain\nfrom get_mask.models.resnet import resnet50\n\n\nclass ResDownS(nn.Module):\n    def __init__(self, inplane, outplane):\n        super(ResDownS, self).__init__()\n        self.downsample = nn.Sequential(\n                nn.Conv2d(inplane, outplane, kernel_size=1, bias=False),\n                nn.BatchNorm2d(outplane))\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if x.size(3) < 20:\n            l, r = 4, -4\n            x = x[:, :, l:r, l:r]\n        return x\n\n\nclass ResDown(Features):\n    def __init__(self, pretrain=False):\n        super(ResDown, self).__init__()\n        self.features = resnet50(layer3=True, layer4=False)\n        if pretrain:\n            load_pretrain(self.features, \'resnet.model\')\n\n        self.downsample = ResDownS(1024, 256)\n\n    def forward(self, x):\n        output = self.features(x)\n        p3 = self.downsample(output[-1])\n        return p3\n\n    def forward_all(self, x):\n        output = self.features(x)\n        p3 = self.downsample(output[-1])\n        return output, p3\n\n\nclass UP(RPN):\n    def __init__(self, anchor_num=5, feature_in=256, feature_out=256):\n        super(UP, self).__init__()\n\n        self.anchor_num = anchor_num\n        self.feature_in = feature_in\n        self.feature_out = feature_out\n\n        self.cls_output = 2 * self.anchor_num\n        self.loc_output = 4 * self.anchor_num\n\n        self.cls = DepthCorr(feature_in, feature_out, self.cls_output)\n        self.loc = DepthCorr(feature_in, feature_out, self.loc_output)\n\n    def forward(self, z_f, x_f):\n        cls = self.cls(z_f, x_f)\n        loc = self.loc(z_f, x_f)\n        return cls, loc\n\n\nclass MaskCorr(Mask):\n    def __init__(self, oSz=63):\n        super(MaskCorr, self).__init__()\n        self.oSz = oSz\n        self.mask = DepthCorr(256, 256, self.oSz**2)\n\n    def forward(self, z, x):\n        return self.mask(z, x)\n\n\nclass Refine(nn.Module):\n    def __init__(self):\n        """"""\n        Mask refinement module\n        Please refer SiamMask (Appendix A)\n        https://arxiv.org/abs/1812.05050\n        """"""\n        super(Refine, self).__init__()\n        self.v0 = nn.Sequential(nn.Conv2d(64, 16, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(16, 4, 3, padding=1), nn.ReLU())\n\n        self.v1 = nn.Sequential(nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(64, 16, 3, padding=1), nn.ReLU())\n\n        self.v2 = nn.Sequential(nn.Conv2d(512, 128, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(128, 32, 3, padding=1), nn.ReLU())\n\n        self.h2 = nn.Sequential(nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(32, 32, 3, padding=1), nn.ReLU())\n\n        self.h1 = nn.Sequential(nn.Conv2d(16, 16, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(16, 16, 3, padding=1), nn.ReLU())\n\n        self.h0 = nn.Sequential(nn.Conv2d(4, 4, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(4, 4, 3, padding=1), nn.ReLU())\n\n        self.deconv = nn.ConvTranspose2d(256, 32, 15, 15)\n\n        self.post0 = nn.Conv2d(32, 16, 3, padding=1)\n        self.post1 = nn.Conv2d(16, 4, 3, padding=1)\n        self.post2 = nn.Conv2d(4, 1, 3, padding=1)\n\n    def forward(self, f, corr_feature, pos=None):\n        p0 = torch.nn.functional.pad(f[0], [16,16,16,16])[:, :, 4*pos[0]:4*pos[0]+60, 4*pos[1]:4*pos[1]+60]\n        p1 = torch.nn.functional.pad(f[1], [8,8,8,8])[:, :, 2*pos[0]:2*pos[0]+30, 2*pos[1]:2*pos[1]+30]\n        p2 = torch.nn.functional.pad(f[2], [4,4,4,4])[:, :, pos[0]:pos[0]+15, pos[1]:pos[1]+15]\n\n        p3 = corr_feature[:, :, pos[0], pos[1]].view(-1, 256, 1, 1)\n\n        out = self.deconv(p3)\n        out = self.post0(F.upsample(self.h2(out) + self.v2(p2), size=(30, 30)))\n        out = self.post1(F.upsample(self.h1(out) + self.v1(p1), size=(60, 60)))\n        out = self.post2(F.upsample(self.h0(out) + self.v0(p0), size=(120, 120)))\n        out = out.view(-1, 120*120)\n        return out\n\n\nclass Custom(SiamMask):\n    def __init__(self, pretrain=False, **kwargs):\n        super(Custom, self).__init__(**kwargs)\n        self.features = ResDown(pretrain=pretrain)\n        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=256, feature_out=256)\n        self.mask_model = MaskCorr()\n        self.refine_model = Refine()\n\n    def refine(self, f, pos=None):\n        return self.refine_model(f, pos)\n\n    def template(self, template):\n        self.zf = self.features(template)\n\n    def track(self, search):\n        search = self.features(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)\n        return rpn_pred_cls, rpn_pred_loc\n\n    def track_mask(self, search):\n        self.feature, self.search = self.features.forward_all(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, self.search)\n        self.corr_feature = self.mask_model.mask.forward_corr(self.zf, self.search)\n        pred_mask = self.mask_model.mask.head(self.corr_feature)\n        return rpn_pred_cls, rpn_pred_loc, pred_mask\n\n    def track_refine(self, pos):\n        pred_mask = self.refine_model(self.feature, self.corr_feature, pos=pos)\n        return pred_mask\n\n'"
get_mask/models/features.py,1,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\n\nclass Features(nn.Module):\n    def __init__(self):\n        super(Features, self).__init__()\n        self.feature_size = -1\n\n    def forward(self, x):\n        raise NotImplementedError\n'"
get_mask/models/mask.py,1,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\n\n\nclass Mask(nn.Module):\n    def __init__(self):\n        super(Mask, self).__init__()\n\n    def forward(self, z_f, x_f):\n        raise NotImplementedError\n\n    def template(self, template):\n        raise NotImplementedError\n\n    def track(self, search):\n        raise NotImplementedError\n'"
get_mask/models/resnet.py,11,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom get_mask.models.features import Features\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(Features):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # padding = (2 - stride) + (dilation // 2 - 1)\n        padding = 2 - stride\n        assert stride==1 or dilation==1, ""stride and dilation must have one equals to zero at least""\n        if dilation > 1:\n            padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if out.size() != residual.size():\n            print(out.size(), residual.size())\n        out += residual\n\n        out = self.relu(out)\n\n        return out\n\n\n\nclass Bottleneck_nop(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck_nop, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        s = residual.size(3)\n        residual = residual[:, :, 1:s-1, 1:s-1]\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, layer4=False, layer3=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, # 3\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 31x31, 15x15\n\n        self.feature_size = 128 * block.expansion\n\n        if layer3:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2) # 15x15, 7x7\n            self.feature_size = (256 + 128) * block.expansion\n        else:\n            self.layer3 = lambda x:x # identity\n\n        if layer4:\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4) # 7x7, 3x3\n            self.feature_size = 512 * block.expansion\n        else:\n            self.layer4 = lambda x:x  # identity\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        dd = dilation\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if stride == 1 and dilation == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n            else:\n                if dilation > 1:\n                    dd = dilation // 2\n                    padding = dd\n                else:\n                    dd = 1\n                    padding = 0\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=3, stride=stride, bias=False,\n                              padding=padding, dilation=dd),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n\n        layers = []\n        # layers.append(block(self.inplanes, planes, stride, downsample, dilation=dilation))\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation=dd))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        p0 = self.relu(x)\n        x = self.maxpool(p0)\n\n        p1 = self.layer1(x)\n        p2 = self.layer2(p1)\n        p3 = self.layer3(p2)\n\n        return p0, p1, p2, p3\n\n\nclass ResAdjust(nn.Module):\n    def __init__(self,\n            block=Bottleneck,\n            out_channels=256,\n            adjust_number=1,\n            fuse_layers=[2,3,4]):\n        super(ResAdjust, self).__init__()\n        self.fuse_layers = set(fuse_layers)\n\n        if 2 in self.fuse_layers:\n            self.layer2 = self._make_layer(block, 128, 1, out_channels, adjust_number)\n        if 3 in self.fuse_layers:\n            self.layer3 = self._make_layer(block, 256, 2, out_channels, adjust_number)\n        if 4 in self.fuse_layers:\n            self.layer4 = self._make_layer(block, 512, 4, out_channels, adjust_number)\n\n        self.feature_size = out_channels * len(self.fuse_layers)\n\n\n    def _make_layer(self, block, plances, dilation, out, number=1):\n\n        layers = []\n\n        for _ in range(number):\n            layer = block(plances * block.expansion, plances, dilation=dilation)\n            layers.append(layer)\n\n        downsample = nn.Sequential(\n                nn.Conv2d(plances * block.expansion, out, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out)\n                )\n        layers.append(downsample)\n\n        return nn.Sequential(*layers)\n\n    def forward(self, p2, p3, p4):\n\n        outputs = []\n\n        if 2 in self.fuse_layers:\n            outputs.append(self.layer2(p2))\n        if 3 in self.fuse_layers:\n            outputs.append(self.layer3(p3))\n        if 4 in self.fuse_layers:\n            outputs.append(self.layer4(p4))\n        # return torch.cat(outputs, 1)\n        return outputs\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\nif __name__ == \'__main__\':\n    net = resnet50()\n    print(net)\n    net = net.cuda()\n\n    var = torch.FloatTensor(1,3,127,127).cuda()\n    var = Variable(var)\n\n    net(var)\n    print(\'*************\')\n    var = torch.FloatTensor(1,3,255,255).cuda()\n    var = Variable(var)\n\n    net(var)\n\n'"
get_mask/models/rpn.py,2,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RPN(nn.Module):\n    def __init__(self):\n        super(RPN, self).__init__()\n\n    def forward(self, z_f, x_f):\n        raise NotImplementedError\n\n    def template(self, template):\n        raise NotImplementedError\n\n    def track(self, search):\n        raise NotImplementedError\n\n\ndef conv2d_dw_group(x, kernel):\n    batch, channel = kernel.shape[:2]\n    x = x.view(1, batch*channel, x.size(2), x.size(3))  # 1 * (b*c) * k * k\n    kernel = kernel.view(batch*channel, 1, kernel.size(2), kernel.size(3))  # (b*c) * 1 * H * W\n    out = F.conv2d(x, kernel, groups=batch*channel)\n    out = out.view(batch, channel, out.size(2), out.size(3))\n    return out\n\n\nclass DepthCorr(nn.Module):\n    def __init__(self, in_channels, hidden, out_channels, kernel_size=3):\n        super(DepthCorr, self).__init__()\n        # adjust layer for asymmetrical features\n        self.conv_kernel = nn.Sequential(\n                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                )\n        self.conv_search = nn.Sequential(\n                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                )\n\n        self.head = nn.Sequential(\n                nn.Conv2d(hidden, hidden, kernel_size=1, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden, out_channels, kernel_size=1)\n                )\n\n    def forward_corr(self, kernel, input):\n        kernel = self.conv_kernel(kernel)\n        input = self.conv_search(input)\n        feature = conv2d_dw_group(input, kernel)\n        return feature\n\n    def forward(self, kernel, search):\n        feature = self.forward_corr(kernel, search)\n        out = self.head(feature)\n        return out\n'"
get_mask/models/siammask.py,2,"b'import torch\nimport torch.nn as nn\nfrom get_mask.utils.anchors import Anchors\n\n\nclass SiamMask(nn.Module):\n    def __init__(self, anchors=None, o_sz=127, g_sz=127):\n        super(SiamMask, self).__init__()\n        self.anchors = anchors  # anchor_cfg\n        self.anchor_num = len(self.anchors[""ratios""]) * len(self.anchors[""scales""])\n        self.anchor = Anchors(anchors)\n        self.features = None\n        self.rpn_model = None\n        self.mask_model = None\n        self.o_sz = o_sz\n        self.g_sz = g_sz\n        self.all_anchors = None\n\n    def set_all_anchors(self, image_center, size):\n        # cx,cy,w,h\n        if not self.anchor.generate_all_anchors(image_center, size):\n            return\n        all_anchors = self.anchor.all_anchors[1]  # cx, cy, w, h\n        self.all_anchors = torch.from_numpy(all_anchors).float().cuda()\n        self.all_anchors = [self.all_anchors[i] for i in range(4)]\n\n    def feature_extractor(self, x):\n        return self.features(x)\n\n    def rpn(self, template, search):\n        pred_cls, pred_loc = self.rpn_model(template, search)\n        return pred_cls, pred_loc\n\n    def mask(self, template, search):\n        pred_mask = self.mask_model(template, search)\n        return pred_mask\n\n    def template(self, z):\n        self.zf = self.feature_extractor(z)\n        cls_kernel, loc_kernel = self.rpn_model.template(self.zf)\n        return cls_kernel, loc_kernel\n\n    def track(self, x, cls_kernel=None, loc_kernel=None, softmax=False):\n        xf = self.feature_extractor(x)\n        rpn_pred_cls, rpn_pred_loc = self.rpn_model.track(xf, cls_kernel, loc_kernel)\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc\n'"
get_mask/utils/__init__.py,0,b''
get_mask/utils/anchors.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\nimport math\nfrom get_mask.utils.bbox_helper import center2corner, corner2center\n\n\nclass Anchors:\n    def __init__(self, cfg):\n        self.stride = 8\n        self.ratios = [0.33, 0.5, 1, 2, 3]\n        self.scales = [8]\n        self.round_dight = 0\n        self.image_center = 0\n        self.size = 0\n\n        self.__dict__.update(cfg)\n\n        self.anchor_num = len(self.scales) * len(self.ratios)\n        self.anchors = None  # in single position (anchor_num*4)\n        self.all_anchors = None  # in all position 2*(4*anchor_num*h*w)\n        self.generate_anchors()\n\n    def generate_anchors(self):\n        self.anchors = np.zeros((self.anchor_num, 4), dtype=np.float32)\n\n        size = self.stride * self.stride\n        count = 0\n        for r in self.ratios:\n            if self.round_dight > 0:\n                ws = round(math.sqrt(size*1. / r), self.round_dight)\n                hs = round(ws * r, self.round_dight)\n            else:\n                ws = int(math.sqrt(size*1. / r))\n                hs = int(ws * r)\n\n            for s in self.scales:\n                w = ws * s\n                h = hs * s\n                self.anchors[count][:] = [-w*0.5, -h*0.5, w*0.5, h*0.5][:]\n                count += 1\n\n    def generate_all_anchors(self, im_c, size):\n        if self.image_center == im_c and self.size == size:\n            return False\n        self.image_center = im_c\n        self.size = size\n\n        a0x = im_c - size // 2 * self.stride\n        ori = np.array([a0x] * 4, dtype=np.float32)\n        zero_anchors = self.anchors + ori\n\n        x1 = zero_anchors[:, 0]\n        y1 = zero_anchors[:, 1]\n        x2 = zero_anchors[:, 2]\n        y2 = zero_anchors[:, 3]\n\n        x1, y1, x2, y2 = map(lambda x: x.reshape(self.anchor_num, 1, 1), [x1, y1, x2, y2])\n        cx, cy, w, h = corner2center([x1, y1, x2, y2])\n\n        disp_x = np.arange(0, size).reshape(1, 1, -1) * self.stride\n        disp_y = np.arange(0, size).reshape(1, -1, 1) * self.stride\n\n        cx = cx + disp_x\n        cy = cy + disp_y\n\n        # broadcast\n        zero = np.zeros((self.anchor_num, size, size), dtype=np.float32)\n        cx, cy, w, h = map(lambda x: x + zero, [cx, cy, w, h])\n        x1, y1, x2, y2 = center2corner([cx, cy, w, h])\n\n        self.all_anchors = np.stack([x1, y1, x2, y2]), np.stack([cx, cy, w, h])\n        return True\n\n\n'"
get_mask/utils/bbox_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\nfrom collections import namedtuple\n\nCorner = namedtuple(\'Corner\', \'x1 y1 x2 y2\')\nBBox = Corner\nCenter = namedtuple(\'Center\', \'x y w h\')\n\n\ndef corner2center(corner):\n    """"""\n    :param corner: Corner or np.array 4*N\n    :return: Center or 4 np.array N\n    """"""\n    if isinstance(corner, Corner):\n        x1, y1, x2, y2 = corner\n        return Center((x1 + x2) * 0.5, (y1 + y2) * 0.5, (x2 - x1), (y2 - y1))\n    else:\n        x1, y1, x2, y2 = corner[0], corner[1], corner[2], corner[3]\n        x = (x1 + x2) * 0.5\n        y = (y1 + y2) * 0.5\n        w = x2 - x1\n        h = y2 - y1\n        return x, y, w, h\n\n\ndef center2corner(center):\n    """"""\n    :param center: Center or np.array 4*N\n    :return: Corner or np.array 4*N\n    """"""\n    if isinstance(center, Center):\n        x, y, w, h = center\n        return Corner(x - w * 0.5, y - h * 0.5, x + w * 0.5, y + h * 0.5)\n    else:\n        x, y, w, h = center[0], center[1], center[2], center[3]\n        x1 = x - w * 0.5\n        y1 = y - h * 0.5\n        x2 = x + w * 0.5\n        y2 = y + h * 0.5\n        return x1, y1, x2, y2\n\n\ndef cxy_wh_2_rect(pos, sz):\n    return np.array([pos[0]-sz[0]/2, pos[1]-sz[1]/2, sz[0], sz[1]])  # 0-index\n\n\ndef get_axis_aligned_bbox(region):\n    nv = region.size\n    if nv == 8:\n        cx = np.mean(region[0::2])\n        cy = np.mean(region[1::2])\n        x1 = min(region[0::2])\n        x2 = max(region[0::2])\n        y1 = min(region[1::2])\n        y2 = max(region[1::2])\n        A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n        A2 = (x2 - x1) * (y2 - y1)\n        s = np.sqrt(A1 / A2)\n        w = s * (x2 - x1) + 1\n        h = s * (y2 - y1) + 1\n    else:\n        x = region[0]\n        y = region[1]\n        w = region[2]\n        h = region[3]\n        cx = x+w/2\n        cy = y+h/2\n\n    return cx, cy, w, h\n\n\n'"
get_mask/utils/benchmark_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join, realpath, dirname, exists, isdir\nfrom os import listdir\nimport logging\nimport glob\nimport numpy as np\nimport json\nfrom collections import OrderedDict\n\n\ndef get_dataset_zoo():\n    root = realpath(join(dirname(__file__), \'../data\'))\n    zoos = listdir(root)\n\n    def valid(x):\n        y = join(root, x)\n        if not isdir(y): return False\n\n        return exists(join(y, \'list.txt\')) \\\n               or exists(join(y, \'train\', \'meta.json\'))\\\n               or exists(join(y, \'ImageSets\', \'2016\', \'val.txt\'))\n\n    zoos = list(filter(valid, zoos))\n    return zoos\n\n\ndataset_zoo = get_dataset_zoo()\n\n\ndef load_dataset(dataset):\n    info = OrderedDict()\n    if \'VOT\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', dataset)\n        if not exists(base_path):\n            logging.error(""Please download test dataset!!!"")\n            exit()\n        list_path = join(base_path, \'list.txt\')\n        with open(list_path) as f:\n            videos = [v.strip() for v in f.readlines()]\n        for video in videos:\n            video_path = join(base_path, video)\n            image_path = join(video_path, \'*.jpg\')\n            image_files = sorted(glob.glob(image_path))\n            if len(image_files) == 0:  # VOT2018\n                image_path = join(video_path, \'color\', \'*.jpg\')\n                image_files = sorted(glob.glob(image_path))\n            gt_path = join(video_path, \'groundtruth.txt\')\n            gt = np.loadtxt(gt_path, delimiter=\',\').astype(np.float64)\n            if gt.shape[1] == 4:\n                gt = np.column_stack((gt[:, 0], gt[:, 1], gt[:, 0], gt[:, 1] + gt[:, 3]-1,\n                                      gt[:, 0] + gt[:, 2]-1, gt[:, 1] + gt[:, 3]-1, gt[:, 0] + gt[:, 2]-1, gt[:, 1]))\n            info[video] = {\'image_files\': image_files, \'gt\': gt, \'name\': video}\n    elif \'DAVIS\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS\')\n        list_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS\', \'ImageSets\', dataset[-4:], \'val.txt\')\n        with open(list_path) as f:\n            videos = [v.strip() for v in f.readlines()]\n        for video in videos:\n            info[video] = {}\n            info[video][\'anno_files\'] = sorted(glob.glob(join(base_path, \'Annotations/480p\', video, \'*.png\')))\n            info[video][\'image_files\'] = sorted(glob.glob(join(base_path, \'JPEGImages/480p\', video, \'*.jpg\')))\n            info[video][\'name\'] = video\n    elif \'ytb_vos\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', \'ytb_vos\', \'valid\')\n        json_path = join(realpath(dirname(__file__)), \'../data\', \'ytb_vos\', \'valid\', \'meta.json\')\n        meta = json.load(open(json_path, \'r\'))\n        meta = meta[\'videos\']\n        info = dict()\n        for v in meta.keys():\n            objects = meta[v][\'objects\']\n            frames = []\n            anno_frames = []\n            info[v] = dict()\n            for obj in objects:\n                frames += objects[obj][\'frames\']\n                anno_frames += [objects[obj][\'frames\'][0]]\n            frames = sorted(np.unique(frames))\n            info[v][\'anno_files\'] = [join(base_path, \'Annotations\', v, im_f+\'.png\') for im_f in frames]\n            info[v][\'anno_init_files\'] = [join(base_path, \'Annotations\', v, im_f + \'.png\') for im_f in anno_frames]\n            info[v][\'image_files\'] = [join(base_path, \'JPEGImages\', v, im_f+\'.jpg\') for im_f in frames]\n            info[v][\'name\'] = v\n\n            info[v][\'start_frame\'] = dict()\n            info[v][\'end_frame\'] = dict()\n            for obj in objects:\n                start_file = objects[obj][\'frames\'][0]\n                end_file = objects[obj][\'frames\'][-1]\n                info[v][\'start_frame\'][obj] = frames.index(start_file)\n                info[v][\'end_frame\'][obj] = frames.index(end_file)\n    else:\n        logging.error(\'Not support\')\n        exit()\n    return info\n'"
get_mask/utils/config_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport json\nfrom os.path import exists\n\n\ndef load_config(args):\n    assert exists(args.config), \'""{}"" not exists\'.format(args.config)\n    config = json.load(open(args.config))\n\n    # deal with network\n    if \'network\' not in config:\n        print(\'Warning: network lost in config. This will be error in next version\')\n\n        config[\'network\'] = {}\n\n        if not args.arch:\n            raise Exception(\'no arch provided\')\n    args.arch = config[\'network\'][\'arch\']\n\n    return config\n\n'"
get_mask/utils/load_helper.py,2,"b'import torch\nimport logging\nlogger = logging.getLogger(\'global\')\n\n\ndef check_keys(model, pretrained_state_dict):\n    ckpt_keys = set(pretrained_state_dict.keys())\n    model_keys = set(model.state_dict().keys())\n    used_pretrained_keys = model_keys & ckpt_keys\n    unused_pretrained_keys = ckpt_keys - model_keys\n    missing_keys = model_keys - ckpt_keys\n    if len(missing_keys) > 0:\n        logger.info(\'[Warning] missing keys: {}\'.format(missing_keys))\n        logger.info(\'missing keys:{}\'.format(len(missing_keys)))\n    if len(unused_pretrained_keys) > 0:\n        logger.info(\'[Warning] unused_pretrained_keys: {}\'.format(unused_pretrained_keys))\n        logger.info(\'unused checkpoint keys:{}\'.format(len(unused_pretrained_keys)))\n    logger.info(\'used keys:{}\'.format(len(used_pretrained_keys)))\n    assert len(used_pretrained_keys) > 0, \'load NONE from pretrained checkpoint\'\n    return True\n\n\ndef remove_prefix(state_dict, prefix):\n    \'\'\' Old style model is stored with all names of parameters share common prefix \'module.\' \'\'\'\n    logger.info(\'remove prefix \\\'{}\\\'\'.format(prefix))\n    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n    return {f(key): value for key, value in state_dict.items()}\n\n\ndef load_pretrain(model, pretrained_path):\n    logger.info(\'load pretrained model from {}\'.format(pretrained_path))\n    device = torch.cuda.current_device()\n    pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n    if ""state_dict"" in pretrained_dict.keys():\n        pretrained_dict = remove_prefix(pretrained_dict[\'state_dict\'], \'module.\')\n    else:\n        pretrained_dict = remove_prefix(pretrained_dict, \'module.\')\n\n    try:\n        check_keys(model, pretrained_dict)\n    except:\n        logger.info(\'[Warning]: using pretrain as features. Adding ""features."" as prefix\')\n        new_dict = {}\n        for k, v in pretrained_dict.items():\n            k = \'features.\' + k\n            new_dict[k] = v\n        pretrained_dict = new_dict\n        check_keys(model, pretrained_dict)\n    model.load_state_dict(pretrained_dict, strict=False)\n    return model\n'"
get_mask/utils/log_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\n\nimport os\nimport logging\nimport sys\n\nif hasattr(sys, \'frozen\'):  # support for py2exe\n    _srcfile = ""logging%s__init__%s"" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in [\'.pyc\', \'.pyo\']:\n    _srcfile = __file__[:-4] + \'.py\'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n\nlogs = set()\n\n\nclass Filter:\n    def __init__(self, flag):\n        self.flag = flag\n\n    def filter(self, x): return self.flag\n\n\nclass Dummy:\n    def __init__(self, *arg, **kwargs):\n        pass\n\n    def __getattr__(self, arg):\n        def dummy(*args, **kwargs): pass\n        return dummy\n\n\ndef get_format(logger, level):\n    if \'SLURM_PROCID\' in os.environ:\n        rank = int(os.environ[\'SLURM_PROCID\'])\n\n        if level == logging.INFO:\n            logger.addFilter(Filter(rank == 0))\n    else:\n        rank = 0\n    format_str = \'[%(asctime)s-rk{}-%(filename)s#%(lineno)3d] %(message)s\'.format(rank)\n    formatter = logging.Formatter(format_str)\n    return formatter\n\n\ndef init_log(name, level = logging.INFO, format_func=get_format):\n    if (name, level) in logs: return\n    logs.add((name, level))\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    formatter = format_func(logger, level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    return logger\n\n\ndef add_file_handler(name, log_file, level = logging.INFO):\n    logger = logging.getLogger(name)\n    fh = logging.FileHandler(log_file)\n    fh.setFormatter(get_format(logger, level))\n    logger.addHandler(fh)\n\n\ninit_log(\'global\')\n\n'"
get_mask/utils/tracker_config.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nfrom get_mask.utils.anchors import Anchors\n\n\nclass TrackerConfig(object):\n    # These are the default hyper-params for SiamMask\n    penalty_k = 0.04\n    window_influence = 0.42\n    lr = 0.25\n    seg_thr = 0.3  # for mask\n    windowing = 'cosine'  # to penalize large displacements [cosine/uniform]\n    # Params from the network architecture, have to be consistent with the training\n    exemplar_size = 127  # input z size\n    instance_size = 255  # input x size (search region)\n    total_stride = 8\n    out_size = 63  # for mask\n    base_size = 8\n    score_size = (instance_size-exemplar_size)//total_stride+1+base_size\n    context_amount = 0.5  # context amount for the exemplar\n    ratios = [0.33, 0.5, 1, 2, 3]\n    scales = [8, ]\n    anchor_num = len(ratios) * len(scales)\n    round_dight = 0\n    anchor = []\n\n    def update(self, newparam=None, anchors=None):\n        if newparam:\n            for key, value in newparam.items():\n                setattr(self, key, value)\n        if anchors is not None:\n            if isinstance(anchors, dict):\n                anchors = Anchors(anchors)\n            if isinstance(anchors, Anchors):\n                self.total_stride = anchors.stride\n                self.ratios = anchors.ratios\n                self.scales = anchors.scales\n                self.round_dight = anchors.round_dight\n        self.renew()\n\n    def renew(self):\n        self.score_size = (self.instance_size - self.exemplar_size) // self.total_stride + 1 + self.base_size\n        self.anchor_num = len(self.ratios) * len(self.scales)\n\n\n\n\n"""
inpainting/lib/FlowNet2.py,28,"b""import torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\nimport math\r\nimport numpy as np\r\nimport sys\r\n\r\nfrom .resample2d_package.modules.resample2d import Resample2d\r\nfrom .channelnorm_package.modules.channelnorm import ChannelNorm\r\n\r\n#import FlowNetC\r\n#import FlowNetS\r\n#import FlowNetSD\r\n#import FlowNetFusion\r\n\r\nfrom . import FlowNetC\r\nfrom . import FlowNetS\r\nfrom . import FlowNetSD\r\nfrom . import FlowNetFusion\r\n\r\nfrom .submodules import *\r\n'Parameter count = 162,518,834'\r\n\r\nclass FlowNet2(nn.Module):\r\n\r\n    def __init__(self, args, batchNorm=False, div_flow = 20., requires_grad=False):\r\n        super(FlowNet2,self).__init__()\r\n        self.batchNorm = batchNorm\r\n        self.div_flow = div_flow\r\n        self.rgb_max = args.rgb_max\r\n        self.args = args\r\n\r\n        self.channelnorm = ChannelNorm()\r\n\r\n        # First Block (FlowNetC)\r\n        self.flownetc = FlowNetC.FlowNetC(args, batchNorm=self.batchNorm)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n        if args.fp16:\r\n            self.resample1 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample1 = Resample2d()\r\n\r\n        # Block (FlowNetS1)\r\n        self.flownets_1 = FlowNetS.FlowNetS(args, batchNorm=self.batchNorm)\r\n        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n        if args.fp16:\r\n            self.resample2 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample2 = Resample2d()\r\n\r\n\r\n        # Block (FlowNetS2)\r\n        self.flownets_2 = FlowNetS.FlowNetS(args, batchNorm=self.batchNorm)\r\n\r\n        # Block (FlowNetSD)\r\n        self.flownets_d = FlowNetSD.FlowNetSD(args, batchNorm=self.batchNorm) \r\n        self.upsample3 = nn.Upsample(scale_factor=4, mode='nearest') \r\n        self.upsample4 = nn.Upsample(scale_factor=4, mode='nearest') \r\n\r\n        if args.fp16:\r\n            self.resample3 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample3 = Resample2d()\r\n\r\n        if args.fp16:\r\n            self.resample4 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample4 = Resample2d()\r\n\r\n        # Block (FLowNetFusion)\r\n        self.flownetfusion = FlowNetFusion.FlowNetFusion(args, batchNorm=self.batchNorm)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n\r\n        if not requires_grad:\r\n            for param in self.parameters():\r\n                param.requires_grad = False\r\n                \r\n\r\n    def init_deconv_bilinear(self, weight):\r\n        f_shape = weight.size()\r\n        heigh, width = f_shape[-2], f_shape[-1]\r\n        f = np.ceil(width/2.0)\r\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\r\n        bilinear = np.zeros([heigh, width])\r\n        for x in range(width):\r\n            for y in range(heigh):\r\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\r\n                bilinear[x, y] = value\r\n        min_dim = min(f_shape[0], f_shape[1])\r\n        weight.data.fill_(0.)\r\n        for i in range(min_dim):\r\n            weight.data[i,i,:,:] = torch.from_numpy(bilinear)\r\n        return \r\n\r\n    def forward(self, img1, img2):\r\n\r\n        ### normaize input\r\n        sz = img1.size()\r\n        img1 = img1.view(sz[0], sz[1], 1, sz[2], sz[3] )\r\n        img2 = img2.view(sz[0], sz[1], 1, sz[2], sz[3] )\r\n\r\n        inputs = torch.cat((img1, img2), dim=2)\r\n\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        \r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x1 = x[:,:,0,:,:]\r\n        x2 = x[:,:,1,:,:]\r\n        x = torch.cat((x1,x2), dim = 1)\r\n\r\n        # flownetc\r\n        flownetc_flow2 = self.flownetc(x)[0]\r\n        flownetc_flow = self.upsample1(flownetc_flow2*self.div_flow)\r\n        \r\n        # warp img1 to img0; magnitude of diff between img0 and and warped_img1, \r\n        resampled_img1 = self.resample1(x[:,3:,:,:], flownetc_flow)\r\n        diff_img0 = x[:,:3,:,:] - resampled_img1 \r\n        norm_diff_img0 = self.channelnorm(diff_img0)\r\n\r\n        # concat img0, img1, img1->img0, flow, diff-mag ; \r\n        concat1 = torch.cat((x, resampled_img1, flownetc_flow/self.div_flow, norm_diff_img0), dim=1)\r\n        \r\n        # flownets1\r\n        flownets1_flow2 = self.flownets_1(concat1)[0]\r\n        flownets1_flow = self.upsample2(flownets1_flow2*self.div_flow) \r\n\r\n        # warp img1 to img0 using flownets1; magnitude of diff between img0 and and warped_img1\r\n        resampled_img1 = self.resample2(x[:,3:,:,:], flownets1_flow)\r\n        diff_img0 = x[:,:3,:,:] - resampled_img1\r\n        norm_diff_img0 = self.channelnorm(diff_img0)\r\n\r\n        # concat img0, img1, img1->img0, flow, diff-mag\r\n        concat2 = torch.cat((x, resampled_img1, flownets1_flow/self.div_flow, norm_diff_img0), dim=1)\r\n\r\n        # flownets2\r\n        flownets2_flow2 = self.flownets_2(concat2)[0]\r\n        flownets2_flow = self.upsample4(flownets2_flow2 * self.div_flow)\r\n        norm_flownets2_flow = self.channelnorm(flownets2_flow)\r\n\r\n        diff_flownets2_flow = self.resample4(x[:,3:,:,:], flownets2_flow)\r\n        #if not diff_flownets2_flow.volatile:\r\n        #    diff_flownets2_flow.register_hook(save_grad(self.args.grads, 'diff_flownets2_flow'))\r\n\r\n        diff_flownets2_img1 = self.channelnorm((x[:,:3,:,:]-diff_flownets2_flow))\r\n        #if not diff_flownets2_img1.volatile:\r\n        #    diff_flownets2_img1.register_hook(save_grad(self.args.grads, 'diff_flownets2_img1'))\r\n\r\n        # flownetsd\r\n        flownetsd_flow2 = self.flownets_d(x)[0]\r\n        flownetsd_flow = self.upsample3(flownetsd_flow2 / self.div_flow)\r\n        norm_flownetsd_flow = self.channelnorm(flownetsd_flow)\r\n        \r\n        diff_flownetsd_flow = self.resample3(x[:,3:,:,:], flownetsd_flow)\r\n        #if not diff_flownetsd_flow.volatile:\r\n        #    diff_flownetsd_flow.register_hook(save_grad(self.args.grads, 'diff_flownetsd_flow'))\r\n\r\n        diff_flownetsd_img1 = self.channelnorm((x[:,:3,:,:]-diff_flownetsd_flow))\r\n        #if not diff_flownetsd_img1.volatile:\r\n        #    diff_flownetsd_img1.register_hook(save_grad(self.args.grads, 'diff_flownetsd_img1'))\r\n\r\n        # concat img1 flownetsd, flownets2, norm_flownetsd, norm_flownets2, diff_flownetsd_img1, diff_flownets2_img1\r\n        concat3 = torch.cat((x[:,:3,:,:], flownetsd_flow, flownets2_flow, norm_flownetsd_flow, norm_flownets2_flow, diff_flownetsd_img1, diff_flownets2_img1), dim=1)\r\n        flownetfusion_flow = self.flownetfusion(concat3)\r\n\r\n        #if not flownetfusion_flow.volatile:\r\n        #    flownetfusion_flow.register_hook(save_grad(self.args.grads, 'flownetfusion_flow'))\r\n\r\n        return flownetfusion_flow\r\n\r\nclass FlowNet2C(FlowNetC.FlowNetC):\r\n    def __init__(self, args, batchNorm=False, div_flow=20):\r\n        super(FlowNet2C,self).__init__(args, batchNorm=batchNorm, div_flow=20)\r\n        self.rgb_max = args.rgb_max\r\n\r\n    def forward(self, inputs):\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        \r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x1 = x[:,:,0,:,:]\r\n        x2 = x[:,:,1,:,:]\r\n\r\n        # FlownetC top input stream\r\n        out_conv1a = self.conv1(x1)\r\n        out_conv2a = self.conv2(out_conv1a)\r\n        out_conv3a = self.conv3(out_conv2a)\r\n\r\n        # FlownetC bottom input stream\r\n        out_conv1b = self.conv1(x2)\r\n        \r\n        out_conv2b = self.conv2(out_conv1b)\r\n        out_conv3b = self.conv3(out_conv2b)\r\n\r\n        # Merge streams\r\n        out_corr = self.corr(out_conv3a, out_conv3b) # False\r\n        out_corr = self.corr_activation(out_corr)\r\n\r\n        # Redirect top input stream and concatenate\r\n        out_conv_redir = self.conv_redir(out_conv3a)\r\n\r\n        in_conv3_1 = torch.cat((out_conv_redir, out_corr), 1)\r\n\r\n        # Merged conv layers\r\n        out_conv3_1 = self.conv3_1(in_conv3_1)\r\n\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3_1))\r\n\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n\r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n\r\n        flow5       = self.predict_flow5(concat5)\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n\r\n        flow4       = self.predict_flow4(concat4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        concat3 = torch.cat((out_conv3_1,out_deconv3,flow4_up),1)\r\n\r\n        flow3       = self.predict_flow3(concat3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n        concat2 = torch.cat((out_conv2a,out_deconv2,flow3_up),1)\r\n\r\n        flow2 = self.predict_flow2(concat2)\r\n\r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6\r\n        else:\r\n            return self.upsample1(flow2*self.div_flow)\r\n\r\nclass FlowNet2S(FlowNetS.FlowNetS):\r\n    def __init__(self, args, batchNorm=False, div_flow=20):\r\n        super(FlowNet2S,self).__init__(args, input_channels = 6, batchNorm=batchNorm)\r\n        self.rgb_max = args.rgb_max\r\n        self.div_flow = div_flow\r\n        \r\n    def forward(self, inputs):\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x = torch.cat( (x[:,:,0,:,:], x[:,:,1,:,:]), dim = 1)\r\n\r\n        out_conv1 = self.conv1(x)\r\n\r\n        out_conv2 = self.conv2(out_conv1)\r\n        out_conv3 = self.conv3_1(self.conv3(out_conv2))\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n        \r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n        flow5       = self.predict_flow5(concat5)\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        \r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n        flow4       = self.predict_flow4(concat4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        \r\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\r\n        flow3       = self.predict_flow3(concat3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n\r\n        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\r\n        flow2 = self.predict_flow2(concat2)\r\n\r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6\r\n        else:\r\n            return self.upsample1(flow2*self.div_flow)\r\n\r\nclass FlowNet2SD(FlowNetSD.FlowNetSD):\r\n    def __init__(self, args, batchNorm=False, div_flow=20):\r\n        super(FlowNet2SD,self).__init__(args, batchNorm=batchNorm)\r\n        self.rgb_max = args.rgb_max\r\n        self.div_flow = div_flow\r\n\r\n    def forward(self, inputs):\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x = torch.cat( (x[:,:,0,:,:], x[:,:,1,:,:]), dim = 1)\r\n\r\n        out_conv0 = self.conv0(x)\r\n        out_conv1 = self.conv1_1(self.conv1(out_conv0))\r\n        out_conv2 = self.conv2_1(self.conv2(out_conv1))\r\n\r\n        out_conv3 = self.conv3_1(self.conv3(out_conv2))\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n        \r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n        out_interconv5 = self.inter_conv5(concat5)\r\n        flow5       = self.predict_flow5(out_interconv5)\r\n\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        \r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n        out_interconv4 = self.inter_conv4(concat4)\r\n        flow4       = self.predict_flow4(out_interconv4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        \r\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\r\n        out_interconv3 = self.inter_conv3(concat3)\r\n        flow3       = self.predict_flow3(out_interconv3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n\r\n        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\r\n        out_interconv2 = self.inter_conv2(concat2)\r\n        flow2 = self.predict_flow2(out_interconv2)\r\n\r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6\r\n        else:\r\n            return self.upsample1(flow2*self.div_flow)\r\n\r\nclass FlowNet2CS(nn.Module):\r\n\r\n    def __init__(self, args, batchNorm=False, div_flow = 20.):\r\n        super(FlowNet2CS,self).__init__()\r\n        self.batchNorm = batchNorm\r\n        self.div_flow = div_flow\r\n        self.rgb_max = args.rgb_max\r\n        self.args = args\r\n\r\n        self.channelnorm = ChannelNorm()\r\n\r\n        # First Block (FlowNetC)\r\n        self.flownetc = FlowNetC.FlowNetC(args, batchNorm=self.batchNorm)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n        if args.fp16:\r\n            self.resample1 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample1 = Resample2d()\r\n\r\n        # Block (FlowNetS1)\r\n        self.flownets_1 = FlowNetS.FlowNetS(args, batchNorm=self.batchNorm)\r\n        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n\r\n    def forward(self, inputs):\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        \r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x1 = x[:,:,0,:,:]\r\n        x2 = x[:,:,1,:,:]\r\n        x = torch.cat((x1,x2), dim = 1)\r\n\r\n        # flownetc\r\n        flownetc_flow2 = self.flownetc(x)[0]\r\n        flownetc_flow = self.upsample1(flownetc_flow2*self.div_flow)\r\n        \r\n        # warp img1 to img0; magnitude of diff between img0 and and warped_img1, \r\n        resampled_img1 = self.resample1(x[:,3:,:,:], flownetc_flow)\r\n        diff_img0 = x[:,:3,:,:] - resampled_img1 \r\n        norm_diff_img0 = self.channelnorm(diff_img0)\r\n\r\n        # concat img0, img1, img1->img0, flow, diff-mag ; \r\n        concat1 = torch.cat((x, resampled_img1, flownetc_flow/self.div_flow, norm_diff_img0), dim=1)\r\n        \r\n        # flownets1\r\n        flownets1_flow2 = self.flownets_1(concat1)[0]\r\n        flownets1_flow = self.upsample2(flownets1_flow2*self.div_flow) \r\n\r\n        return flownets1_flow\r\n\r\nclass FlowNet2CSS(nn.Module):\r\n\r\n    def __init__(self, args, batchNorm=False, div_flow = 20.):\r\n        super(FlowNet2CSS,self).__init__()\r\n        self.batchNorm = batchNorm\r\n        self.div_flow = div_flow\r\n        self.rgb_max = args.rgb_max\r\n        self.args = args\r\n\r\n        self.channelnorm = ChannelNorm()\r\n\r\n        # First Block (FlowNetC)\r\n        self.flownetc = FlowNetC.FlowNetC(args, batchNorm=self.batchNorm)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n        if args.fp16:\r\n            self.resample1 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample1 = Resample2d()\r\n\r\n        # Block (FlowNetS1)\r\n        self.flownets_1 = FlowNetS.FlowNetS(args, batchNorm=self.batchNorm)\r\n        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n        if args.fp16:\r\n            self.resample2 = nn.Sequential(\r\n                            tofp32(), \r\n                            Resample2d(),\r\n                            tofp16()) \r\n        else:\r\n            self.resample2 = Resample2d()\r\n\r\n\r\n        # Block (FlowNetS2)\r\n        self.flownets_2 = FlowNetS.FlowNetS(args, batchNorm=self.batchNorm)\r\n        self.upsample3 = nn.Upsample(scale_factor=4, mode='nearest') \r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n\r\n    def forward(self, inputs):\r\n        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))\r\n        \r\n        x = (inputs - rgb_mean) / self.rgb_max\r\n        x1 = x[:,:,0,:,:]\r\n        x2 = x[:,:,1,:,:]\r\n        x = torch.cat((x1,x2), dim = 1)\r\n\r\n        # flownetc\r\n        flownetc_flow2 = self.flownetc(x)[0]\r\n        flownetc_flow = self.upsample1(flownetc_flow2*self.div_flow)\r\n        \r\n        # warp img1 to img0; magnitude of diff between img0 and and warped_img1, \r\n        resampled_img1 = self.resample1(x[:,3:,:,:], flownetc_flow)\r\n        diff_img0 = x[:,:3,:,:] - resampled_img1 \r\n        norm_diff_img0 = self.channelnorm(diff_img0)\r\n\r\n        # concat img0, img1, img1->img0, flow, diff-mag ; \r\n        concat1 = torch.cat((x, resampled_img1, flownetc_flow/self.div_flow, norm_diff_img0), dim=1)\r\n        \r\n        # flownets1\r\n        flownets1_flow2 = self.flownets_1(concat1)[0]\r\n        flownets1_flow = self.upsample2(flownets1_flow2*self.div_flow) \r\n\r\n        # warp img1 to img0 using flownets1; magnitude of diff between img0 and and warped_img1\r\n        resampled_img1 = self.resample2(x[:,3:,:,:], flownets1_flow)\r\n        diff_img0 = x[:,:3,:,:] - resampled_img1\r\n        norm_diff_img0 = self.channelnorm(diff_img0)\r\n\r\n        # concat img0, img1, img1->img0, flow, diff-mag\r\n        concat2 = torch.cat((x, resampled_img1, flownets1_flow/self.div_flow, norm_diff_img0), dim=1)\r\n\r\n        # flownets2\r\n        flownets2_flow2 = self.flownets_2(concat2)[0]\r\n        flownets2_flow = self.upsample3(flownets2_flow2 * self.div_flow)\r\n\r\n        return flownets2_flow\r\n\r\n"""
inpainting/lib/FlowNetC.py,7,"b""import torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\nimport math\r\nimport numpy as np\r\n\r\nfrom .correlation_package.modules.correlation import Correlation\r\n\r\nfrom .submodules import *\r\n'Parameter count , 39,175,298 '\r\n\r\nclass FlowNetC(nn.Module):\r\n    def __init__(self,args, batchNorm=True, div_flow = 20):\r\n        super(FlowNetC,self).__init__()\r\n\r\n        self.batchNorm = batchNorm\r\n        self.div_flow = div_flow\r\n\r\n        self.conv1   = conv(self.batchNorm,   3,   64, kernel_size=7, stride=2)\r\n        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\r\n        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\r\n        self.conv_redir  = conv(self.batchNorm, 256,   32, kernel_size=1, stride=1)\r\n\r\n        if args.fp16:\r\n            self.corr = nn.Sequential(\r\n                tofp32(),\r\n                Correlation(pad_size=20, kernel_size=1, max_displacement=20, stride1=1, stride2=2, corr_multiply=1),\r\n                tofp16())\r\n        else:\r\n            self.corr = Correlation(pad_size=20, kernel_size=1, max_displacement=20, stride1=1, stride2=2, corr_multiply=1)\r\n\r\n        self.corr_activation = nn.LeakyReLU(0.1,inplace=True)\r\n        self.conv3_1 = conv(self.batchNorm, 473,  256)\r\n        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\r\n        self.conv4_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\r\n        self.conv5_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\r\n        self.conv6_1 = conv(self.batchNorm,1024, 1024)\r\n\r\n        self.deconv5 = deconv(1024,512)\r\n        self.deconv4 = deconv(1026,256)\r\n        self.deconv3 = deconv(770,128)\r\n        self.deconv2 = deconv(386,64)\r\n\r\n        self.predict_flow6 = predict_flow(1024)\r\n        self.predict_flow5 = predict_flow(1026)\r\n        self.predict_flow4 = predict_flow(770)\r\n        self.predict_flow3 = predict_flow(386)\r\n        self.predict_flow2 = predict_flow(194)\r\n\r\n        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=True)\r\n        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=True)\r\n        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=True)\r\n        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=True)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n    def forward(self, x):\r\n        x1 = x[:,0:3,:,:]\r\n        x2 = x[:,3::,:,:]\r\n\r\n        out_conv1a = self.conv1(x1)\r\n        out_conv2a = self.conv2(out_conv1a)\r\n        out_conv3a = self.conv3(out_conv2a)\r\n\r\n        # FlownetC bottom input stream\r\n        out_conv1b = self.conv1(x2)\r\n        \r\n        out_conv2b = self.conv2(out_conv1b)\r\n        out_conv3b = self.conv3(out_conv2b)\r\n\r\n        # Merge streams\r\n        out_corr = self.corr(out_conv3a, out_conv3b) # False\r\n        out_corr = self.corr_activation(out_corr)\r\n\r\n        # Redirect top input stream and concatenate\r\n        out_conv_redir = self.conv_redir(out_conv3a)\r\n\r\n        in_conv3_1 = torch.cat((out_conv_redir, out_corr), 1)\r\n\r\n        # Merged conv layers\r\n        out_conv3_1 = self.conv3_1(in_conv3_1)\r\n\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3_1))\r\n\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n\r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n\r\n        flow5       = self.predict_flow5(concat5)\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n\r\n        flow4       = self.predict_flow4(concat4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        concat3 = torch.cat((out_conv3_1,out_deconv3,flow4_up),1)\r\n\r\n        flow3       = self.predict_flow3(concat3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n        concat2 = torch.cat((out_conv2a,out_deconv2,flow3_up),1)\r\n\r\n        flow2 = self.predict_flow2(concat2)\r\n\r\n        \r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6,out_conv3_1,out_conv4,out_conv5,out_conv6\r\n        else:\r\n            return flow2,\r\n        \r\n        return flow2,flow3,flow4,flow5,flow6"""
inpainting/lib/FlowNetFusion.py,4,"b""import torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\nimport math\r\nimport numpy as np\r\n\r\nfrom .submodules import *\r\n'Parameter count = 581,226'\r\n\r\nclass FlowNetFusion(nn.Module):\r\n    def __init__(self,args, batchNorm=True):\r\n        super(FlowNetFusion,self).__init__()\r\n\r\n        self.batchNorm = batchNorm\r\n        self.conv0   = conv(self.batchNorm,  11,   64)\r\n        self.conv1   = conv(self.batchNorm,  64,   64, stride=2)\r\n        self.conv1_1 = conv(self.batchNorm,  64,   128)\r\n        self.conv2   = conv(self.batchNorm,  128,  128, stride=2)\r\n        self.conv2_1 = conv(self.batchNorm,  128,  128)\r\n\r\n        self.deconv1 = deconv(128,32)\r\n        self.deconv0 = deconv(162,16)\r\n\r\n        self.inter_conv1 = i_conv(self.batchNorm,  162,   32)\r\n        self.inter_conv0 = i_conv(self.batchNorm,  82,   16)\r\n\r\n        self.predict_flow2 = predict_flow(128)\r\n        self.predict_flow1 = predict_flow(32)\r\n        self.predict_flow0 = predict_flow(16)\r\n\r\n        self.upsampled_flow2_to_1 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n        self.upsampled_flow1_to_0 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n\r\n    def forward(self, x):\r\n        out_conv0 = self.conv0(x)\r\n        out_conv1 = self.conv1_1(self.conv1(out_conv0))\r\n        out_conv2 = self.conv2_1(self.conv2(out_conv1))\r\n\r\n        flow2       = self.predict_flow2(out_conv2)\r\n        flow2_up    = self.upsampled_flow2_to_1(flow2)\r\n        out_deconv1 = self.deconv1(out_conv2)\r\n        \r\n        concat1 = torch.cat((out_conv1,out_deconv1,flow2_up),1)\r\n        out_interconv1 = self.inter_conv1(concat1)\r\n        flow1       = self.predict_flow1(out_interconv1)\r\n        flow1_up    = self.upsampled_flow1_to_0(flow1)\r\n        out_deconv0 = self.deconv0(concat1)\r\n        \r\n        concat0 = torch.cat((out_conv0,out_deconv0,flow1_up),1)\r\n        out_interconv0 = self.inter_conv0(concat0)\r\n        flow0       = self.predict_flow0(out_interconv0)\r\n\r\n        return flow0\r\n\r\n"""
inpainting/lib/FlowNetS.py,6,"b""'''\r\nPortions of this code copyright 2017, Clement Pinard\r\n'''\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\nimport math\r\nimport numpy as np\r\n\r\nfrom .submodules import *\r\n'Parameter count : 38,676,504 '\r\n\r\nclass FlowNetS(nn.Module):\r\n    def __init__(self, args, input_channels = 12, batchNorm=True):\r\n        super(FlowNetS,self).__init__()\r\n\r\n        self.batchNorm = batchNorm\r\n        self.conv1   = conv(self.batchNorm,  input_channels,   64, kernel_size=7, stride=2)\r\n        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\r\n        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\r\n        self.conv3_1 = conv(self.batchNorm, 256,  256)\r\n        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\r\n        self.conv4_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\r\n        self.conv5_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\r\n        self.conv6_1 = conv(self.batchNorm,1024, 1024)\r\n\r\n        self.deconv5 = deconv(1024,512)\r\n        self.deconv4 = deconv(1026,256)\r\n        self.deconv3 = deconv(770,128)\r\n        self.deconv2 = deconv(386,64)\r\n\r\n        self.predict_flow6 = predict_flow(1024)\r\n        self.predict_flow5 = predict_flow(1026)\r\n        self.predict_flow4 = predict_flow(770)\r\n        self.predict_flow3 = predict_flow(386)\r\n        self.predict_flow2 = predict_flow(194)\r\n\r\n        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\r\n        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\r\n        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\r\n        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n    def forward(self, x):\r\n        out_conv1 = self.conv1(x)\r\n\r\n        out_conv2 = self.conv2(out_conv1)\r\n        out_conv3 = self.conv3_1(self.conv3(out_conv2))\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n        \r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n        flow5       = self.predict_flow5(concat5)\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        \r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n        flow4       = self.predict_flow4(concat4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        \r\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\r\n        flow3       = self.predict_flow3(concat3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n\r\n        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\r\n        flow2 = self.predict_flow2(concat2)\r\n\r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6\r\n        else:\r\n            return flow2,\r\n\r\n"""
inpainting/lib/FlowNetSD.py,6,"b""import torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\nimport math\r\nimport numpy as np\r\n\r\nfrom .submodules import *\r\n'Parameter count = 45,371,666'\r\n\r\nclass FlowNetSD(nn.Module):\r\n    def __init__(self, args, batchNorm=True):\r\n        super(FlowNetSD,self).__init__()\r\n\r\n        self.batchNorm = batchNorm\r\n        self.conv0   = conv(self.batchNorm,  6,   64)\r\n        self.conv1   = conv(self.batchNorm,  64,   64, stride=2)\r\n        self.conv1_1 = conv(self.batchNorm,  64,   128)\r\n        self.conv2   = conv(self.batchNorm,  128,  128, stride=2)\r\n        self.conv2_1 = conv(self.batchNorm,  128,  128)\r\n        self.conv3   = conv(self.batchNorm, 128,  256, stride=2)\r\n        self.conv3_1 = conv(self.batchNorm, 256,  256)\r\n        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\r\n        self.conv4_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\r\n        self.conv5_1 = conv(self.batchNorm, 512,  512)\r\n        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\r\n        self.conv6_1 = conv(self.batchNorm,1024, 1024)\r\n\r\n        self.deconv5 = deconv(1024,512)\r\n        self.deconv4 = deconv(1026,256)\r\n        self.deconv3 = deconv(770,128)\r\n        self.deconv2 = deconv(386,64)\r\n\r\n        self.inter_conv5 = i_conv(self.batchNorm,  1026,   512)\r\n        self.inter_conv4 = i_conv(self.batchNorm,  770,   256)\r\n        self.inter_conv3 = i_conv(self.batchNorm,  386,   128)\r\n        self.inter_conv2 = i_conv(self.batchNorm,  194,   64)\r\n\r\n        self.predict_flow6 = predict_flow(1024)\r\n        self.predict_flow5 = predict_flow(512)\r\n        self.predict_flow4 = predict_flow(256)\r\n        self.predict_flow3 = predict_flow(128)\r\n        self.predict_flow2 = predict_flow(64)\r\n\r\n        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                if m.bias is not None:\r\n                    init.uniform_(m.bias)\r\n                init.xavier_uniform_(m.weight)\r\n                # init_deconv_bilinear(m.weight)\r\n        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n\r\n\r\n\r\n    def forward(self, x):\r\n        out_conv0 = self.conv0(x)\r\n        out_conv1 = self.conv1_1(self.conv1(out_conv0))\r\n        out_conv2 = self.conv2_1(self.conv2(out_conv1))\r\n\r\n        out_conv3 = self.conv3_1(self.conv3(out_conv2))\r\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\r\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\r\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\r\n\r\n        flow6       = self.predict_flow6(out_conv6)\r\n        flow6_up    = self.upsampled_flow6_to_5(flow6)\r\n        out_deconv5 = self.deconv5(out_conv6)\r\n        \r\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\r\n        out_interconv5 = self.inter_conv5(concat5)\r\n        flow5       = self.predict_flow5(out_interconv5)\r\n\r\n        flow5_up    = self.upsampled_flow5_to_4(flow5)\r\n        out_deconv4 = self.deconv4(concat5)\r\n        \r\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\r\n        out_interconv4 = self.inter_conv4(concat4)\r\n        flow4       = self.predict_flow4(out_interconv4)\r\n        flow4_up    = self.upsampled_flow4_to_3(flow4)\r\n        out_deconv3 = self.deconv3(concat4)\r\n        \r\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\r\n        out_interconv3 = self.inter_conv3(concat3)\r\n        flow3       = self.predict_flow3(out_interconv3)\r\n        flow3_up    = self.upsampled_flow3_to_2(flow3)\r\n        out_deconv2 = self.deconv2(concat3)\r\n\r\n        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\r\n        out_interconv2 = self.inter_conv2(concat2)\r\n        flow2 = self.predict_flow2(out_interconv2)\r\n\r\n        if self.training:\r\n            return flow2,flow3,flow4,flow5,flow6\r\n        else:\r\n            return flow2,\r\n"""
inpainting/lib/TransformNet.py,5,"b'## torch lib\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nfrom .ConvLSTM import ConvLSTM\nimport pdb\n\nclass TransformNet(nn.Module):\n\n    def __init__(self, opts, nc_in, nc_out):\n        super(TransformNet, self).__init__()\n\n        self.blocks = opts.blocks\n        self.epoch = 0\n        nf = opts.nf\n        use_bias = (opts.norm == ""IN"")\n        \n        ## convolution layers\n        self.conv1a = ConvLayer(3 + 3, nf * 1, kernel_size=7, stride=1, bias=use_bias, norm=opts.norm) ## input: P_t, O_t-1\n        self.conv1b = ConvLayer(3 + 3, nf * 1, kernel_size=7, stride=1, bias=use_bias, norm=opts.norm) ## input: I_t, I_t-1\n        self.conv2a = ConvLayer(nf * 1, nf * 2, kernel_size=3, stride=2, bias=use_bias, norm=opts.norm)\n        self.conv2b = ConvLayer(nf * 1, nf * 2, kernel_size=3, stride=2, bias=use_bias, norm=opts.norm)\n        self.conv3  = ConvLayer(nf * 4, nf * 4, kernel_size=3, stride=2, bias=use_bias, norm=opts.norm)\n        \n        # Residual blocks\n        self.ResBlocks = nn.ModuleList()\n        for b in range(self.blocks):\n            self.ResBlocks.append(ResidualBlock(nf * 4, bias=use_bias, norm=opts.norm))\n\n        ## LSTM\n        self.convlstm = ConvLSTM(input_size=nf * 4, hidden_size = nf * 4, kernel_size=3)\n        \n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(nf * 4, nf * 2, kernel_size=3, stride=1, upsample=2, bias=use_bias, norm=opts.norm)\n        self.deconv2 = UpsampleConvLayer(nf * 4, nf * 1, kernel_size=3, stride=1, upsample=2, bias=use_bias, norm=opts.norm)\n        self.deconv3 = ConvLayer(nf * 2, nc_out, kernel_size=7, stride=1) ## output one channel mask\n        \n        # Non-linearities\n        self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        self.tanh = nn.Tanh()\n\n\n    def forward(self, X, prev_state):\n        \n        Xa = X[:,:6,:,:] ## P_t, O_t-1\n        Xb = X[:,6:,:,:] ## I_t, I_t-1\n\n        E1a = self.relu(self.conv1a(Xa))\n        E1b = self.relu(self.conv1b(Xb))\n\n        E2a = self.relu(self.conv2a(E1a))\n        E2b = self.relu(self.conv2b(E1b))\n\n        E3 = self.relu(self.conv3(torch.cat((E2a, E2b), 1)))\n        \n        RB = E3\n        for b in range(self.blocks):\n            RB = self.ResBlocks[b](RB)\n        pdb.set_trace()\n        state = self.convlstm(RB, prev_state)\n\n        D2 = self.relu(self.deconv1(state[0]))\n        C2 = torch.cat((D2, E2a), 1)\n        D1 = self.relu(self.deconv2(C2))\n        C1 = torch.cat((D1, E1a), 1)\n        Y = self.deconv3(C1)\n\n        Y = self.tanh(Y)\n        \n        return Y, state\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=None, bias=True):\n        super(ConvLayer, self).__init__()\n\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=bias)\n\n        self.norm = norm\n        if norm == ""BN"":\n            self.norm_layer = nn.BatchNorm2d(out_channels)\n        elif norm == ""IN"":\n            self.norm_layer = nn.InstanceNorm2d(out_channels, track_running_stats=True)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n\n        if self.norm in [""BN"" or ""IN""]:\n            out = self.norm_layer(out)\n\n        return out\n\n\nclass UpsampleConvLayer(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None, norm=None, bias=True):\n        super(UpsampleConvLayer, self).__init__()\n\n        self.upsample = upsample\n        if upsample:\n            self.upsample_layer = nn.Upsample(scale_factor=upsample, mode=\'nearest\')\n\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=bias)\n\n        self.norm = norm\n        if norm == ""BN"":\n            self.norm_layer = nn.BatchNorm2d(out_channels)\n        elif norm == ""IN"":\n            self.norm_layer = nn.InstanceNorm2d(out_channels, track_running_stats=True)\n\n    def forward(self, x):\n\n        x_in = x\n        if self.upsample:\n            x_in = self.upsample_layer(x_in)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n\n        if self.norm in [""BN"" or ""IN""]:\n            out = self.norm_layer(out)\n\n        return out\n\n\n\nclass ResidualBlock(nn.Module):\n    \n    def __init__(self, channels, norm=None, bias=True):\n        super(ResidualBlock, self).__init__()\n        self.conv1  = ConvLayer(channels, channels, kernel_size=3, stride=1, bias=bias, norm=norm)\n        self.conv2  = ConvLayer(channels, channels, kernel_size=3, stride=1, bias=bias, norm=norm)\n\n        self.relu   = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        \n    def forward(self, x):\n        \n        input = x\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        out = out + input\n\n        return out\n\n\n'"
inpainting/lib/__init__.py,0,b'from .FlowNet2 import *\n#from .TransformNet import *'
inpainting/lib/submodules.py,2,"b'# freda (todo) : \r\n\r\nimport torch.nn as nn\r\nimport torch\r\nimport numpy as np \r\n\r\ndef conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\r\n    if batchNorm:\r\n        return nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\r\n            nn.BatchNorm2d(out_planes),\r\n            nn.LeakyReLU(0.1,inplace=True)\r\n        )\r\n    else:\r\n        return nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\r\n            nn.LeakyReLU(0.1,inplace=True)\r\n        )\r\n\r\ndef i_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, bias = True):\r\n    if batchNorm:\r\n        return nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\r\n            nn.BatchNorm2d(out_planes),\r\n        )\r\n    else:\r\n        return nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\r\n        )\r\n\r\ndef predict_flow(in_planes):\r\n    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)\r\n\r\ndef deconv(in_planes, out_planes):\r\n    return nn.Sequential(\r\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=True),\r\n        nn.LeakyReLU(0.1,inplace=True)\r\n    )\r\n\r\nclass tofp16(nn.Module):\r\n    def __init__(self):\r\n        super(tofp16, self).__init__()\r\n\r\n    def forward(self, input):\r\n        return input.half()\r\n\r\n\r\nclass tofp32(nn.Module):\r\n    def __init__(self):\r\n        super(tofp32, self).__init__()\r\n\r\n    def forward(self, input):\r\n        return input.float()\r\n\r\n\r\ndef init_deconv_bilinear(weight):\r\n    f_shape = weight.size()\r\n    heigh, width = f_shape[-2], f_shape[-1]\r\n    f = np.ceil(width/2.0)\r\n    c = (2 * f - 1 - f % 2) / (2.0 * f)\r\n    bilinear = np.zeros([heigh, width])\r\n    for x in range(width):\r\n        for y in range(heigh):\r\n            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\r\n            bilinear[x, y] = value\r\n    weight.data.fill_(0.)\r\n    for i in range(f_shape[0]):\r\n        for j in range(f_shape[1]):\r\n            weight.data[i,j,:,:] = torch.from_numpy(bilinear)\r\n\r\n\r\ndef save_grad(grads, name):\r\n    def hook(grad):\r\n        grads[name] = grad\r\n    return hook\r\n'"
inpainting/models/ConvLSTM.py,4,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as f\n\n### modified from https://github.com/Atcold/pytorch-CortexNet/blob/master/model/ConvLSTMCell.py\n\nclass ConvLSTM(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, kernel_size, type='2d'):\n        super(ConvLSTM, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        pad = kernel_size // 2\n        if type == '3d':\n            self.Gates = nn.Conv3d(input_size + hidden_size, 4 * hidden_size, (1,kernel_size,kernel_size), padding=(0,pad,pad))\n        else:\n            self.Gates = nn.Conv2d(input_size + hidden_size, 4 * hidden_size, kernel_size, padding=pad)\n\n\n    def forward(self, input_, prev_state=None):\n\n        # get batch and spatial sizes\n        batch_size = input_.data.size()[0]\n        spatial_size = input_.data.size()[2:]\n\n        # generate empty prev_state, if None is provided\n        if prev_state is None:\n            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n            prev_state = (\n                torch.zeros(state_size).to(input_.device),\n                torch.zeros(state_size).to(input_.device)\n            )\n\n        prev_hidden, prev_cell = prev_state\n\n        # data size is [batch, channel, height, width]\n        stacked_inputs = torch.cat((input_, prev_hidden), 1)\n        gates = self.Gates(stacked_inputs)\n\n        # chunk across channel dimension\n        in_gate, remember_gate, out_gate, cell_gate = gates.chunk(4, 1)\n\n        # apply sigmoid non linearity\n        in_gate = f.sigmoid(in_gate)\n        remember_gate = f.sigmoid(remember_gate)\n        out_gate = f.sigmoid(out_gate)\n\n        # apply tanh non linearity\n        cell_gate = f.tanh(cell_gate)\n\n        # compute current cell and hidden state\n        cell = (remember_gate * prev_cell) + (in_gate * cell_gate)\n        hidden = out_gate * f.tanh(cell)\n\n        return hidden, cell\n"""
inpainting/models/__init__.py,0,b''
inpainting/models/flow_modules.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport sys\nfrom time import time\nfrom inpainting.models.correlation_package.modules.correlation import Correlation\nfrom inpainting.models.gated_conv import GatedConvolution, GatedUpConvolution\nimport pdb\n\n\ndef get_grid(x):\n    torchHorizontal = torch.linspace(-1.0, 1.0, x.size(3)).view(1, 1, 1, x.size(3)).expand(x.size(0), 1, x.size(2), x.size(3))\n    torchVertical = torch.linspace(-1.0, 1.0, x.size(2)).view(1, 1, x.size(2), 1).expand(x.size(0), 1, x.size(2), x.size(3))\n    grid = torch.cat([torchHorizontal, torchVertical], 1)\n    return grid\n\n\ndef conv(batch_norm, in_planes, out_planes, kernel_size = 3, stride = 1, dilation = 1):\n    if batch_norm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size = kernel_size, stride = stride, dilation = dilation, padding = ((kernel_size - 1) * dilation) // 2, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size = kernel_size, stride = stride, dilation = dilation, padding = ((kernel_size - 1) * dilation) // 2, bias=True),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n\ndef conv_(batch_norm, in_planes, out_planes, kernel_size = 3, stride = 1, dilation = 1):\n    if batch_norm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size = kernel_size, stride = stride, dilation = dilation, padding = ((kernel_size - 1) * dilation) // 2, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size = kernel_size, stride = stride, dilation = dilation, padding = ((kernel_size - 1) * dilation) // 2, bias=True),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n\n\nclass MaskEstimator_(nn.Module):\n    def __init__(self, args, ch_in):\n        super(MaskEstimator_, self).__init__()\n        self.args = args\n        self.convs = nn.Sequential(\n            conv_(False, ch_in, ch_in//2),\n            conv_(False, ch_in//2, ch_in//2),\n            nn.Conv2d(in_channels = ch_in//2, out_channels = 1, kernel_size = 3, stride = 1, padding = 1, dilation = 1, groups = 1, bias = True),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.convs(x)\n\nclass WarpingLayer(nn.Module):\n    \n    def __init__(self):\n        super(WarpingLayer, self).__init__()\n    \n    def forward(self, x, flow):\n        # WarpingLayer uses F.grid_sample, which expects normalized grid\n        # we still output unnormalized flow for the convenience of comparing EPEs with FlowNet2 and original code\n        # so here we need to denormalize the flow\n        flow_for_grip = torch.zeros_like(flow).cuda()\n        flow_for_grip[:,0,:,:] = flow[:,0,:,:] / ((flow.size(3) - 1.0) / 2.0)\n        flow_for_grip[:,1,:,:] = flow[:,1,:,:] / ((flow.size(2) - 1.0) / 2.0)\n\n        grid = (get_grid(x).cuda() + flow_for_grip).permute(0, 2, 3, 1)\n        x_warp = F.grid_sample(x, grid)\n        return x_warp\n\n\nclass ContextNetwork(nn.Module):\n    def __init__(self, args, ch_in):\n        super(ContextNetwork, self).__init__()\n        self.args = args\n        self.convs = nn.Sequential(\n            conv(args.batch_norm, ch_in, 128, 3, 1, 1),\n            conv(args.batch_norm, 128, 128, 3, 1, 2),\n            conv(args.batch_norm, 128, 128, 3, 1, 4),\n            conv(args.batch_norm, 128, 96, 3, 1, 8),\n            conv(args.batch_norm, 96, 64, 3, 1, 16),\n            conv(args.batch_norm, 64, 32, 3, 1, 1),\n            conv(args.batch_norm, 32, 2, 3, 1, 1)\n        )\n    def forward(self, x):\n        return self.convs(x)\n\n\nclass LongFlowEstimatorCorr(nn.Module):\n    def __init__(self, args, ch_in):\n        super(LongFlowEstimatorCorr, self).__init__()\n        self.args = args\n        self.convs = nn.Sequential(\n            conv(args.batch_norm, ch_in, 128),\n            conv(args.batch_norm, 128, 128),\n            conv(args.batch_norm, 128, 96),\n            conv(args.batch_norm, 96, 64),\n            conv(args.batch_norm, 64, 32)\n        )\n        self.conv1 = nn.Conv2d(in_channels = 32, out_channels = 2, kernel_size = 3, stride = 1, padding = 1, dilation = 1, groups = 1, bias = True)\n        self.convs_fine = ContextNetwork(args, 32+2)\n        \n    def forward(self, x):\n        x = self.convs(x)\n        flo_coarse = self.conv1(x)\n        flo_fine = self.convs_fine(torch.cat([x, flo_coarse],1))\n        flo = flo_coarse + flo_fine\n        return flo\n\n\nclass LongFlowNetCorr(nn.Module):\n    def __init__(self, args, in_ch):\n        super(LongFlowNetCorr, self).__init__()\n        self.args = args\n        self.corr = Correlation(pad_size = args.search_range, kernel_size = 1, max_displacement = args.search_range, stride1 = 1, stride2 = 1, corr_multiply = 1).cuda()\n        self.flow_estimator = LongFlowEstimatorCorr(args, in_ch+(args.search_range*2+1)**2)\n        # init\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv3d):\n                if m.bias is not None: nn.init.uniform_(m.bias)\n                nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.ConvTranspose3d):\n                if m.bias is not None: nn.init.uniform_(m.bias)\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x1, x2, upflow=None):\n        corr = self.corr(x1.contiguous(), x2.contiguous())\n        if upflow is not None:\n            flow = self.flow_estimator(torch.cat([x1, corr, upflow], dim = 1))\n        else:\n            flow = self.flow_estimator(torch.cat([x1, corr], dim = 1))\n        return flow\n\n'"
inpainting/models/gated_conv.py,8,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass GatedConvolution(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation=1, padding=0, bias=False, type='3d', status='train'):\n        super(GatedConvolution, self).__init__()\n        assert type in ['2d', '3d']\n        assert status in ['train', 'test']\n\n        self.status = status\n        self.type = type\n\n        if type == '3d':\n            self.conv = nn.Conv3d(in_channels, out_channels*2, kernel_size,  stride=stride, dilation=dilation, padding=padding, bias=bias)\n        elif type == '2d':\n            self.conv = nn.Conv2d(in_channels, out_channels*2, kernel_size,  stride=stride, dilation=dilation, padding=padding, bias=bias)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.conv(x)\n        c = x.size(1)\n        phi, gate = torch.split(x,c//2,1)\n\n        if self.status == 'train':\n            return torch.sigmoid(gate)*self.relu(phi)\n        else:\n            return torch.sigmoid(gate)*self.relu(phi), torch.sigmoid(gate)\n        \n\nclass GatedUpConvolution(nn.Module):\n    def __init__(self, size, in_channels, out_channels, kernel_size, stride, padding, bias, mode='trilinear', type='3d', status='train'):\n        super(GatedUpConvolution, self).__init__()\n        assert type in ['2d', '3d']\n        assert status in ['train', 'test']\n        self.status = status\n        self.type = type\n        self.leaky_relu = nn.LeakyReLU(0.2)\n\n        if type == '3d':\n            self.conv = nn.Sequential(\n                        nn.Upsample(size=size, mode=mode),\n                        nn.Conv3d(in_channels, out_channels*2, kernel_size, stride=stride, padding=padding, bias=bias))\n                            \n        elif type == '2d':\n            self.conv = nn.Sequential(\n                        nn.Upsample(size=size, mode=mode),\n                        nn.Conv2d(in_channels, out_channels*2, kernel_size, stride=stride, padding=padding, bias=bias))\n\n    def forward(self, x):\n        x = self.conv(x)\n        c = x.size(1)\n        phi, gate = torch.split(x,c//2,1)\n\n        if self.status == 'train':\n            return torch.sigmoid(gate)*self.leaky_relu(phi)\n        else:\n            return torch.sigmoid(gate)*self.leaky_relu(phi), torch.sigmoid(gate)\n\n"""
inpainting/models/utils.py,11,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom random import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\ndef down_sample(x, size=None, scale_factor=None, mode='nearest'):\n    # define size if user has specified scale_factor\n    if size is None: size = (int(scale_factor*x.size(2)), int(scale_factor*x.size(3)))\n    # create coordinates\n    h = torch.arange(0,size[0]) / (size[0]-1) * 2 - 1\n    w = torch.arange(0,size[1]) / (size[1]-1) * 2 - 1\n    # create grid\n    grid =torch.zeros(size[0],size[1],2)\n    grid[:,:,0] = w.unsqueeze(0).repeat(size[0],1)\n    grid[:,:,1] = h.unsqueeze(0).repeat(size[1],1).transpose(0,1)\n    # expand to match batch size\n    grid = grid.unsqueeze(0).repeat(x.size(0),1,1,1)\n    if x.is_cuda: grid = Variable(grid).cuda()\n    # do sampling\n    return F.grid_sample(x, grid, mode=mode)\n\n\ndef reduce_mean(x):\n    for i in range(4):\n        if i==1: continue\n        x = torch.mean(x, dim=i, keepdim=True)\n    return x\n\n\ndef l2_norm(x):\n    def reduce_sum(x):\n        for i in range(4):\n            if i==1: continue\n            x = torch.sum(x, dim=i, keepdim=True)\n        return x\n\n    x = x**2\n    x = reduce_sum(x)\n    return torch.sqrt(x)\n\n\ndef show_image(real, masked, stage_1, stage_2, fake, offset_flow):\n    batch_size = real.shape[0]\n\n    (real, masked, stage_1, stage_2, fake, offset_flow) = (\n                                var_to_numpy(real), \n                                var_to_numpy(masked), \n                                var_to_numpy(stage_1),\n                                var_to_numpy(stage_2),\n                                var_to_numpy(fake),\n                                var_to_numpy(offset_flow)\n                              )\n    # offset_flow = (offset_flow*2).astype(int) -1\n    for x in range(batch_size):\n        if x > 5 :\n            break\n        fig, axs = plt.subplots(ncols=5, figsize=(15,3))\n        axs[0].set_title('real image')\n        axs[0].imshow(real[x])\n        axs[0].axis('off')\n\n        axs[1].set_title('masked image')\n        axs[1].imshow(masked[x])\n        axs[1].axis('off')\n\n        axs[2].set_title('stage_1 image')\n        axs[2].imshow(stage_1[x])\n        axs[2].axis('off')\n\n        axs[3].set_title('stage_2 image')\n        axs[3].imshow(stage_2[x])\n        axs[3].axis('off')\n\n        axs[4].set_title('fake_image')\n        axs[4].imshow(fake[x])\n        axs[4].axis('off')\n\n        # axs[5].set_title('C_Attn')\n        # axs[5].imshow(offset_flow[x])\n        # axs[5].axis('off')\n\n        plt.show()\n\n\ndef var_to_numpy(obj, for_vis=True):\n    if for_vis:\n        obj = obj.permute(0,2,3,1)\n        obj = (obj+1) / 2\n    return obj.data.cpu().numpy()\n\n\ndef to_var(x, volatile=False):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, volatile=volatile)"""
inpainting/models/vinet.py,21,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom inpainting.models.flow_modules import (WarpingLayer, LongFlowNetCorr, MaskEstimator_ )\nfrom inpainting.models.gated_conv import GatedConvolution, GatedUpConvolution\nfrom inpainting.models.utils import *\nfrom inpainting.models.ConvLSTM import ConvLSTM\nimport pdb\n\nclass VI_2D_Encoder_3(nn.Module):\n    def __init__(self, opt):\n        super(VI_2D_Encoder_3, self).__init__()\n        self.opt = opt\n        ### ENCODER\n        st = 2 if self.opt.double_size else 1\n        self.ec0 = GatedConvolution(5, 32, kernel_size=(3,3), stride=(st,st), padding=(1,1), bias=False, type='2d')\n        self.ec1 = GatedConvolution(32, 64, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False, type='2d')\n        self.ec2 = GatedConvolution(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False, type='2d')\n        self.ec3_1 = GatedConvolution(64, 96, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False, type='2d')\n        self.ec3_2 = GatedConvolution(96, 96, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False, type='2d')\n        self.ec4_1 = GatedConvolution(96, 128, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False, type='2d')\n        self.ec4 = GatedConvolution(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False, type='2d')\n        self.ec5 = GatedConvolution(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False, type='2d')\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n    \n    def forward(self, x):\n        out_1 = self.ec0(x)\n        out_2 = self.ec2(self.ec1(out_1))\n        out_4 = self.ec3_2(self.ec3_1(out_2))\n        out = self.ec5(self.ec4(self.ec4_1(out_4)))\n        \n        return out, out_4, out_2, out_1\n\nclass VI_2D_Decoder_3(nn.Module):\n    def __init__(self, opt):\n        super(VI_2D_Decoder_3, self).__init__()\n        self.opt=opt\n        dv = 2 if self.opt.double_size else 1\n        ### decoder\n        self.dc0 = GatedConvolution(128, 128, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        self.dc1 = GatedConvolution(128, 128, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        #### UPCONV\n        self.dc1_1 = GatedUpConvolution((1, opt.crop_size//4//dv, opt.crop_size//4//dv), 128 , 96, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        \n        self.dc2_1 = GatedConvolution(96+96, 96, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        self.dc2_bt1 =  GatedConvolution(96, 96, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,2,2),padding=(0,2,2), bias=False)\n        self.dc2_bt2 =  GatedConvolution(96, 96, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,4,4),padding=(0,4,4), bias=False)\n        self.dc2_bt3 =  GatedConvolution(96, 96, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,8,8),padding=(0,8,8), bias=False)\n        #### UPCONV\n        self.dc2_2 = GatedUpConvolution((1, opt.crop_size//2//dv, opt.crop_size//2//dv), 96, 64, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        self.dc3_1 = GatedConvolution(64+64, 64, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        self.dc3_2 =  GatedConvolution(64, 64, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        #### UPCONV\n        self.dc4 = GatedUpConvolution((1, opt.crop_size//dv, opt.crop_size//dv), 64 , 32, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        if self.opt.double_size:\n            self.upsample = nn.Upsample(size=(1,opt.crop_size, opt.crop_size), mode='trilinear')\n        self.dc5 = GatedConvolution(32, 16, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        self.dc6 = nn.Conv3d(16, 3, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x, x2_64_warp=None, x2_128_warp=None):\n        x1_64 = self.dc1_1(self.dc1(self.dc0(x)))\n        if  x2_64_warp is not None and x2_128_warp is not None:\n            x1_64 = self.dc2_bt3(self.dc2_bt2(self.dc2_bt1(self.dc2_1(torch.cat([x1_64, x2_64_warp],1)))))\n            x1_128 = self.dc2_2(x1_64)\n            if self.opt.double_size:\n                d6 = self.dc6(self.dc5(self.upsample(self.dc4(self.dc3_2(self.dc3_1(torch.cat([x1_128, x2_128_warp],1)))))))\n            else:\n                d6 = self.dc6(self.dc5(self.dc4(self.dc3_2(self.dc3_1(torch.cat([x1_128, x2_128_warp],1))))))\n        return d6, None\n\nclass VI_2D_BottleNeck(nn.Module):\n    def __init__(self, opt, in_ch):\n        super(VI_2D_BottleNeck, self).__init__()\n        self.opt = opt\n        ### bottleneck\n        self.bt0 =  GatedConvolution(in_ch, 128, kernel_size=(3,3), stride=(1,1), dilation=(1,1),padding=(1,1), bias=False, type='2d')\n        self.bt1 =  GatedConvolution(128, 128, kernel_size=(3,3), stride=(1,1), dilation=(2,2),padding=(2,2), bias=False, type='2d')\n        self.bt2 =  GatedConvolution(128, 128, kernel_size=(3,3), stride=(1,1), dilation=(4,4),padding=(4,4), bias=False, type='2d')\n        self.bt3 =  GatedConvolution(128, 128, kernel_size=(3,3), stride=(1,1), dilation=(8,8),padding=(8,8), bias=False, type='2d')\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        res = self.bt3(self.bt2(self.bt1(self.bt0(x))))\n        return res\n\nclass VI_Aggregator(nn.Module):\n    def __init__(self, opt, in_ch, T):\n        super(VI_Aggregator, self).__init__()\n        self.opt = opt\n        ### spatio-temporal aggregation\n        self.stAgg =  GatedConvolution(in_ch, in_ch, kernel_size=(T,3,3), stride=(1,1,1), padding=(0,1,1), bias=False, type='3d')\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n    def forward(self, x):\n        return self.stAgg(x)\n\n\nclass VINet_final(nn.Module):\n    def __init__(self, opt):\n        super(VINet_final, self).__init__()\n        self.opt = opt\n        self.encoder1 = VI_2D_Encoder_3(self.opt)\n        self.encoder2 = VI_2D_Encoder_3(self.opt)\n        self.bottleneck = VI_2D_BottleNeck(self.opt, in_ch=256)\n\n        self.convlstm = ConvLSTM(input_size=128, hidden_size=128, kernel_size=3)\n\n        self.decoder = VI_2D_Decoder_3(self.opt)\n        self.flownet = LongFlowNetCorr(self.opt, 128)\n        self.flownet_64 = LongFlowNetCorr(self.opt, 96+2)\n        self.flownet_128 = LongFlowNetCorr(self.opt, 64+2)\n\n        if self.opt.prev_warp:\n            self.flownet_256 = LongFlowNetCorr(self.opt, 32+2)\n            self.masknet_256 = MaskEstimator_(self.opt, 32)\n            from inpainting.lib.resample2d_package.modules.resample2d import Resample2d\n            self.warping_256 = Resample2d().cuda()\n\n        self.masknet = MaskEstimator_(self.opt, 128)\n        self.masknet_64 = MaskEstimator_(self.opt, 96)\n        self.masknet_128 = MaskEstimator_(self.opt, 64)\n\n        self.st_agg = VI_Aggregator(self.opt, in_ch=128, T=5)\n        self.st_agg_64 = VI_Aggregator(self.opt, in_ch=96, T=5)\n        self.st_agg_128 = VI_Aggregator(self.opt, in_ch=64, T=5)\n        \n        self.warping = WarpingLayer()\n        self.warping_64 = WarpingLayer()        \n        self.warping_128 = WarpingLayer()\n\n    def forward(self, masked_img, mask, prev_state=None, prev_feed=None, idx=0): # masked img: b x C x TxHxW\n        T = masked_img.size(2)\n        ref_idx = (T-1)//2 # 2 --> 0\n        ones = to_var(torch.ones(mask.size()))\n        \n        # encoder\n        enc_output = []\n        enc_input = torch.cat([masked_img, ones, ones*mask], dim=1)\n\n        f1, f1_64, f1_128, f1_256 = self.encoder1(enc_input[:,:,ref_idx,:,:])\n\n        f2, f2_64, f2_128, _ = self.encoder2(enc_input[:,:,ref_idx-2,:,:])\n        f3, f3_64, f3_128, _ = self.encoder2(enc_input[:,:,ref_idx-1,:,:])\n        f4, f4_64, f4_128, _ = self.encoder2(enc_input[:,:,ref_idx+1,:,:])\n        f5, f5_64, f5_128, _ = self.encoder2(enc_input[:,:,ref_idx+2,:,:])\n        f6, f6_64, f6_128, f6_256 = self.encoder2(prev_feed)\n\n        flow2 = self.flownet(f1, f2)\n        flow3 = self.flownet(f1, f3)\n        flow4 = self.flownet(f1, f4)\n        flow5 = self.flownet(f1, f5)\n        flow6 = self.flownet(f1, f6)\n\n        f2_warp = self.warping(f2, flow2)\n        f3_warp = self.warping(f3, flow3)\n        f4_warp = self.warping(f4, flow4)\n        f5_warp = self.warping(f5, flow5)\n        f6_warp = self.warping(f6, flow6)\n\n        f_stack_oth = torch.stack([f2_warp, f3_warp, f4_warp, f5_warp, f6_warp],2)\n        f_agg = self.st_agg(f_stack_oth).squeeze(2)\n        occlusion_mask = self.masknet(torch.abs(f1 - f_agg))\n        f_syn = (1-occlusion_mask) * f1 + occlusion_mask * f_agg\n\n        bott_input = torch.cat([f1, f_syn],1)\n        output = self.bottleneck(bott_input)\n\n        # CONV LSTM\n        state = self.convlstm(output, prev_state)\n\n        # ============================ SCALE - 1/4 : 64 =============================\n        flow2_64 = F.upsample(flow2, scale_factor = 2, mode = 'bilinear')*2\n        flow3_64 = F.upsample(flow3, scale_factor = 2, mode = 'bilinear')*2\n        flow4_64 = F.upsample(flow4, scale_factor = 2, mode = 'bilinear')*2\n        flow5_64 = F.upsample(flow5, scale_factor = 2, mode = 'bilinear')*2\n        flow6_64 = F.upsample(flow6, scale_factor = 2, mode = 'bilinear')*2\n\n        f2_64_warp = self.warping_64(f2_64, flow2_64)\n        f3_64_warp = self.warping_64(f3_64, flow3_64)\n        f4_64_warp = self.warping_64(f4_64, flow4_64)\n        f5_64_warp = self.warping_64(f5_64, flow5_64)\n        f6_64_warp = self.warping_64(f6_64, flow6_64)\n\n        flow2_64 = self.flownet_64(f1_64, f2_64_warp, flow2_64) + flow2_64\n        flow3_64 = self.flownet_64(f1_64, f3_64_warp, flow3_64) + flow3_64\n        flow4_64 = self.flownet_64(f1_64, f4_64_warp, flow4_64) + flow4_64\n        flow5_64 = self.flownet_64(f1_64, f5_64_warp, flow5_64) + flow5_64\n        flow6_64 = self.flownet_64(f1_64, f6_64_warp, flow6_64) + flow6_64\n\n        f2_64_warp = self.warping_64(f2_64, flow2_64)\n        f3_64_warp = self.warping_64(f3_64, flow3_64)\n        f4_64_warp = self.warping_64(f4_64, flow4_64)\n        f5_64_warp = self.warping_64(f5_64, flow5_64)\n        f6_64_warp = self.warping_64(f6_64, flow6_64)\n        \n        f_stack_64_oth = torch.stack([f2_64_warp, f3_64_warp, f4_64_warp, f5_64_warp, f6_64_warp],2)\n        f_agg_64 = self.st_agg_64(f_stack_64_oth).squeeze(2)\n        occlusion_mask_64 = self.masknet_64(torch.abs(f1_64 - f_agg_64))\n        f_syn_64 = (1-occlusion_mask_64) * f1_64 + occlusion_mask_64 * f_agg_64\n\n        # ============================= SCALE - 1/2 : 128 ===============================\n        flow2_128 = F.upsample(flow2_64, scale_factor = 2, mode = 'bilinear')*2\n        flow3_128 = F.upsample(flow3_64, scale_factor = 2, mode = 'bilinear')*2\n        flow4_128 = F.upsample(flow4_64, scale_factor = 2, mode = 'bilinear')*2\n        flow5_128 = F.upsample(flow5_64, scale_factor = 2, mode = 'bilinear')*2\n        flow6_128 = F.upsample(flow6_64, scale_factor = 2, mode = 'bilinear')*2\n\n        f2_128_warp = self.warping_128(f2_128, flow2_128)\n        f3_128_warp = self.warping_128(f3_128, flow3_128)\n        f4_128_warp = self.warping_128(f4_128, flow4_128)\n        f5_128_warp = self.warping_128(f5_128, flow5_128)\n        f6_128_warp = self.warping_128(f6_128, flow6_128)\n\n        flow2_128 = self.flownet_128(f1_128, f2_128_warp, flow2_128) + flow2_128\n        flow3_128 = self.flownet_128(f1_128, f3_128_warp, flow3_128) + flow3_128\n        flow4_128 = self.flownet_128(f1_128, f4_128_warp, flow4_128) + flow4_128\n        flow5_128 = self.flownet_128(f1_128, f5_128_warp, flow5_128) + flow5_128\n        flow6_128 = self.flownet_128(f1_128, f6_128_warp, flow6_128) + flow6_128\n\n        f2_128_warp = self.warping_128(f2_128, flow2_128)\n        f3_128_warp = self.warping_128(f3_128, flow3_128)\n        f4_128_warp = self.warping_128(f4_128, flow4_128)\n        f5_128_warp = self.warping_128(f5_128, flow5_128)\n        f6_128_warp = self.warping_128(f6_128, flow6_128)\n\n        f_stack_128_oth = torch.stack([f2_128_warp, f3_128_warp, f4_128_warp, f5_128_warp, f6_128_warp],2)\n        f_agg_128 = self.st_agg_128(f_stack_128_oth).squeeze(2)\n        occlusion_mask_128 = self.masknet_128(torch.abs(f1_128 - f_agg_128))\n        f_syn_128 = (1-occlusion_mask_128) * f1_128 + occlusion_mask_128 * f_agg_128\n\n        output, _ = self.decoder(state[0].unsqueeze(2), x2_64_warp=f_syn_64.unsqueeze(2), x2_128_warp=f_syn_128.unsqueeze(2))\n        occ_mask = F.upsample(occlusion_mask, scale_factor=8, mode='bilinear')\n        occ_mask_64 = F.upsample(occlusion_mask_64, scale_factor=4, mode='bilinear')\n        occ_mask_128 = F.upsample(occlusion_mask_128, scale_factor=2, mode='bilinear')\n\n        flow6_256, flow6_512 = None, None\n        if self.opt.prev_warp:\n            if prev_state is not None or idx != 0:\n                flow6_256 = F.upsample(flow6_128, scale_factor = 2, mode = 'bilinear')*2\n                flow6_512 = F.upsample(flow6_128, scale_factor = 4, mode = 'bilinear')*4\n                f6_256_warp = self.warping_256(f6_256, flow6_256)\n                flow6_256 = self.flownet_256(f1_256, f6_256_warp, flow6_256) + flow6_256\n                occlusion_mask_256 = self.masknet_256(torch.abs(f1_256 - f6_256_warp))\n                output_ = output\n                if self.opt.double_size:\n                    prev_feed_warp = self.warping_256(prev_feed[:,:3], flow6_512)\n                    occlusion_mask_512 = F.upsample(occlusion_mask_256, scale_factor=2, mode='nearest')\n                    output = (1-occlusion_mask_512.unsqueeze(2)) * output + occlusion_mask_512 * prev_feed_warp.unsqueeze(2)\n                    flow6_256=flow6_512\n                else:\n                    prev_feed_warp = self.warping_256(prev_feed[:,:3], flow6_256)                \n                    output = (1-occlusion_mask_256.unsqueeze(2)) * output + occlusion_mask_256 * prev_feed_warp.unsqueeze(2)\n                if self.opt.loss_on_raw:\n                    output = (output, output_)\n\n        return output, torch.stack([flow2_128,flow3_128,flow4_128,flow5_128, flow6_128],2), state, torch.stack([occ_mask, 1-occ_mask, occ_mask_64, 1-occ_mask_64, occ_mask_128, 1-occ_mask_128], 2), flow6_256\n\n\n\n        # if not self.opt.no_train:\n        #     if self.opt.prev_warp and prev_state is not None:\n        #         if self.opt.loss_on_raw:\n        #             output = (output, output_)\n        #         return output, torch.stack([flow2_128,flow3_128,flow4_128,flow5_128, flow6_128],2), state, None, torch.stack([occ_mask, 1-occ_mask, occ_mask_64, 1-occ_mask_64, occ_mask_128, occlusion_mask_256], 2), flow6_256\n        #     else:\n        #         return output, torch.stack([flow2_128,flow3_128,flow4_128,flow5_128, flow6_128],2), state, None, torch.stack([occ_mask, 1-occ_mask, occ_mask_64, 1-occ_mask_64, occ_mask_128, 1-occ_mask_128], 2)\n\n        # elif self.opt.test:\n        #     if self.opt.prev_warp and idx !=0:         \n        #         return output, torch.stack([flow2_128,flow3_128,flow4_128,flow5_128, flow6_128],2), state, None, torch.stack([occ_mask, 1-occ_mask, occ_mask_64, 1-occ_mask_64, occ_mask_128, occlusion_mask_256], 2), flow6_256\n        #     else:\n        #         return output, torch.stack([flow2_128,flow3_128,flow4_128,flow5_128, flow6_128],2), state, None, torch.stack([occ_mask, 1-occ_mask, occ_mask_64, 1-occ_mask_64, occ_mask_128, 1-occ_mask_128], 2)\n\n"""
get_mask/utils/pysot/__init__.py,0,b''
get_mask/utils/pyvotkit/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom . import region\n'
get_mask/utils/pyvotkit/setup.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize([Extension(""region"", [""region.pyx""])])\n)\n\n'"
inpainting/lib/channelnorm_package/__init__.py,0,b''
inpainting/lib/channelnorm_package/build.py,3,"b""import os\nimport torch\nimport torch.utils.ffi\n\nthis_folder = os.path.dirname(os.path.abspath(__file__)) + '/'\n\nHeaders = []\nSources = []\nDefines = []\nObjects = []\n\nif torch.cuda.is_available() == True:\n    Headers += ['src/ChannelNorm_cuda.h']\n    Sources += ['src/ChannelNorm_cuda.c']\n    Defines += [('WITH_CUDA', None)]\n    Objects += ['src/ChannelNorm_kernel.o']\n\nffi = torch.utils.ffi.create_extension(\n    name='_ext.channelnorm',\n    headers=Headers,\n    sources=Sources,\n    verbose=False,\n    with_cuda=True,\n    package=False,\n    relative_to=this_folder,\n    define_macros=Defines,\n    extra_objects=[os.path.join(this_folder, Object) for Object in Objects]\n)\n\nif __name__ == '__main__':\n    ffi.build()"""
inpainting/lib/correlation_package/__init__.py,0,b''
inpainting/lib/correlation_package/build.py,3,"b""import os\nimport torch\nimport torch.utils.ffi\n\nthis_folder = os.path.dirname(os.path.abspath(__file__)) + '/'\n\nHeaders = []\nSources = []\nDefines = []\nObjects = []\n\nif torch.cuda.is_available() == True:\n    Headers += ['src/correlation_cuda.h']\n    Sources += ['src/correlation_cuda.c']\n    Defines += [('WITH_CUDA', None)]\n    Objects += ['src/correlation_cuda_kernel.o']\n\nffi = torch.utils.ffi.create_extension(\n    name='_ext.correlation',\n    headers=Headers,\n    sources=Sources,\n    verbose=False,\n    with_cuda=True,\n    package=False,\n    relative_to=this_folder,\n    define_macros=Defines,\n    extra_objects=[os.path.join(this_folder, Object) for Object in Objects]\n)\n\nif __name__ == '__main__':\n    ffi.build()"""
inpainting/lib/resample2d_package/__init__.py,0,b''
inpainting/lib/resample2d_package/build.py,3,"b""import os\nimport torch\nimport torch.utils.ffi\n\nthis_folder = os.path.dirname(os.path.abspath(__file__)) + '/'\n\nHeaders = []\nSources = []\nDefines = []\nObjects = []\n\nif torch.cuda.is_available() == True:\n    Headers += ['src/Resample2d_cuda.h']\n    Sources += ['src/Resample2d_cuda.c']\n    Defines += [('WITH_CUDA', None)]\n    Objects += ['src/Resample2d_kernel.o']\n\nffi = torch.utils.ffi.create_extension(\n    name='_ext.resample2d',\n    headers=Headers,\n    sources=Sources,\n    verbose=False,\n    with_cuda=True,\n    package=False,\n    relative_to=this_folder,\n    define_macros=Defines,\n    extra_objects=[os.path.join(this_folder, Object) for Object in Objects]\n)\n\nif __name__ == '__main__':\n    ffi.build()"""
inpainting/models/correlation_package/__init__.py,0,b''
inpainting/models/correlation_package/build.py,3,"b""import os\nimport torch\nimport torch.utils.ffi\n\nthis_folder = os.path.dirname(os.path.abspath(__file__)) + '/'\n\nHeaders = []\nSources = []\nDefines = []\nObjects = []\n\nif torch.cuda.is_available() == True:\n    Headers += ['src/correlation_cuda.h']\n    Sources += ['src/correlation_cuda.c']\n    Defines += [('WITH_CUDA', None)]\n    Objects += ['src/correlation_cuda_kernel.o']\n\nffi = torch.utils.ffi.create_extension(\n    name='_ext.correlation',\n    headers=Headers,\n    sources=Sources,\n    verbose=False,\n    with_cuda=True,\n    package=False,\n    relative_to=this_folder,\n    define_macros=Defines,\n    extra_objects=[os.path.join(this_folder, Object) for Object in Objects]\n)\n\nif __name__ == '__main__':\n    ffi.build()"""
get_mask/utils/pysot/datasets/__init__.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom .vot import VOTDataset\n\n\nclass DatasetFactory(object):\n    @staticmethod\n    def create_dataset(**kwargs):\n        """"""\n        Args:\n            name: dataset name \'VOT2018\', \'VOT2016\'\n            dataset_root: dataset root\n        Return:\n            dataset\n        """"""\n        assert \'name\' in kwargs, ""should provide dataset name""\n        name = kwargs[\'name\']\n        if \'VOT2018\' == name or \'VOT2016\' == name:\n            dataset = VOTDataset(**kwargs)\n        else:\n            raise Exception(""unknow dataset {}"".format(kwargs[\'name\']))\n        return dataset\n\n'"
get_mask/utils/pysot/datasets/dataset.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nclass Dataset(object):\n    def __init__(self, name, dataset_root):\n        self.name = name\n        self.dataset_root = dataset_root\n        self.videos = None\n\n    def __getitem__(self, idx):\n        if isinstance(idx, str):\n            return self.videos[idx]\n        elif isinstance(idx, int):\n            return self.videos[sorted(list(self.videos.keys()))[idx]]\n\n    def __len__(self):\n        return len(self.videos)\n\n    def __iter__(self):\n        keys = sorted(list(self.videos.keys()))\n        for key in keys:\n            yield self.videos[key]\n\n    def set_tracker(self, path, tracker_names):\n        """"""\n        Args:\n            path: path to tracker results,\n            tracker_names: list of tracker name\n        """"""\n        self.tracker_path = path\n        self.tracker_names = tracker_names\n'"
get_mask/utils/pysot/datasets/video.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport os\nimport cv2\n\nfrom glob import glob\n\n\nclass Video(object):\n    def __init__(self, name, root, video_dir, init_rect, img_names,\n            gt_rect, attr):\n        self.name = name\n        self.video_dir = video_dir\n        self.init_rect = init_rect\n        self.gt_traj = gt_rect\n        self.attr = attr\n        self.pred_trajs = {}\n        self.img_names = [os.path.join(root, x) for x in img_names]\n        self.imgs = None\n\n    def load_tracker(self, path, tracker_names=None, store=True):\n        """"""\n        Args:\n            path(str): path to result\n            tracker_name(list): name of tracker\n        """"""\n        if not tracker_names:\n            tracker_names = [x.split(\'/\')[-1] for x in glob(path)\n                    if os.path.isdir(x)]\n        if isinstance(tracker_names, str):\n            tracker_names = [tracker_names]\n        for name in tracker_names:\n            traj_file = os.path.join(path, name, self.name+\'.txt\')\n            if os.path.exists(traj_file):\n                with open(traj_file, \'r\') as f :\n                    pred_traj = [list(map(float, x.strip().split(\',\')))\n                            for x in f.readlines()]\n                if len(pred_traj) != len(self.gt_traj):\n                    print(name, len(pred_traj), len(self.gt_traj), self.name)\n                if store:\n                    self.pred_trajs[name] = pred_traj\n                else:\n                    return pred_traj\n            else:\n                print(traj_file)\n        self.tracker_names = list(self.pred_trajs.keys())\n\n    def load_img(self):\n        if self.imgs is None:\n            self.imgs = [cv2.imread(x)\n                            for x in self.img_names]\n            self.width = self.imgs[0].shape[1]\n            self.height = self.imgs[0].shape[0]\n\n    def free_img(self):\n        self.imgs = None\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        if self.imgs is None:\n            return cv2.imread(self.img_names[idx]), \\\n                    self.gt_traj[idx]\n        else:\n            return self.imgs[idx], self.gt_traj[idx]\n\n    def __iter__(self):\n        for i in range(len(self.img_names)):\n            if self.imgs is not None:\n                yield self.imgs[i], self.gt_traj[i]\n            else:\n                yield cv2.imread(self.img_names[i]), \\\n                        self.gt_traj[i]\n'"
get_mask/utils/pysot/datasets/vot.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport os\nimport json\nimport numpy as np\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nfrom .dataset import Dataset\nfrom .video import Video\n\n\nclass VOTVideo(Video):\n    """"""\n    Args:\n        name: video name\n        root: dataset root\n        video_dir: video directory\n        init_rect: init rectangle\n        img_names: image names\n        gt_rect: groundtruth rectangle\n        camera_motion: camera motion tag\n        illum_change: illum change tag\n        motion_change: motion change tag\n        size_change: size change\n        occlusion: occlusion\n    """"""\n    def __init__(self, name, root, video_dir, init_rect, img_names, gt_rect,\n            camera_motion, illum_change, motion_change, size_change, occlusion, width, height):\n        super(VOTVideo, self).__init__(name, root, video_dir, init_rect, img_names, gt_rect, None)\n        self.tags= {\'all\': [1] * len(gt_rect)}\n        self.tags[\'camera_motion\'] = camera_motion\n        self.tags[\'illum_change\'] = illum_change\n        self.tags[\'motion_change\'] = motion_change\n        self.tags[\'size_change\'] = size_change\n        self.tags[\'occlusion\'] = occlusion\n\n        self.width = width\n        self.height = height\n\n        # empty tag\n        all_tag = [v for k, v in self.tags.items() if len(v) > 0 ]\n        self.tags[\'empty\'] = np.all(1 - np.array(all_tag), axis=1).astype(np.int32).tolist()\n\n        self.tag_names = list(self.tags.keys())\n\n    def select_tag(self, tag, start=0, end=0):\n        if tag == \'empty\':\n            return self.tags[tag]\n        return self.tags[tag][start:end]\n\n    def load_tracker(self, path, tracker_names=None, store=True):\n        """"""\n        Args:\n            path(str): path to result\n            tracker_name(list): name of tracker\n        """"""\n        if not tracker_names:\n            tracker_names = [x.split(\'/\')[-1] for x in glob(path)\n                    if os.path.isdir(x)]\n        if isinstance(tracker_names, str):\n            tracker_names = [tracker_names]\n        for name in tracker_names:\n            traj_files = glob(os.path.join(path, name, \'baseline\', self.name, \'*0*.txt\'))\n            if len(traj_files) == 15:\n                traj_files = traj_files\n            else:\n                traj_files = traj_files[0:1]\n            pred_traj = []\n            for traj_file in traj_files:\n                with open(traj_file, \'r\') as f:\n                    traj = [list(map(float, x.strip().split(\',\')))\n                            for x in f.readlines()]\n                    pred_traj.append(traj)\n            if store:\n                self.pred_trajs[name] = pred_traj\n            else:\n                return pred_traj\n\n\nclass VOTDataset(Dataset):\n    """"""\n    Args:\n        name: dataset name, should be \'VOT2018\', \'VOT2016\'\n        dataset_root: dataset root\n        load_img: wether to load all imgs\n    """"""\n    def __init__(self, name, dataset_root):\n        super(VOTDataset, self).__init__(name, dataset_root)\n        try:\n            with open(os.path.join(dataset_root, name+\'.json\'), \'r\') as f:\n                meta_data = json.load(f)\n        except:\n            download_str = \'# download json file for eval toolkit\\n\'+\\\n                           \'cd $SiamMask/data\\n\'+\\\n                           \'wget http://www.robots.ox.ac.uk/~qwang/VOT2016.json\\n\'+\\\n                           \'wget http://www.robots.ox.ac.uk/~qwang/VOT2018.json\'\n            print(download_str)\n            exit()\n\n        # load videos\n        pbar = tqdm(meta_data.keys(), desc=\'loading \'+name, ncols=100)\n        self.videos = {}\n        for video in pbar:\n            pbar.set_postfix_str(video)\n            self.videos[video] = VOTVideo(video,\n                                          dataset_root,\n                                          meta_data[video][\'video_dir\'],\n                                          meta_data[video][\'init_rect\'],\n                                          meta_data[video][\'img_names\'],\n                                          meta_data[video][\'gt_rect\'],\n                                          meta_data[video][\'camera_motion\'],\n                                          meta_data[video][\'illum_change\'],\n                                          meta_data[video][\'motion_change\'],\n                                          meta_data[video][\'size_change\'],\n                                          meta_data[video][\'occlusion\'],\n                                          meta_data[video][\'width\'],\n                                          meta_data[video][\'height\'])\n\n        self.tags = [\'all\', \'camera_motion\', \'illum_change\', \'motion_change\',\n                     \'size_change\', \'occlusion\', \'empty\']\n'"
get_mask/utils/pysot/evaluation/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom .ar_benchmark import AccuracyRobustnessBenchmark\nfrom .eao_benchmark import EAOBenchmark\n'
get_mask/utils/pysot/evaluation/ar_benchmark.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\n\nimport warnings\nimport itertools\nimport numpy as np\n\nfrom colorama import Style, Fore\nfrom ..utils import calculate_failures, calculate_accuracy\n\n\nclass AccuracyRobustnessBenchmark:\n    """"""\n    Args:\n        dataset:\n        burnin:\n    """"""\n    def __init__(self, dataset, burnin=10):\n        self.dataset = dataset\n        self.burnin = burnin\n\n    def eval(self, eval_trackers=None):\n        """"""\n        Args:\n            eval_tags: list of tag\n            eval_trackers: list of tracker name\n        Returns:\n            ret: dict of results\n        """"""\n        if eval_trackers is None:\n            eval_trackers = self.dataset.tracker_names\n        if isinstance(eval_trackers, str):\n            eval_trackers = [eval_trackers]\n\n        result = {}\n        for tracker_name in eval_trackers:\n            accuracy, failures = self._calculate_accuracy_robustness(tracker_name)\n            result[tracker_name] = {\'overlaps\': accuracy,\n                                    \'failures\': failures}\n        return result\n\n    def show_result(self, result, eao_result=None, show_video_level=False, helight_threshold=0.5):\n        """"""pretty print result\n        Args:\n            result: returned dict from function eval\n        """"""\n        tracker_name_len = max((max([len(x) for x in result.keys()])+2), 12)\n        if eao_result is not None:\n            header = ""|{:^""+str(tracker_name_len)+""}|{:^10}|{:^12}|{:^13}|{:^7}|""\n            header = header.format(\'Tracker Name\',\n                    \'Accuracy\', \'Robustness\', \'Lost Number\', \'EAO\')\n            formatter = ""|{:^""+str(tracker_name_len)+""}|{:^10.3f}|{:^12.3f}|{:^13.1f}|{:^7.3f}|""\n        else:\n            header = ""|{:^""+str(tracker_name_len)+""}|{:^10}|{:^12}|{:^13}|""\n            header = header.format(\'Tracker Name\',\n                    \'Accuracy\', \'Robustness\', \'Lost Number\')\n            formatter = ""|{:^""+str(tracker_name_len)+""}|{:^10.3f}|{:^12.3f}|{:^13.1f}|""\n        bar = \'-\'*len(header)\n        print(bar)\n        print(header)\n        print(bar)\n        if eao_result is not None:\n            tracker_eao = sorted(eao_result.items(),\n                                 key=lambda x:x[1][\'all\'],\n                                 reverse=True)[:20]\n            tracker_names = [x[0] for x in tracker_eao]\n        else:\n            tracker_names = list(result.keys())\n        for tracker_name in tracker_names:\n            ret = result[tracker_name]\n            overlaps = list(itertools.chain(*ret[\'overlaps\'].values()))\n            accuracy = np.nanmean(overlaps)\n            length = sum([len(x) for x in ret[\'overlaps\'].values()])\n            failures = list(ret[\'failures\'].values())\n            lost_number = np.mean(np.sum(failures, axis=0))\n            robustness = np.mean(np.sum(np.array(failures), axis=0) / length) * 100\n            if eao_result is None:\n                print(formatter.format(tracker_name, accuracy, robustness, lost_number))\n            else:\n                print(formatter.format(tracker_name, accuracy, robustness, lost_number, eao_result[tracker_name][\'all\']))\n        print(bar)\n\n        if show_video_level and len(result) < 10:\n            print(\'\\n\\n\')\n            header1 = ""|{:^14}|"".format(""Tracker name"")\n            header2 = ""|{:^14}|"".format(""Video name"")\n            for tracker_name in result.keys():\n                header1 += (""{:^17}|"").format(tracker_name)\n                header2 += ""{:^8}|{:^8}|"".format(""Acc"", ""LN"")\n            print(\'-\'*len(header1))\n            print(header1)\n            print(\'-\'*len(header1))\n            print(header2)\n            print(\'-\'*len(header1))\n            videos = list(result[tracker_name][\'overlaps\'].keys())\n            for video in videos:\n                row = ""|{:^14}|"".format(video)\n                for tracker_name in result.keys():\n                    overlaps = result[tracker_name][\'overlaps\'][video]\n                    accuracy = np.nanmean(overlaps)\n                    failures = result[tracker_name][\'failures\'][video]\n                    lost_number = np.mean(failures)\n\n                    accuracy_str = ""{:^8.3f}"".format(accuracy)\n                    if accuracy < helight_threshold:\n                        row += f\'{Fore.RED}{accuracy_str}{Style.RESET_ALL}|\'\n                    else:\n                        row += accuracy_str+\'|\'\n                    lost_num_str = ""{:^8.3f}"".format(lost_number)\n                    if lost_number > 0:\n                        row += f\'{Fore.RED}{lost_num_str}{Style.RESET_ALL}|\'\n                    else:\n                        row += lost_num_str+\'|\'\n                print(row)\n            print(\'-\'*len(header1))\n\n    def _calculate_accuracy_robustness(self, tracker_name):\n        overlaps = {}\n        failures = {}\n        all_length = {}\n        for i in range(len(self.dataset)):\n            video = self.dataset[i]\n            gt_traj = video.gt_traj\n            if tracker_name not in video.pred_trajs:\n                tracker_trajs = video.load_tracker(self.dataset.tracker_path, tracker_name, False)\n            else:\n                tracker_trajs = video.pred_trajs[tracker_name]\n            overlaps_group = []\n            num_failures_group = []\n            for tracker_traj in tracker_trajs:\n                num_failures = calculate_failures(tracker_traj)[0]\n                overlaps_ = calculate_accuracy(tracker_traj, gt_traj,\n                        burnin=10, bound=(video.width, video.height))[1]\n                overlaps_group.append(overlaps_)\n                num_failures_group.append(num_failures)\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"", category=RuntimeWarning)\n                overlaps[video.name] = np.nanmean(overlaps_group, axis=0).tolist()\n                failures[video.name] = num_failures_group\n        return overlaps, failures\n'"
get_mask/utils/pysot/evaluation/eao_benchmark.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport numpy as np\n\nfrom ..utils import calculate_failures, calculate_accuracy, calculate_expected_overlap\n\n\nclass EAOBenchmark:\n    """"""\n    Args:\n        dataset:\n    """"""\n    def __init__(self, dataset, skipping=5, tags=[\'all\']):\n        self.dataset = dataset\n        self.skipping = skipping\n        self.tags = tags\n        # NOTE we not use gmm to generate low, high, peak value\n        if dataset.name in [\'VOT2019\']:\n            self.low = 46\n            self.high = 291\n            self.peak = 128\n        elif dataset.name in [\'VOT2018\', \'VOT2017\']:\n            self.low = 100\n            self.high = 356\n            self.peak = 160\n        elif dataset.name == \'VOT2016\':\n            self.low = 100  # TODO\n            self.high = 356\n            self.peak = 160\n\n    def eval(self, eval_trackers=None):\n        """"""\n        Args:\n            eval_tags: list of tag\n            eval_trackers: list of tracker name\n        Returns:\n            eao: dict of results\n        """"""\n        if eval_trackers is None:\n            eval_trackers = self.dataset.tracker_names\n        if isinstance(eval_trackers, str):\n            eval_trackers = [eval_trackers]\n\n        ret = {}\n        for tracker_name in eval_trackers:\n            eao = self._calculate_eao(tracker_name, self.tags)\n            ret[tracker_name] = eao\n        return ret\n\n    def show_result(self, result, topk=10):\n        """"""pretty print result\n        Args:\n            result: returned dict from function eval\n        """"""\n        if len(self.tags) == 1:\n            tracker_name_len = max((max([len(x) for x in result.keys()])+2), 12)\n            header = (""|{:^""+str(tracker_name_len)+""}|{:^10}|"").format(\'Tracker Name\', \'EAO\')\n            bar = \'-\'*len(header)\n            formatter = ""|{:^20}|{:^10.3f}|""\n            print(bar)\n            print(header)\n            print(bar)\n            tracker_eao = sorted(result.items(), \n                                 key=lambda x: x[1][\'all\'], \n                                 reverse=True)[:topk]\n            for tracker_name, eao in tracker_eao:\n                print(formatter.format(tracker_name, eao))\n            print(bar)\n        else:\n            header = ""|{:^20}|"".format(\'Tracker Name\')\n            header += ""{:^7}|{:^15}|{:^14}|{:^15}|{:^13}|{:^11}|{:^7}|"".format(*self.tags)\n            bar = \'-\'*len(header)\n            formatter = ""{:^7.3f}|{:^15.3f}|{:^14.3f}|{:^15.3f}|{:^13.3f}|{:^11.3f}|{:^7.3f}|""\n            print(bar)\n            print(header)\n            print(bar)\n            sorted_tacker = sorted(result.items(), \n                                   key=lambda x: x[1][\'all\'],\n                                   reverse=True)[:topk]\n            sorted_tacker = [x[0] for x in sorted_tacker]\n            for tracker_name in sorted_tacker:\n                print(""|{:^20}|"".format(tracker_name)+formatter.format(\n                    *[result[tracker_name][x] for x in self.tags]))\n            print(bar)\n\n    def _calculate_eao(self, tracker_name, tags):\n        all_overlaps = []\n        all_failures = []\n        video_names = []\n        gt_traj_length = []\n        for video in self.dataset:\n            gt_traj = video.gt_traj\n            if tracker_name not in video.pred_trajs:\n                tracker_trajs = video.load_tracker(self.dataset.tracker_path, tracker_name, False)\n            else:\n                tracker_trajs = video.pred_trajs[tracker_name]\n            for tracker_traj in tracker_trajs:\n                gt_traj_length.append(len(gt_traj))\n                video_names.append(video.name)\n                overlaps = calculate_accuracy(tracker_traj, gt_traj, bound=(video.width-1, video.height-1))[1]\n                failures = calculate_failures(tracker_traj)[1]\n                all_overlaps.append(overlaps)\n                all_failures.append(failures)\n        fragment_num = sum([len(x)+1 for x in all_failures])\n        max_len = max([len(x) for x in all_overlaps])\n        seq_weight = 1 / len(tracker_trajs)\n\n        eao = {}\n        for tag in tags:\n            # prepare segments\n            fweights = np.ones((fragment_num)) * np.nan\n            fragments = np.ones((fragment_num, max_len)) * np.nan\n            seg_counter = 0\n            for name, traj_len, failures, overlaps in zip(video_names, gt_traj_length,\n                    all_failures, all_overlaps):\n                if len(failures) > 0:\n                    points = [x+self.skipping for x in failures if\n                            x+self.skipping <= len(overlaps)]\n                    points.insert(0, 0)\n                    for i in range(len(points)):\n                        if i != len(points) - 1:\n                            fragment = np.array(overlaps[points[i]:points[i+1]+1])\n                            fragments[seg_counter, :] = 0\n                        else:\n                            fragment = np.array(overlaps[points[i]:])\n                        fragment[np.isnan(fragment)] = 0\n                        fragments[seg_counter, :len(fragment)] = fragment\n                        if i != len(points) - 1:\n                            tag_value = self.dataset[name].select_tag(tag, points[i], points[i+1]+1)\n                            w = sum(tag_value) / (points[i+1] - points[i]+1)\n                            fweights[seg_counter] = seq_weight * w\n                        else:\n                            tag_value = self.dataset[name].select_tag(tag, points[i], len(overlaps))\n                            w = sum(tag_value) / (traj_len - points[i]+1e-16)\n                            fweights[seg_counter] = seq_weight * w\n                        seg_counter += 1\n                else:\n                    # no failure\n                    max_idx = min(len(overlaps), max_len)\n                    fragments[seg_counter, :max_idx] = overlaps[:max_idx]\n                    tag_value = self.dataset[name].select_tag(tag, 0, max_idx)\n                    w = sum(tag_value) / max_idx\n                    fweights[seg_counter] = seq_weight * w\n                    seg_counter += 1\n\n            expected_overlaps = calculate_expected_overlap(fragments, fweights)\n            # caculate eao\n            weight = np.zeros((len(expected_overlaps)))\n            weight[self.low-1:self.high-1+1] = 1\n            is_valid = np.logical_not(np.isnan(expected_overlaps))\n            eao_ = np.sum(expected_overlaps[is_valid] * weight[is_valid]) / np.sum(weight[is_valid])\n            eao[tag] = eao_\n        return eao\n'"
get_mask/utils/pysot/utils/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom . import region\nfrom .statistics import *\n'
get_mask/utils/pysot/utils/misc.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport numpy as np\n\ndef determine_thresholds(confidence, resolution=100):\n    """"""choose threshold according to confidence\n\n    Args:\n        confidence: list or numpy array or numpy array\n        reolution: number of threshold to choose\n\n    Restures:\n        threshold: numpy array\n    """"""\n    if isinstance(confidence, list):\n        confidence = np.array(confidence)\n    confidence = confidence.flatten()\n    confidence = confidence[~np.isnan(confidence)]\n    confidence.sort()\n\n    assert len(confidence) > resolution and resolution > 2\n\n    thresholds = np.ones((resolution))\n    thresholds[0] = - np.inf\n    thresholds[-1] = np.inf\n    delta = np.floor(len(confidence) / (resolution - 2))\n    idxs = np.linspace(delta, len(confidence)-delta, resolution-2, dtype=np.int32)\n    thresholds[1:-1] =  confidence[idxs]\n    return thresholds\n'"
get_mask/utils/pysot/utils/setup.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize([Extension(""region"", [""region.pyx"", ""src/region.c""])]),\n)\n\n'"
get_mask/utils/pysot/utils/statistics.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\n\nimport numpy as np\nfrom numba import jit\nfrom . import region\n\ndef calculate_failures(trajectory):\n    """""" Calculate number of failures\n    Args:\n        trajectory: list of bbox\n    Returns:\n        num_failures: number of failures\n        failures: failures point in trajectory, start with 0\n    """"""\n    failures = [i for i, x in zip(range(len(trajectory)), trajectory)\n            if len(x) == 1 and x[0] == 2]\n    num_failures = len(failures)\n    return num_failures, failures\n\ndef calculate_accuracy(pred_trajectory, gt_trajectory,\n        burnin=0, ignore_unknown=True, bound=None):\n    """"""Caculate accuracy socre as average overlap over the entire sequence\n    Args:\n        trajectory: list of bbox\n        gt_trajectory: list of bbox\n        burnin: number of frames that have to be ignored after the failure\n        ignore_unknown: ignore frames where the overlap is unknown\n        bound: bounding region\n    Return:\n        acc: average overlap\n        overlaps: per frame overlaps\n    """"""\n    pred_trajectory_ = pred_trajectory\n    if not ignore_unknown:\n        unkown = [len(x)==1 and x[0] == 0 for x in pred_trajectory]\n    \n    if burnin > 0:\n        pred_trajectory_ = pred_trajectory[:]\n        mask = [len(x)==1 and x[0] == 1 for x in pred_trajectory]\n        for i in range(len(mask)):\n            if mask[i]:\n                for j in range(burnin):\n                    if i + j < len(mask):\n                        pred_trajectory_[i+j] = [0]\n    min_len = min(len(pred_trajectory_), len(gt_trajectory))\n    overlaps = region.vot_overlap_traj(pred_trajectory_[:min_len],\n            gt_trajectory[:min_len], bound)\n\n    if not ignore_unknown:\n        overlaps = [u if u else 0 for u in unkown]\n\n    acc = 0\n    if len(overlaps) > 0:\n        acc = np.nanmean(overlaps)\n    return acc, overlaps\n\n@jit(nopython=True)\ndef overlap_ratio(rect1, rect2):\n    \'\'\'Compute overlap ratio between two rects\n    Args\n        rect:2d array of N x [x,y,w,h]\n    Return:\n        iou\n    \'\'\'\n    # if rect1.ndim==1:\n    #     rect1 = rect1[np.newaxis, :]\n    # if rect2.ndim==1:\n    #     rect2 = rect2[np.newaxis, :]\n    left = np.maximum(rect1[:,0], rect2[:,0])\n    right = np.minimum(rect1[:,0]+rect1[:,2], rect2[:,0]+rect2[:,2])\n    top = np.maximum(rect1[:,1], rect2[:,1])\n    bottom = np.minimum(rect1[:,1]+rect1[:,3], rect2[:,1]+rect2[:,3])\n\n    intersect = np.maximum(0,right - left) * np.maximum(0,bottom - top)\n    union = rect1[:,2]*rect1[:,3] + rect2[:,2]*rect2[:,3] - intersect\n    iou = intersect / union\n    iou = np.maximum(np.minimum(1, iou), 0)\n    return iou\n\n@jit(nopython=True)\ndef success_overlap(gt_bb, result_bb, n_frame):\n    thresholds_overlap = np.arange(0, 1.05, 0.05)\n    success = np.zeros(len(thresholds_overlap))\n    iou = np.ones(len(gt_bb)) * (-1)\n    mask = np.sum(gt_bb > 0, axis=1) == 4\n    iou[mask] = overlap_ratio(gt_bb[mask], result_bb[mask])\n    for i in range(len(thresholds_overlap)):\n        success[i] = np.sum(iou > thresholds_overlap[i]) / float(n_frame)\n    return success\n\n@jit(nopython=True)\ndef success_error(gt_center, result_center, thresholds, n_frame):\n    # n_frame = len(gt_center)\n    success = np.zeros(len(thresholds))\n    dist = np.ones(len(gt_center)) * (-1)\n    mask = np.sum(gt_center > 0, axis=1) == 2\n    dist[mask] = np.sqrt(np.sum(\n        np.power(gt_center[mask] - result_center[mask], 2), axis=1))\n    for i in range(len(thresholds)):\n        success[i] = np.sum(dist <= thresholds[i]) / float(n_frame)\n    return success\n\n@jit(nopython=True)\ndef determine_thresholds(scores, resolution=100):\n    """"""\n    Args:\n        scores: 1d array of score\n    """"""\n    scores = np.sort(scores[np.logical_not(np.isnan(scores))])\n    delta = np.floor(len(scores) / (resolution - 2))\n    idxs = np.floor(np.linspace(delta-1, len(scores)-delta, resolution-2)+0.5).astype(np.int32)\n    thresholds = np.zeros((resolution))\n    thresholds[0] = - np.inf\n    thresholds[-1] = np.inf\n    thresholds[1:-1] = scores[idxs]\n    return thresholds\n\n@jit(nopython=True)\ndef calculate_f1(overlaps, score, bound, thresholds, N):\n    overlaps = np.array(overlaps)\n    overlaps[np.isnan(overlaps)] = 0\n    score = np.array(score)\n    score[np.isnan(score)] = 0\n    precision = np.zeros(len(thresholds))\n    recall = np.zeros(len(thresholds))\n    for i, th in enumerate(thresholds):\n        if th == - np.inf:\n            idx = score > 0\n        else:\n            idx = score >= th\n        if np.sum(idx) == 0:\n            precision[i] = 1\n            recall[i] = 0\n        else:\n            precision[i] = np.mean(overlaps[idx])\n            recall[i] = np.sum(overlaps[idx]) / N\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1, precision, recall\n\n@jit(nopython=True)\ndef calculate_expected_overlap(fragments, fweights):\n    max_len = fragments.shape[1]\n    expected_overlaps = np.zeros((max_len), np.float32)\n    expected_overlaps[0] = 1\n\n    # TODO Speed Up \n    for i in range(1, max_len):\n        mask = np.logical_not(np.isnan(fragments[:, i]))\n        if np.any(mask):\n            fragment = fragments[mask, 1:i+1]\n            seq_mean = np.sum(fragment, 1) / fragment.shape[1]\n            expected_overlaps[i] = np.sum(seq_mean *\n                fweights[mask]) / np.sum(fweights[mask])\n    return expected_overlaps\n'"
inpainting/lib/channelnorm_package/_ext/__init__.py,0,b''
inpainting/lib/channelnorm_package/functions/__init__.py,0,b''
inpainting/lib/channelnorm_package/functions/channelnorm.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import channelnorm\n\n\nclass ChannelNormFunction(Function):\n\n    def __init__(self, norm_deg=2):\n        super(ChannelNormFunction, self).__init__()\n        self.norm_deg = norm_deg\n\n    def forward(self, input1):\n        # self.save_for_backward(input1)\n\n        assert(input1.is_contiguous() == True)\n\n        with torch.cuda.device_of(input1):\n            b, _, h, w = input1.size()\n            output = input1.new().resize_(b, 1, h, w).zero_()\n\n            channelnorm.ChannelNorm_cuda_forward(input1, output, self.norm_deg)\n        self.save_for_backward(input1, output)\n\n        return output\n\n    def backward(self, gradOutput):\n        input1, output = self.saved_tensors\n\n        with torch.cuda.device_of(input1):\n            b, c, h, w = input1.size()\n            gradInput1 = input1.new().resize_(b,c,h,w).zero_()\n\n            channelnorm.ChannelNorm_cuda_backward(input1, output, gradOutput, gradInput1, self.norm_deg)\n\n        return gradInput1'"
inpainting/lib/channelnorm_package/modules/__init__.py,0,b''
inpainting/lib/channelnorm_package/modules/channelnorm.py,1,"b'from torch.nn.modules.module import Module\n\nfrom ..functions.channelnorm import ChannelNormFunction\n\nclass ChannelNorm(Module):\n    def __init__(self, norm_deg=2):\n        super(ChannelNorm, self).__init__()\n        self.norm_deg = norm_deg\n\n    def forward(self, input1):\n\n        result = ChannelNormFunction(self.norm_deg)(input1)\n\n        return result\n'"
inpainting/lib/correlation_package/_ext/__init__.py,0,b''
inpainting/lib/correlation_package/functions/__init__.py,0,b''
inpainting/lib/correlation_package/functions/correlation.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import correlation\n\n\nclass CorrelationFunction(Function):\n\n    def __init__(self, pad_size=3, kernel_size=3, max_displacement=20, stride1=1, stride2=2, corr_multiply=1):\n        super(CorrelationFunction, self).__init__()\n        self.pad_size = pad_size\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride1 = stride1\n        self.stride2 = stride2\n        self.corr_multiply = corr_multiply\n        # self.out_channel = ((max_displacement/stride2)*2 + 1) * ((max_displacement/stride2)*2 + 1)\n\n    def forward(self, input1, input2):\n        self.save_for_backward(input1, input2)\n\n        assert(input1.is_contiguous() == True)\n        assert(input2.is_contiguous() == True)\n\n        with torch.cuda.device_of(input1):\n            rbot1 = input1.new()\n            rbot2 = input2.new()\n            output = input1.new()\n\n            correlation.Correlation_forward_cuda(input1, input2, rbot1, rbot2, output, \n                self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)\n\n        return output\n\n    def backward(self, grad_output):\n        input1, input2 = self.saved_tensors\n        \n        assert(grad_output.is_contiguous() == True)\n\n        with torch.cuda.device_of(input1):\n            rbot1 = input1.new()\n            rbot2 = input2.new()\n\n            grad_input1 = input1.new()\n            grad_input2 = input2.new()\n\n            correlation.Correlation_backward_cuda(input1, input2, rbot1, rbot2, grad_output, grad_input1, grad_input2,\n                self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)\n\n        return grad_input1, grad_input2'"
inpainting/lib/correlation_package/modules/__init__.py,0,b''
inpainting/lib/correlation_package/modules/correlation.py,1,"b'from torch.nn.modules.module import Module\n\nfrom ..functions.correlation import CorrelationFunction\n\nclass Correlation(Module):\n    def __init__(self, pad_size=0, kernel_size=0, max_displacement=0, stride1=1, stride2=2, corr_multiply=1):\n        super(Correlation, self).__init__()\n        self.pad_size = pad_size\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride1 = stride1\n        self.stride2 = stride2\n        self.corr_multiply = corr_multiply\n\n    def forward(self, input1, input2):\n\n        result = CorrelationFunction(self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)(input1, input2)\n\n        return result\n'"
inpainting/lib/resample2d_package/_ext/__init__.py,0,b''
inpainting/lib/resample2d_package/functions/__init__.py,0,b''
inpainting/lib/resample2d_package/functions/resample2d.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import resample2d\n\n\nclass Resample2dFunction(Function):\n\n    def __init__(self, kernel_size=1):\n        super(Resample2dFunction, self).__init__()\n        self.kernel_size = kernel_size\n\n    def forward(self, input1, input2):\n        self.save_for_backward(input1, input2)\n\n        assert(input1.is_contiguous() == True)\n        assert(input2.is_contiguous() == True)\n\n        with torch.cuda.device_of(input1):\n            _, d, _, _ = input1.size() \n            b, _, h, w = input2.size()\n            output = input1.new().resize_(b, d, h, w).zero_()\n\n            resample2d.Resample2d_cuda_forward(input1, input2, output, self.kernel_size)\n\n        return output\n\n    def backward(self, gradOutput):\n        input1, input2 = self.saved_tensors\n\n        assert(gradOutput.is_contiguous() == True)\n        \n        with torch.cuda.device_of(input1):\n            b, c, h, w = input1.size()\n            gradInput1 = input1.new().resize_(b,c,h,w).zero_()\n\n            b, c, h, w = input2.size()\n            gradInput2 = input2.new().resize_(b,c,h,w).zero_()\n\n            resample2d.Resample2d_cuda_backward(input1, input2, gradOutput, gradInput1, gradInput2, self.kernel_size)\n\n        return gradInput1, gradInput2'"
inpainting/lib/resample2d_package/modules/__init__.py,0,b''
inpainting/lib/resample2d_package/modules/resample2d.py,1,"b'from torch.nn.modules.module import Module\n\nfrom ..functions.resample2d import Resample2dFunction\n\nclass Resample2d(Module):\n    def __init__(self, kernel_size=1):\n        super(Resample2d, self).__init__()\n        self.kernel_size = kernel_size\n\n    def forward(self, input1, input2):\n        input1_c = input1.contiguous()\n\n        result = Resample2dFunction(self.kernel_size)(input1_c, input2)\n\n        return result\n'"
inpainting/models/correlation_package/_ext/__init__.py,0,b''
inpainting/models/correlation_package/functions/__init__.py,0,b''
inpainting/models/correlation_package/functions/correlation.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import correlation\n\n\nclass CorrelationFunction(Function):\n\n    def __init__(self, pad_size=3, kernel_size=3, max_displacement=20, stride1=1, stride2=2, corr_multiply=1):\n        super(CorrelationFunction, self).__init__()\n        self.pad_size = pad_size\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride1 = stride1\n        self.stride2 = stride2\n        self.corr_multiply = corr_multiply\n        # self.out_channel = ((max_displacement/stride2)*2 + 1) * ((max_displacement/stride2)*2 + 1)\n\n    def forward(self, input1, input2):\n        self.save_for_backward(input1, input2)\n\n        assert(input1.is_contiguous() == True)\n        assert(input2.is_contiguous() == True)\n\n        with torch.cuda.device_of(input1):\n            rbot1 = input1.new()\n            rbot2 = input2.new()\n            output = input1.new()\n\n            correlation.Correlation_forward_cuda(input1, input2, rbot1, rbot2, output, \n                self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)\n\n        return output\n\n    def backward(self, grad_output):\n        input1, input2 = self.saved_tensors\n        \n        # assert(grad_output.is_contiguous() == True)\n        if not grad_output.is_contiguous():\n            grad_output = grad_output.contiguous()\n            \n        with torch.cuda.device_of(input1):\n            rbot1 = input1.new()\n            rbot2 = input2.new()\n\n            grad_input1 = input1.new()\n            grad_input2 = input2.new()\n\n            correlation.Correlation_backward_cuda(input1, input2, rbot1, rbot2, grad_output, grad_input1, grad_input2,\n                self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)\n\n        return grad_input1, grad_input2'"
inpainting/models/correlation_package/modules/__init__.py,0,b''
inpainting/models/correlation_package/modules/correlation.py,1,"b'from torch.nn.modules.module import Module\n\nfrom ..functions.correlation import CorrelationFunction\n\nclass Correlation(Module):\n    def __init__(self, pad_size=0, kernel_size=0, max_displacement=0, stride1=1, stride2=2, corr_multiply=1):\n        super(Correlation, self).__init__()\n        self.pad_size = pad_size\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride1 = stride1\n        self.stride2 = stride2\n        self.corr_multiply = corr_multiply\n\n    def forward(self, input1, input2):\n\n        result = CorrelationFunction(self.pad_size, self.kernel_size, self.max_displacement,self.stride1, self.stride2, self.corr_multiply)(input1, input2)\n\n        return result\n'"
inpainting/lib/channelnorm_package/_ext/channelnorm/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._channelnorm import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
inpainting/lib/correlation_package/_ext/correlation/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._correlation import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
inpainting/lib/resample2d_package/_ext/resample2d/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._resample2d import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
inpainting/models/correlation_package/_ext/correlation/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._correlation import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
