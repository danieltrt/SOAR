file_path,api_count,code
celebA_data_preprocess.py,0,"b""import os\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imresize\n\n# root path depends on your computer\nroot = 'data/celebA/celebA/'\nsave_root = 'data/resized_celebA/'\nresize_size = 64\n\nif not os.path.isdir(save_root):\n    os.mkdir(save_root)\nif not os.path.isdir(save_root + 'celebA'):\n    os.mkdir(save_root + 'celebA')\nimg_list = os.listdir(root)\n\n# ten_percent = len(img_list) // 10\n\nfor i in range(len(img_list)):\n    img = plt.imread(root + img_list[i])\n    img = imresize(img, (resize_size, resize_size))\n    plt.imsave(fname=save_root + 'celebA/' + img_list[i], arr=img)\n\n    if (i % 1000) == 0:\n        print('%d images complete' % i)"""
pytorch_CelebA_DCGAN.py,18,"b'import os, time, sys\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(generator, self).__init__()\n        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n        self.deconv1_bn = nn.BatchNorm2d(d*8)\n        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d*4)\n        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d*2)\n        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(d)\n        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        # x = F.relu(self.deconv1(input))\n        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n        x = F.tanh(self.deconv5(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d*2)\n        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d*4)\n        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(d*8)\n        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        x = F.leaky_relu(self.conv1(input), 0.2)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n        x = F.sigmoid(self.conv5(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n\nfixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\nfixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\', isFix=False):\n    z_ = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n    z_ = Variable(z_.cuda(), volatile=True)\n\n    G.eval()\n    if isFix:\n        test_images = G(fixed_z_)\n    else:\n        test_images = G(z_)\n    G.train()\n\n    size_figure_grid = 5\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(5*5):\n        i = k // 5\n        j = k % 5\n        ax[i, j].cla()\n        ax[i, j].imshow((test_images[k].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n    plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Iter\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 20\n\n# data_loader\nimg_size = 64\nisCrop = False\nif isCrop:\n    transform = transforms.Compose([\n        transforms.Scale(108),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    ])\nelse:\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    ])\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ndata_dir = \'data/resized_celebA\'          # this path depends on your computer\ndset = datasets.ImageFolder(data_dir, transform)\ntrain_loader = torch.utils.data.DataLoader(dset, batch_size=128, shuffle=True)\ntemp = plt.imread(train_loader.dataset.imgs[0][0])\nif (temp.shape[0] != img_size) or (temp.shape[0] != img_size):\n    sys.stderr.write(\'Error! image size is not 64 x 64! run \\""celebA_data_preprocess.py\\"" !!!\')\n    sys.exit(1)\n\n# network\nG = generator(128)\nD = discriminator(128)\nG.weight_init(mean=0.0, std=0.02)\nD.weight_init(mean=0.0, std=0.02)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# results save folder\nif not os.path.isdir(\'CelebA_DCGAN_results\'):\n    os.mkdir(\'CelebA_DCGAN_results\')\nif not os.path.isdir(\'CelebA_DCGAN_results/Random_results\'):\n    os.mkdir(\'CelebA_DCGAN_results/Random_results\')\nif not os.path.isdir(\'CelebA_DCGAN_results/Fixed_results\'):\n    os.mkdir(\'CelebA_DCGAN_results/Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\ntrain_hist[\'per_epoch_ptimes\'] = []\ntrain_hist[\'total_ptime\'] = []\n\nprint(\'Training start!\')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n\n    # learning rate decay\n    if (epoch+1) == 11:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    if (epoch+1) == 16:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    num_iter = 0\n\n    epoch_start_time = time.time()\n    for x_, _ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n        \n        if isCrop:\n            x_ = x_[:, :, 22:86, 22:86]\n\n        mini_batch = x_.size()[0]\n\n        y_real_ = torch.ones(mini_batch)\n        y_fake_ = torch.zeros(mini_batch)\n\n        x_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n        D_result = D(x_).squeeze()\n        D_real_loss = BCE_loss(D_result, y_real_)\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        z_ = Variable(z_.cuda())\n        G_result = G(z_)\n\n        D_result = D(G_result).squeeze()\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result.data.mean()\n\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        z_ = Variable(z_.cuda())\n\n        G_result = G(z_)\n        D_result = D(G_result).squeeze()\n        G_train_loss = BCE_loss(D_result, y_real_)\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n        num_iter += 1\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n\n    print(\'[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f\' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    p = \'CelebA_DCGAN_results/Random_results/CelebA_DCGAN_\' + str(epoch + 1) + \'.png\'\n    fixed_p = \'CelebA_DCGAN_results/Fixed_results/CelebA_DCGAN_\' + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=p, isFix=False)\n    show_result((epoch+1), save=True, path=fixed_p, isFix=True)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n    train_hist[\'per_epoch_ptimes\'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist[\'total_ptime\'].append(total_ptime)\n\nprint(""Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f"" % (torch.mean(torch.FloatTensor(train_hist[\'per_epoch_ptimes\'])), train_epoch, total_ptime))\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), ""CelebA_DCGAN_results/generator_param.pkl"")\ntorch.save(D.state_dict(), ""CelebA_DCGAN_results/discriminator_param.pkl"")\nwith open(\'CelebA_DCGAN_results/train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=\'CelebA_DCGAN_results/CelebA_DCGAN_train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = \'CelebA_DCGAN_results/Fixed_results/CelebA_DCGAN_\' + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(\'CelebA_DCGAN_results/generation_animation.gif\', images, fps=5)\n'"
pytorch_MNIST_DCGAN.py,18,"b'import os, time\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(generator, self).__init__()\n        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n        self.deconv1_bn = nn.BatchNorm2d(d*8)\n        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d*4)\n        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d*2)\n        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(d)\n        self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        # x = F.relu(self.deconv1(input))\n        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n        x = F.tanh(self.deconv5(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d*2)\n        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d*4)\n        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(d*8)\n        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        x = F.leaky_relu(self.conv1(input), 0.2)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n        x = F.sigmoid(self.conv5(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n\nfixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\nfixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\', isFix=False):\n    z_ = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n    z_ = Variable(z_.cuda(), volatile=True)\n\n    G.eval()\n    if isFix:\n        test_images = G(fixed_z_)\n    else:\n        test_images = G(z_)\n    G.train()\n\n    size_figure_grid = 5\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(5*5):\n        i = k // 5\n        j = k % 5\n        ax[i, j].cla()\n        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap=\'gray\')\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n    plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Iter\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 20\n\n# data_loader\nimg_size = 64\ntransform = transforms.Compose([\n        transforms.Scale(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'data\', train=True, download=True, transform=transform),\n    batch_size=batch_size, shuffle=True)\n\n# network\nG = generator(128)\nD = discriminator(128)\nG.weight_init(mean=0.0, std=0.02)\nD.weight_init(mean=0.0, std=0.02)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# results save folder\nif not os.path.isdir(\'MNIST_DCGAN_results\'):\n    os.mkdir(\'MNIST_DCGAN_results\')\nif not os.path.isdir(\'MNIST_DCGAN_results/Random_results\'):\n    os.mkdir(\'MNIST_DCGAN_results/Random_results\')\nif not os.path.isdir(\'MNIST_DCGAN_results/Fixed_results\'):\n    os.mkdir(\'MNIST_DCGAN_results/Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\ntrain_hist[\'per_epoch_ptimes\'] = []\ntrain_hist[\'total_ptime\'] = []\nnum_iter = 0\n\nprint(\'training start!\')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n    epoch_start_time = time.time()\n    for x_, _ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n\n        mini_batch = x_.size()[0]\n\n        y_real_ = torch.ones(mini_batch)\n        y_fake_ = torch.zeros(mini_batch)\n\n        x_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n        D_result = D(x_).squeeze()\n        D_real_loss = BCE_loss(D_result, y_real_)\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        z_ = Variable(z_.cuda())\n        G_result = G(z_)\n\n        D_result = D(G_result).squeeze()\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result.data.mean()\n\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        # D_losses.append(D_train_loss.data[0])\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        z_ = Variable(z_.cuda())\n\n        G_result = G(z_)\n        D_result = D(G_result).squeeze()\n        G_train_loss = BCE_loss(D_result, y_real_)\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n        num_iter += 1\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n\n    print(\'[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f\' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    p = \'MNIST_DCGAN_results/Random_results/MNIST_DCGAN_\' + str(epoch + 1) + \'.png\'\n    fixed_p = \'MNIST_DCGAN_results/Fixed_results/MNIST_DCGAN_\' + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=p, isFix=False)\n    show_result((epoch+1), save=True, path=fixed_p, isFix=True)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n    train_hist[\'per_epoch_ptimes\'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist[\'total_ptime\'].append(total_ptime)\n\nprint(""Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f"" % (torch.mean(torch.FloatTensor(train_hist[\'per_epoch_ptimes\'])), train_epoch, total_ptime))\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), ""MNIST_DCGAN_results/generator_param.pkl"")\ntorch.save(D.state_dict(), ""MNIST_DCGAN_results/discriminator_param.pkl"")\nwith open(\'MNIST_DCGAN_results/train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=\'MNIST_DCGAN_results/MNIST_DCGAN_train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = \'MNIST_DCGAN_results/Fixed_results/MNIST_DCGAN_\' + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(\'MNIST_DCGAN_results/generation_animation.gif\', images, fps=5)\n'"
pytorch_MNIST_GAN.py,17,"b'import os\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self, input_size=32, n_class = 10):\n        super(generator, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n        self.fc3 = nn.Linear(self.fc2.out_features, 1024)\n        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n\n    # forward method\n    def forward(self, input):\n        x = F.leaky_relu(self.fc1(input), 0.2)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = F.tanh(self.fc4(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, input_size=32, n_class=10):\n        super(discriminator, self).__init__()\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n        self.fc3 = nn.Linear(self.fc2.out_features, 256)\n        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n\n    # forward method\n    def forward(self, input):\n        x = F.leaky_relu(self.fc1(input), 0.2)\n        x = F.dropout(x, 0.3)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = F.dropout(x, 0.3)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = F.dropout(x, 0.3)\n        x = F.sigmoid(self.fc4(x))\n\n        return x\n\nfixed_z_ = torch.randn((5 * 5, 100))    # fixed noise\nfixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\', isFix=False):\n    z_ = torch.randn((5*5, 100))\n    z_ = Variable(z_.cuda(), volatile=True)\n\n    G.eval()\n    if isFix:\n        test_images = G(fixed_z_)\n    else:\n        test_images = G(z_)\n    G.train()\n\n    size_figure_grid = 5\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(5*5):\n        i = k // 5\n        j = k % 5\n        ax[i, j].cla()\n        ax[i, j].imshow(test_images[k, :].cpu().data.view(28, 28).numpy(), cmap=\'gray\')\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n    plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 100\n\n# data_loader\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'data\', train=True, download=True, transform=transform),\n    batch_size=batch_size, shuffle=True)\n\n# network\nG = generator(input_size=100, n_class=28*28)\nD = discriminator(input_size=28*28, n_class=1)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr)\nD_optimizer = optim.Adam(D.parameters(), lr=lr)\n\n# results save folder\nif not os.path.isdir(\'MNIST_GAN_results\'):\n    os.mkdir(\'MNIST_GAN_results\')\nif not os.path.isdir(\'MNIST_GAN_results/Random_results\'):\n    os.mkdir(\'MNIST_GAN_results/Random_results\')\nif not os.path.isdir(\'MNIST_GAN_results/Fixed_results\'):\n    os.mkdir(\'MNIST_GAN_results/Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n    for x_, _ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n\n        x_ = x_.view(-1, 28 * 28)\n\n        mini_batch = x_.size()[0]\n\n        y_real_ = torch.ones(mini_batch)\n        y_fake_ = torch.zeros(mini_batch)\n\n        x_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n        D_result = D(x_)\n        D_real_loss = BCE_loss(D_result, y_real_)\n        D_real_score = D_result\n\n        z_ = torch.randn((mini_batch, 100))\n        z_ = Variable(z_.cuda())\n        G_result = G(z_)\n\n        D_result = D(G_result)\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result\n\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.randn((mini_batch, 100))\n        y_ = torch.ones(mini_batch)\n\n        z_, y_ = Variable(z_.cuda()), Variable(y_.cuda())\n        G_result = G(z_)\n        D_result = D(G_result)\n        G_train_loss = BCE_loss(D_result, y_)\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n    print(\'[%d/%d]: loss_d: %.3f, loss_g: %.3f\' % (\n        (epoch + 1), train_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))\n    p = \'MNIST_GAN_results/Random_results/MNIST_GAN_\' + str(epoch + 1) + \'.png\'\n    fixed_p = \'MNIST_GAN_results/Fixed_results/MNIST_GAN_\' + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=p, isFix=False)\n    show_result((epoch+1), save=True, path=fixed_p, isFix=True)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n\n\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), ""MNIST_GAN_results/generator_param.pkl"")\ntorch.save(D.state_dict(), ""MNIST_GAN_results/discriminator_param.pkl"")\nwith open(\'MNIST_GAN_results/train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=\'MNIST_GAN_results/MNIST_GAN_train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = \'MNIST_GAN_results/Fixed_results/MNIST_GAN_\' + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(\'MNIST_GAN_results/generation_animation.gif\', images, fps=5)\n'"
