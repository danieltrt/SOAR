file_path,api_count,code
celebA_data_preprocess.py,0,"b""import os\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imresize\n\n# root path depends on your computer\nroot = 'data/celebA/celebA/'\nsave_root = 'data/resized_celebA/'\nresize_size = 64\n\nif not os.path.isdir(save_root):\n    os.mkdir(save_root)\nif not os.path.isdir(save_root + 'celebA'):\n    os.mkdir(save_root + 'celebA')\nimg_list = os.listdir(root)\n\n# ten_percent = len(img_list) // 10\n\nfor i in range(len(img_list)):\n    img = plt.imread(root + img_list[i])\n    img = imresize(img, (resize_size, resize_size))\n    plt.imsave(fname=save_root + 'celebA/' + img_list[i], arr=img)\n\n    if (i % 1000) == 0:\n        print('%d images complete' % i)"""
pytorch_CelebA_cDCGAN.py,36,"b'import os, time, sys\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(generator, self).__init__()\n        self.deconv1_1 = nn.ConvTranspose2d(100, d*4, 4, 1, 0)\n        self.deconv1_1_bn = nn.BatchNorm2d(d*4)\n        self.deconv1_2 = nn.ConvTranspose2d(2, d*4, 4, 1, 0)\n        self.deconv1_2_bn = nn.BatchNorm2d(d*4)\n        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d*4)\n        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d*2)\n        # self.deconv4 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(d)\n        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    # def forward(self, input):\n    def forward(self, input, label):\n        x = F.leaky_relu(self.deconv1_1_bn(self.deconv1_1(input)), 0.2)\n        y = F.leaky_relu(self.deconv1_2_bn(self.deconv1_2(label)), 0.2)\n        x = torch.cat([x, y], 1)\n        x = F.leaky_relu(self.deconv2_bn(self.deconv2(x)), 0.2)\n        x = F.leaky_relu(self.deconv3_bn(self.deconv3(x)), 0.2)\n        # x = F.tanh(self.deconv4(x))\n        x = F.leaky_relu(self.deconv4_bn(self.deconv4(x)), 0.2)\n        x = F.tanh(self.deconv5(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(discriminator, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, d/2, 4, 2, 1)\n        self.conv1_2 = nn.Conv2d(2, d/2, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d*2)\n        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d*4)\n        # self.conv4 = nn.Conv2d(d*4, 1, 4, 1, 0)\n        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(d*8)\n        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    # def forward(self, input):\n    def forward(self, input, label):\n        x = F.leaky_relu(self.conv1_1(input), 0.2)\n        y = F.leaky_relu(self.conv1_2(label), 0.2)\n        x = torch.cat([x, y], 1)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        # x = F.sigmoid(self.conv4(x))\n        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n        x = F.sigmoid(self.conv5(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n\n# label preprocess\nimg_size = 64\nonehot = torch.zeros(2, 2)\nonehot = onehot.scatter_(1, torch.LongTensor([0, 1]).view(2, 1), 1).view(2, 2, 1, 1)\nfill = torch.zeros([2, 2, img_size, img_size])\nfor i in range(2):\n    fill[i, i, :, :] = 1\n\nwith open(\'data/resized_celebA/gender_label.pkl\', \'rb\') as fp:\n    y_gender_ = pickle.load(fp)\n\ny_gender_ = torch.LongTensor(y_gender_).squeeze()\n\n# fixed noise & label\ntemp_z0_ = torch.randn(4, 100)\ntemp_z0_ = torch.cat([temp_z0_, temp_z0_], 0)\ntemp_z1_ = torch.randn(4, 100)\ntemp_z1_ = torch.cat([temp_z1_, temp_z1_], 0)\n\nfixed_z_ = torch.cat([temp_z0_, temp_z1_], 0)\nfixed_y_ = torch.cat([torch.zeros(4), torch.ones(4), torch.zeros(4), torch.ones(4)], 0).type(torch.LongTensor).squeeze()\n\nfixed_z_ = fixed_z_.view(-1, 100, 1, 1)\nfixed_y_label_ = onehot[fixed_y_]\nfixed_z_, fixed_y_label_ = Variable(fixed_z_.cuda(), volatile=True), Variable(fixed_y_label_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\'):\n    G.eval()\n    test_images = G(fixed_z_, fixed_y_label_)\n    G.train()\n\n    size_figure_grid = 4\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(size_figure_grid*size_figure_grid):\n        i = k // size_figure_grid\n        j = k % size_figure_grid\n        ax[i, j].cla()\n        ax[i, j].imshow((test_images[k].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_noise_morp(show=False, save=False, path=\'result.png\'):\n    source_z_ = torch.randn(10, 100)\n    z_ = torch.zeros(100, 100)\n    for i in range(5):\n        for j in range(10):\n            z_[i*20 + j] = (source_z_[i*2+1] - source_z_[i*2]) / 9 * (j+1) + source_z_[i*2]\n\n    for i in range(5):\n        z_[i*20+10:i*20+20] = z_[i*20:i*20+10]\n\n    y_ = torch.cat([torch.zeros(10, 1), torch.ones(10, 1)], 0).type(torch.LongTensor).squeeze()\n    y_ = torch.cat([y_, y_, y_, y_, y_], 0)\n    y_label_ = onehot[y_]\n    z_ = z_.view(-1, 100, 1, 1)\n    y_label_ = y_label_.view(-1, 2, 1, 1)\n\n    z_, y_label_ = Variable(z_.cuda(), volatile=True), Variable(y_label_.cuda(), volatile=True)\n\n    G.eval()\n    test_images = G(z_, y_label_)\n    G.train()\n\n    size_figure_grid = 10\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(img_size, img_size))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(10 * 10):\n        i = k // 10\n        j = k % 10\n        ax[i, j].cla()\n        ax[i, j].imshow((test_images[k].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 20\n\n# data_loader\nisCrop = False\nif isCrop:\n    transform = transforms.Compose([\n        transforms.Scale(108),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    ])\nelse:\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    ])\ndata_dir = \'data/resized_celebA\'          # this path depends on your computer\ndset = datasets.ImageFolder(data_dir, transform)\ndset.imgs.sort()\ntrain_loader = torch.utils.data.DataLoader(dset, batch_size=128, shuffle=False)\ntemp = plt.imread(train_loader.dataset.imgs[0][0])\nif (temp.shape[0] != img_size) or (temp.shape[0] != img_size):\n    sys.stderr.write(\'Error! image size is not 64 x 64! run \\""celebA_data_preprocess.py\\"" !!!\')\n    sys.exit(1)\n\n# network\nG = generator(128)\nD = discriminator(128)\nG.weight_init(mean=0.0, std=0.02)\nD.weight_init(mean=0.0, std=0.02)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# results save folder\nroot = \'CelebA_cDCGAN_results/\'\nmodel = \'CelebA_cDCGAN_\'\nif not os.path.isdir(root):\n    os.mkdir(root)\nif not os.path.isdir(root + \'Fixed_results\'):\n    os.mkdir(root + \'Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\ntrain_hist[\'per_epoch_ptimes\'] = []\ntrain_hist[\'total_ptime\'] = []\n\nprint(\'training start!\')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n\n    # learning rate decay\n    if (epoch+1) == 11:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    if (epoch+1) == 16:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    y_real_ = torch.ones(batch_size)\n    y_fake_ = torch.zeros(batch_size)\n    y_real_, y_fake_ = Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n    epoch_start_time = time.time()\n    num_iter = 0\n    for x_, _ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n        \n        if isCrop:\n            x_ = x_[:, :, 22:86, 22:86]\n\n        mini_batch = x_.size()[0]\n\n        if mini_batch != batch_size:\n            y_real_ = torch.ones(mini_batch)\n            y_fake_ = torch.zeros(mini_batch)\n            y_real_, y_fake_ = Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n            y_ = y_gender_[batch_size*num_iter:]\n        else:\n            y_ = y_gender_[batch_size*num_iter:batch_size*(num_iter+1)]\n\n        y_fill_ = fill[y_]\n        x_, y_fill_ = Variable(x_.cuda()), Variable(y_fill_.cuda())\n\n        D_result = D(x_, y_fill_).squeeze()\n\n        D_real_loss = BCE_loss(D_result, y_real_)\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        y_ = (torch.rand(mini_batch, 1) * 2).type(torch.LongTensor).squeeze()\n        y_label_ = onehot[y_]\n        y_fill_ = fill[y_]\n        z_, y_label_, y_fill_ = Variable(z_.cuda()), Variable(y_label_.cuda()), Variable(y_fill_.cuda())\n\n        G_result = G(z_, y_label_)\n        D_result = D(G_result, y_fill_).squeeze()\n\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result.data.mean()\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        y_ = (torch.rand(mini_batch, 1) * 2).type(torch.LongTensor).squeeze()\n        y_label_ = onehot[y_]\n        y_fill_ = fill[y_]\n        z_, y_label_, y_fill_ = Variable(z_.cuda()), Variable(y_label_.cuda()), Variable(y_fill_.cuda())\n\n        G_result = G(z_, y_label_)\n        D_result = D(G_result, y_fill_).squeeze()\n\n        G_train_loss = BCE_loss(D_result, y_real_)\n\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n        num_iter += 1\n\n        if (num_iter % 100) == 0:\n            print(\'%d - %d complete!\' % ((epoch+1), num_iter))\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n    print(\'[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f\' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    fixed_p = root + \'Fixed_results/\' + model + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=fixed_p)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n    train_hist[\'per_epoch_ptimes\'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist[\'total_ptime\'].append(total_ptime)\n\nprint(""Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f"" % (torch.mean(torch.FloatTensor(train_hist[\'per_epoch_ptimes\'])), train_epoch, total_ptime))\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), root + model + \'generator_param.pkl\')\ntorch.save(D.state_dict(), root + model + \'discriminator_param.pkl\')\nwith open(root + model + \'train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=root + model + \'train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = root + \'Fixed_results/\' + model + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(root + model + \'generation_animation.gif\', images, fps=5)\n\nshow_noise_morp(save=True, path=root + model + \'warp.png\')\n'"
pytorch_MNIST_cDCGAN.py,32,"b'import os, time\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(generator, self).__init__()\n        self.deconv1_1 = nn.ConvTranspose2d(100, d*2, 4, 1, 0)\n        self.deconv1_1_bn = nn.BatchNorm2d(d*2)\n        self.deconv1_2 = nn.ConvTranspose2d(10, d*2, 4, 1, 0)\n        self.deconv1_2_bn = nn.BatchNorm2d(d*2)\n        self.deconv2 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d*2)\n        self.deconv3 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d)\n        self.deconv4 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label):\n        x = F.relu(self.deconv1_1_bn(self.deconv1_1(input)))\n        y = F.relu(self.deconv1_2_bn(self.deconv1_2(label)))\n        x = torch.cat([x, y], 1)\n        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n        x = F.tanh(self.deconv4(x))\n        # x = F.relu(self.deconv4_bn(self.deconv4(x)))\n        # x = F.tanh(self.deconv5(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=128):\n        super(discriminator, self).__init__()\n        self.conv1_1 = nn.Conv2d(1, d/2, 4, 2, 1)\n        self.conv1_2 = nn.Conv2d(10, d/2, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d*2)\n        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d*4)\n        self.conv4 = nn.Conv2d(d * 4, 1, 4, 1, 0)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label):\n        x = F.leaky_relu(self.conv1_1(input), 0.2)\n        y = F.leaky_relu(self.conv1_2(label), 0.2)\n        x = torch.cat([x, y], 1)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        x = F.sigmoid(self.conv4(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n\n# fixed noise & label\ntemp_z_ = torch.randn(10, 100)\nfixed_z_ = temp_z_\nfixed_y_ = torch.zeros(10, 1)\nfor i in range(9):\n    fixed_z_ = torch.cat([fixed_z_, temp_z_], 0)\n    temp = torch.ones(10, 1) + i\n    fixed_y_ = torch.cat([fixed_y_, temp], 0)\n\nfixed_z_ = fixed_z_.view(-1, 100, 1, 1)\nfixed_y_label_ = torch.zeros(100, 10)\nfixed_y_label_.scatter_(1, fixed_y_.type(torch.LongTensor), 1)\nfixed_y_label_ = fixed_y_label_.view(-1, 10, 1, 1)\nfixed_z_, fixed_y_label_ = Variable(fixed_z_.cuda(), volatile=True), Variable(fixed_y_label_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\'):\n\n    G.eval()\n    test_images = G(fixed_z_, fixed_y_label_)\n    G.train()\n\n    size_figure_grid = 10\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(10*10):\n        i = k // 10\n        j = k % 10\n        ax[i, j].cla()\n        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap=\'gray\')\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n    plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 20\n\n# data_loader\nimg_size = 32\ntransform = transforms.Compose([\n        transforms.Scale(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'data\', train=True, download=True, transform=transform),\n    batch_size=batch_size, shuffle=True)\n\n# network\nG = generator(128)\nD = discriminator(128)\nG.weight_init(mean=0.0, std=0.02)\nD.weight_init(mean=0.0, std=0.02)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# results save folder\nroot = \'MNIST_cDCGAN_results/\'\nmodel = \'MNIST_cDCGAN_\'\nif not os.path.isdir(root):\n    os.mkdir(root)\nif not os.path.isdir(root + \'Fixed_results\'):\n    os.mkdir(root + \'Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\ntrain_hist[\'per_epoch_ptimes\'] = []\ntrain_hist[\'total_ptime\'] = []\n\n# label preprocess\nonehot = torch.zeros(10, 10)\nonehot = onehot.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1).view(10, 10, 1, 1)\nfill = torch.zeros([10, 10, img_size, img_size])\nfor i in range(10):\n    fill[i, i, :, :] = 1\n\nprint(\'training start!\')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n\n    # learning rate decay\n    if (epoch+1) == 11:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    if (epoch+1) == 16:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    epoch_start_time = time.time()\n    y_real_ = torch.ones(batch_size)\n    y_fake_ = torch.zeros(batch_size)\n    y_real_, y_fake_ = Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n    for x_, y_ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n\n        mini_batch = x_.size()[0]\n\n        if mini_batch != batch_size:\n            y_real_ = torch.ones(mini_batch)\n            y_fake_ = torch.zeros(mini_batch)\n            y_real_, y_fake_ = Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n\n        y_fill_ = fill[y_]\n        x_, y_fill_ = Variable(x_.cuda()), Variable(y_fill_.cuda())\n\n        D_result = D(x_, y_fill_).squeeze()\n        D_real_loss = BCE_loss(D_result, y_real_)\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor).squeeze()\n        y_label_ = onehot[y_]\n        y_fill_ = fill[y_]\n        z_, y_label_, y_fill_ = Variable(z_.cuda()), Variable(y_label_.cuda()), Variable(y_fill_.cuda())\n\n        G_result = G(z_, y_label_)\n        D_result = D(G_result, y_fill_).squeeze()\n\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result.data.mean()\n\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor).squeeze()\n        y_label_ = onehot[y_]\n        y_fill_ = fill[y_]\n        z_, y_label_, y_fill_ = Variable(z_.cuda()), Variable(y_label_.cuda()), Variable(y_fill_.cuda())\n\n        G_result = G(z_, y_label_)\n        D_result = D(G_result, y_fill_).squeeze()\n\n        G_train_loss = BCE_loss(D_result, y_real_)\n\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n    print(\'[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f\' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    fixed_p = root + \'Fixed_results/\' + model + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=fixed_p)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n    train_hist[\'per_epoch_ptimes\'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist[\'total_ptime\'].append(total_ptime)\n\nprint(""Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f"" % (torch.mean(torch.FloatTensor(train_hist[\'per_epoch_ptimes\'])), train_epoch, total_ptime))\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), root + model + \'generator_param.pkl\')\ntorch.save(D.state_dict(), root + model + \'discriminator_param.pkl\')\nwith open(root + model + \'train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=root + model + \'train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = root + \'Fixed_results/\' + model + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(root + model + \'generation_animation.gif\', images, fps=5)\n'"
pytorch_MNIST_cGAN.py,30,"b'import os, time\nimport matplotlib.pyplot as plt\nimport itertools\nimport pickle\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# G(z)\nclass generator(nn.Module):\n    # initializers\n    def __init__(self):\n        super(generator, self).__init__()\n        self.fc1_1 = nn.Linear(100, 256)\n        self.fc1_1_bn = nn.BatchNorm1d(256)\n        self.fc1_2 = nn.Linear(10, 256)\n        self.fc1_2_bn = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc2_bn = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 1024)\n        self.fc3_bn = nn.BatchNorm1d(1024)\n        self.fc4 = nn.Linear(1024, 784)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label):\n        x = F.relu(self.fc1_1_bn(self.fc1_1(input)))\n        y = F.relu(self.fc1_2_bn(self.fc1_2(label)))\n        x = torch.cat([x, y], 1)\n        x = F.relu(self.fc2_bn(self.fc2(x)))\n        x = F.relu(self.fc3_bn(self.fc3(x)))\n        x = F.tanh(self.fc4(x))\n\n        return x\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self):\n        super(discriminator, self).__init__()\n        self.fc1_1 = nn.Linear(784, 1024)\n        self.fc1_2 = nn.Linear(10, 1024)\n        self.fc2 = nn.Linear(2048, 512)\n        self.fc2_bn = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc3_bn = nn.BatchNorm1d(256)\n        self.fc4 = nn.Linear(256, 1)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label):\n        x = F.leaky_relu(self.fc1_1(input), 0.2)\n        y = F.leaky_relu(self.fc1_2(label), 0.2)\n        x = torch.cat([x, y], 1)\n        x = F.leaky_relu(self.fc2_bn(self.fc2(x)), 0.2)\n        x = F.leaky_relu(self.fc3_bn(self.fc3(x)), 0.2)\n        x = F.sigmoid(self.fc4(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.Linear):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n\ntemp_z_ = torch.rand(10, 100)\nfixed_z_ = temp_z_\nfixed_y_ = torch.zeros(10, 1)\nfor i in range(9):\n    fixed_z_ = torch.cat([fixed_z_, temp_z_], 0)\n    temp = torch.ones(10,1) + i\n    fixed_y_ = torch.cat([fixed_y_, temp], 0)\n\n\nfixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\nfixed_y_label_ = torch.zeros(100, 10)\nfixed_y_label_.scatter_(1, fixed_y_.type(torch.LongTensor), 1)\nfixed_y_label_ = Variable(fixed_y_label_.cuda(), volatile=True)\ndef show_result(num_epoch, show = False, save = False, path = \'result.png\'):\n\n    G.eval()\n    test_images = G(fixed_z_, fixed_y_label_)\n    G.train()\n\n    size_figure_grid = 10\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(10*10):\n        i = k // 10\n        j = k % 10\n        ax[i, j].cla()\n        ax[i, j].imshow(test_images[k].cpu().data.view(28, 28).numpy(), cmap=\'gray\')\n\n    label = \'Epoch {0}\'.format(num_epoch)\n    fig.text(0.5, 0.04, label, ha=\'center\')\n    plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\ndef show_train_hist(hist, show = False, save = False, path = \'Train_hist.png\'):\n    x = range(len(hist[\'D_losses\']))\n\n    y1 = hist[\'D_losses\']\n    y2 = hist[\'G_losses\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n# training parameters\nbatch_size = 128\nlr = 0.0002\ntrain_epoch = 50\n\n# data_loader\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'data\', train=True, download=True, transform=transform),\n    batch_size=batch_size, shuffle=True)\n\n# network\nG = generator()\nD = discriminator()\nG.weight_init(mean=0, std=0.02)\nD.weight_init(mean=0, std=0.02)\nG.cuda()\nD.cuda()\n\n# Binary Cross Entropy loss\nBCE_loss = nn.BCELoss()\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# results save folder\nif not os.path.isdir(\'MNIST_cGAN_results\'):\n    os.mkdir(\'MNIST_cGAN_results\')\nif not os.path.isdir(\'MNIST_cGAN_results/Fixed_results\'):\n    os.mkdir(\'MNIST_cGAN_results/Fixed_results\')\n\ntrain_hist = {}\ntrain_hist[\'D_losses\'] = []\ntrain_hist[\'G_losses\'] = []\ntrain_hist[\'per_epoch_ptimes\'] = []\ntrain_hist[\'total_ptime\'] = []\n\nprint(\'training start!\')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n\n    # learning rate decay\n    if (epoch+1) == 30:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    if (epoch+1) == 40:\n        G_optimizer.param_groups[0][\'lr\'] /= 10\n        D_optimizer.param_groups[0][\'lr\'] /= 10\n        print(""learning rate change!"")\n\n    epoch_start_time = time.time()\n    for x_, y_ in train_loader:\n        # train discriminator D\n        D.zero_grad()\n\n        mini_batch = x_.size()[0]\n\n        y_real_ = torch.ones(mini_batch)\n        y_fake_ = torch.zeros(mini_batch)\n        y_label_ = torch.zeros(mini_batch, 10)\n        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n\n        x_ = x_.view(-1, 28 * 28)\n        x_, y_label_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_label_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n        D_result = D(x_, y_label_).squeeze()\n        D_real_loss = BCE_loss(D_result, y_real_)\n\n        z_ = torch.rand((mini_batch, 100))\n        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor)\n        y_label_ = torch.zeros(mini_batch, 10)\n        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n\n        z_, y_label_ = Variable(z_.cuda()), Variable(y_label_.cuda())\n        G_result = G(z_, y_label_)\n\n        D_result = D(G_result, y_label_).squeeze()\n        D_fake_loss = BCE_loss(D_result, y_fake_)\n        D_fake_score = D_result.data.mean()\n\n        D_train_loss = D_real_loss + D_fake_loss\n\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        D_losses.append(D_train_loss.data[0])\n\n        # train generator G\n        G.zero_grad()\n\n        z_ = torch.rand((mini_batch, 100))\n        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor)\n        y_label_ = torch.zeros(mini_batch, 10)\n        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n\n        z_, y_label_ = Variable(z_.cuda()), Variable(y_label_.cuda())\n\n        G_result = G(z_, y_label_)\n        D_result = D(G_result, y_label_).squeeze()\n        G_train_loss = BCE_loss(D_result, y_real_)\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        G_losses.append(G_train_loss.data[0])\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n\n    print(\'[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f\' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    fixed_p = \'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_\' + str(epoch + 1) + \'.png\'\n    show_result((epoch+1), save=True, path=fixed_p)\n    train_hist[\'D_losses\'].append(torch.mean(torch.FloatTensor(D_losses)))\n    train_hist[\'G_losses\'].append(torch.mean(torch.FloatTensor(G_losses)))\n    train_hist[\'per_epoch_ptimes\'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist[\'total_ptime\'].append(total_ptime)\n\nprint(""Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f"" % (torch.mean(torch.FloatTensor(train_hist[\'per_epoch_ptimes\'])), train_epoch, total_ptime))\nprint(""Training finish!... save training results"")\ntorch.save(G.state_dict(), ""MNIST_cGAN_results/generator_param.pkl"")\ntorch.save(D.state_dict(), ""MNIST_cGAN_results/discriminator_param.pkl"")\nwith open(\'MNIST_cGAN_results/train_hist.pkl\', \'wb\') as f:\n    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=True, path=\'MNIST_cGAN_results/MNIST_cGAN_train_hist.png\')\n\nimages = []\nfor e in range(train_epoch):\n    img_name = \'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_\' + str(e + 1) + \'.png\'\n    images.append(imageio.imread(img_name))\nimageio.mimsave(\'MNIST_cGAN_results/generation_animation.gif\', images, fps=5)\n'"
