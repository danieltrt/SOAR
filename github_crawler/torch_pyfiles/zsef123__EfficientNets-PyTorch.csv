file_path,api_count,code
loader.py,1,"b'from torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\n\n\ndef get_loaders(root, batch_size, resolution, num_workers=32):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    train_dataset = ImageFolder(\n        root + ""/train"",\n        transforms.Compose([\n            transforms.Resize([resolution, resolution]),\n            transforms.RandomResizedCrop(resolution),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ])\n    )\n\n    val_dataset = ImageFolder(\n        root + ""/val"",\n        transforms.Compose([\n            transforms.Resize([resolution, resolution]),\n            transforms.ToTensor(),\n            normalize,\n        ])\n    )\n\n    train_loader = DataLoader(train_dataset,\n        batch_size=batch_size, shuffle=True,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(val_dataset,\n        batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n    return train_loader, val_loader\n'"
logger.py,0,"b'import os\nimport json\nfrom cycler import cycler\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n# x axis of plot\nLOG_KEYS = {\n    ""train"":""epoch"",\n    ""valid"":""epoch"",\n    ""test"": ""fname""\n}\n\n# y axis of plot\n# save datas like loss, f1-score, PSNR, SSIM ..\n# can multiple datas\nLOG_VALUES = {\n    ""train"":[""loss"", ],\n    ""valid"":[""acc"",""valid_acc""],\n    ""test"": [""train_acc"", ""valid_acc""]\n}\n\n\nclass Logger:\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n        self.log_file = save_dir + ""/log.txt""\n        self.buffers = []\n\n    def will_write(self, line):\n        print(line)\n        self.buffers.append(line)\n\n    def flush(self):\n        with open(self.log_file, ""a"", encoding=""utf-8"") as f:\n            f.write(""\\n"".join(self.buffers))\n            f.write(""\\n"")\n        self.buffers = []\n\n    def write(self, line):\n        self.will_write(line)\n        self.flush()\n\n    def log_write(self, learn_type, **values):\n        """"""log write in buffers\n\n        ex ) log_write(""train"", epoch=1, loss=0.3)\n\n        Parmeters:\n            learn_type : it must be train, valid or test\n            values : values keys in LOG_VALUES\n        """"""\n        for k in values.keys():\n            if k not in LOG_VALUES[learn_type] and k != LOG_KEYS[learn_type]:\n                raise KeyError(""%s Log %s keys not in log"" % (learn_type, k))\n\n        log = ""[%s] %s"" % (learn_type, json.dumps(values))\n        self.will_write(log)\n        if learn_type != ""train"":\n            self.flush()\n\n    def log_parse(self, log_key):\n        log_dict = OrderedDict()\n        with open(self.log_file, ""r"", encoding=""utf-8"") as f:\n            for line in f.readlines():\n                if len(line) == 1 or not line.startswith(""[%s]"" % (log_key)):\n                    continue\n                # line : ~~\n                line = line[line.find(""] "") + 2:]  # ~~\n                line_log = json.loads(line)\n\n                train_log_key = line_log[LOG_KEYS[log_key]]\n                line_log.pop(LOG_KEYS[log_key], None)\n                log_dict[train_log_key] = line_log\n        return log_dict\n\n    def log_plot(self, log_key,\n                 figsize=(12, 12), title=""plot"", colors=[""C1"", ""C2""]):\n        fig = plt.figure(figsize=figsize)\n        plt.title(title)\n        plt.legend(LOG_VALUES[log_key], loc=""best"")\n\n        ax = plt.subplot(111)\n        colors = plt.cm.nipy_spectral(np.linspace(0.1, 0.9, len(LOG_VALUES[log_key])))\n        ax.set_prop_cycle(cycler(\'color\', colors))\n\n        log_dict = self.log_parse(log_key)\n        x = log_dict.keys()\n        for keys in LOG_VALUES[log_key]:\n            if keys not in list(log_dict.values())[0]:\n                continue\n            y = [v[keys] for v in log_dict.values()]\n\n            label = keys + "", max : %f"" % (max(y))\n            ax.plot(x, y, marker=""o"", linestyle=""solid"", label=label)\n            if max(y) > 1:\n                ax.set_ylim([min(y) - 1, y[0] + 1])\n        ax.legend(fontsize=30)\n\n        plt.show()\n'"
main.py,6,"b'import os\nimport argparse\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom effnet import EfficientNet\nfrom runner import Runner\nfrom loader import get_loaders\n\n\nfrom logger import Logger\n\n\ndef arg_parse():\n    # projects description\n    desc = ""Pytorch EfficientNet""\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument(\'--save_dir\', type=str, required=True,\n                        help=\'Directory name to save the model\')\n\n    parser.add_argument(\'--root\', type=str, default=""/data2/imagenet"",\n                        help=""The Directory of data path."")\n    parser.add_argument(\'--gpus\', type=str, default=""0,1,2,3"",\n                        help=""Select GPU Numbers | 0,1,2,3 | "")\n    parser.add_argument(\'--num_workers\', type=int, default=""32"",\n                        help=""Select CPU Number workers"")\n\n    parser.add_argument(\'--model\', type=str, default=\'b0\',\n                        choices=[""b0""],\n                        help=\'The type of Efficient net.\')\n\n    parser.add_argument(\'--epoch\', type=int, default=350, help=\'The number of epochs\')\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'The size of batch\')\n    parser.add_argument(\'--test\', action=""store_true"", help=\'Only Test\')\n\n    parser.add_argument(\'--ema_decay\', type=float, default=0.9999,\n                        help=""Exponential Moving Average Term"")\n\n    parser.add_argument(\'--dropout_rate\', type=float, default=0.2)\n    parser.add_argument(\'--dropconnect_rate\', type=float, default=0.2)\n\n    parser.add_argument(\'--optim\', type=str, default=\'rmsprop\', choices=[""rmsprop""])\n    parser.add_argument(\'--lr\',    type=float, default=0.016, help=""Base learning rate when train batch size is 256."")\n    # Adam Optimizer\n    parser.add_argument(\'--beta\', nargs=""*"", type=float, default=(0.5, 0.999))\n\n    parser.add_argument(\'--momentum\', type=float, default=0.9)\n    parser.add_argument(\'--eps\',      type=float, default=0.001)\n    parser.add_argument(\'--decay\',    type=float, default=1e-5)\n\n    parser.add_argument(\'--scheduler\', type=str, default=\'exp\', choices=[""exp"", ""cosine"", ""none""],\n                        help=""Learning rate scheduler type"")\n\n    return parser.parse_args()\n\n\ndef get_model(arg, classes=1000):\n    if arg.model == ""b0"":\n        return EfficientNet(1, 1, num_classes=classes)\n\n\ndef get_scheduler(optim, sche_type, step_size, t_max):\n    if sche_type == ""exp"":\n        return StepLR(optim, step_size, 0.97)\n    elif sche_type == ""cosine"":\n        return CosineAnnealingLR(optim, t_max)\n    else:\n        return None\n\n\nif __name__ == ""__main__"":\n    arg = arg_parse()\n\n    arg.save_dir = ""%s/outs/%s"" % (os.getcwd(), arg.save_dir)\n    if os.path.exists(arg.save_dir) is False:\n        os.mkdir(arg.save_dir)\n\n    logger = Logger(arg.save_dir)\n    logger.will_write(str(arg) + ""\\n"")\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = arg.gpus\n    torch_device = torch.device(""cuda"")\n\n    train_loader, val_loader = get_loaders(arg.root, arg.batch_size, 224, arg.num_workers)\n\n    net = get_model(arg, classes=1000)\n    net = nn.DataParallel(net).to(torch_device)\n    loss = nn.CrossEntropyLoss()\n\n    scaled_lr = arg.lr * arg.batch_size / 256\n    optim = {\n        # ""adam"" : lambda : torch.optim.Adam(net.parameters(), lr=arg.lr, betas=arg.beta, weight_decay=arg.decay),\n        ""rmsprop"" : lambda : torch.optim.RMSprop(net.parameters(), lr=scaled_lr, momentum=arg.momentum, eps=arg.eps, weight_decay=arg.decay)\n    }[arg.optim]()\n\n    scheduler = get_scheduler(optim, arg.scheduler, int(2.4 * len(train_loader)), arg.epoch * len(train_loader))\n\n    model = Runner(arg, net, optim, torch_device, loss, logger, scheduler)\n    if arg.test is False:\n        model.train(train_loader, val_loader)\n    model.test(train_loader, val_loader)\n'"
runner.py,6,"b'import os\nimport copy\nimport time\nfrom glob import glob\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Runner():\n    def __init__(self, arg, net, optim, torch_device, loss, logger, scheduler=None):\n        self.arg = arg\n        self.save_dir = arg.save_dir\n\n        self.logger = logger\n\n        self.torch_device = torch_device\n\n        self.net = net\n        self.ema = copy.deepcopy(net.module).cpu()\n        self.ema.eval()\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n        self.ema_decay = arg.ema_decay\n\n        self.loss = loss\n        self.optim = optim\n        self.scheduler = scheduler\n\n        self.start_epoch = 0\n        self.best_metric = -1\n\n        self.load()\n\n    def save(self, epoch, filename=""train""):\n        """"""Save current epoch model\n\n        Save Elements:\n            model_type : arg.model\n            start_epoch : current epoch\n            network : network parameters\n            optimizer: optimizer parameters\n            best_metric : current best score\n\n        Parameters:\n            epoch : current epoch\n            filename : model save file name\n        """"""\n\n        torch.save({""model_type"": self.arg.model,\n                    ""start_epoch"": epoch + 1,\n                    ""network"": self.net.module.state_dict(),\n                    ""ema"": self.ema.state_dict(),\n                    ""optimizer"": self.optim.state_dict(),\n                    ""best_metric"": self.best_metric\n                    }, self.save_dir + ""/%s.pth.tar"" % (filename))\n        print(""Model saved %d epoch"" % (epoch))\n\n    def load(self, filename=""""):\n        """""" Model load. same with save""""""\n        if filename == """":\n            # load last epoch model\n            filenames = sorted(glob(self.save_dir + ""/*.pth.tar""))\n            if len(filenames) == 0:\n                print(""Not Load"")\n                return\n            else:\n                filename = os.path.basename(filenames[-1])\n\n        file_path = self.save_dir + ""/"" + filename\n        if os.path.exists(file_path) is True:\n            print(""Load %s to %s File"" % (self.save_dir, filename))\n            ckpoint = torch.load(file_path)\n            if ckpoint[""model_type""] != self.arg.model:\n                raise ValueError(""Ckpoint Model Type is %s"" %\n                                 (ckpoint[""model_type""]))\n\n            self.net.module.load_state_dict(ckpoint[\'network\'])\n            self.ema.load_state_dict(ckpoint[\'ema\'])\n            self.optim.load_state_dict(ckpoint[\'optimizer\'])\n            self.start_epoch = ckpoint[\'start_epoch\']\n            self.best_metric = ckpoint[""best_metric""]\n            print(""Load Model Type : %s, epoch : %d acc : %f"" %\n                  (ckpoint[""model_type""], self.start_epoch, self.best_metric))\n        else:\n            print(""Load Failed, not exists file"")\n\n    def update_ema(self):\n        with torch.no_grad():\n            named_param = dict(self.net.module.named_parameters())\n            for k, v in self.ema.named_parameters():\n                param = named_param[k].detach().cpu()\n                v.copy_(self.ema_decay * v + (1 - self.ema_decay) * param)\n\n    def train(self, train_loader, val_loader=None):\n        print(""\\nStart Train len :"", len(train_loader.dataset))        \n        self.net.train()\n        for epoch in range(self.start_epoch, self.arg.epoch):\n            for i, (input_, target_) in enumerate(train_loader):\n                target_ = target_.to(self.torch_device, non_blocking=True)\n\n                if self.scheduler:\n                    self.scheduler.step()\n\n                out = self.net(input_)\n                loss = self.loss(out, target_)\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n                self.update_ema()\n\n                if (i % 50) == 0:\n                    self.logger.log_write(""train"", epoch=epoch, loss=loss.item())\n            if val_loader is not None:\n                self.valid(epoch, val_loader)\n\n    def _get_acc(self, loader):\n        correct = 0\n        with torch.no_grad():\n            for input_, target_ in loader:\n                out = self.ema(input_)\n                out = F.softmax(out, dim=1)\n\n                _, idx = out.max(dim=1)\n                correct += (target_ == idx).sum().item()\n        return correct / len(loader.dataset)\n\n    def valid(self, epoch, val_loader):\n        acc = self._get_acc(val_loader)\n        self.logger.log_write(""valid"", epoch=epoch, acc=acc)\n\n        if acc > self.best_metric:\n            self.best_metric = acc\n            self.save(epoch, ""epoch[%05d]_acc[%.4f]"" % (\n                epoch, acc))\n\n    def test(self, train_loader, val_loader):\n        print(""\\n Start Test"")\n        self.load()\n        train_acc = self._get_acc(train_loader)\n        valid_acc = self._get_acc(val_loader)\n        self.logger.log_write(""test"", fname=""test"", train_acc=train_acc, valid_acc=valid_acc)\n        return train_acc, valid_acc\n'"
conversion/convert.py,12,"b'import os\r\nimport argparse\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\n\r\nimport tensorflow as tf\r\n\r\nimport sys\r\nsys.path.append("".."")\r\nfrom models.effnet import EfficientNet\r\n\r\nsys.path.append(""tf_repo"")\r\nfrom eval_ckpt_main import EvalCkptDriver\r\n\r\n\r\ndef convert_conv(m, weight, bias=None):\r\n    m.weight.data = torch.from_numpy(np.transpose(weight, (3, 2, 0, 1)))\r\n    if bias is not None:\r\n        m.bias.data = torch.from_numpy(bias)\r\n\r\n\r\ndef convert_depthwise_conv(m, weight, bias=None):\r\n    m.weight.data = torch.from_numpy(np.transpose(weight, (2, 3, 0, 1)))\r\n    if bias is not None:\r\n        m.bias.data = torch.from_numpy(bias)\r\n\r\n\r\ndef convert_linear(m, weight, bias=None):\r\n    m.weight.data = torch.from_numpy(np.transpose(weight))\r\n    if bias is not None:\r\n        m.bias.data = torch.from_numpy(bias)\r\n\r\n\r\ndef convert_bn(m, gamma, beta, mean, var):\r\n    m.weight.data = torch.from_numpy(gamma)\r\n    m.bias.data = torch.from_numpy(beta)\r\n    m.running_mean.data = torch.from_numpy(mean)\r\n    m.running_var.data = torch.from_numpy(var)\r\n\r\n\r\ndef convert_stem(stem, tf_params):\r\n    convert_conv(stem[0], tf_params[0])\r\n    convert_bn(stem[1], *tf_params[1:])\r\n\r\n\r\ndef convert_head(head, tf_params):\r\n    convert_conv(head[0], tf_params[0])\r\n    convert_bn(head[1], *tf_params[1:5])\r\n    convert_linear(head[-1], *tf_params[5:])\r\n\r\n\r\ndef convert_se(m, tf_params):\r\n    convert_conv(m.se[1], tf_params[0], tf_params[1])\r\n    convert_conv(m.se[3], tf_params[2], tf_params[3])\r\n\r\n\r\ndef convert_MBConv(m, tf_params):\r\n    if isinstance(m.expand_conv, torch.nn.Identity):\r\n        tf_params = [None] * 5 + tf_params\r\n    else:\r\n        convert_conv(m.expand_conv[0], tf_params[0])\r\n        convert_bn(m.expand_conv[1], *tf_params[1:5])\r\n\r\n    convert_depthwise_conv(m.depth_wise_conv[0], tf_params[5])\r\n    convert_bn(m.depth_wise_conv[1], *tf_params[6:10])\r\n    convert_se(m.se, tf_params[10:14])\r\n    convert_conv(m.project_conv[0], tf_params[14])\r\n    convert_bn(m.project_conv[1], *tf_params[15:])\r\n\r\n\r\ndef arg_parse():\r\n    desc = ""TF EfficientNet to Pytorch EfficientNet""\r\n    parser = argparse.ArgumentParser(description=desc)\r\n    parser.add_argument(\'--model\', type=str, default=\'efficientnet-b0\')\r\n    parser.add_argument(\'--tf_weight\', type=str, required=True,\r\n                        help=\'Directory name to save the TF chekpoint\')\r\n    parser.add_argument(\'--pth_weight\', type=str, default=\'model\',\r\n                        help=\'output PyTorch model file name\')\r\n    return parser.parse_args()\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    args = arg_parse()\r\n\r\n    # Loading TF Network\r\n    driver = EvalCkptDriver(args.model)\r\n    image_files = [""dummy""]\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        images, _ = driver.build_dataset(image_files, [0] * len(image_files), False)\r\n        _ = driver.build_model(images, is_training=False)\r\n        tf_keys = [k.name for k in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\r\n\r\n    def key_to_param(ks):\r\n        return [tf.train.load_variable(args.tf_weight, k) for k in ks]\r\n\r\n    model = EfficientNet(1, 1)\r\n\r\n    tf_stem = [k for k in tf_keys if ""stem"" in k]\r\n    convert_stem(model.stem, key_to_param(tf_stem))\r\n\r\n    tf_head = [k for k in tf_keys if ""head"" in k]\r\n    convert_head(model.head, key_to_param(tf_head))\r\n\r\n    blocks = list(set([k for k in tf_keys if ""block"" in k]))\r\n    tf_block = [list() for _ in blocks]\r\n    for k in tf_keys:\r\n        if ""block"" in k:\r\n            # efficientnet-b0/blocks_5/...\r\n            block_id = int(k.split(""/"")[1].split(""_"")[1])\r\n            tf_block[block_id].append(k)\r\n\r\n    # Flatten All MBConv in MBBlock\r\n    mbconvs = [mbconv for mbblock in model.blocks for mbconv in mbblock.layers]\r\n    for i, (mbconv, tf_mbconv) in enumerate(zip(mbconvs, tf_block)):\r\n        convert_MBConv(mbconv, key_to_param(tf_mbconv))\r\n\r\n    torch.save(model.state_dict(), args.pth_weight + "".pth"")\r\n\r\n    print(""\\n\\nTF to Pytorch Conversion Done"")\r\n'"
models/effnet.py,1,"b'import math\n\nimport torch\nimport torch.nn as nn\n\nfrom models.layers import conv_bn_act\nfrom models.layers import SamePadConv2d\nfrom models.layers import Flatten\nfrom models.layers import SEModule\nfrom models.layers import DropConnect\n\n\nclass MBConv(nn.Module):\n    def __init__(self, in_, out_, expand,\n                 kernel_size, stride, skip,\n                 se_ratio, dc_ratio=0.2):\n        super().__init__()\n        mid_ = in_ * expand\n        self.expand_conv = conv_bn_act(in_, mid_, kernel_size=1, bias=False) if expand != 1 else nn.Identity()\n\n        self.depth_wise_conv = conv_bn_act(mid_, mid_,\n                                           kernel_size=kernel_size, stride=stride,\n                                           groups=mid_, bias=False)\n\n        self.se = SEModule(mid_, int(in_ * se_ratio)) if se_ratio > 0 else nn.Identity()\n\n        self.project_conv = nn.Sequential(\n            SamePadConv2d(mid_, out_, kernel_size=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_, 1e-3, 0.01)\n        )\n\n        # if _block_args.id_skip:\n        # and all(s == 1 for s in self._block_args.strides)\n        # and self._block_args.input_filters == self._block_args.output_filters:\n        self.skip = skip and (stride == 1) and (in_ == out_)\n\n        # DropConnect\n        # self.dropconnect = DropConnect(dc_ratio) if dc_ratio > 0 else nn.Identity()\n        # Original TF Repo not using drop_rate\n        # https://github.com/tensorflow/tpu/blob/05f7b15cdf0ae36bac84beb4aef0a09983ce8f66/models/official/efficientnet/efficientnet_model.py#L408\n        self.dropconnect = nn.Identity()\n\n    def forward(self, inputs):\n        expand = self.expand_conv(inputs)\n        x = self.depth_wise_conv(expand)\n        x = self.se(x)\n        x = self.project_conv(x)\n        if self.skip:\n            x = self.dropconnect(x)\n            x = x + inputs\n        return x\n\n\nclass MBBlock(nn.Module):\n    def __init__(self, in_, out_, expand, kernel, stride, num_repeat, skip, se_ratio, drop_connect_ratio=0.2):\n        super().__init__()\n        layers = [MBConv(in_, out_, expand, kernel, stride, skip, se_ratio, drop_connect_ratio)]\n        for i in range(1, num_repeat):\n            layers.append(MBConv(out_, out_, expand, kernel, 1, skip, se_ratio, drop_connect_ratio))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass EfficientNet(nn.Module):\n    def __init__(self, width_coeff, depth_coeff,\n                 depth_div=8, min_depth=None,\n                 dropout_rate=0.2, drop_connect_rate=0.2,\n                 num_classes=1000):\n        super().__init__()\n        min_depth = min_depth or depth_div\n        \n        def renew_ch(x):\n            if not width_coeff:\n                return x\n\n            x *= width_coeff\n            new_x = max(min_depth, int(x + depth_div / 2) // depth_div * depth_div)\n            if new_x < 0.9 * x:\n                new_x += depth_div\n            return int(new_x)\n\n        def renew_repeat(x):\n            return int(math.ceil(x * depth_coeff))\n\n        self.stem = conv_bn_act(3, renew_ch(32), kernel_size=3, stride=2, bias=False)\n        \n        self.blocks = nn.Sequential(\n            #       input channel  output    expand  k  s                   skip  se\n            MBBlock(renew_ch(32), renew_ch(16), 1, 3, 1, renew_repeat(1), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(16), renew_ch(24), 6, 3, 2, renew_repeat(2), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(24), renew_ch(40), 6, 5, 2, renew_repeat(2), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(40), renew_ch(80), 6, 3, 2, renew_repeat(3), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(80), renew_ch(112), 6, 5, 1, renew_repeat(3), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(112), renew_ch(192), 6, 5, 2, renew_repeat(4), True, 0.25, drop_connect_rate),\n            MBBlock(renew_ch(192), renew_ch(320), 6, 3, 1, renew_repeat(1), True, 0.25, drop_connect_rate)\n        )\n\n        self.head = nn.Sequential(\n            *conv_bn_act(renew_ch(320), renew_ch(1280), kernel_size=1, bias=False),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Dropout2d(dropout_rate, True) if dropout_rate > 0 else nn.Identity(),\n            Flatten(),\n            nn.Linear(renew_ch(1280), num_classes)\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, SamePadConv2d):\n                nn.init.kaiming_normal_(m.weight, mode=""fan_out"")\n            elif isinstance(m, nn.Linear):\n                init_range = 1.0 / math.sqrt(m.weight.shape[1])\n                nn.init.uniform_(m.weight, -init_range, init_range)\n\n    def forward(self, inputs):\n        stem = self.stem(inputs)\n        x = self.blocks(stem)\n        head = self.head(x)\n        return head\n\n\nif __name__ == ""__main__"":\n    print(""Efficient B0 Summary"")\n    net = EfficientNet(1, 1)\n    from torchsummary import summary\n    summary(net.cuda(), (3, 224, 224))\n'"
models/layers.py,5,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_bn_act(in_, out_, kernel_size,\n                stride=1, groups=1, bias=True,\n                eps=1e-3, momentum=0.01):\n    return nn.Sequential(\n        SamePadConv2d(in_, out_, kernel_size, stride, groups=groups, bias=bias),\n        nn.BatchNorm2d(out_, eps, momentum),\n        Swish()\n    )\n\n\nclass SamePadConv2d(nn.Conv2d):\n    """"""\n    Conv with TF padding=\'same\'\n    https://github.com/pytorch/pytorch/issues/3867#issuecomment-349279036\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True, padding_mode=""zeros""):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias, padding_mode)\n\n    def get_pad_odd(self, in_, weight, stride, dilation):\n        effective_filter_size_rows = (weight - 1) * dilation + 1\n        out_rows = (in_ + stride - 1) // stride\n        padding_needed = max(0, (out_rows - 1) * stride + effective_filter_size_rows - in_)\n        padding_rows = max(0, (out_rows - 1) * stride + (weight - 1) * dilation + 1 - in_)\n        rows_odd = (padding_rows % 2 != 0)\n        return padding_rows, rows_odd\n\n    def forward(self, x):\n        padding_rows, rows_odd = self.get_pad_odd(x.shape[2], self.weight.shape[2], self.stride[0], self.dilation[0])\n        padding_cols, cols_odd = self.get_pad_odd(x.shape[3], self.weight.shape[3], self.stride[1], self.dilation[1])\n\n        if rows_odd or cols_odd:\n            x = F.pad(x, [0, int(cols_odd), 0, int(rows_odd)])\n\n        return F.conv2d(x, self.weight, self.bias, self.stride,\n                        padding=(padding_rows // 2, padding_cols // 2),\n                        dilation=self.dilation, groups=self.groups)\n\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, in_, squeeze_ch):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_, squeeze_ch, kernel_size=1, stride=1, padding=0, bias=True),\n            Swish(),\n            nn.Conv2d(squeeze_ch, in_, kernel_size=1, stride=1, padding=0, bias=True),\n        )\n\n    def forward(self, x):\n        return x * torch.sigmoid(self.se(x))\n\n\nclass DropConnect(nn.Module):\n    def __init__(self, ratio):\n        super().__init__()\n        self.ratio = 1.0 - ratio\n\n    def forward(self, x):\n        if not self.training:\n            return x\n\n        random_tensor = self.ratio\n        random_tensor += torch.rand([x.shape[0], 1, 1, 1], dtype=torch.float, device=x.device)\n        random_tensor.requires_grad_(False)\n        return x / self.ratio * random_tensor.floor()\n'"
