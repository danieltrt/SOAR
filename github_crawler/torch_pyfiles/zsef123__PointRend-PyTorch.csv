file_path,api_count,code
infer.py,2,"b'import time\nimport logging\n\nimport torch\n\nfrom utils.metrics import ConfusionMatrix\n\n\n@torch.no_grad()\ndef infer(loader, net, device):\n    net.eval()\n    num_classes = 19 # Hard coding for Cityscapes\n    metric = ConfusionMatrix(num_classes)\n    for i, (x, gt) in enumerate(loader):\n        x = x.to(device, non_blocking=True)\n        gt = gt.squeeze(1).to(device, dtype=torch.long, non_blocking=True)\n\n        pred = net(x)[""fine""].argmax(1)\n\n        metric.update(pred, gt)\n\n    mIoU = metric.mIoU()\n    logging.info(f""[Infer] mIOU : {mIoU}"")\n    return mIoU\n'"
main.py,5,"b'import os\nimport sys\nimport argparse\nimport logging\nfrom configs.parser import Parser\n\nimport torch\n\nfrom apex import amp\nfrom apex.parallel import DistributedDataParallel as ApexDDP\n\nfrom model import deeplabv3, PointHead, PointRend\nfrom datas import get_loader\nfrom train import train\nfrom utils.gpus import synchronize, is_main_process\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""PyTorch Object Detection Training"")\n    parser.add_argument(""config"", type=str, help=""It must be config/*.yaml"")\n    parser.add_argument(""save"", type=str, help=""Save path in out directory"")\n    parser.add_argument(""--local_rank"", type=int, default=0, help=""Using for Apex DDP"")\n    return parser.parse_args()\n\n\ndef amp_init(args):\n    # Apex Initialize\n    args.distributed = False\n    if \'WORLD_SIZE\' in os.environ:\n        args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n\n    torch.backends.cudnn.benchmark = True\n\n\ndef set_loggging(save_dir):\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    log_format = \'%(asctime)s %(message)s\'\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                        format=log_format, datefmt=\'[%y/%m/%d %H:%M:%S]\')\n\n    fh = logging.FileHandler(f""{save_dir}/log.txt"")\n    fh.setFormatter(logging.Formatter(log_format))\n    logging.getLogger().addHandler(fh)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    amp_init(args)\n\n    parser = Parser(args.config)\n    C = parser.C\n    save_dir = f""{os.getcwd()}/outs/{args.save}""\n\n    if is_main_process():\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir, mode=0o775)\n\n        parser.dump(f""{save_dir}/config.yaml"")\n\n        set_loggging(save_dir)\n\n    device = torch.device(""cuda"")\n    train_loader = get_loader(C.data, ""train"", distributed=args.distributed)\n    valid_loader = get_loader(C.data, ""val"", distributed=args.distributed)\n\n    net = PointRend(\n        deeplabv3(**C.net.deeplab),\n        PointHead(**C.net.pointhead)\n    ).to(device)\n\n    params = [{""params"": net.backbone.backbone.parameters(),   ""lr"": C.train.lr},\n              {""params"": net.head.parameters(),                ""lr"": C.train.lr},\n              {""params"": net.backbone.classifier.parameters(), ""lr"": C.train.lr * 10}]\n\n    optim = torch.optim.SGD(params, momentum=C.train.momentum, weight_decay=C.train.weight_decay)\n\n    net, optim = amp.initialize(net, optim, opt_level=C.apex.opt)\n    if args.distributed:\n        net = ApexDDP(net, delay_allreduce=True)\n\n    train(C.run, save_dir, train_loader, valid_loader, net, optim, device)\n'"
train.py,3,"b'import logging\n\nimport torch\nimport torch.nn.functional as F\n\nfrom model import point_sample\n\nfrom infer import infer\n\nfrom apex import amp\nfrom utils.gpus import is_main_process, reduce_tensor\n\n\ndef step(epoch, loader, net, optim, device):\n    net.train()\n    loss_sum = 0\n    for i, (x, gt) in enumerate(loader):\n        x = x.to(device, non_blocking=True)\n        gt = gt.squeeze_(1).to(device, dtype=torch.long, non_blocking=True)\n\n        result = net(x)\n\n        pred = F.interpolate(result[""coarse""], x.shape[-2:], mode=""bilinear"", align_corners=True)\n        seg_loss = F.cross_entropy(pred, gt, ignore_index=255)\n\n        gt_points = point_sample(\n            gt.float().unsqueeze(1),\n            result[""points""],\n            mode=""nearest"",\n            align_corners=False\n        ).squeeze_(1).long()\n        points_loss = F.cross_entropy(result[""rend""], gt_points, ignore_index=255)\n\n        loss = seg_loss + points_loss\n\n        reduce_seg = reduce_tensor(seg_loss)\n        reduce_point = reduce_tensor(points_loss)\n        reduce_loss = reduce_seg + reduce_point\n\n        if (i % 10) == 0:\n            logging.info(f""[Train] Epoch[{epoch:04d}:{i:03d}/{len(loader):03d}] loss : {reduce_loss.item():.5f} seg : {reduce_seg.item():.5f} points : {reduce_point.item():.5f}"")\n\n        optim.zero_grad()\n        with amp.scale_loss(loss, optim) as scaled_loss:\n            scaled_loss.backward()\n        optim.step()\n\n        loss_sum += reduce_loss.item()\n    return loss_sum / len(loader)\n\n\ndef train(C, save_dir, loader, val_loader, net, optim, device):\n    for e in range(C.epochs):\n        loss = step(e, loader, net, optim, device)\n        if is_main_process() and (e % 10) == 0:\n            torch.save(net.state_dict(),\n                       f""{save_dir}/epoch_{e:04d}_loss_{loss:.5f}.pth"")\n        infer(val_loader, net, device)\n'"
configs/parser.py,0,"b'import os\nimport yaml\nimport logging\nimport argparse\n\n\nclass YamlStructure(dict):\n    def __init__(self, data):\n        super().__init__()\n        assert isinstance(data, dict), ""Check Data Type""\n        self.update(data)\n\n    def __getattr__(self, name):\n        if name in self.keys():\n            return self[name]\n\n    def __repr__(self, path=None):\n        tmp = {}\n\n        def update(src, dst):\n            for k, v in src.items():\n                if isinstance(v, dict):\n                    dst[k] = {}\n                    update(v, dst[k])\n                else:\n                    dst[k] = v\n\n        update(self, tmp)\n\n        if path is not None:\n            with open(path, \'w\') as f:\n                yaml.dump(tmp, f)\n            return f""Yaml Dump in {path}""\n        else:\n            return yaml.dump(tmp)\n\n\nclass Parser:\n    @staticmethod\n    def load(path):\n        with open(path, \'r\') as f:\n            data = yaml.load(f, Loader=yaml.FullLoader)\n        C = YamlStructure(data)\n\n        def to_structure(d):\n            for k, v in d.items():\n                if isinstance(v, (YamlStructure, dict)):\n                    d[k] = YamlStructure(v)\n                    to_structure(v)\n\n        to_structure(C)\n        return C\n\n    def __init__(self, path, args=None):\n        if args is not None:\n            raise NotImplementedError(""Don\'t use args"")\n            assert isinstance(args, argparse.Namespace), ""Check args""\n\n        full_path = f""{os.getcwd()}/{path}""\n        default_path = full_path.replace(path, ""configs/default.yaml"")\n        self.init_yaml(default_path)\n        self.update_yaml(full_path)\n\n    def init_yaml(self, path):\n        with open(path, \'r\') as f:\n            data = yaml.load(f, Loader=yaml.FullLoader)\n        self.C = YamlStructure(data)\n\n        def to_structure(d):\n            for k, v in d.items():\n                if isinstance(v, (YamlStructure, dict)):\n                    d[k] = YamlStructure(v)\n                    to_structure(v)\n\n        to_structure(self.C)\n\n    def update_yaml(self, path):\n        with open(path, \'r\') as f:\n            data = yaml.load(f, Loader=yaml.FullLoader)\n\n        def update(src, dst):\n            for k, v in src.items():\n                if isinstance(v, dict):\n                    update(v, dst[k])\n                elif k in dst.keys():\n                    dst[k] = v\n                else:\n                    raise EnvironmentError(f""key({k}) must be in default.yaml"")\n\n        update(data, self.C)\n\n    def dump(self, path=None):\n        self.C.__repr__(path)\n\n\ndef arg_parse():\n    # projects description\n    desc = ""Test Parser""\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument(\'--save_dir\', type=str, default=""asdf"")\n    parser.add_argument(\'--gpu\', type=int, default=0, help=""Only single gpu"")\n    parser.add_argument(\'--const_top_k\', type=float, default=0.01)\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    path = ""/data2/DW/automl/auto-dip/config/default.yaml""\n    a = {""b"": 1}\n    args = arg_parse()\n    p = Parser(path)\n    print(p.C)\n    print(p.C.data.name)\n    print(p.C.search)\n    p.dump()\n'"
datas/__init__.py,1,"b'from torch.utils.data import DataLoader, DistributedSampler\nfrom torchvision.datasets.voc import VOCSegmentation\nfrom torchvision.datasets.cityscapes import Cityscapes\n\nfrom .transforms import Compose, Resize, ToTensor, Normalize, RandomCrop, RandomFlip, ConvertMaskID\n\n\ndef get_voc(C, split=""train""):\n    if split == ""train"":\n        transforms = Compose([\n            ToTensor(),\n            RandomCrop((256, 256)),\n            Resize((256, 256)),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n    else:\n        transforms = Compose([\n            ToTensor(),\n            Resize((256, 256)),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    return VOCSegmentation(C[\'root\'], download=True, image_set=split, transforms=transforms)\n\n\ndef get_cityscapes(C, split=""train""):\n    if split == ""train"":\n        # Appendix B. Semantic Segmentation Details\n        transforms = Compose([\n            ToTensor(),\n            RandomCrop(768),\n            ConvertMaskID(Cityscapes.classes),\n            RandomFlip(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    else:\n        transforms = Compose([\n            ToTensor(),\n            Resize(768),\n            ConvertMaskID(Cityscapes.classes),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    return Cityscapes(**C, split=split, transforms=transforms)\n\n\ndef get_loader(C, split, distributed):\n    """"""\n    Args:\n        C (Config): C.data\n        split (str): args of dataset,\n                    The image split to use, ``train``, ``test`` or ``val`` if split=""gtFine""\n                    otherwise ``train``, ``train_extra`` or ``val`\n    """"""\n    print(C.name, C.dataset, split)\n    if C.name == ""cityscapes"":\n        dset = get_cityscapes(C.dataset, split)\n    elif C.name == ""pascalvoc"":\n        dset = get_voc(C.dataset, split)\n\n    if split == ""train"":\n        shuffle = True\n        drop_last = False\n    else:\n        shuffle = False\n        drop_last = False\n\n    sampler = None\n    if distributed:\n        sampler = DistributedSampler(dset, shuffle=shuffle)\n        shuffle = None\n\n    return DataLoader(dset, **C.loader, sampler=sampler,\n                      shuffle=shuffle, drop_last=drop_last,\n                      pin_memory=True)\n'"
datas/transforms.py,2,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import normalize\n\n\nclass Resize:\n    def __init__(self, shape):\n        self.shape = [shape, shape] if isinstance(shape, int) else shape\n\n    def __call__(self, img, mask):\n        img, mask = img.unsqueeze(0), mask.unsqueeze(0).float()\n        img = F.interpolate(img, size=self.shape, mode=""bilinear"", align_corners=False)\n        mask = F.interpolate(mask, size=self.shape, mode=""nearest"")\n        return img[0], mask[0].byte()\n\n\nclass RandomCrop:\n    def __init__(self, shape):\n        self.shape = [shape, shape] if isinstance(shape, int) else shape\n        self.fill = 0\n        self.padding_mode = \'constant\'\n\n    def _get_range(self, shape, crop_shape):\n        if shape == crop_shape:\n            start = 0\n        else:\n            start = random.randint(0, shape - crop_shape)\n        end = start + crop_shape\n        return start, end\n\n    def __call__(self, img, mask):\n        _, h, w = img.shape\n        sh, eh = self._get_range(h, self.shape[0])\n        sw, ew = self._get_range(w, self.shape[1])\n        return img[:, sh:eh, sw:ew], mask[:, sh:eh, sw:ew]\n\n\nclass RandomFlip:\n    def __init__(self, prob=0.5, side=(False, True)):\n        self.prob = prob\n        self.side = side\n\n    def _flip(self, img, prob):\n        if self.side[0] and prob[0] <= self.prob:\n            img = img.flip(1)\n        if self.side[1] and prob[1] <= self.prob:\n            img = img.flip(2)\n        return img\n\n    def __call__(self, img, mask):\n        prob = (random.uniform(0, 1), random.uniform(0, 1))\n        return self._flip(img, prob), self._flip(mask, prob)\n\n\nclass ToTensor:\n    def __init__(self):\n        self.to_tensor = transforms.ToTensor()\n\n    def __call__(self, img, mask):\n        img = self.to_tensor(img)\n        mask = torch.from_numpy(np.array(mask))\n        return img, mask[None]\n\n\nclass Normalize:\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img, mask):\n        return normalize(img, self.mean, self.std, False), mask\n\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n\nclass ConvertMaskID:\n    """"""\n    Convert 34 classes to 19 classes\n\n    Change the `id` value of CityscapesClass to `train_id`\n    """"""\n    def __init__(self, classes):\n        self.classes = classes\n\n    def __call__(self, img, mask):\n        mask_train_id = mask.clone()\n        for c in self.classes:\n            mask_train_id[mask == c.id] = c.train_id\n\n        return img, mask_train_id\n'"
model/__init__.py,0,"b'from .sampling_points import sampling_points, point_sample\nfrom torchvision.models.resnet import resnet50, resnet101\nfrom .deeplab import deeplabv3\nfrom .pointrend import PointRend, PointHead\n'"
model/deeplab.py,1,"b'from collections import OrderedDict\n\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torchvision.models.utils import load_state_dict_from_url\nfrom torchvision.models.segmentation._utils import _SimpleSegmentationModel\nfrom torchvision.models.segmentation.deeplabv3 import DeepLabHead\nfrom torchvision.models.segmentation.fcn import FCNHead\nfrom .resnet import resnet103, resnet53\nfrom torchvision.models import resnet50, resnet101\n\n\nclass SmallDeepLab(_SimpleSegmentationModel):\n    def forward(self, input_):\n        result = self.backbone(input_)\n        result[""coarse""] = self.classifier(result[""out""])\n        return result\n\n\ndef deeplabv3(pretrained=False, resnet=""res103"", head_in_ch=2048, num_classes=21):\n    resnet = {\n        ""res53"":  resnet53,\n        ""res103"": resnet103,\n        ""res50"":  resnet50,\n        ""res101"": resnet101\n    }[resnet]\n\n    net = SmallDeepLab(\n        backbone=IntermediateLayerGetter(\n            resnet(pretrained=True, replace_stride_with_dilation=[False, True, True]),\n            return_layers={\'layer2\': \'res2\', \'layer4\': \'out\'}\n        ),\n        classifier=DeepLabHead(head_in_ch, num_classes)\n    )\n    return net\n\n\nif __name__ == ""__main__"":\n    import torch\n    x = torch.randn(3, 3, 512, 1024).cuda()\n    net = deeplabv3(False).cuda()\n    result = net(x)\n    for k, v in result.items():\n        print(k, v.shape)\n'"
model/pointrend.py,6,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import sampling_points, point_sample\n\n\nclass PointHead(nn.Module):\n    def __init__(self, in_c=532, num_classes=21, k=3, beta=0.75):\n        super().__init__()\n        self.mlp = nn.Conv1d(in_c, num_classes, 1)\n        self.k = k\n        self.beta = beta\n\n    def forward(self, x, res2, out):\n        """"""\n        1. Fine-grained features are interpolated from res2 for DeeplabV3\n        2. During training we sample as many points as there are on a stride 16 feature map of the input\n        3. To measure prediction uncertainty\n           we use the same strategy during training and inference: the difference between the most\n           confident and second most confident class probabilities.\n        """"""\n        if not self.training:\n            return self.inference(x, res2, out)\n\n        points = sampling_points(out, x.shape[-1] // 16, self.k, self.beta)\n\n        coarse = point_sample(out, points, align_corners=False)\n        fine = point_sample(res2, points, align_corners=False)\n\n        feature_representation = torch.cat([coarse, fine], dim=1)\n\n        rend = self.mlp(feature_representation)\n\n        return {""rend"": rend, ""points"": points}\n\n    @torch.no_grad()\n    def inference(self, x, res2, out):\n        """"""\n        During inference, subdivision uses N=8096\n        (i.e., the number of points in the stride 16 map of a 1024\xc3\x972048 image)\n        """"""\n        num_points = 8096\n\n        while out.shape[-1] != x.shape[-1]:\n            out = F.interpolate(out, scale_factor=2, mode=""bilinear"", align_corners=True)\n\n            points_idx, points = sampling_points(out, num_points, training=self.training)\n\n            coarse = point_sample(out, points, align_corners=False)\n            fine = point_sample(res2, points, align_corners=False)\n\n            feature_representation = torch.cat([coarse, fine], dim=1)\n\n            rend = self.mlp(feature_representation)\n\n            B, C, H, W = out.shape\n            points_idx = points_idx.unsqueeze(1).expand(-1, C, -1)\n            out = (out.reshape(B, C, -1)\n                      .scatter_(2, points_idx, rend)\n                      .view(B, C, H, W))\n\n        return {""fine"": out}\n\n\nclass PointRend(nn.Module):\n    def __init__(self, backbone, head):\n        super().__init__()\n        self.backbone = backbone\n        self.head = head\n\n    def forward(self, x):\n        result = self.backbone(x)\n        result.update(self.head(x, result[""res2""], result[""coarse""]))\n        return result\n\n\nif __name__ == ""__main__"":\n    x = torch.randn(3, 3, 256, 512).cuda()\n    from deeplab import deeplabv3\n    net = PointRend(deeplabv3(False), PointHead()).cuda()\n    out = net(x)\n    for k, v in out.items():\n        print(k, v.shape)\n'"
model/resnet.py,1,"b'import torch.nn as nn\nfrom torchvision.models.resnet import ResNet, Bottleneck\n\n\nclass ResNetXX3(ResNet):\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super().__init__(block, layers, num_classes, zero_init_residual,\n                         groups, width_per_group, replace_stride_with_dilation,\n                         norm_layer)\n        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        nn.init.kaiming_normal_(self.conv1.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\n\ndef resnet53(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return ResNetXX3(Bottleneck, [3, 4, 6, 3], **kwargs)\n\n\ndef resnet103(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return ResNetXX3(Bottleneck, [3, 4, 23, 3], **kwargs)\n'"
model/sampling_points.py,12,"b'import torch\nimport torch.nn.functional as F\n\n\ndef point_sample(input, point_coords, **kwargs):\n    """"""\n    From Detectron2, point_features.py#19\n\n    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.\n    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside\n    [0, 1] x [0, 1] square.\n\n    Args:\n        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.\n        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains\n        [0, 1] x [0, 1] normalized point coordinates.\n\n    Returns:\n        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains\n            features for points in `point_coords`. The features are obtained via bilinear\n            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.\n    """"""\n    add_dim = False\n    if point_coords.dim() == 3:\n        add_dim = True\n        point_coords = point_coords.unsqueeze(2)\n    output = F.grid_sample(input, 2.0 * point_coords - 1.0, **kwargs)\n    if add_dim:\n        output = output.squeeze(3)\n    return output\n\n\n@torch.no_grad()\ndef sampling_points(mask, N, k=3, beta=0.75, training=True):\n    """"""\n    Follows 3.1. Point Selection for Inference and Training\n\n    In Train:, `The sampling strategy selects N points on a feature map to train on.`\n\n    In Inference, `then selects the N most uncertain points`\n\n    Args:\n        mask(Tensor): [B, C, H, W]\n        N(int): `During training we sample as many points as there are on a stride 16 feature map of the input`\n        k(int): Over generation multiplier\n        beta(float): ratio of importance points\n        training(bool): flag\n\n    Return:\n        selected_point(Tensor) : flattened indexing points [B, num_points, 2]\n    """"""\n    assert mask.dim() == 4, ""Dim must be N(Batch)CHW""\n    device = mask.device\n    B, _, H, W = mask.shape\n    mask, _ = mask.sort(1, descending=True)\n\n    if not training:\n        H_step, W_step = 1 / H, 1 / W\n        N = min(H * W, N)\n        uncertainty_map = -1 * (mask[:, 0] - mask[:, 1])\n        _, idx = uncertainty_map.view(B, -1).topk(N, dim=1)\n\n        points = torch.zeros(B, N, 2, dtype=torch.float, device=device)\n        points[:, :, 0] = W_step / 2.0 + (idx  % W).to(torch.float) * W_step\n        points[:, :, 1] = H_step / 2.0 + (idx // W).to(torch.float) * H_step\n        return idx, points\n\n    # Official Comment : point_features.py#92\n    # It is crucial to calculate uncertanty based on the sampled prediction value for the points.\n    # Calculating uncertainties of the coarse predictions first and sampling them for points leads\n    # to worse results. To illustrate the difference: a sampled point between two coarse predictions\n    # with -1 and 1 logits has 0 logit prediction and therefore 0 uncertainty value, however, if one\n    # calculates uncertainties for the coarse predictions first (-1 and -1) and sampe it for the\n    # center point, they will get -1 unceratinty.\n\n    over_generation = torch.rand(B, k * N, 2, device=device)\n    over_generation_map = point_sample(mask, over_generation, align_corners=False)\n\n    uncertainty_map = -1 * (over_generation_map[:, 0] - over_generation_map[:, 1])\n    _, idx = uncertainty_map.topk(int(beta * N), -1)\n\n    shift = (k * N) * torch.arange(B, dtype=torch.long, device=device)\n\n    idx += shift[:, None]\n\n    importance = over_generation.view(-1, 2)[idx.view(-1), :].view(B, int(beta * N), 2)\n    coverage = torch.rand(B, N - int(beta * N), 2, device=device)\n    return torch.cat([importance, coverage], 1).to(device)\n'"
utils/gpus.py,11,"b'\nimport pickle\nimport time\n\nimport torch\nimport torch.distributed as dist\n\n\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.LongTensor([tensor.numel()]).to(""cuda"")\n    size_list = [torch.LongTensor([0]).to(""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(""cuda""))\n    if local_size != max_size:\n        padding = torch.ByteTensor(size=(max_size - local_size,)).to(""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\n@torch.no_grad()\ndef reduce_tensor(tensor, average=True):\n    world_size = get_world_size()\n    if world_size < 2:\n        return tensor\n\n    dist.reduce(tensor, dst=0)\n    if dist.get_rank() == 0 and average:\n        # only main process gets accumulated, so only divide by\n        # world_size in this case\n        tensor /= world_size\n    return tensor\n\n\n@torch.no_grad()\ndef reduce_dict(input_dict, average=True):\n    """"""\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    names = []\n    values = []\n    # sort the keys so that they are consistent across processes\n    for k in sorted(input_dict.keys()):\n        names.append(k)\n        values.append(input_dict[k])\n    values = torch.stack(values, dim=0)\n    dist.reduce(values, dst=0)\n    if dist.get_rank() == 0 and average:\n        # only main process gets accumulated, so only divide by\n        # world_size in this case\n        values /= world_size\n    return {k: v for k, v in zip(names, values)}\n'"
utils/metrics.py,2,"b'import torch\n\nfrom utils.gpus import reduce_tensor\n\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes, ignore_index=255):\n        self.N = num_classes\n        self.ignore_index = ignore_index\n        self.cm = torch.zeros(self.N, self.N, dtype=torch.float)\n\n    def update(self, pred, gt):\n        idx = (gt != self.ignore_index)\n        indices = self.N * gt[idx] + pred[idx]\n        # cpu version is faster\n        self.cm += torch.bincount(indices.cpu(), minlength=self.N**2).reshape(self.N, self.N)\n\n    def mIoU(self):\n        cm = reduce_tensor(self.cm.cuda(), False)\n        iou = cm.diag() / (cm.sum(dim=1) + cm.sum(dim=0) - cm.diag() + 1e-15)\n        return iou.mean().cpu().item()\n'"
