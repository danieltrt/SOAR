file_path,api_count,code
setup.py,0,"b""from importlib.machinery import SourceFileLoader\nimport pkg_resources\nfrom distutils.version import LooseVersion\nimport re\nimport codecs\nfrom setuptools import setup\nfrom setuptools import find_packages\n\n\n# Read long description of README markdown, shows in Python Package Index\nwith codecs.open('README.md', encoding='utf-8') as f:\n    long_description = f.read()\n\n# Minimal requried dependencies (full dependencies in requirements.txt)\ninstall_requires = ['numpy', \n                    'scipy', \n                    'gym>=0.15.3', \n                    'cloudpickle', \n                    'pyyaml', \n                    'colorama']\ntests_require = ['pytest', \n                 'flake8', \n                 'sphinx', \n                 'sphinx_rtd_theme']\n\nsetup(name='lagom',\n      version=SourceFileLoader('version', 'lagom/version.py').load_module().__version__,\n      # List all lagom packages (folder with __init__.py), useful to distribute a release\n      packages=find_packages(), \n      \n      install_requires=install_requires,\n      tests_require=tests_require,\n      python_requires='>=3',\n      \n      author='Xingdong Zuo',\n      author_email='zuoxingdong@hotmail.com',\n      description='lagom: A light PyTorch infrastructure to quickly prototype reinforcement learning algorithms.',\n      long_description=long_description, \n      long_description_content_type='text/markdown',\n      url='https://github.com/zuoxingdong/lagom',      \n      # tell pip some metadata (e.g. Python version, OS etc.)\n      classifiers=['Programming Language :: Python :: 3', \n                   'License :: OSI Approved :: MIT License', \n                   'Operating System :: OS Independent', \n                   'Natural Language :: English', \n                   'Topic :: Scientific/Engineering :: Artificial Intelligence']\n)\n\n\n# check PyTorch installation\npkg = None\nfor name in ['torch', 'torch-nightly']:\n    try:\n        pkg = pkg_resources.get_distribution(name)\n    except pkg_resources.DistributionNotFound:\n        pass\nassert pkg is not None, 'PyTorch is not correctly installed.'\nversion_msg = 'PyTorch of version above 1.2.0 expected'\nassert LooseVersion(re.search(r'\\d+[.]\\d+[.]\\d+', pkg.version)[0]) >= LooseVersion('1.2.0'), version_msg\n"""
baselines/__init__.py,0,b''
baselines/unzip_logs.py,0,"b""import shutil\nimport tarfile\nfrom pathlib import Path\n\n\n# extract all_logs to each algorithm folder\npath = Path.cwd()\nassert path.name == 'baselines'\nassert (path / 'all_logs.tar.gz').exists()\nalgos = [x for x in path.iterdir() if x.is_dir() and (x / 'experiment.py').exists()]\nall_logs_path = path / 'all_logs'\nif all_logs_path.exists():\n    shutil.rmtree(all_logs_path)\nprint('Extracting all logs folders...')\nwith tarfile.open('all_logs.tar.gz') as tar:\n    tar.extractall()\nassert all_logs_path.exists()\nprint('Done !')\nprint('Moving all logs folders...')\n[shutil.move((all_logs_path / algo.name / 'logs').as_posix(), algo.as_posix()) for algo in algos]\nprint('Done !')\nshutil.rmtree(all_logs_path)\nprint('Clean up all_logs')\n"""
baselines/zip_logs.py,0,"b""import shutil\nimport tarfile\nfrom pathlib import Path\n\n\n# move logs folder for each algorithm to a common all_logs folder and zip it\npath = Path.cwd()\nassert path.name == 'baselines'\nalgos = [x for x in path.iterdir() if x.is_dir() and (x / 'logs').exists()]\nall_logs_path = path / 'all_logs'\nif all_logs_path.exists():\n    shutil.rmtree(all_logs_path)\nif (path / 'all_logs.tar.gz').exists():\n    (path / 'all_logs.tar.gz').unlink()\nall_logs_path.mkdir(parents=True)\n[(all_logs_path / algo.name).mkdir(parents=True) for algo in algos]\nprint('Moving all logs folders...')\n[shutil.move((algo / 'logs').as_posix(), (all_logs_path / algo.name).as_posix()) for algo in algos]\nprint('Done !')\nprint('Creating tar.gz file...')\nwith tarfile.open('all_logs.tar.gz', 'x:gz') as tar:\n    tar.add(all_logs_path.name)\nprint('Done !')\nshutil.rmtree(all_logs_path)\nprint('Clean up all_logs folder')\n"""
examples/__init__.py,0,b''
lagom/__init__.py,0,"b'from .version import __version__\n\nfrom .agent import BaseAgent\nfrom .agent import RandomAgent\n\nfrom .data import StepType\nfrom .data import TimeStep\nfrom .data import Trajectory\n\nfrom .engine import BaseEngine\n\nfrom .es import BaseES\nfrom .es import CMAES\nfrom .es import CEM\n\nfrom .logger import Logger\n\nfrom .runner import BaseRunner\nfrom .runner import EpisodeRunner\nfrom .runner import StepRunner\n\nSEEDS = [1084389005, 1096831319, 1107694401, 1117177635, 1129282672,\n         1140653297, 1150794856, 1160564931, 1172473350, 1182646876,\n         1194869402, 1204124363, 1215002816, 1224696448, 1234004931,\n         1244955702, 1253984800, 1264542370, 1275783021, 1286407000,\n         1297948033, 1308368048, 1318561877, 1328120147, 1340427696,\n         1350227676, 1359860441, 1369776250, 1381816286, 1392715469,\n         1402458368, 1413180126, 1422627425, 1435082944, 1447324129,\n         1456328541, 1465779442, 1475999157, 1486310528, 1494679302,\n         1506172141, 1518028617, 1527780914, 1538526038, 1549325168,\n         1560536970, 1571383129, 1581495367, 1592889501, 1602245422,\n         1614303361, 1624632858, 1633118977, 1642094049, 1654421916,\n         1665793034, 1677022745, 1686043992, 1697281408, 1706820597,\n         1718965098, 1728564048, 1739519808, 1750007859, 1761378110,\n         1771784242, 1781137871, 1791372216, 1800938711, 1810365863,\n         1820490687, 1831406387, 1845967304, 1856705628, 1869071543,\n         1879559156, 1890816383, 1900393289, 1909815351, 1918608275,\n         1932280555, 1943384584, 1953589340, 1965014885, 1975516203,\n         1987315053, 1998853165, 2010844016, 2022228780, 2031808701,\n         2042192235, 2053396598, 2066616655, 2077143627, 2086966747,\n         2097132773, 2108167629, 2118561231, 2128474801, 2138280732]\n'"
lagom/agent.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\nfrom lagom.networks import Module\n\n\nclass BaseAgent(Module, ABC):\n    r""""""Base class for all agents. \n    \n    The agent could select an action from some data (e.g. observation) and update itself by\n    defining a certain learning mechanism. \n    \n    Any agent should subclass this class, e.g. policy-based or value-based. \n    \n    Args:\n        config (dict): a dictionary of configurations\n        env (Env): environment object. \n        device (Device): a PyTorch device\n        **kwargs: keyword aguments used to specify the agent\n    \n    """"""\n    def __init__(self, config, env, device, **kwargs):\n        super(Module, self).__init__(**kwargs)\n        \n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.info = {}\n        self.is_recurrent = None\n        \n    @abstractmethod\n    def choose_action(self, x, **kwargs):\n        r""""""Returns the selected action given the data.\n        \n        .. note::\n        \n            It\'s recommended to handle all dtype/device conversions between CPU/GPU or Tensor/Numpy here.\n        \n        The output is a dictionary containing useful items, \n        \n        Args:\n            obs (object): batched observation returned from the environment. First dimension is treated\n                as batch dimension. \n            **kwargs: keyword arguments to specify action selection.\n            \n        Returns:\n            dict: a dictionary of action selection output. It contains all useful information (e.g. action, \n                action_logprob, state_value). This allows the API to be generic and compatible with\n                different kinds of runner and agents. \n        """"""\n        pass\n        \n    @abstractmethod\n    def learn(self, D, **kwargs):\n        r""""""Defines learning mechanism to update the agent from a batched data. \n        \n        Args:\n            D (list): a list of batched data to train the agent e.g. in policy gradient, this can be \n                a list of :class:`Trajectory`.\n            **kwargs: keyword arguments to specify learning mechanism\n            \n        Returns:\n            dict: a dictionary of learning output. This could contain the loss and other useful metrics. \n        """"""\n        pass\n\n\nclass RandomAgent(BaseAgent):\n    r""""""A random agent samples action uniformly from action space. """"""    \n    def choose_action(self, x, **kwargs):\n        if hasattr(self.env, \'num_envs\'):\n            action = [self.env.action_space.sample() for _ in range(self.env.num_envs)]\n        else:\n            action = self.env.action_space.sample()\n        out = {\'raw_action\': action}\n        return out  \n\n    def learn(self, D, **kwargs):\n        pass\n'"
lagom/data.py,0,"b""from enum import IntEnum\nfrom dataclasses import dataclass\nimport numpy as np\n\n\nclass StepType(IntEnum):\n    FIRST = 0\n    MID = 1\n    LAST = 2\n\n\n@dataclass\nclass TimeStep:\n    step_type: StepType\n    observation: object\n    reward: float\n    done: bool\n    info: dict\n    \n    def __getitem__(self, key):\n        return self.info[key]\n    \n    def first(self):\n        if self.step_type == StepType.FIRST:\n            assert all([x is None for x in [self.reward, self.done, self.info]])\n        return self.step_type == StepType.FIRST\n    \n    def mid(self):\n        if self.step_type == StepType.MID:\n            assert not self.first() and not self.last()\n        return self.step_type == StepType.MID\n        \n    def last(self):\n        if self.step_type == StepType.LAST:\n            assert self.done is not None and self.done\n        return self.step_type == StepType.LAST\n        \n    def time_limit(self):\n        return self.last() and self.info.get('TimeLimit.truncated', False)\n    \n    def terminal(self):\n        return self.last() and not self.time_limit()\n    \n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.step_type.name})'\n\n\nclass Trajectory(object):\n    def __init__(self):\n        self.timesteps = []\n        self._actions = []\n        self._extra_info = {}\n        \n    def __len__(self):\n        return len(self.timesteps)\n    \n    @property\n    def T(self):\n        return max(0, len(self) - 1)\n    \n    def __getitem__(self, index):\n        return self.timesteps[index]\n    \n    def __iter__(self):\n        self.i = 0\n        return self\n    \n    def __next__(self):\n        if self.i < len(self):\n            timestep = self.timesteps[self.i]\n            self.i += 1\n            return timestep\n        else:\n            raise StopIteration\n    \n    @property\n    def finished(self):\n        return len(self) > 0 and self.timesteps[-1].last()\n    \n    @property\n    def reach_time_limit(self):\n        return len(self) > 0 and self.timesteps[-1].time_limit()\n    \n    @property\n    def reach_terminal(self):\n        return len(self) > 0 and self.timesteps[-1].terminal()\n\n    def add(self, timestep, action):\n        assert not self.finished\n        if len(self) == 0:\n            assert timestep.first()\n            assert action is None\n        else:\n            assert action is not None\n            self._actions.append(action)\n        self.timesteps.append(timestep)\n\n    @property\n    def observations(self):\n        return [timestep.observation for timestep in self.timesteps]\n    \n    @property\n    def actions(self):\n        return self._actions\n        \n    @property\n    def rewards(self):\n        return [timestep.reward for timestep in self.timesteps[1:]]\n    \n    @property\n    def dones(self):\n        return [timestep.done for timestep in self.timesteps[1:]]\n    \n    @property\n    def infos(self):\n        return [timestep.info for timestep in self.timesteps[1:]]\n    \n    def get_infos(self, key):\n        return [timestep.info[key] for timestep in self.timesteps[1:] if key in timestep.info]\n    \n    @property\n    def extra_info(self):\n        return self._extra_info\n    \n    @extra_info.setter\n    def extra_info(self, info):\n        self._extra_info = info\n    \n    def __repr__(self):\n        return f'Trajectory(T: {self.T}, Finished: {self.finished}, Reach time limit: {self.reach_time_limit}, Reach terminal: {self.reach_terminal})'\n"""
lagom/engine.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\n\nclass BaseEngine(ABC):\n    r""""""Base class for all engines. \n    \n    It defines the training and evaluation process. \n    \n    """"""\n    def __init__(self, config, **kwargs):\n        self.config = config\n        \n        for key, value in kwargs.items():\n            self.__setattr__(key, value)\n        \n    @abstractmethod\n    def train(self, n=None, **kwargs):\n        r""""""Training process for one iteration. \n        \n        .. note::\n        \n            It is recommended to use :class:`Logger` to store loggings. \n            \n        .. note::\n        \n            All parameterized modules should be called `.train()` to specify training mode.\n        \n        Args:\n            n (int, optional): n-th iteration for training. \n            **kwargs: keyword aguments used for logging. \n        \n        Returns:\n            dict: a dictionary of training output    \n        """"""\n        pass\n        \n    @abstractmethod\n    def eval(self, n=None, **kwargs):\n        r""""""Evaluation process for one iteration. \n        \n        .. note::\n        \n            It is recommended to use :class:`Logger` to store loggings. \n            \n        .. note::\n        \n            All parameterized modules should be called `.eval()` to specify evaluation mode.\n        \n        Args:   \n            n (int, optional): n-th iteration for evaluation. \n            **kwargs: keyword aguments used for logging. \n        \n        Returns:\n            dict: a dictionary of evluation output\n        """"""\n        pass\n'"
lagom/es.py,1,"b'from abc import ABC\nfrom abc import abstractmethod\n\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom lagom.transform import LinearSchedule\n\n\nclass BaseES(ABC):\n    r""""""Base class for all evolution strategies. \n    \n    .. note::\n    \n        The optimization is treated as minimization. e.g. maximize rewards is equivalent to minimize negative rewards.\n        \n    .. note::\n    \n        For painless parallelization, we highly recommend to use `concurrent.futures.ProcessPoolExecutor` with a few \n        practical tips. \n        \n        * Set `max_workers` argument to control the max parallelization capacity. \n        * When execution get stuck, try to use :class:`CloudpickleWrapper` to wrap the objective function\n          e.g. particularly for lambda, class methods\n        * Use `with ProcessPoolExecutor` once to wrap entire iterative ES generations. Because using this \n          internally for each generation, it can slow down the parallelization dramatically due to overheads.\n        * To reduce overheads further (e.g. PyTorch models, gym environments)\n            * Recreate such models for each generation will be very expensive. \n            * Use initializer function for ProcessPoolExecutor\n            * Within initializer function, define PyTorch models and gym environments as global variables\n              Note that the global variables are defined to each worker independently\n            * Don\'t forget to use `with torch.no_grad` to increase forward pass speed.\n\n    """""" \n    @abstractmethod\n    def ask(self):\n        r""""""Sample a set of new candidate solutions. \n        \n        Returns:\n            list: a list of sampled candidate solutions\n        """"""\n        pass\n        \n    @abstractmethod\n    def tell(self, solutions, function_values):\n        r""""""Update the parameters of the population for a new generation based on the values of the objective\n        function evaluated for sampled solutions. \n        \n        Args:\n            solutions (list/ndarray): candidate solutions returned from :meth:`ask`\n            function_values (list): a list of objective function values evaluated for the sampled solutions.\n        """"""\n        pass\n        \n    @property\n    @abstractmethod\n    def result(self):\n        r""""""Return a namedtuple of all results for the optimization. \n        \n        It contains:\n        * xbest: best solution evaluated\n        * fbest: objective function value of the best solution\n        * evals_best: evaluation count when xbest was evaluated\n        * evaluations: evaluations overall done\n        * iterations: number of iterations\n        * xfavorite: distribution mean in ""phenotype"" space, to be considered as current best estimate of the optimum\n        * stds: effective standard deviations\n        """"""\n        pass\n\n    \nclass CMAES(BaseES):\n    r""""""Implements CMA-ES algorithm. \n    \n    .. note::\n    \n        It is a wrapper of the `original CMA-ES implementation`_. \n        \n    Args:\n        x0 (list): initial solution\n        sigma0 (list): initial standard deviation\n        opts (dict): a dictionary of options, e.g. [\'popsize\', \'seed\']\n        \n    .. _original CMA-ES implementation:\n        https://github.com/CMA-ES/pycma\n    \n    """"""\n    def __init__(self, x0, sigma0, opts=None):\n        import cma\n        self.es = cma.CMAEvolutionStrategy(x0, sigma0, opts)\n        \n        self.x0 = self.es.x0\n        self.sigma0 = self.es.sigma0\n        self.popsize = self.es.popsize\n        \n    def ask(self):\n        return self.es.ask()\n    \n    def tell(self, solutions, function_values):\n        self.es.tell(solutions, function_values)\n        \n    @property\n    def result(self):\n        return self.es.result\n\n    \nclass CEM(BaseES):\n    def __init__(self, \n                 x0, \n                 sigma0, \n                 opts=None):\n        self.x0 = x0\n        self.sigma0 = sigma0\n        self.popsize = opts[\'popsize\']\n        self.elite_ratio = opts[\'elite_ratio\']\n        self.elite_size = max(1, int(self.elite_ratio*self.popsize))\n        \n        self.seed = opts[\'seed\'] if \'seed\' in opts else np.random.randint(1, 2**32)\n        self.np_random = np.random.RandomState(self.seed)\n        \n        self.noise_scheduler = LinearSchedule(*opts[\'noise_scheduler_args\'])\n        self.iter = 0\n        \n        # initialize mean and std\n        self.x = np.asarray(x0).astype(np.float32)\n        self.shape = self.x.shape\n        if np.isscalar(sigma0):\n            self.sigma = np.full(self.shape, sigma0, dtype=np.float32)\n        else:\n            self.sigma = np.asarray(sigma0).astype(np.float32)\n            \n        self.xbest = None\n        self.fbest = None\n\n    def ask(self):\n        extra_noise = self.noise_scheduler(self.iter)\n        sigma = np.sqrt(self.sigma**2 + extra_noise)\n        solutions = self.np_random.normal(self.x, sigma, size=(self.popsize,) + self.shape)\n        return solutions\n        \n    def tell(self, solutions, function_values):\n        solutions = np.asarray(solutions).astype(np.float32)\n        elite_idx = np.argsort(function_values)[:self.elite_size]\n        elite = solutions[elite_idx]\n        \n        self.x = elite.mean(axis=0)\n        self.sigma = elite.std(axis=0)\n        self.iter += 1\n        \n        self.xbest = elite[0]\n        self.fbest = function_values[elite_idx[0]]\n        \n    @property\n    def result(self):\n        CEMResult = namedtuple(\'CEMResult\', \n                               [\'xbest\', \'fbest\', \'evals_best\', \'evaluations\', \'iterations\', \'xfavorite\', \'stds\'],\n                               defaults=[None]*7)\n        result = CEMResult(xbest=self.xbest, fbest=self.fbest, iterations=self.iter, xfavorite=self.x, stds=self.sigma)\n        return result\n    \n    def __repr__(self):\n        return f\'CEM in dimension {len(self.x0)} (seed={self.seed})\'\n'"
lagom/logger.py,0,"b'from collections import OrderedDict\nfrom operator import itemgetter  # get list items with multiple indicies\n\nfrom .utils import pickle_dump\n\n\nclass Logger(object):\n    r""""""Log the information in a dictionary. \n    \n    If a key is logged more than once, then the new value will be appended to a list. \n    \n    .. note::\n    \n        It uses pickle to serialize the data. Empirically, ``pickle`` is 2x faster than ``numpy.save``\n        and other alternatives like ``yaml`` is too slow and ``JSON`` does not support numpy array. \n    \n    .. warning::\n    \n        It is discouraged to store hierarchical structure, e.g. list of dict of list of ndarray.\n        Because pickling such complex and large data structure is extremely slow. Put dictionary\n        only at the topmost level. Large numpy array should be saved separately. \n    \n    Example:\n    \n    * Default::\n    \n        >>> logger = Logger()\n        >>> logger(\'iteration\', 1)\n        >>> logger(\'train_loss\', 0.12)\n        >>> logger(\'iteration\', 2)\n        >>> logger(\'train_loss\', 0.11)\n        >>> logger(\'iteration\', 3)\n        >>> logger(\'train_loss\', 0.09)\n        \n        >>> logger\n        OrderedDict([(\'iteration\', [1, 2, 3]), (\'train_loss\', [0.12, 0.11, 0.09])])\n        \n        >>> logger.dump()\n        Iteration: [1, 2, 3]\n        Train Loss: [0.12, 0.11, 0.09]\n        \n    * With indentation::\n    \n        >>> logger.dump(indent=1)\n            Iteration: [1, 2, 3]\n            Train Loss: [0.12, 0.11, 0.09]\n        \n    * With specific keys::\n    \n        >>> logger.dump(keys=[\'iteration\'])\n        Iteration: [1, 2, 3]\n        \n    * With specific index::\n    \n        >>> logger.dump(index=0)\n        Iteration: 1\n        Train Loss: 0.12\n        \n    * With specific list of indices::\n    \n        >>> logger.dump(index=[0, 2])\n        Iteration: [1, 3]\n        Train Loss: [0.12, 0.09]\n    \n    """"""\n    def __init__(self):\n        # TODO: wait for popularity of Python 3.7 which dict preserves the order, then drop OrderedDict()\n        self.logs = OrderedDict()\n        \n    def __call__(self, key, value):\n        r""""""Log the information with given key and value. \n        \n        .. note::\n        \n            The key should be semantic and each word is separated by ``_``. \n        \n        Args:\n            key (str): key of the information\n            value (object): value to be logged\n        """"""\n        if key not in self.logs:\n            self.logs[key] = []\n        \n        self.logs[key].append(value)\n        \n    def dump(self, keys=None, index=None, indent=0, border=\'\'):\n        r""""""Dump the loggings to the screen.\n        \n        Args:\n            keys (list, optional): a list of selected keys. If ``None``, then use all keys. Default: ``None``\n            index (int/list, optional): the index of logged values. It has following use cases:\n                \n                - ``scalar``: a specific index. If ``-1``, then use last element.\n                - ``list``: a list of indicies. \n                - ``None``: all indicies.\n                \n            indent (int, optional): the number of tab indentation. Default: ``0``\n            border (str, optional): the string to print as header and footer\n        """"""\n        if keys is None:\n            keys = list(self.logs.keys())\n        assert isinstance(keys, list), f\'expected list, got {type(keys)}\'\n        \n        if index is None:\n            index = \'all\'\n        \n        print(border)\n        for key in keys:\n            if indent > 0:\n                print(\'\\t\'*indent, end=\'\')  # do not create a new line\n            \n            if index == \'all\':\n                value = self.logs[key]\n            elif isinstance(index, int):\n                value = self.logs[key][index]\n            elif isinstance(index, list):\n                value = list(itemgetter(*index)(self.logs[key]))\n            \n            # Polish key string\n            key = key.strip().replace(\'_\', \' \').title()\n            \n            print(f\'{key}: {value}\')\n        print(border)\n\n    def save(self, f):\n        r""""""Save loggings to a file. \n        \n        Args:\n            f (str): file path\n        """"""\n        pickle_dump(obj=self.logs, f=f, ext=\'.pkl\')\n        \n    def clear(self):\n        r""""""Remove all loggings in the dictionary. """"""\n        self.logs.clear()\n        \n    def __repr__(self):\n        return repr(self.logs)\n'"
lagom/runner.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\nfrom lagom.data import StepType\nfrom lagom.data import TimeStep\nfrom lagom.data import Trajectory\nfrom lagom.envs.timestep_env import TimeStepEnv\n\n\nclass BaseRunner(ABC):\n    r""""""Base class for all runners.\n    \n    A runner is a data collection interface between the agent and the environment. \n        \n    """""" \n    @abstractmethod\n    def __call__(self, agent, env, **kwargs):\n        r""""""Defines data collection via interactions between the agent and the environment.\n        \n        Args:\n            agent (BaseAgent): agent\n            env (Env): environment\n            **kwargs: keyword arguments for more specifications. \n            \n        """"""\n        pass\n\n\nclass EpisodeRunner(BaseRunner):\n    def __call__(self, agent, env, N, **kwargs):\n        assert isinstance(env, TimeStepEnv)\n        D = []\n        for _ in range(N):\n            traj = Trajectory()\n            timestep = env.reset()\n            traj.add(timestep, None)\n            while not timestep.last():\n                out_agent = agent.choose_action(timestep, **kwargs)\n                action = out_agent.pop(\'raw_action\')\n                timestep = env.step(action)\n                timestep.info = {**timestep.info, **out_agent}\n                traj.add(timestep, action)\n            traj.extra_info[\'last_info\'] = agent.choose_action(timestep, last_info=True, **kwargs)\n            D.append(traj)\n        return D\n\n\nclass StepRunner(BaseRunner):\n    def __init__(self, reset_on_call=True):\n        self.reset_on_call = reset_on_call\n        self.observation = None\n        \n    def __call__(self, agent, env, T, **kwargs):\n        assert isinstance(env, TimeStepEnv)\n        D = []\n        traj = Trajectory()\n        if self.reset_on_call or self.observation is None:\n            timestep = env.reset()\n        else:\n            timestep = TimeStep(StepType.FIRST, observation=self.observation, reward=None, done=None, info=None)\n        traj.add(timestep, None)\n        for t in range(T):\n            out_agent = agent.choose_action(timestep, **kwargs)\n            action = out_agent.pop(\'raw_action\')\n            timestep = env.step(action)\n            timestep.info = {**timestep.info, **out_agent}\n            traj.add(timestep, action)\n            if timestep.last():\n                traj.extra_info[\'last_info\'] = agent.choose_action(timestep, last_info=True, **kwargs)\n                D.append(traj)\n                traj = Trajectory()\n                timestep = env.reset()\n                traj.add(timestep, None)\n        if traj.T > 0:\n            traj.extra_info[\'last_info\'] = agent.choose_action(timestep, last_info=True, **kwargs)\n            D.append(traj)\n        if not self.reset_on_call:\n            self.observation = timestep.observation\n        return D\n'"
lagom/version.py,0,"b""# Useful for setup.py and when lagom is not installed yet\n# Versioning format: Major.Minor.Maintenance\n__version__ = '0.0.3'\n"""
legacy/clip_action.py,0,"b'import numpy as np\n\nfrom gym import ActionWrapper\n\n\nclass ClipAction(ActionWrapper):\n    r""""""Clip the continuous action within the valid bound. """"""\n    def action(self, action):\n        return np.clip(action, self.action_space.low, self.action_space.high)\n\n\ndef test_clip_action():\n    # mountaincar: action-based rewards\n    env = gym.make(\'MountainCarContinuous-v0\')\n    clipped_env = ClipAction(env)\n\n    env.reset()\n    clipped_env.reset()\n\n    action = [10000.]\n\n    _, reward, _, _ = env.step(action)\n    _, clipped_reward, _, _ = clipped_env.step(action)\n\n    assert abs(clipped_reward) < abs(reward)\n'"
legacy/clip_reward.py,0,"b'import numpy as np\n\nfrom gym import RewardWrapper\n\n\nclass ClipReward(RewardWrapper):\n    r""""""""Clip reward to [min, max]. """"""\n    def __init__(self, env, min_r, max_r):\n        super().__init__(env)\n        self.min_r = min_r\n        self.max_r = max_r\n            \n    def reward(self, reward):\n        return np.clip(reward, self.min_r, self.max_r)\n\n\n@pytest.mark.parametrize(\'env_id\', [\'CartPole-v1\', \'Pendulum-v0\', \'MountainCar-v0\'])\ndef test_clip_reward(env_id):\n    env = gym.make(env_id)\n    wrapped_env = ClipReward(env, -0.0005, 0.0002)\n\n    env.reset()\n    wrapped_env.reset()\n\n    action = env.action_space.sample()\n\n    _, reward, _, _ = env.step(action)\n    _, wrapped_reward, _, _ = wrapped_env.step(action)\n\n    assert abs(wrapped_reward) < abs(reward)\n    assert wrapped_reward == -0.0005 or wrapped_reward == 0.0002\n'"
legacy/episode_runner.py,0,"b""class EpisodeRunner(BaseRunner):\n    def __init__(self, reset_on_call=True):\n        self.reset_on_call = reset_on_call\n        self.observation = None\n    \n    def __call__(self, agent, env, T, **kwargs):\n        assert isinstance(env, VecEnv) and isinstance(env, VecStepInfo) and len(env) == 1\n        \n        D = [Trajectory()]\n        if self.reset_on_call:\n            observation, _ = env.reset()\n        else:\n            if self.observation is None:\n                self.observation, _ = env.reset()\n            observation = self.observation\n        D[-1].add_observation(observation)\n        for t in range(T):\n            out_agent = agent.choose_action(observation, **kwargs)\n            action = out_agent.pop('raw_action')\n            next_observation, [reward], [step_info] = env.step(action)\n            step_info.info = {**step_info.info, **out_agent}\n            if step_info.last:\n                D[-1].add_observation([step_info['last_observation']])  # add a batch dim    \n            else:\n                D[-1].add_observation(next_observation)\n            D[-1].add_action(action)\n            D[-1].add_reward(reward)\n            D[-1].add_step_info(step_info)\n            if step_info.last:\n                assert D[-1].completed\n                D.append(Trajectory())\n                D[-1].add_observation(next_observation)  # initial observation\n            observation = next_observation\n        if len(D[-1]) == 0:\n            D = D[:-1]\n        self.observation = observation\n        return D\n\n\n@pytest.mark.parametrize('env_id', ['Sanity', 'CartPole-v1', 'Pendulum-v0', 'Pong-v0'])\n@pytest.mark.parametrize('num_env', [1, 3])\n@pytest.mark.parametrize('init_seed', [0, 10])\n@pytest.mark.parametrize('T', [1, 5, 100])\ndef test_episode_runner(env_id, num_env, init_seed, T):    \n    if env_id == 'Sanity':\n        make_env = lambda: TimeLimit(SanityEnv())\n    else:\n        make_env = lambda: gym.make(env_id)\n    env = make_vec_env(make_env, num_env, init_seed)\n    env = VecStepInfo(env)\n    agent = RandomAgent(None, env, None)\n    runner = EpisodeRunner()\n    \n    if num_env > 1:\n        with pytest.raises(AssertionError):\n            D = runner(agent, env, T)\n    else:\n        with pytest.raises(AssertionError):\n            runner(agent, env.env, T)  # must be VecStepInfo\n        D = runner(agent, env, T)\n        for traj in D:\n            assert isinstance(traj, Trajectory)\n            assert len(traj) <= env.spec.max_episode_steps\n            assert traj.numpy_observations.shape == (len(traj) + 1, *env.observation_space.shape)\n            if isinstance(env.action_space, gym.spaces.Discrete):\n                assert traj.numpy_actions.shape == (len(traj),)\n            else:\n                assert traj.numpy_actions.shape == (len(traj), *env.action_space.shape)\n            assert traj.numpy_rewards.shape == (len(traj),)\n            assert traj.numpy_dones.shape == (len(traj), )\n            assert traj.numpy_masks.shape == (len(traj), )\n            assert len(traj.step_infos) == len(traj)\n            if traj.completed:\n                assert np.allclose(traj.observations[-1], traj.step_infos[-1]['last_observation'])\n"""
legacy/experiment_master.py,0,"b'from itertools import product\n\nfrom lagom.multiprocessing import ProcessMaster\n\n\nclass ExperimentMaster(ProcessMaster):\n    r""""""The master of parallelized experiment. \n    \n    It receives a :class:`Config` object and generate a list of all possible configurations. It also receives\n    a list of random seeds, and each configuration runs with each random seeds. Then it distributes each\n    pair of configuration and seed to the workers. \n\n    """"""\n    def __init__(self, worker_class, num_worker, run, config, seeds):\n        super().__init__(worker_class, num_worker)\n        \n        self.run = run\n        self.config = config\n        self.seeds = seeds\n        self.configs = self.config.make_configs()\n    \n    def make_tasks(self):\n        tasks = list(product(self.configs, self.seeds))\n        tasks = [(config, seed, self.run) for config, seed in tasks]\n        return tasks\n'"
legacy/experiment_worker.py,5,"b'import torch\n\nfrom lagom.multiprocessing import ProcessWorker\n\n\nclass ExperimentWorker(ProcessWorker):\n    r""""""The worker of parallelized experiment. \n    \n    It assigns a PyTorch device object to each received task according to their ID. \n    \n    .. note::\n    \n        If the configuration indicates to use GPU (i.e. ``config[\'cuda\']=True``), then each worker will\n        assign a specific CUDA device for PyTorch in rolling manner. Concretely, if there are 5 GPUs\n        available and the master assigns 30 workers, then each GPU will be assigned by 6 workers. \n        The GPU is chosen by the worker ID modulus total number of GPUs. In other words, the \n        workers iterate over all GPUs in rolling manner trying to use all GPUs exhaustively for maximal speedup. \n    \n    """"""    \n    def work(self, task_id, task):\n        config, seed, run = task\n        device = self.make_device(config, task_id)\n        \n        print(f\'@ Seed for following configuration: {seed}\')\n        print(\'#\'*50)\n        [print(f\'# {key}: {value}\') for key, value in config.items()]\n        print(\'#\'*50)\n        \n        result = run(config=config, seed=seed, device=device)\n        \n        return result\n    \n    def make_device(self, config, task_id):\n        if \'cuda\' in config and config[\'cuda\']:\n            if \'cuda_ids\' in config:  # use specific GPUs\n                device_id = config[\'cuda_ids\'][task_id % len(config[\'cuda_ids\'])]\n            else:  # use all GPUs\n                num_gpu = torch.cuda.device_count()\n                device_id = task_id % num_gpu\n            \n            torch.cuda.set_device(device_id)\n            device = torch.device(f\'cuda:{device_id}\')\n        else:\n            device = torch.device(\'cpu\')\n            torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK, e.g. with LayerNorm\n            \n        return device\n'"
legacy/flatten_observation.py,0,"b'from gym import ObservationWrapper\nfrom gym.spaces import flatten\n\n\nclass FlattenObservation(ObservationWrapper):\n    r""""""Observation wrapper that flattens the observation. \n    \n    .. note:\n    \n        Keep the original observation space, because e.g. unflatten maybe used\n    \n    """"""\n    def observation(self, observation):\n        return flatten(self.observation_space, observation)\n'"
legacy/frame_stack.py,0,"b'from collections import deque\nfrom lz4.block import compress\nfrom lz4.block import decompress\n\nimport numpy as np\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass LazyFrames(object):\n    r""""""Ensures common frames are only stored once to optimize memory use. \n    \n    To further reduce the memory use, it is optionally to turn on lz4 to \n    compress the observations.\n    \n    .. note::\n    \n        This object should only be converted to numpy array just before forward pass. \n        \n    """"""\n    def __init__(self, frames, lz4_compress=False):\n        if lz4_compress:\n            self.shape = frames[0].shape\n            frames = [compress(frame) for frame in frames]\n        self._frames = frames\n        self.lz4_compress = lz4_compress\n        \n    def __array__(self, dtype=None):\n        if self.lz4_compress:\n            frames = [np.frombuffer(decompress(frame), dtype=np.uint8).reshape(self.shape) for frame in self._frames]\n        else:\n            frames = self._frames\n        out = np.stack(frames, axis=0)\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n    \n    def __len__(self):\n        return len(self.__array__())\n    \n    def __getitem__(self, i):\n        return self.__array__()[i]\n        \n    \nclass FrameStack(ObservationWrapper):\n    r""""""Observation wrapper that stacks the observations in a rolling manner. \n    \n    For example, if the number os stacks is 4, then returned observation constains\n    the most recent 4 observations. For environment \'Pendulum-v0\', the original observation\n    is an array with shape [3], so if we stack 4 observations, the processed observation\n    has shape [3, 4]. \n    \n    .. note::\n    \n        To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n    \n    .. note::\n    \n        The observation space must be `Box` type. If one uses `Dict`\n        as observation space, it should apply `FlattenDictWrapper` at first. \n    \n    Example::\n    \n        >>> import gym\n        >>> env = gym.make(\'PongNoFrameskip-v0\')\n        >>> env = FrameStack(env, 4)\n        >>> env.observation_space\n        Box(4, 210, 160, 3)\n    \n    Args:\n        env (Env): environment object\n        num_stack (int): number of stacks\n    \n    """"""\n\n    def __init__(self, env, num_stack, lz4_compress=False):\n        super().__init__(env)\n        self.num_stack = num_stack\n        self.lz4_compress = lz4_compress\n\n        self.frames = deque(maxlen=num_stack)\n        \n        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n        high = np.repeat(self.observation_space.high[np.newaxis, ...], num_stack, axis=0)\n        self.observation_space = Box(low=low, high=high, dtype=self.observation_space.dtype)\n        \n    def _get_observation(self):\n        assert len(self.frames) == self.num_stack\n        return LazyFrames(list(self.frames), self.lz4_compress)\n        \n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        self.frames.append(observation)\n        return self._get_observation(), reward, done, info\n        \n    def reset(self, **kwargs):\n        observation = self.env.reset(**kwargs)\n        [self.frames.append(observation) for _ in range(self.num_stack)]\n        return self._get_observation()\n'"
legacy/gray_scale_observation.py,0,"b'import numpy as np\nimport cv2\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass GrayScaleObservation(ObservationWrapper):\n    r""""""Convert the image observation from RGB to gray scale. """"""\n    def __init__(self, env, keep_dim=False):\n        super().__init__(env)\n        self.keep_dim = keep_dim\n        \n        obs_shape = self.observation_space.shape[:2]\n        if self.keep_dim:\n            self.observation_space = Box(low=0, high=255, shape=(*obs_shape, 1), dtype=np.uint8)\n        else:\n            self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n        \n    def observation(self, observation):\n        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n        if self.keep_dim:\n            observation = np.expand_dims(observation, -1)\n        return observation\n'"
legacy/ln_rnn.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef make_rnncell(cell_type, input_dim, hidden_sizes):\n    r""""""Returns a ModuleList of RNN Cells.\n    \n    .. note::\n    \n        All submodules can be automatically tracked because it uses nn.ModuleList. One can\n        use this function to generate parameters in :class:`BaseNetwork`. \n\n    Example::\n    \n        >>> make_rnncell(\'LSTMCell\', 3, [51, 32, 16])\n        ModuleList(\n          (0): LSTMCell(3, 51)\n          (1): LSTMCell(51, 32)\n          (2): LSTMCell(32, 16)\n        )\n    \n    Args:\n        cell_type (str): RNNCell type e.g. [\'RNNCell\', \'LSTMCell\', \'GRUCell\', \'LayerNormLSTMCell\']\n        input_dim (int): input dimension in the first recurrent layer. \n        hidden_sizes (list): a list of hidden sizes, each for one recurrent layer. \n    \n    Returns\n    -------\n    rnncell : nn.ModuleList\n        a ModuleList of recurrent layers (cells).\n        \n    """"""\n    assert isinstance(hidden_sizes, list), f\'expected as list, got {type(hidden_sizes)}\'\n    \n    if cell_type == \'RNNCell\':\n        cell_f = nn.RNNCell\n    elif cell_type == \'LSTMCell\':\n        cell_f = nn.LSTMCell\n    elif cell_type == \'GRUCell\':\n        cell_f = nn.GRUCell\n    elif cell_type == \'LayerNormLSTMCell\':\n        cell_f = LayerNormLSTMCell\n    else:\n        raise ValueError(f\'expected RNNCell/LSTMCell/GRUCell/LayerNormLSTMCell, got {cell_type}\')\n    \n    hidden_sizes = [input_dim] + hidden_sizes\n    \n    rnncell = []\n    for input_size, hidden_size in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n        rnncell.append(cell_f(input_size=input_size, hidden_size=hidden_size))\n    \n    rnncell = nn.ModuleList(rnncell)\n    \n    return rnncell\n\n\nclass LayerNormLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        if dropout > 0 and num_layers == 1:\n            raise ValueError(\'dropout used for at least two layers, and apply all but last recurrent layer.\')\n        \n        self.ln_cells = make_rnncell(\'LayerNormLSTMCell\', input_size, [hidden_size]*num_layers)\n        \n    def forward(self, input, hx):\n        x = input\n        h, c = hx\n        for i, ln_cell in enumerate(self.ln_cells):\n            output = []\n            for t in range(x.size(0)):\n                h[i], c[i] = ln_cell(x[t], (h[i].clone(), c[i].clone()))\n                output.append(h[i])\n            x = torch.stack(output, dim=0)\n            if self.dropout > 0 and i < self.num_layers - 1:\n                x = F.dropout(x, self.dropout, self.training)\n        return x, (h, c)\n'"
legacy/ln_rnncell.py,1,"b""import torch\nimport torch.nn as nn\n\n\nclass LayerNormLSTMCell(nn.RNNCellBase):\n    def __init__(self, input_size, hidden_size, bias=True, ln_preact=True):\n        super(LayerNormLSTMCell, self).__init__(input_size, hidden_size, bias, num_chunks=4)\n        \n        self.ln_preact = ln_preact\n        if self.ln_preact:\n            self.ln_ih = nn.LayerNorm(4*self.hidden_size)\n            self.ln_hh = nn.LayerNorm(4*self.hidden_size)\n        self.ln_cell = nn.LayerNorm(self.hidden_size)\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            hx = (hx, hx)\n        self.check_forward_hidden(input, hx[0], '[0]')\n        self.check_forward_hidden(input, hx[1], '[1]')\n        \n        # hidden states and preactivations\n        h, c = hx\n        ih = input @ self.weight_ih.t() + self.bias_ih\n        hh = h @ self.weight_hh.t() + self.bias_hh\n        if self.ln_preact:\n            ih = self.ln_ih(ih)\n            hh = self.ln_hh(hh)\n        preact = ih + hh\n        \n        # Gates\n        f, i, o, g = preact.chunk(4, dim=1)\n        g = g.tanh()\n        f = f.sigmoid()\n        i = i.sigmoid()\n        o = o.sigmoid()\n        \n        # cell computations\n        c = f*c + i*g\n        c = self.ln_cell(c)\n        h = o*c.tanh()\n        \n        return h, c\n"""
legacy/make_vec_env.py,0,"b'from functools import partial  # argument-free functions\n\nfrom lagom.utils import Seeder\nfrom lagom.utils import CloudpickleWrapper\n\nfrom .vec_env import VecEnv\n\n\ndef make_vec_env(make_env, num_env, init_seed):\n    r""""""Create a vectorized environment, each associated with a different random seed.\n    \n    Example::\n        \n        >>> import gym\n        >>> make_vec_env(lambda: gym.make(\'CartPole-v1\'), 3, 0)\n        <VecEnv: 3, CartPole-v1>\n    \n    Args:\n        make_env (function): a function to create an environment\n        num_env (int): number of environments to create. \n        init_seed (int): initial seed for :class:`Seeder` to sample random seeds. \n    \n    Returns:\n        VecEnv: created vectorized environment\n    """"""\n    # Generate different seeds for each environment\n    seeder = Seeder(init_seed=init_seed)\n    seeds = seeder(size=num_env)\n    \n    def f(seed):\n        env = make_env()\n        env.seed(seed)\n        env.observation_space.seed(seed)\n        env.action_space.seed(seed)\n        return env\n    \n    # Use partial to generate a list of argument-free make_env, each with different seed\n    # partial object is not picklable, so wrap it with magical CloudpickleWrapper\n    list_make_env = [CloudpickleWrapper(partial(f, seed=seed)) for seed in seeds]\n    return VecEnv(list_make_env)\n\n\n\n\n\n@pytest.mark.parametrize(\'env_id\', [\'CartPole-v0\', \'Pendulum-v0\'])\n@pytest.mark.parametrize(\'num_env\', [1, 3, 5])\n@pytest.mark.parametrize(\'init_seed\', [0, 10])\ndef test_make_vec_env(env_id, num_env, init_seed):\n    def make_env():\n        return gym.make(env_id)\n    env = make_vec_env(make_env, num_env, init_seed)\n    assert isinstance(env, VecEnv)\n    seeds = [x.keywords[\'seed\'] for x in env.list_make_env]\n    seeder = Seeder(init_seed)\n    assert seeds == seeder(num_env)'"
legacy/parallel_vec_env.py,0,"b'import numpy as np\n\nfrom multiprocessing import Process  # easier than threading\nfrom multiprocessing import Pipe  # faster than Queue\n\nfrom lagom.utils import CloudpickleWrapper\n\nfrom .vec_env import VecEnv\n\n\ndef worker(master_conn, worker_conn, make_env):\n    r""""""Environment worker to do working for master and send back result via Pipe connection. \n    \n    Args:\n        master_conn (Connection): master connection terminal\n        worker_conn (Connection): worker connection terminal\n        make_env (function): an argument-free function to generate an environment. \n    """"""\n    # Close forked master connection as it is not used here\n    master_conn.close()\n    \n    env = make_env()\n    \n    while True:\n        cmd, data = worker_conn.recv()\n        \n        if cmd == \'step\':\n            observation, reward, done, info = env.step(data)\n            # If done=True, reset environment, store last observation in info and report new initial observation\n            if done:\n                info[\'last_observation\'] = observation\n                observation = env.reset()\n            worker_conn.send([observation, reward, done, info])\n        elif cmd == \'reset\':\n            observation = env.reset()\n            worker_conn.send(observation)\n        elif cmd == \'render\':\n            img = env.render(mode=\'rgb_array\')\n            worker_conn.send(img)\n        elif cmd == \'close\':\n            env.close()\n            worker_conn.close()\n            break\n        elif cmd == \'env_info\':\n            worker_conn.send([env.observation_space, env.action_space, env.reward_range, env.spec])\n        elif cmd == \'get_env\':\n            worker_conn.send(env)\n        elif cmd == \'set_env\':\n            env = data\n\n\nclass ParallelVecEnv(VecEnv):\n    r""""""A vectorized environment runs in parallel. Each sub-environment uses an individual Process.\n    \n    For each :meth:`step` and :meth:`reset`, the command is executed for each sub-environment\n    all at once in parallel. \n    \n    .. note::\n    \n        It is recommended to use this if the simulator is very computationally expensive. In this\n        case, :class:`SerialVecEnv` would be too slow. However, if the simulator is very fast, one\n        should use :class:`SerialVecEnv` instead. \n        \n    Example::\n        \n        >>> from lagom.envs import make_envs, make_gym_env\n        >>> list_make_env = make_envs(make_env=make_gym_env, env_id=\'CartPole-v1\', num_env=3, init_seed=0)\n        >>> env = ParallelVecEnv(list_make_env=list_make_env)\n        >>> env\n        <ParallelVecEnv: CartPole-v1, n: 3>\n        \n        >>> env.reset()\n        [array([-0.04002427,  0.00464987, -0.01704236, -0.03673052]),\n         array([ 0.00854682,  0.00830137, -0.03052506,  0.03439879]),\n         array([0.00025361, 0.02915667, 0.01103413, 0.04977449])]\n         \n    Args:\n            list_make_env (list): a list of functions to generate environments.\n    \n    """"""\n    def __init__(self, list_make_env):\n        self.master_conns, self.worker_conns = zip(*[Pipe() for _ in range(len(list_make_env))])\n        self.list_process = [Process(target=worker, \n                                     args=[master_conn, worker_conn, CloudpickleWrapper(make_env)], \n                                     daemon=True)\n                             for master_conn, worker_conn, make_env \n                             in zip(self.master_conns, self.worker_conns, list_make_env)]\n        [process.start() for process in self.list_process]\n        [worker_conn.close() for worker_conn in self.worker_conns]\n        \n        self.master_conns[0].send([\'env_info\', None])\n        observation_space, action_space, reward_range, spec = self.master_conns[0].recv()\n        super().__init__(list_make_env=list_make_env, \n                         observation_space=observation_space, \n                         action_space=action_space, \n                         reward_range=reward_range, \n                         spec=spec)\n        \n        self.waiting = False  # If True, then workers are still working\n        \n    def step(self, actions):\n        [master_conn.send([\'step\', action]) for master_conn, action in zip(self.master_conns, actions)]\n        self.waiting = True\n        # Note that different worker finishes the job differently, but list comprehension\n        # automatically preserve the order. This order is very important, otherwise it is a BUG !\n        results = [master_conn.recv() for master_conn in self.master_conns]\n        self.waiting = False\n        observations, rewards, dones, infos = zip(*results)\n        return list(observations), list(rewards), list(dones), list(infos)  # zip produces tuples\n    \n    def reset(self):\n        [master_conn.send([\'reset\', None]) for master_conn in self.master_conns]\n        observations = [master_conn.recv() for master_conn in self.master_conns]\n        return observations\n    \n    def get_images(self):\n        [master_conn.send([\'render\', None]) for master_conn in self.master_conns]\n        imgs = [master_conn.recv() for master_conn in self.master_conns]\n        return imgs\n    \n    def close_extras(self):\n        if self.closed:  # all environments already closed\n            return None\n        \n        # Waiting to receive data from all the workers if they are still working\n        if self.waiting:\n            [master_conn.recv() for master_conn in self.master_conns]\n        [master_conn.send([\'close\', None]) for master_conn in self.master_conns]\n        [master_conn.close() for master_conn in self.master_conns]\n        [process.join() for process in self.list_process]\n        self.closed = True\n        \n    def __getitem__(self, index):\n        self.master_conns[index].send([\'get_env\', None])\n        return self.master_conns[index].recv()\n    \n    def __setitem__(self, index, x):\n        self.master_conns[index].send([\'set_env\', x])\n    \n    def __del__(self):\n        if not self.closed:\n            self.close()\n'"
legacy/resize_observation.py,0,"b'import numpy as np\nimport cv2\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass ResizeObservation(ObservationWrapper):\n    r""""""Downsample the image observation to a square image. """"""\n    def __init__(self, env, size):\n        super().__init__(env)\n        assert size > 0\n        self.size = size\n        \n        shape = (self.size, self.size) + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=shape, dtype=np.uint8)\n        \n    def observation(self, observation):\n        observation = cv2.resize(observation, (self.size, self.size), interpolation=cv2.INTER_AREA)\n        if observation.ndim == 2:\n            observation = np.expand_dims(observation, -1)\n        return observation\n\n\n@pytest.mark.parametrize(\'env_id\', [\'Pong-v0\', \'SpaceInvaders-v0\'])\n@pytest.mark.parametrize(\'size\', [16, 32])\ndef test_resize_observation(env_id, size):\n    env = gym.make(env_id)\n    env = ResizeObservation(env, size)\n\n    assert env.observation_space.shape[-1] == 3\n    assert env.observation_space.shape[:2] == (size, size)\n    obs = env.reset()\n    assert obs.shape == (size, size, 3)\n'"
legacy/run_experiment.py,0,"b'from shutil import rmtree\nfrom shutil import copyfile\nfrom pathlib import Path\nimport inspect\n\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import yaml_dump\nfrom lagom.utils import ask_yes_or_no\nfrom lagom.utils import timeit\n\nfrom .experiment_master import ExperimentMaster\nfrom .experiment_worker import ExperimentWorker\n\n\n@timeit(color=\'green\', attribute=\'bold\')\ndef run_experiment(run, config, seeds, num_worker):\n    r""""""A convenient function to launch a parallelized experiment (Master-Worker). \n    \n    .. note::\n    \n        It automatically creates all subfolders for logging the experiment. The topmost\n        folder is indicated by the logging directory specified in the configuration. \n        Then all subfolders for each configuration are created with the name of their ID.\n        Finally, under each configuration subfolder, a set subfolders are created for each\n        random seed (the random seed as folder name). Intuitively, an experiment could have \n        following directory structure::\n        \n            - logs\n                - 0  # ID number\n                    - 123  # random seed\n                    - 345\n                    - 567\n                - 1\n                    - 123\n                    - 345\n                    - 567\n                - 2\n                    - 123\n                    - 345\n                    - 567\n                - 3\n                    - 123\n                    - 345\n                    - 567\n                - 4\n                    - 123\n                    - 345\n                    - 567\n    \n    Args:\n        run (function): an algorithm function to train on.\n        config (Config): a :class:`Config` object defining all configuration settings\n        seeds (list): a list of random seeds\n        num_worker (int): number of workers\n        \n    """"""\n    experiment = ExperimentMaster(ExperimentWorker, num_worker, run, config, seeds)\n    \n    log_path = Path(experiment.configs[0][\'log.dir\'])\n    if not log_path.exists():\n        log_path.mkdir(parents=True)\n    else:\n        msg = f""Logging directory \'{log_path.absolute()}\' already existed, do you want to clean it ?""\n        answer = ask_yes_or_no(msg)\n        if answer:\n            rmtree(log_path)\n            log_path.mkdir(parents=True)\n        else:  # back up\n            old_log_path = log_path.with_name(\'old_\' + log_path.name)\n            log_path.rename(old_log_path)\n            log_path.mkdir(parents=True)\n            print(f""The old logging directory is renamed to \'{old_log_path.absolute()}\'. "")\n            input(\'Please, press Enter to continue\\n>>> \')\n            \n    # save source files\n    source_path = Path(log_path / \'source_files/\')\n    source_path.mkdir(parents=True)\n    [copyfile(s, source_path / s.name) for s in Path(inspect.getsourcefile(run)).parent.glob(\'*.py\')]\n            \n    # Create subfolders for each ID and subsubfolders for each random seed\n    for config in experiment.configs:\n        ID = config[\'ID\']\n        for seed in experiment.seeds:\n            p = log_path / f\'{ID}\' / f\'{seed}\'\n            p.mkdir(parents=True)\n        yaml_dump(obj=config, f=log_path/f\'{ID}\'/\'config\', ext=\'.yml\')\n\n    pickle_dump(experiment.configs, log_path / \'configs\', ext=\'.pkl\')\n            \n    # Run experiment in parallel\n    results = experiment()\n    return results\n'"
legacy/scale_reward.py,0,"b'from gym import RewardWrapper\n\n\nclass ScaleReward(RewardWrapper):\n    r""""""Scale the reward. \n    \n    .. note::\n    \n        This is incredibly important and drastically impact on performance e.g. PPO. \n        \n    Example::\n    \n        >>> from lagom.envs import make_gym_env\n        >>> env = make_gym_env(env_id=\'CartPole-v1\', seed=0)\n        >>> env = ScaleReward(env, scale=0.1)\n        >>> env.reset()\n        >>> observation, reward, done, info = env.step(env.action_space.sample())\n        >>> reward\n        0.1\n        \n    Args:\n            env (Env): environment\n            scale (float): reward scaling factor\n        \n    """"""\n    def __init__(self, env, scale=0.01):\n        super().__init__(env)\n        self.scale = scale\n        \n    def reward(self, reward):\n        return self.scale*reward\n'"
legacy/scaled_float_frame.py,0,"b'import numpy as np\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass ScaledFloatFrame(ObservationWrapper):\n    r""""""Convert image frame to float range [0, 1] by dividing 255. \n    \n    .. warning::\n    \n        Do NOT use this wrapper for DQN ! It will break the memory optimization.\n    \n    """"""\n    def __init__(self, env):\n        super().__init__(env)\n        self.observation_space = Box(low=0, high=1, shape=self.observation_space.shape, dtype=np.float32)\n    \n    def observation(self, observation):\n        return observation.astype(np.float32)/255.\n\n\n@pytest.mark.parametrize(\'env_id\', [\'Pong-v0\', \'SpaceInvaders-v0\'])\ndef test_scaled_float_frame(env_id):\n    env = gym.make(env_id)\n    env = ScaledFloatFrame(env)\n    assert np.allclose(env.observation_space.high, 1.0)\n    assert np.allclose(env.observation_space.low, 0.0)\n    obs = env.reset()\n    assert np.alltrue(obs <= 1.0) and np.alltrue(obs >= 0.0)\n    obs, _, _, _ = env.step(env.action_space.sample())\n    assert np.alltrue(obs <= 1.0) and np.alltrue(obs >= 0.0)\n'"
legacy/sign_clip_reward.py,0,"b'import numpy as np\n\nfrom gym import RewardWrapper\n\n\nclass SignClipReward(RewardWrapper):\n    r""""""""Bin reward to {-1, 0, +1} by its sign. """"""   \n    def reward(self, reward):\n        return np.sign(reward)\n\n\n@pytest.mark.parametrize(\'env_id\', [\'CartPole-v1\', \'Pendulum-v0\', \'MountainCar-v0\', \n                                    \'Pong-v0\', \'SpaceInvaders-v0\'])\ndef test_sign_clip_reward(env_id):\n    env = gym.make(env_id)\n    wrapped_env = SignClipReward(env)\n    \n    env.reset()\n    wrapped_env.reset()\n    \n    for _ in range(1000):\n        action = env.action_space.sample()\n        _, wrapped_reward, done, _ = wrapped_env.step(action)\n        assert wrapped_reward in [-1.0, 0.0, 1.0]\n        if done:\n            break\n'"
legacy/space_utils.py,0,"b""import numpy as np\n\nfrom gym.spaces import Box\nfrom gym.spaces import Discrete\nfrom gym.spaces import MultiDiscrete\nfrom gym.spaces import MultiBinary\nfrom gym.spaces import Tuple\nfrom gym.spaces import Dict\n\n\ndef flatdim(space):\n    if isinstance(space, Box):\n        return int(np.prod(space.shape))\n    elif isinstance(space, Discrete):\n        return int(space.n)\n    elif isinstance(space, Tuple):\n        return int(sum([flatdim(s) for s in space.spaces]))\n    elif isinstance(space, Dict):\n        return int(sum([flatdim(s) for s in space.spaces.values()]))\n    elif isinstance(space, MultiBinary):\n        return int(space.n)\n    elif isinstance(space, MultiDiscrete):\n        return int(np.prod(space.shape))\n    else:\n        raise NotImplementedError\n\n\ndef flatten(space, x):\n    if isinstance(space, Box):\n        return np.asarray(x, dtype=np.float32).flatten()\n    elif isinstance(space, Discrete):\n        onehot = np.zeros(space.n, dtype=np.float32)\n        onehot[x] = 1.0\n        return onehot\n    elif isinstance(space, Tuple):\n        return np.concatenate([flatten(s, x_part) for x_part, s in zip(x, space.spaces)])\n    elif isinstance(space, Dict):\n        return np.concatenate([flatten(space.spaces[key], item) for key, item in x.items()])\n    elif isinstance(space, MultiBinary):\n        return np.asarray(x).flatten()\n    elif isinstance(space, MultiDiscrete):\n        return np.asarray(x).flatten()\n    else:\n        raise NotImplementedError\n\n\ndef unflatten(space, x):\n    if isinstance(space, Box):\n        return np.asarray(x, dtype=np.float32).reshape(space.shape)\n    elif isinstance(space, Discrete):\n        return int(np.nonzero(x)[0][0])\n    elif isinstance(space, Tuple):\n        dims = [flatdim(s) for s in space.spaces]\n        list_flattened = np.split(x, np.cumsum(dims)[:-1])\n        list_unflattened = [unflatten(s, flattened) \n                            for flattened, s in zip(list_flattened, space.spaces)]\n        return tuple(list_unflattened)\n    elif isinstance(space, Dict):\n        dims = [flatdim(s) for s in space.spaces.values()]\n        list_flattened = np.split(x, np.cumsum(dims)[:-1])\n        list_unflattened = [(key, unflatten(s, flattened)) \n                            for flattened, (key, s) in zip(list_flattened, space.spaces.items())]\n        return dict(list_unflattened)\n    elif isinstance(space, MultiBinary):\n        return np.asarray(x).reshape(space.shape)\n    elif isinstance(space, MultiDiscrete):\n        return np.asarray(x).reshape(space.shape)\n    else:\n        raise NotImplementedError\n\n\ndef test_space_utils():\n    # Box\n    box = Box(-1.0, 1.0, shape=[2, 3], dtype=np.float32)\n    sample = box.sample()\n    assert flatdim(box) == 2*3\n    assert flatten(box, sample).shape == (2*3,)\n    assert np.allclose(sample, unflatten(box, flatten(box, sample)))\n\n    x = np.array([[1.0, 1.0], [1.0, 1.0]])\n    box = Box(low=-x, high=x, dtype=np.float32)\n    sample = box.sample()\n    assert flatdim(box) == 2*2\n    assert flatten(box, sample).shape == (2*2,)\n    assert np.allclose(sample, unflatten(box, flatten(box, sample)))\n\n    # Discrete\n    discrete = Discrete(5)\n    sample = discrete.sample()\n    assert flatdim(discrete) == 5\n    assert flatten(discrete, sample).shape == (5,)\n    assert sample == unflatten(discrete, flatten(discrete, sample))\n\n    # Tuple\n    S = Tuple([Discrete(5), \n               Box(-1.0, 1.0, shape=(2, 3), dtype=np.float32), \n               Dict({'success': Discrete(2), 'velocity': Box(-1, 1, shape=(1, 3), dtype=np.float32)})])\n    sample = S.sample()\n    assert flatdim(S) == 5+2*3+2+3\n    assert flatten(S, sample).shape == (16,)\n    _sample = unflatten(S, flatten(S, sample))\n    assert sample[0] == _sample[0]\n    assert np.allclose(sample[1], _sample[1])\n    assert sample[2]['success'] == _sample[2]['success']\n    assert np.allclose(sample[2]['velocity'], _sample[2]['velocity'])\n\n    # Dict\n    D0 = Dict({'position': Box(-100, 100, shape=(3,), dtype=np.float32), \n               'velocity': Box(-1, 1, shape=(4,), dtype=np.float32)})\n    D = Dict({'sensors': D0, 'score': Discrete(100)})\n    sample = D.sample()\n    assert flatdim(D) == 3+4+100\n    assert flatten(D, sample).shape == (107,)\n    _sample = unflatten(D, flatten(D, sample))\n    assert sample['score'] == _sample['score']\n    assert np.allclose(sample['sensors']['position'], _sample['sensors']['position'])\n    assert np.allclose(sample['sensors']['velocity'], _sample['sensors']['velocity'])\n"""
legacy/time_aware_observation.py,0,"b'import numpy as np\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\nfrom .utils import get_all_wrappers\n\n\nclass TimeAwareObservation(ObservationWrapper):\n    r""""""Augment the observation with current time step in the trajectory. \n    \n    .. note::\n        Currently it only works with one-dimensional observation space. It doesn\'t\n        support pixel observation space yet. \n    \n    """"""\n    def __init__(self, env):\n        assert self.__class__.__name__ not in get_all_wrappers(env), \'TimeAwareObservation cannot be wrapped twice\'\n        super().__init__(env)\n        low = np.append(self.observation_space.low, 0.0)\n        high = np.append(self.observation_space.high, np.inf)\n        self.observation_space = Box(low, high, dtype=np.float32)\n        \n    def observation(self, observation):\n        return np.append(observation, self.t)\n        \n    def step(self, action):\n        self.t += 1\n        return super().step(action)\n        \n    def reset(self, **kwargs):\n        self.t = 0\n        return super().reset(**kwargs)\n'"
legacy/time_limit.py,0,"b'import gym\n\nfrom .utils import get_all_wrappers\n\n\n# TODO: temporary, remove after it is officially ported to gym\n# So, no unit test in lagom\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        assert self.__class__.__name__ not in get_all_wrappers(env), \'TimeLimit cannot be wrapped twice\' \n        super().__init__(env)\n        if max_episode_steps is None:\n            max_episode_steps = env.spec.max_episode_steps\n        self.env.spec.max_episode_steps = max_episode_steps\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = None\n\n    def step(self, action):\n        assert self._elapsed_steps is not None, ""Cannot call env.step() before calling reset()""\n        observation, reward, done, info = self.env.step(action)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            if not done:\n                info[\'TimeLimit.truncated\'] = True\n            done = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n'"
legacy/trajectory.py,0,"b""import numpy as np\n\n\nclass Trajectory(object):\n    def __init__(self):\n        self.observations = []\n        self.actions = []\n        self.rewards = []\n        self.step_infos = []\n        \n    def __len__(self):\n        return len(self.step_infos)\n        \n    @property\n    def completed(self):\n        return len(self.step_infos) > 0 and self.step_infos[-1].last\n    \n    @property\n    def reach_time_limit(self):\n        return self.step_infos[-1].time_limit\n    \n    @property\n    def reach_terminal(self):\n        return self.step_infos[-1].terminal\n        \n    def add_observation(self, observation):\n        assert not self.completed\n        self.observations.append(observation)\n    \n    def add_action(self, action):\n        assert not self.completed\n        self.actions.append(action)\n    \n    def add_reward(self, reward):\n        assert not self.completed\n        self.rewards.append(reward)\n        \n    def add_step_info(self, step_info):\n        assert not self.completed\n        self.step_infos.append(step_info)\n        if step_info.last:\n            assert self.completed\n    \n    @property\n    def last_observation(self):\n        return self.observations[-1]\n    \n    @property\n    def numpy_observations(self):\n        return np.concatenate(self.observations, axis=0)\n    \n    @property\n    def numpy_actions(self):\n        return np.concatenate(self.actions, axis=0)\n        \n    @property\n    def numpy_rewards(self):\n        return np.asarray(self.rewards)\n    \n    @property\n    def numpy_dones(self):\n        return np.asarray([step_info.done for step_info in self.step_infos])\n    \n    @property\n    def numpy_masks(self):\n        return 1. - self.numpy_dones\n    \n    @property\n    def infos(self):\n        return [step_info.info for step_info in self.step_infos]\n    \n    def get_all_info(self, key):\n        return [step_info[key] for step_info in self.step_infos]\n    \n    def __repr__(self):\n        return f'Trajectory(T: {len(self)}, Completed: {self.completed}, Reach time limit: {self.reach_time_limit}, Reach terminal: {self.reach_terminal})'\n\n\n\n@pytest.mark.parametrize('init_seed', [0, 10])\n@pytest.mark.parametrize('T', [1, 5, 100])\ndef test_trajectory(init_seed, T):\n    make_env = lambda: TimeLimit(SanityEnv())\n    env = make_vec_env(make_env, 1, init_seed)  # single environment\n    env = VecStepInfo(env)\n    D = Trajectory()\n    assert len(D) == 0\n    assert not D.completed\n    \n    observation, _ = env.reset()\n    D.add_observation(observation)\n    for t in range(T):\n        action = [env.action_space.sample()]\n        next_observation, reward, [step_info] = env.step(action)\n        if step_info.last:\n            D.add_observation([step_info['last_observation']])\n        else:\n            D.add_observation(next_observation)\n        D.add_action(action)\n        D.add_reward(reward)\n        D.add_step_info(step_info)\n        observation = next_observation\n        if step_info.last:\n            with pytest.raises(AssertionError):\n                D.add_observation(observation)\n            break\n    assert len(D) > 0\n    assert len(D) <= T\n    assert len(D) + 1 == len(D.observations)\n    assert len(D) + 1 == len(D.numpy_observations)\n    assert len(D) == len(D.actions)\n    assert len(D) == len(D.numpy_actions)\n    assert len(D) == len(D.rewards)\n    assert len(D) == len(D.numpy_rewards)\n    assert len(D) == len(D.numpy_dones)\n    assert len(D) == len(D.numpy_masks)\n    assert np.allclose(np.logical_not(D.numpy_dones), D.numpy_masks)\n    assert len(D) == len(D.step_infos)\n    if len(D) < T:\n        assert step_info.last\n        assert D.completed\n        assert D.reach_terminal\n        assert not D.reach_time_limit\n        assert np.allclose(D.observations[-1], [step_info['last_observation']])\n    if not step_info.last:\n        assert not D.completed\n        assert not D.reach_terminal\n        assert not D.reach_time_limit\n"""
legacy/utils.py,0,"b'def get_wrapper(env, name):\n    r""""""Return a wrapped environment of a specific wrapper. \n    \n    .. note::\n        If no such wrapper found, then an ``None`` is returned. \n    \n    Args:\n        env (Env): environment\n        name (str): name of the wrapper\n        \n    Returns:\n        Env: wrapped environment\n    """"""\n    if name == env.__class__.__name__:\n        return env\n    elif env.unwrapped is env:  # reaching underlying environment\n        return None\n    else:\n        return get_wrapper(env.env, name)\n\n\ndef get_all_wrappers(env):\n    r""""""Returns a list of wrapper names of a wrapped environment. \n    \n    Args:\n        env (Env): wrapped environment\n    \n    Returns:\n        list: a list of string names of wrappers\n    """"""\n    out = []\n    while env is not env.unwrapped:\n        out.append(env.__class__.__name__)\n        env = env.env\n    return out\n'"
legacy/vec_env.py,0,"b'import numpy as np\n\nfrom lagom.vis import GridImage\n\ntry:  # workaround on server without fake screen but still running other things well\n    from lagom.vis import ImageViewer\nexcept ImportError:\n    pass\n\n\nclass VecEnv(object):\n    r""""""A vectorized environment runs serially for each sub-environment. \n    \n    Each observation returned from vectorized environment is a batch of observations \n    for each sub-environment. And :meth:`step` is expected to receive a batch of \n    actions for each sub-environment. \n    \n    .. note::\n    \n        All sub-environments should share the identical observation and action spaces.\n        In other words, a vector of multiple different environments is not supported. \n    \n    Args:\n        list_make_env (list): a list of functions each returns an instantiated enviroment. \n        observation_space (Space): observation space of the environment\n        action_space (Space): action space of the environment\n        \n    """"""\n    metadata = {\'render.modes\': [\'human\', \'rgb_array\']}\n    closed = False\n    viewer = None\n\n    def __init__(self, list_make_env):\n        self.list_make_env = list_make_env\n        self.list_env = [make_env() for make_env in list_make_env]\n        self.observation_space = self.list_env[0].observation_space\n        self.action_space = self.list_env[0].action_space\n        self.reward_range = self.list_env[0].reward_range\n        self.spec = self.list_env[0].spec\n\n    def step(self, actions):\n        r""""""Ask all the environments to take a step with a list of actions, each for one environment. \n        \n        Args:\n            actions (list): a list of actions, each for one environment. \n            \n        Returns:\n            tuple: a tuple of (observations, rewards, dones, infos)\n                * observations (list): a list of observations, each returned from one environment \n                  after executing the given action. \n                * rewards (list): a list of scalar rewards, each returned from one environment. \n                * dones (list): a list of booleans indicating whether the episode terminates, each \n                  returned from one environment. \n                * infos (list): a list of dictionaries of additional informations, each returned \n                  from one environment. \n        """"""\n        assert len(actions) == len(self)\n        observations = []\n        rewards = []\n        dones = []\n        infos = []\n        for i, (env, action) in enumerate(zip(self.list_env, actions)):\n            observation, reward, done, info = env.step(action)\n            # If done=True, reset environment, store last observation in info and report new initial observation\n            if done:\n                info[\'last_observation\'] = observation\n                observation = env.reset()\n            observations.append(observation)\n            rewards.append(reward)\n            dones.append(done)\n            infos.append(info)\n        return observations, rewards, dones, infos\n    \n    def reset(self):\n        r""""""Reset all the environments and return a list of initial observations from each environment. \n        \n        .. warning::\n        \n            If :meth:`step_async` is still working, then it will be aborted. \n        \n        Returns:\n            list: a list of initial observations from all environments. \n        """"""\n        observations = [env.reset() for env in self.list_env]\n        return observations\n    \n    def render(self, mode=\'human\'):\n        r""""""Render all the environments. \n        \n        It firstly retrieve RGB images from all environments and use :class:`GridImage`\n        to make a grid of them as a single image. Then it either returns the image array\n        or display the image to the screen by using :class:`ImageViewer`. \n        \n        See docstring in :class:`Env` for more detais about rendering. \n        """"""\n        # Get images from all environments with shape [N, H, W, C]\n        imgs = self.get_images()\n        imgs = np.stack(imgs)\n        # Make a grid of images\n        grid = GridImage(ncol=5, padding=5, pad_value=0)\n        imgs = imgs.transpose(0, 3, 1, 2)  # to shape [N, C, H, W]\n        grid.add(imgs)\n        gridimg = np.asarray(grid())\n        gridimg = gridimg.transpose(0, 2, 3, 1)  # back to shape [N, H, W, C]\n        \n        # render the grid of image\n        if mode == \'human\':\n            self.get_viewer()(gridimg)\n        elif mode == \'rgb_array\':\n            return gridimg\n        else:\n            raise ValueError(f\'expected human or rgb_array, got {mode}\')\n\n    def get_images(self):\n        r""""""Returns a batched RGB array with shape [N, H, W, C] from all environments. \n        \n        Returns:\n            ndarray: a batched RGB array with shape [N, H, W, C]    \n        """"""\n        return [env.render(mode=\'rgb_array\') for env in self.list_env]\n    \n    def get_viewer(self):\n        r""""""Returns an instantiated :class:`ImageViewer`. \n        \n        Returns:\n            ImageViewer: an image viewer\n        """"""\n        if self.viewer is None:  # create viewer is not existed\n            self.viewer = ImageViewer(max_width=500)  # set a max width here\n        return self.viewer\n    \n    def close_extras(self):\n        r""""""Clean up the extra resources e.g. beyond what\'s in this base class. """"""\n        return [env.close() for env in self.list_env]\n    \n    def close(self):\n        r""""""Close all environments. \n        \n        It closes all the existing image viewers, then calls :meth:`close_extras` and set\n        :attr:`closed` as ``True``. \n        \n        .. warning::\n        \n            This function itself does not close the environments, it should be handled\n            in :meth:`close_extras`. This is useful for parallelized environments. \n        \n        .. note::\n        \n            This will be automatically called when garbage collected or program exited. \n            \n        """"""\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n    \n    @property\n    def unwrapped(self):\n        r""""""Unwrap this vectorized environment. \n        \n        Useful for sequential wrappers applied, it can access information from the original \n        vectorized environment. \n        """"""\n        return self\n    \n    def __len__(self):\n        return len(self.list_make_env)\n    \n    def __getitem__(self, index):\n        return self.list_env[index]\n    \n    def __setitem__(self, index, x):\n        self.list_env[index] = x\n    \n    def __repr__(self):\n        return f\'<{self.__class__.__name__}: {len(self)}, {self.spec.id}>\'\n    \n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n        # propagate exception\n        return False \n\n    \nclass VecEnvWrapper(VecEnv):\n    r""""""Wraps the vectorized environment to allow a modular transformation. \n    \n    This class is the base class for all wrappers for vectorized environments. The subclass\n    could override some methods to change the behavior of the original vectorized environment\n    without touching the original code. \n    \n    .. note::\n    \n        Don\'t forget to call ``super().__init__(env)`` if the subclass overrides :meth:`__init__`.\n    \n    """"""\n    def __init__(self, env):\n        assert isinstance(env, VecEnv)\n        self.env = env\n        self.metadata = env.metadata\n        \n        self.list_make_env = env.list_make_env\n        self.list_env = env.list_env\n        self.observation_space = env.observation_space\n        self.action_space = env.action_space\n        self.reward_range = env.reward_range\n        self.spec = env.spec\n        \n    def step(self, actions):\n        return self.env.step(actions)\n    \n    def reset(self):\n        return self.env.reset()\n    \n    def get_images(self):\n        return self.env.get_images()\n    \n    def close_extras(self):\n        return self.env.close_extras()\n    \n    @property\n    def unwrapped(self):\n        return self.env.unwrapped\n    \n    def __len__(self):\n        return len(self.env)\n    \n    def __getitem__(self, index):\n        return self.env[index]\n    \n    def __setitem__(self, index, x):\n        self.env[index] = x\n    \n    def __repr__(self):\n        return f\'<{self.__class__.__name__}, {self.env}>\'\n\n    \n    \n    \n    \n    \n    \n@pytest.mark.parametrize(\'env_id\', [\'CartPole-v0\', \'Pendulum-v0\'])\n@pytest.mark.parametrize(\'num_env\', [1, 3, 5])\ndef test_vec_env(env_id, num_env):\n    def make_env():\n        return gym.make(env_id)\n    base_env = make_env()\n    list_make_env = [make_env for _ in range(num_env)]\n    env = VecEnv(list_make_env)\n    assert isinstance(env, VecEnv)\n    assert len(env) == num_env\n    assert len(list(env)) == num_env\n    assert env.observation_space == base_env.observation_space\n    assert env.action_space == base_env.action_space\n    assert env.reward_range == base_env.reward_range\n    assert env.spec.id == base_env.spec.id\n    obs = env.reset()\n    assert isinstance(obs, list) and len(obs) == num_env\n    assert all([x in env.observation_space for x in obs])\n    actions = [env.action_space.sample() for _ in range(num_env)]\n    observations, rewards, dones, infos = env.step(actions)\n    assert isinstance(observations, list) and len(observations) == num_env\n    assert isinstance(rewards, list) and len(rewards) == num_env\n    assert isinstance(dones, list) and len(dones) == num_env\n    assert isinstance(infos, list) and len(infos) == num_env\n    env.close()\n    assert env.closed'"
legacy/vec_monitor.py,0,"b'from time import perf_counter\nfrom collections import deque\n\nimport numpy as np\n\nfrom lagom.envs import VecEnvWrapper\n\n\nclass VecMonitor(VecEnvWrapper):\n    r""""""Record episode reward, horizon and time and report it when an episode terminates. """"""\n    def __init__(self, env, deque_size=100):\n        super().__init__(env)\n        \n        self.t0 = perf_counter()\n        self.episode_rewards = np.zeros(len(env), dtype=np.float32)\n        self.episode_horizons = np.zeros(len(env), dtype=np.int32)\n        \n        self.return_queue = deque(maxlen=deque_size)\n        self.horizon_queue = deque(maxlen=deque_size)\n        \n    def step(self, actions):\n        observations, rewards, dones, infos = self.env.step(actions)\n        \n        self.episode_rewards += rewards\n        self.episode_horizons += 1\n        for i, done in enumerate(dones):\n            if done:\n                infos[i][\'episode\'] = {\'return\': self.episode_rewards[i], \n                                       \'horizon\': self.episode_horizons[i],\n                                       \'time\': round(perf_counter() - self.t0, 4)}\n                self.return_queue.append(self.episode_rewards[i])\n                self.horizon_queue.append(self.episode_horizons[i])\n                \n                self.episode_rewards[i] = 0.0\n                self.episode_horizons[i] = 0\n        \n        return observations, rewards, dones, infos\n        \n    def reset(self):\n        observations = self.env.reset()\n        \n        self.episode_rewards.fill(0.0)\n        self.episode_horizons.fill(0)\n        \n        return observations\n'"
legacy/vec_standardize_observation.py,0,"b'import numpy as np\n\nfrom lagom.transform import RunningMeanVar\nfrom lagom.envs import VecEnvWrapper\n\n\nclass VecStandardizeObservation(VecEnvWrapper):\n    r""""""Standardizes the observations by running estimation of mean and variance. \n    \n    .. warning::\n    \n        To evaluate the agent trained on standardized observations, remember to\n        save and load observation scalings, otherwise, the performance will be incorrect. \n    \n    Args:\n        env (VecEnv): a vectorized environment\n        clip (float): clipping range of standardized observation, i.e. [-clip, clip]\n        constant_moments (tuple): a tuple of constant mean and variance to standardize observation.\n            Note that if it is provided, then running average will be ignored.\n    \n    """"""\n    def __init__(self, env, clip=10., constant_moments=None):\n        super().__init__(env)\n        self.clip = clip\n        self.constant_moments = constant_moments\n        \n        self.eps = 1e-8\n        \n        if constant_moments is None:\n            self.online = True\n            self.running_moments = RunningMeanVar(shape=env.observation_space.shape)\n        else:\n            self.online = False\n            self.constant_mean, self.constant_var = constant_moments    \n        \n    def step(self, actions):\n        observations, rewards, dones, infos = self.env.step(actions)\n        for i, info in enumerate(infos):  # standardize last_observation\n            if \'last_observation\' in info:\n                infos[i][\'last_observation\'] = self.process_obs([info[\'last_observation\']]).squeeze(0)\n        return self.process_obs(observations), rewards, dones, infos\n    \n    def reset(self):\n        observations = self.env.reset()\n        return self.process_obs(observations)  # initial observation has very small magnitude\n    \n    def process_obs(self, observations):\n        if self.online:\n            self.running_moments(observations)\n            mean = self.running_moments.mean\n            std = np.sqrt(self.running_moments.var + self.eps)\n            observations = (observations - mean)/std\n        else:\n            mean = self.constant_mean\n            std = np.sqrt(self.constant_var + self.eps)\n            observations = (observations - mean)/std\n        observations = np.clip(observations, -self.clip, self.clip)\n        return observations\n    \n    @property\n    def mean(self):\n        return self.running_moments.mean\n    \n    @property\n    def var(self):\n        return self.running_moments.var\n'"
legacy/vec_standardize_reward.py,0,"b'import numpy as np\n\nfrom lagom.transform import RunningMeanVar\nfrom lagom.envs import VecEnvWrapper\n\n\nclass VecStandardizeReward(VecEnvWrapper):\n    r""""""Standardize the reward by running estimation of variance.\n    \n    .. warning::\n    \n        We do not subtract running mean from reward but only divides it by running\n        standard deviation. Because subtraction by mean will alter the reward shape\n        so this might degrade the performance. Note that we perform this transformation \n        from the second incoming reward while keeping first reward unchanged, otherwise \n        it\'ll have too large magnitude (then just being clipped) due to the fact that\n        we do not subtract it from mean. \n        \n    .. note::\n    \n        Each :meth:`reset`, we do not clean up the ``self.all_returns`` buffer. Because\n        of discount factor (:math:`< 1`), the running averages will converge after some iterations. \n        Therefore, we do not allow discounted factor as :math:`1.0` since it will lead to\n        unbounded explosion of reward running averages. \n        \n    Args:\n        env (VecEnv): a vectorized environment\n        clip (float): clipping range of standardized reward, i.e. [-clip, clip]\n        gamma (float): discounted factor. Note that the value 1.0 should not be used. \n        constant_var (ndarray): Constant variance to standardize reward. Note that\n            when it is provided, then running average will be ignored. \n    \n    """"""\n    def __init__(self, env, clip=10., gamma=0.99, constant_var=None):\n        super().__init__(env)\n        self.clip = clip\n        assert gamma > 0.0 and gamma < 1.0, \'we do not allow discounted factor as 1.0. See docstring for details. \'\n        self.gamma = gamma\n        self.constant_var = constant_var\n        \n        self.eps = 1e-8\n        \n        if constant_var is None:\n            self.online = True\n            self.running_moments = RunningMeanVar(shape=())\n        else:\n            self.online = False\n        \n        # Buffer to save discounted returns from each environment\n        self.all_returns = np.zeros(len(env), dtype=np.float64)\n        \n    def step(self, actions):\n        observations, rewards, dones, infos = self.env.step(actions)\n        rewards = self.process_reward(rewards)\n        # Set discounted return buffer as zero for those episodes which terminate\n        self.all_returns[dones] = 0.0\n        return observations, rewards, dones, infos\n    \n    def reset(self):\n        # Reset returns buffer, because all environments are also reset\n        self.all_returns.fill(0.0)\n        return super().reset()\n    \n    def process_reward(self, rewards):\n        # Do NOT subtract from mean, but only divided by std\n        if self.online:\n            self.all_returns = rewards + self.gamma*self.all_returns\n            self.running_moments(self.all_returns)\n            # first reward unchanged, otherwise too large magnitude due to no mean subtraction\n            if self.running_moments.n >= 2:\n                std = np.sqrt(self.running_moments.var + self.eps)\n                rewards = rewards/std\n        else:\n            std = np.sqrt(self.constant_var + self.eps)\n            rewards = rewards/std\n        rewards = np.clip(rewards, -self.clip, self.clip)\n        return rewards\n        \n    @property\n    def var(self):\n        return self.running_moments.var\n'"
legacy/vec_step_info.py,0,"b'from dataclasses import dataclass\nfrom lagom.envs import VecEnvWrapper\n\n\n@dataclass\nclass StepInfo:\n    r""""""Defines a set of information for each time step. \n    \n    A `StepInfo` is returned from each `step` and `reset` of an environment. \n    It contains properties of the transition and additional information. \n    \n    """"""\n    done: bool\n    info: dict\n    \n    def __getitem__(self, key):\n        return self.info[key]\n    \n    @property\n    def first(self):\n        return self.info.get(\'FIRST\', False)\n    \n    @property\n    def mid(self):\n        return not self.first and not self.done\n        \n    @property\n    def last(self):\n        return self.done\n        \n    @property\n    def time_limit(self):\n        return self.info.get(\'TimeLimit.truncated\', False)\n        \n    @property\n    def terminal(self):\n        return self.done and not self.time_limit\n\n\nclass VecStepInfo(VecEnvWrapper):\n    def step(self, actions):\n        observations, rewards, dones, infos = self.env.step(actions)\n        step_infos = [StepInfo(done, info) for done, info in zip(dones, infos)]\n        return observations, rewards, step_infos\n        \n    def reset(self):\n        observations = self.env.reset()\n        step_infos = [StepInfo(False, {\'FIRST\': True}) for _ in range(len(observations))]\n        return observations, step_infos\n'"
test/__init__.py,0,b''
test/sanity_env.py,0,"b""import numpy as np\n\nimport gym\nfrom gym.spaces import Box\n\n\nclass SanityEnv(gym.Env):\n    def __init__(self):\n        self.T = np.random.randint(4, 6+1)\n        \n        self.reward_range = (0.1, float('inf'))\n        self.observation_space = Box(10, 60, shape=(), dtype=np.float32)\n        self.action_space = Box(1, 6, shape=(1,), dtype=np.float32)\n        self.spec = gym.envs.registration.EnvSpec('Sanity-v0', max_episode_steps=self.T)\n        \n    def step(self, action):\n        self.t += 1\n        self.s += 10.0\n        self.r += 0.1\n        done = self.t == self.T\n        \n        return self.s, self.r, done, {}\n        \n    def reset(self):\n        self.s = 0.0\n        self.r = 0.0\n        self.t = 0\n        \n        return self.s\n        \n    def seed(self, seed):\n        return [seed]\n        \n    def render(self, mode='human'):\n        pass\n        \n    def close(self):\n        pass\n"""
test/test_envs.py,0,"b""import pytest\nimport numpy as np\n\nimport gym\nfrom lagom.envs import RecordEpisodeStatistics\nfrom lagom.envs import NormalizeObservation\nfrom lagom.envs import NormalizeReward\nfrom lagom.envs import TimeStepEnv\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v0', 'Pendulum-v0'])\n@pytest.mark.parametrize('deque_size', [2, 5])\ndef test_record_episode_statistics(env_id, deque_size):\n    env = gym.make(env_id)\n    env = RecordEpisodeStatistics(env, deque_size)\n\n    for n in range(5):\n        env.reset()\n        assert env.episode_return == 0.0\n        assert env.episode_horizon == 0\n        for t in range(env.spec.max_episode_steps):\n            _, _, done, info = env.step(env.action_space.sample())\n            if done:\n                assert 'episode' in info\n                assert all([item in info['episode'] for item in ['return', 'horizon', 'time']])\n                break\n    assert len(env.return_queue) == deque_size\n    assert len(env.horizon_queue) == deque_size\n    \n    \n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_normalize_observation(env_id):\n    env = gym.make(env_id)\n    wrapped_env = NormalizeObservation(gym.make(env_id))\n    unbiased = []\n\n    env.seed(0)\n    wrapped_env.seed(0)\n\n    obs = env.reset()\n    wrapped_obs = wrapped_env.reset()\n    unbiased.append(obs)\n\n    for t in range(env.spec.max_episode_steps):\n        action = env.action_space.sample()\n        obs, _, done, _ = env.step(action)\n        wrapped_obs, _, wrapped_done, _ = wrapped_env.step(action)\n        unbiased.append(obs)\n\n        mean = np.mean(unbiased, 0)\n        var = np.var(unbiased, 0)\n        assert np.allclose(wrapped_env.obs_moments.mean, mean, atol=1e-5)\n        assert np.allclose(wrapped_env.obs_moments.var, var, atol=1e-4)\n\n        assert done == wrapped_done\n        if done:\n            break\n            \n            \n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\n@pytest.mark.parametrize('gamma', [0.5, 0.99])\ndef test_normalize_reward(env_id, gamma):\n    env = gym.make(env_id)\n    wrapped_env = NormalizeReward(gym.make(env_id), gamma=gamma)\n    unbiased = []\n\n    env.seed(0)\n    wrapped_env.seed(0)\n\n    for n in range(10):\n        env.reset()\n        wrapped_env.reset()\n        G = 0.0\n        for t in range(env.spec.max_episode_steps):\n            action = env.action_space.sample()\n            _, reward, done, _ = env.step(action)\n            _, wrapped_reward, wrapped_done, _ = wrapped_env.step(action)\n            assert done == wrapped_done\n\n            G = reward + gamma*G\n            unbiased.append(G)\n\n            if done:\n                break\n\n            mean = np.mean(unbiased, 0)\n            var = np.var(unbiased, 0)\n            assert wrapped_env.all_returns == G\n\n            assert np.allclose(wrapped_env.reward_moments.mean, mean, atol=1e-4)\n            assert np.allclose(wrapped_env.reward_moments.var, var, atol=1e-3)\n            \n            \n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_timestep_env(env_id):\n    env = gym.make(env_id)\n    wrapped_env = TimeStepEnv(gym.make(env_id))\n\n    env.seed(0)\n    wrapped_env.seed(0)\n\n    obs = env.reset()\n    timestep = wrapped_env.reset()\n    assert timestep.first()\n    assert np.allclose(timestep.observation, obs)\n\n    for t in range(env.spec.max_episode_steps):\n        action = env.action_space.sample()\n        obs, reward, done, info = env.step(action)\n        timestep = wrapped_env.step(action)\n        assert np.allclose(timestep.observation, obs)\n        assert timestep.reward == reward\n        assert timestep.done == done\n        assert timestep.info == info\n        if done:\n            assert timestep.last()\n            if 'TimeLimit.truncated' in info and info['TimeLimit.truncated']:\n                assert timestep.time_limit()\n            else:\n                assert timestep.terminal()\n            break\n        else:\n            assert timestep.mid()\n"""
test/test_experiment.py,0,"b""from pathlib import Path\nfrom shutil import rmtree\n\nimport numpy as np\n\nimport pytest\n\nfrom lagom.experiment import Grid\nfrom lagom.experiment import Sample\nfrom lagom.experiment import Condition\nfrom lagom.experiment import Config\nfrom lagom.experiment import run_experiment\n\n\n@pytest.mark.parametrize('values', [[1, 2, 3], ['MLP', 'LSTM']])\ndef test_grid(values):\n    grid = Grid(values)\n    assert isinstance(grid, list)\n    assert len(grid) == len(values)\n    assert all([grid[i] == value for i, value in enumerate(values)])\n    \n\ndef test_sample():\n    sampler = Sample(lambda: 5)\n    assert all([sampler() == 5 for _ in range(10)])\n    del sampler\n    \n    sampler = Sample(lambda: np.random.uniform(3, 7))\n    for _ in range(100):\n        x = sampler()\n        assert x >= 3 and x < 7\n        \n        \ndef test_condition():\n    condition = Condition(lambda x: 10 if x['one'] == 'ten' else -1)\n    assert condition({'one': 'ten'}) == 10\n    assert condition({'one': 10}) == -1\n\n\n@pytest.mark.parametrize('num_sample', [1, 5, 10])\n@pytest.mark.parametrize('keep_dict_order', [True, False])\ndef test_config(num_sample, keep_dict_order):\n    with pytest.raises(AssertionError):\n        Config([1, 2, 3])\n        \n    config = Config({'log.dir': 'some path',\n                     'beta': Condition(lambda x: 'small' if x['alpha'] < 5 else 'large'),\n                     'network.type': 'MLP', \n                     'network.hidden_size': [64, 64],\n                     'network.lr': Grid([1e-3, 1e-4]), \n                     'env.id': 'HalfCheetah-v2', \n                     'iter': Grid([10, 20, 30]),\n                     'alpha': Sample(lambda: np.random.uniform(3, 10))}, \n                    num_sample=num_sample, \n                    keep_dict_order=keep_dict_order)\n    list_config = config.make_configs()\n    \n    assert len(list_config) == 2*3*num_sample\n    for ID in range(2*3*num_sample):\n        assert list_config[ID]['ID'] == ID\n        \n    for x in list_config:\n        assert len(x.keys()) == len(config.items.keys()) + 1  # added one more 'ID'\n        for key in config.items.keys():\n            assert key in x\n        assert x['log.dir'] == 'some path'\n        if x['alpha'] < 5:\n            assert x['beta'] == 'small'\n        else:\n            assert x['beta'] == 'large'\n        assert x['network.type'] == 'MLP'\n        assert x['network.hidden_size'] == [64, 64]\n        assert x['network.lr'] in [1e-3, 1e-4]\n        assert x['env.id'] == 'HalfCheetah-v2'\n        assert x['iter'] in [10, 20, 30]\n        assert x['alpha'] >= 3 and x['alpha'] < 10\n        \n        if keep_dict_order:\n            assert list(x.keys()) == ['ID'] + list(config.items.keys())\n        else:\n            assert list(x.keys()) != ['ID'] + list(config.items.keys())\n            assert list(x.keys()) == ['ID', 'log.dir', 'beta', 'network.type', 'network.hidden_size', \n                                      'env.id', 'network.lr', 'iter', 'alpha']\n    \n    # test for non-random sampling\n    config = Config({'log.dir': 'some path',\n                     'network.type': 'MLP', \n                     'network.hidden_size': [64, 64],\n                     'network.lr': Grid([1e-3, 1e-4]), \n                     'env.id': 'HalfCheetah-v2', \n                     'iter': Grid([10, 20, 30]),\n                     'alpha': 0.1}, \n                    num_sample=num_sample, \n                    keep_dict_order=keep_dict_order)\n    list_config = config.make_configs()\n    assert len(list_config) == 2*3*1  # no matter how many num_sample, without repetition\n    for ID in range(2*3*1):\n        assert list_config[ID]['ID'] == ID\n        \n    # test for all fixed\n    config = Config({'log.dir': 'some path',\n                     'network.type': 'MLP', \n                     'network.hidden_size': [64, 64],\n                     'network.lr': 1e-3, \n                     'env.id': 'HalfCheetah-v2', \n                     'iter': 20,\n                     'alpha': 0.1}, \n                    num_sample=num_sample, \n                    keep_dict_order=keep_dict_order)\n    list_config = config.make_configs()\n    assert len(list_config) == 1  # no matter how many num_sample, without repetition\n    for ID in range(1):\n        assert list_config[ID]['ID'] == ID\n\n\n@pytest.mark.parametrize('num_sample', [1, 5])\n@pytest.mark.parametrize('max_workers', [None, 1, 5, 100])\n@pytest.mark.parametrize('chunksize', [1, 7, 40])\ndef test_run_experiment(num_sample, max_workers, chunksize):\n    def run(config, seed, device, logdir):\n        return config['ID'], seed, device, logdir\n    \n    config = Config({'network.lr': Grid([1e-3, 5e-3]), \n                     'network.size': [32, 16],\n                     'env.id': Grid(['CartPole-v1', 'Ant-v2'])}, \n                    num_sample=num_sample, \n                    keep_dict_order=True)\n    seeds = [1, 2, 3]\n    log_dir = './some_path'\n    run_experiment(run, config, seeds, log_dir, max_workers, chunksize, use_gpu=False, gpu_ids=None)\n \n    p = Path('./some_path')\n    assert p.exists()\n    assert (p / 'configs.pkl').exists()\n    assert (p / 'source_files').exists() and (p / 'source_files').is_dir()\n    # Check all configuration folders with their IDs and subfolders for all random seeds\n    for i in range(4):\n        config_p = p / str(i)\n        assert config_p.exists()\n        assert (config_p / 'config.yml').exists()\n        for seed in seeds:\n            assert (config_p / str(seed)).exists()\n    # Clean the logging directory\n    rmtree(p)\n    # Test remove\n    assert not p.exists()\n"""
test/test_lagom.py,0,"b""from pathlib import Path\nimport pytest\nimport numpy as np\n\nimport gym\n\nfrom lagom import Logger\nfrom lagom import RandomAgent\nfrom lagom import StepType\nfrom lagom import TimeStep\nfrom lagom import Trajectory\nfrom lagom import EpisodeRunner\nfrom lagom import StepRunner\nfrom lagom.envs import TimeStepEnv\nfrom lagom.utils import pickle_load\n\n\ndef test_logger():\n    logger = Logger()\n\n    logger('iteration', 1)\n    logger('learning_rate', 1e-3)\n    logger('train_loss', 0.12)\n    logger('eval_loss', 0.14)\n\n    logger('iteration', 2)\n    logger('learning_rate', 5e-4)\n    logger('train_loss', 0.11)\n    logger('eval_loss', 0.13)\n\n    logger('iteration', 3)\n    logger('learning_rate', 1e-4)\n    logger('train_loss', 0.09)\n    logger('eval_loss', 0.10)\n\n    def check(logs):\n        assert len(logs) == 4\n        assert list(logs.keys()) == ['iteration', 'learning_rate', 'train_loss', 'eval_loss']\n        assert logs['iteration'] == [1, 2, 3]\n        assert np.allclose(logs['learning_rate'], [1e-3, 5e-4, 1e-4])\n        assert np.allclose(logs['train_loss'], [0.12, 0.11, 0.09])\n        assert np.allclose(logs['eval_loss'], [0.14, 0.13, 0.10])\n\n    check(logger.logs)\n\n    logger.dump()\n    logger.dump(border='-'*50)\n    logger.dump(keys=['iteration'])\n    logger.dump(keys=['iteration', 'train_loss'])\n    logger.dump(index=0)\n    logger.dump(index=[1, 2])\n    logger.dump(index=0)\n    logger.dump(keys=['iteration', 'eval_loss'], index=1)\n    logger.dump(keys=['iteration', 'learning_rate'], indent=1)\n    logger.dump(keys=['iteration', 'train_loss'], index=[0, 2], indent=1, border='#'*50)\n\n    f = Path('./logger_file')\n    logger.save(f)\n    f = f.with_suffix('.pkl')\n    assert f.exists()\n\n    logs = pickle_load(f)\n    check(logs)\n\n    f.unlink()\n    assert not f.exists()\n\n    logger.clear()\n    assert len(logger.logs) == 0\n\n    \n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\n@pytest.mark.parametrize('num_envs', [1, 5])\ndef test_random_agent(env_id, num_envs):\n    # vanilla environment\n    env = gym.make(env_id)\n    agent = RandomAgent(None, env, 'cpu')\n    out = agent.choose_action(env.reset())\n    assert isinstance(out, dict)\n    assert out['raw_action'] in env.action_space\n    del env, agent, out\n    \n    # vectorized environment\n    env = gym.vector.make(env_id, num_envs)\n    agent = RandomAgent(None, env, 'cpu')\n    out = agent.choose_action(env.reset())\n    assert isinstance(out, dict)\n    assert isinstance(out['raw_action'], list)\n    assert len(out['raw_action']) == env.num_envs\n    assert all([action in env.action_space for action in out['raw_action']])\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_timestep(env_id):\n    env = gym.make(env_id)\n    observation = env.reset()\n    timestep = TimeStep(StepType.FIRST, observation=observation, reward=None, done=None, info=None)\n    assert timestep.first()\n    assert not timestep.mid() and not timestep.last()\n    assert not timestep.time_limit() and not timestep.terminal()\n\n    for t in range(env.spec.max_episode_steps):\n        observation, reward, done, info = env.step(env.action_space.sample())\n        if done:\n            timestep = TimeStep(StepType.LAST, observation=observation, reward=reward, done=done, info=info)\n            assert timestep.last()\n            assert not timestep.first() and not timestep.mid()\n            if 'TimeLimit.truncated' in info:\n                assert timestep.time_limit() and not timestep.terminal()\n            else:\n                assert timestep.terminal() and not timestep.time_limit()\n            break\n        else:\n            timestep = TimeStep(StepType.MID, observation=observation, reward=reward, done=done, info=info)\n            assert timestep.mid()\n            assert not timestep.first() and not timestep.last()\n            assert not timestep.time_limit() and not timestep.terminal()\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_trajectory(env_id):\n    env = gym.make(env_id)\n    env = TimeStepEnv(env)\n    traj = Trajectory()\n    assert len(traj) == 0 and traj.T == 0\n\n    with pytest.raises(AssertionError):\n        traj.add(TimeStep(StepType.MID, 1, 2, True, {}), 0.5)\n\n    timestep = env.reset()\n    traj.add(timestep, None)\n    assert len(traj) == 1 and traj.T == 0\n\n    with pytest.raises(AssertionError):\n        traj.add(TimeStep(StepType.MID, 1, 2, True, {}), None)\n\n    while not timestep.last():\n        action = env.action_space.sample()\n        timestep = env.step(action)\n        traj.add(timestep, action)\n        if not timestep.last():\n            assert len(traj) == traj.T + 1\n            assert not traj.finished\n    with pytest.raises(AssertionError):\n        traj.add(timestep, 5.3)\n    \n    traj.extra_info = {'last_V': 2.4, 'last_action': 1.3}\n    traj.extra_info = {**traj.extra_info, 'last_Q': 5.4}\n    assert 'last_V' in traj.extra_info and traj.extra_info['last_V'] == 2.4\n    assert 'last_action' in traj.extra_info and traj.extra_info['last_action'] == 1.3\n    assert 'last_Q' in traj.extra_info and traj.extra_info['last_Q'] == 5.4\n    \n    assert traj.finished\n    assert traj.reach_time_limit == traj[-1].time_limit()\n    assert traj.reach_terminal == traj[-1].terminal()\n    assert np.asarray(traj.observations).shape == (traj.T + 1, *env.observation_space.shape)\n    assert len(traj.actions) == traj.T\n    assert len(traj.rewards) == traj.T\n    assert len(traj.dones) == traj.T\n    assert len(traj.infos) == traj.T\n    if traj.reach_time_limit:\n        assert len(traj.get_infos('TimeLimit.truncated')) == 1\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\n@pytest.mark.parametrize('N', [1, 5, 10])\ndef test_episode_runner(env_id, N):\n    env = gym.make(env_id)\n    env = TimeStepEnv(env)\n    agent = RandomAgent(None, env, None)\n    runner = EpisodeRunner()\n    D = runner(agent, env, N)\n    assert len(D) == N\n    assert all([isinstance(d, Trajectory) for d in D])\n    assert all([traj.finished for traj in D])\n    assert all([traj[0].first() for traj in D])\n    assert all([traj[-1].last() for traj in D])\n    for traj in D:\n        for timestep in traj[1:-1]:\n            assert timestep.mid()\n    assert all(['last_info' in traj.extra_info and 'raw_action' in traj.extra_info['last_info'] for traj in D])\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\n@pytest.mark.parametrize('T', [1, 100, 500])\ndef test_step_runner(env_id, T):\n    env = gym.make(env_id)\n    env = TimeStepEnv(env)\n    agent = RandomAgent(None, env, None)\n\n    runner = StepRunner(reset_on_call=True)\n    D = runner(agent, env, T)\n    assert runner.observation is None\n    assert all([isinstance(traj, Trajectory) for traj in D])\n    assert all([traj[0].first() for traj in D])\n    assert all([traj[-1].last() for traj in D[:-1]])\n    assert all(['last_info' in traj.extra_info and 'raw_action' in traj.extra_info['last_info'] for traj in D])\n\n    runner = StepRunner(reset_on_call=False)\n    D = runner(agent, env, 1)\n    assert D[0][0].first()\n    assert len(D[0]) == 2\n    assert np.allclose(D[0][-1].observation, runner.observation)\n    assert all(['last_info' in traj.extra_info and 'raw_action' in traj.extra_info['last_info'] for traj in D])\n    D2 = runner(agent, env, 3)\n    assert np.allclose(D2[-1][-1].observation, runner.observation)\n    assert np.allclose(D[0][-1].observation, D2[0][0].observation)\n    assert D2[0][0].first() and D2[0][0].reward is None\n    assert all(['last_info' in traj.extra_info and 'raw_action' in traj.extra_info['last_info'] for traj in D2])\n"""
test/test_metric.py,22,"b""import pytest\nimport numpy as np\nimport torch\n\nimport gym\nfrom gym.wrappers import TimeLimit\n\nfrom lagom import RandomAgent\nfrom lagom import EpisodeRunner\nfrom lagom import StepRunner\nfrom lagom.utils import numpify\n\nfrom lagom.metric import returns\nfrom lagom.metric import bootstrapped_returns\nfrom lagom.metric import td0_target\nfrom lagom.metric import td0_error\nfrom lagom.metric import gae\nfrom lagom.metric import vtrace\n\nfrom .sanity_env import SanityEnv\n\n\n@pytest.mark.parametrize('gamma', [0.1, 0.99, 1.0])\ndef test_returns(gamma):\n    assert np.allclose(returns(1.0, [1, 2, 3]), [6, 5, 3])\n    assert np.allclose(returns(0.1, [1, 2, 3]), [1.23, 2.3, 3])\n    assert np.allclose(returns(1.0, [1, 2, 3, 4, 5]), [15, 14, 12, 9, 5])\n    assert np.allclose(returns(0.1, [1, 2, 3, 4, 5]), [1.2345, 2.345, 3.45, 4.5, 5])\n    assert np.allclose(returns(1.0, [1, 2, 3, 4, 5, 6, 7, 8]), [36, 35, 33, 30, 26, 21, 15, 8])\n    assert np.allclose(returns(0.1, [1, 2, 3, 4, 5, 6, 7, 8]), [1.2345678, 2.345678, 3.45678, 4.5678, 5.678, 6.78, 7.8, 8])\n    \n    y1 = [0.1]\n    y2 = [0.1 + gamma*0.2, 0.2]\n    y3 = [0.1 + gamma*(0.2 + gamma*0.3), \n          0.2 + gamma*0.3, \n          0.3]\n    y4 = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*0.4)), \n          0.2 + gamma*(0.3 + gamma*0.4), \n          0.3 + gamma*0.4, \n          0.4]\n    y5 = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*0.5))), \n          0.2 + gamma*(0.3 + gamma*(0.4 + gamma*0.5)), \n          0.3 + gamma*(0.4 + gamma*0.5), \n          0.4 + gamma*0.5, \n          0.5]\n    y6 = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*0.6)))), \n          0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*0.6))), \n          0.3 + gamma*(0.4 + gamma*(0.5 + gamma*0.6)), \n          0.4 + gamma*(0.5 + gamma*0.6), \n          0.5 + gamma*0.6, \n          0.6]\n    assert np.allclose(returns(gamma, [0.1]), y1)\n    assert np.allclose(returns(gamma, [0.1, 0.2]), y2)\n    assert np.allclose(returns(gamma, [0.1, 0.2, 0.3]), y3)\n    assert np.allclose(returns(gamma, [0.1, 0.2, 0.3, 0.4]), y4)\n    assert np.allclose(returns(gamma, [0.1, 0.2, 0.3, 0.4, 0.5]), y5)\n    assert np.allclose(returns(gamma, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]), y6)\n\n\n@pytest.mark.parametrize('gamma', [0.1, 0.99, 1.0])\n@pytest.mark.parametrize('last_V', [-3.0, 0.0, 2.0])\ndef test_bootstrapped_returns(gamma, last_V):\n    y = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*last_V))), \n         0.2 + gamma*(0.3 + gamma*(0.4 + gamma*last_V)), \n         0.3 + gamma*(0.4 + gamma*last_V), \n         0.4 + gamma*last_V]\n    reach_terminal = False\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    assert np.allclose(bootstrapped_returns(gamma, rewards, last_V, reach_terminal), y)\n    assert np.allclose(bootstrapped_returns(gamma, rewards, torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*last_V*0.0))), \n         0.2 + gamma*(0.3 + gamma*(0.4 + gamma*last_V*0.0)), \n         0.3 + gamma*(0.4 + gamma*last_V*0.0), \n         0.4 + gamma*last_V*0.0]\n    reach_terminal = True\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    assert np.allclose(bootstrapped_returns(gamma, rewards, last_V, reach_terminal), y)\n    assert np.allclose(bootstrapped_returns(gamma, rewards, torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V)))), \n         0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V))), \n         0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V)), \n         0.4 + gamma*(0.5 + gamma*last_V), \n         0.5 + gamma*last_V]\n    reach_terminal = False\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5]\n    assert np.allclose(bootstrapped_returns(gamma, rewards, last_V, reach_terminal), y)\n    assert np.allclose(bootstrapped_returns(gamma, rewards, torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*(0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V*0.0)))), \n         0.2 + gamma*(0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V*0.0))), \n         0.3 + gamma*(0.4 + gamma*(0.5 + gamma*last_V*0.0)), \n         0.4 + gamma*(0.5 + gamma*last_V*0.0),\n         0.5 + gamma*last_V*0.0]\n    reach_terminal = True\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5]\n    assert np.allclose(bootstrapped_returns(gamma, rewards, last_V, reach_terminal), y)\n    assert np.allclose(bootstrapped_returns(gamma, rewards, torch.tensor(last_V), reach_terminal), y)\n\n\n@pytest.mark.parametrize('gamma', [0.1, 0.99, 1.0])\n@pytest.mark.parametrize('last_V', [-3.0, 0.0, 2.0])\ndef test_td0_target(gamma, last_V):\n    y = [0.1 + gamma*2, \n         0.2 + gamma*3,\n         0.3 + gamma*4, \n         0.4 + gamma*last_V*0.0]\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    Vs = [1, 2, 3, 4]\n    reach_terminal = True\n    assert np.allclose(td0_target(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_target(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2, \n         0.2 + gamma*3,\n         0.3 + gamma*4, \n         0.4 + gamma*last_V]\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    Vs = [1, 2, 3, 4]\n    reach_terminal = False\n    assert np.allclose(td0_target(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_target(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2, \n         0.2 + gamma*3, \n         0.3 + gamma*4, \n         0.4 + gamma*5, \n         0.5 + gamma*6, \n         0.6 + gamma*last_V*0.0]\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    Vs = [1, 2, 3, 4, 5, 6]\n    reach_terminal = True\n    assert np.allclose(td0_target(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_target(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2, \n         0.2 + gamma*3, \n         0.3 + gamma*4, \n         0.4 + gamma*5, \n         0.5 + gamma*6, \n         0.6 + gamma*last_V]\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    Vs = [1, 2, 3, 4, 5, 6]\n    reach_terminal = False\n    assert np.allclose(td0_target(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_target(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n\n\n@pytest.mark.parametrize('gamma', [0.1, 0.99, 1.0])\n@pytest.mark.parametrize('last_V', [-3.0, 0.0, 2.0])\ndef test_td0_error(gamma, last_V):\n    y = [0.1 + gamma*2 - 1, \n         0.2 + gamma*3 - 2,\n         0.3 + gamma*4 - 3, \n         0.4 + gamma*last_V*0.0 - 4]\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    Vs = [1, 2, 3, 4]\n    reach_terminal = True\n    assert np.allclose(td0_error(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_error(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2 - 1, \n         0.2 + gamma*3 - 2,\n         0.3 + gamma*4 - 3, \n         0.4 + gamma*last_V - 4]\n    rewards = [0.1, 0.2, 0.3, 0.4]\n    Vs = [1, 2, 3, 4]\n    reach_terminal = False\n    assert np.allclose(td0_error(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_error(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2 - 1, \n         0.2 + gamma*3 - 2, \n         0.3 + gamma*4 - 3, \n         0.4 + gamma*5 - 4, \n         0.5 + gamma*6 - 5,  \n         0.6 + gamma*last_V*0.0 - 6]\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    Vs = [1, 2, 3, 4, 5, 6]\n    reach_terminal = True\n    assert np.allclose(td0_error(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_error(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n    \n    y = [0.1 + gamma*2 - 1, \n         0.2 + gamma*3 - 2, \n         0.3 + gamma*4 - 3, \n         0.4 + gamma*5 - 4, \n         0.5 + gamma*6 - 5,  \n         0.6 + gamma*last_V - 6]\n    rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    Vs = [1, 2, 3, 4, 5, 6]\n    reach_terminal = False\n    assert np.allclose(td0_error(gamma, rewards, Vs, last_V, reach_terminal), y)\n    assert np.allclose(td0_error(gamma, rewards, torch.tensor(Vs), torch.tensor(last_V), reach_terminal), y)\n\n\ndef test_gae():\n    rewards = [1, 2, 3]\n    Vs = [0.1, 1.1, 2.1]\n    assert np.allclose(gae(1.0, 0.5, rewards, Vs, 10, True), \n                       [3.725, 3.45, 0.9])\n    assert np.allclose(gae(1.0, 0.5, rewards, torch.tensor(Vs), torch.tensor(10), True), \n                       [3.725, 3.45, 0.9])\n    assert np.allclose(gae(0.1, 0.2, rewards, Vs, 10, True), \n                       [1.03256, 1.128, 0.9])\n    assert np.allclose(gae(0.1, 0.2, rewards, torch.tensor(Vs), torch.tensor(10), True), \n                       [1.03256, 1.128, 0.9])\n    \n    rewards = [1, 2, 3]\n    Vs = [0.5, 1.5, 2.5]\n    assert np.allclose(gae(1.0, 0.5, rewards, Vs, 99, True), \n                       [3.625, 3.25, 0.5])\n    assert np.allclose(gae(1.0, 0.5, rewards, torch.tensor(Vs), torch.tensor(99), True), \n                       [3.625, 3.25, 0.5])\n    assert np.allclose(gae(0.1, 0.2, rewards, Vs, 99, True), \n                       [0.6652, 0.76, 0.5])\n    assert np.allclose(gae(0.1, 0.2, rewards, torch.tensor(Vs), torch.tensor(99), True), \n                       [0.6652, 0.76, 0.5])\n    \n    rewards = [1, 2, 3, 4, 5]\n    Vs = [0.5, 1.5, 2.5, 3.5, 4.5]\n    assert np.allclose(gae(1.0, 0.5, rewards, Vs, 20, False), \n                       [6.40625, 8.8125, 11.625, 15.25, 20.5])\n    assert np.allclose(gae(1.0, 0.5, rewards, torch.tensor(Vs), torch.tensor(20), False), \n                       [6.40625, 8.8125, 11.625, 15.25, 20.5])\n    assert np.allclose(gae(0.1, 0.2, rewards, Vs, 20, False), \n                       [0.665348, 0.7674, 0.87, 1, 2.5])\n    assert np.allclose(gae(0.1, 0.2, rewards, torch.tensor(Vs), torch.tensor(20), False), \n                       [0.665348, 0.7674, 0.87, 1, 2.5])\n\n    rewards = [1, 2, 3, 4, 5]\n    Vs = [0.1, 1.1, 2.1, 3.1, 4.1]\n    assert np.allclose(gae(1.0, 0.5, rewards, Vs, 10, False), \n                       [5.80625, 7.6125, 9.225, 10.45, 10.9])\n    assert np.allclose(gae(1.0, 0.5, rewards, torch.tensor(Vs), torch.tensor(10), False), \n                       [5.80625, 7.6125, 9.225, 10.45, 10.9])\n    assert np.allclose(gae(0.1, 0.2, rewards, Vs, 10, False), \n                       [1.03269478, 1.1347393, 1.23696, 1.348, 1.9])\n    assert np.allclose(gae(0.1, 0.2, rewards, torch.tensor(Vs), torch.tensor(10), False), \n                       [1.03269478, 1.1347393, 1.23696, 1.348, 1.9])\n    \n    rewards = [1, 2, 3, 4, 5, 6, 7, 8]\n    Vs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n    assert np.allclose(gae(1.0, 0.5, rewards, Vs, 30, True), \n                       [5.84375, 7.6875, 9.375, 10.75, 11.5, 11., 8, 0.])\n    assert np.allclose(gae(1.0, 0.5, rewards, torch.tensor(Vs), torch.tensor(30), True), \n                       [5.84375, 7.6875, 9.375, 10.75, 11.5, 11., 8, 0.])\n    assert np.allclose(gae(0.1, 0.2, rewards, Vs, 30, True), \n                       [0.206164098, 0.308204915, 0.410245728, 0.5122864, 0.61432, 0.716, 0.8, 0])\n    assert np.allclose(gae(0.1, 0.2, rewards, torch.tensor(Vs), torch.tensor(30), True), \n                       [0.206164098, 0.308204915, 0.410245728, 0.5122864, 0.61432, 0.716, 0.8, 0])\n\n\n@pytest.mark.parametrize('gamma', [0.1, 1.0])\n@pytest.mark.parametrize('last_V', [0.3, [0.5]])\n@pytest.mark.parametrize('reach_terminal', [True, False])\n@pytest.mark.parametrize('clip_rho', [0.5, 1.0])\n@pytest.mark.parametrize('clip_pg_rho', [0.3, 1.1])\ndef test_vtrace(gamma, last_V, reach_terminal, clip_rho, clip_pg_rho):\n    behavior_logprobs = [1, 2, 3]\n    target_logprobs = [4, 5, 6]\n    Rs = [7, 8, 9]\n    Vs = [10, 11, 12]\n    \n    vs_test, As_test = vtrace(behavior_logprobs, target_logprobs, gamma, Rs, Vs, last_V, reach_terminal, clip_rho, clip_pg_rho)\n    \n    # ground truth calculation\n    behavior_logprobs = numpify(behavior_logprobs, np.float32)\n    target_logprobs = numpify(target_logprobs, np.float32)\n    Rs = numpify(Rs, np.float32)\n    Vs = numpify(Vs, np.float32)\n    last_V = numpify(last_V, np.float32)\n    \n    rhos = np.exp(target_logprobs - behavior_logprobs)\n    clipped_rhos = np.minimum(clip_rho, rhos)\n    cs = np.minimum(1.0, rhos)\n    deltas = clipped_rhos*td0_error(gamma, Rs, Vs, last_V, reach_terminal)\n    \n    vs = np.array([Vs[0] + gamma**0*1*deltas[0] + gamma*cs[0]*deltas[1] + gamma**2*cs[0]*cs[1]*deltas[2], \n                   Vs[1] + gamma**0*1*deltas[1] + gamma*cs[1]*deltas[2], \n                   Vs[2] + gamma**0*1*deltas[2]])\n    vs_next = np.append(vs[1:], (1. - reach_terminal)*last_V)\n    clipped_pg_rhos = np.minimum(clip_pg_rho, rhos)\n    As = clipped_pg_rhos*(Rs + gamma*vs_next - Vs)\n    \n    assert np.allclose(vs, vs_test)\n    assert np.allclose(As, As_test)\n"""
test/test_networks.py,30,"b""import numpy as np\n\nimport pytest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import PackedSequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\nimport torch.optim as optim\n\nfrom torch.distributions import Categorical\nfrom torch.distributions import Normal\nfrom torch.distributions import Independent\n\nfrom lagom.networks import Module\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.networks import ortho_init\nfrom lagom.networks import make_fc\nfrom lagom.networks import make_cnn\nfrom lagom.networks import make_transposed_cnn\nfrom lagom.networks import make_lnlstm\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\n\n\nclass TestMakeBlocks(object):\n    def test_make_fc(self):\n        # Single layer\n        fc = make_fc(3, [4])\n        assert len(fc) == 1\n        \n        # Multiple layers\n        fc = make_fc(3, [4, 5, 6])\n        assert len(fc) == 3\n        \n        # Raise Exception\n        with pytest.raises(AssertionError):\n            make_fc(3, 4)\n            \n    def test_make_cnn(self):\n        # Single layer\n        cnn = make_cnn(input_channel=3, channels=[16], kernels=[4], strides=[2], paddings=[1])\n        assert len(cnn) == 1\n        \n        # Multiple layers\n        cnn = make_cnn(input_channel=3, channels=[16, 32, 64], kernels=[4, 3, 3], strides=[2, 1, 1], paddings=[2, 1, 0])\n        assert len(cnn) == 3\n        \n        # Raise Exception\n        with pytest.raises(AssertionError):\n            # Non-list\n            make_cnn(input_channel=3, channels=[16], kernels=4, strides=[2], paddings=[1])\n        with pytest.raises(AssertionError):\n            # Inconsistent length\n            make_cnn(input_channel=3, channels=[16], kernels=[4, 2], strides=[2], paddings=[1])\n            \n    def test_make_transposed_cnn(self):\n        # Single layer\n        transposed_cnn = make_transposed_cnn(input_channel=3, \n                                             channels=[16], \n                                             kernels=[4], \n                                             strides=[2], \n                                             paddings=[1], \n                                             output_paddings=[1])\n        assert len(transposed_cnn) == 1\n        \n        # Multiple layers\n        transposed_cnn = make_transposed_cnn(input_channel=3, \n                                             channels=[16, 32, 64], \n                                             kernels=[4, 3, 3], \n                                             strides=[2, 1, 1], \n                                             paddings=[2, 1, 0],\n                                             output_paddings=[3, 1, 0])\n        assert len(transposed_cnn) == 3\n        \n        # Raise Exception\n        with pytest.raises(AssertionError):\n            # Non-list\n            make_transposed_cnn(input_channel=3, \n                                channels=[16], \n                                kernels=[4], \n                                strides=2, \n                                paddings=[1], \n                                output_paddings=[1])\n        with pytest.raises(AssertionError):\n            # Inconsistent length\n            make_transposed_cnn(input_channel=3, \n                                channels=[16], \n                                kernels=[4], \n                                strides=[2, 1], \n                                paddings=[1], \n                                output_paddings=[1])\n    \n    @pytest.mark.parametrize('input_size', [1, 10])\n    @pytest.mark.parametrize('hidden_size', [1, 8])\n    @pytest.mark.parametrize('batch_size', [1, 5])\n    @pytest.mark.parametrize('seq_len', [1, 3])\n    def test_make_lnlstm(self, input_size, hidden_size, batch_size, seq_len):\n        rnn = make_lnlstm(input_size, hidden_size)\n        input = torch.randn(seq_len, batch_size, input_size)\n        states = [(torch.randn(batch_size, hidden_size), torch.randn(batch_size, hidden_size))]\n        output, output_states = rnn(input, states)\n        output, output_states = rnn(input, output_states)\n        \n        assert output.shape == (seq_len, batch_size, hidden_size)\n        hx, cx = output_states[0]\n        assert hx.shape == (batch_size, hidden_size)\n        assert cx.shape == (batch_size, hidden_size)\n        \n        # use case: PackedSequence\n        x1 = torch.randn(3, input_size)\n        x2 = torch.randn(1, input_size)\n        x3 = torch.randn(4, input_size)\n        x = pad_sequence([x1, x2, x3])\n        lengths = [x1.shape[0], x2.shape[0], x3.shape[0]]\n        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n        assert x.data.shape == (sum(lengths), input_size)\n        h = torch.zeros(3, hidden_size)\n        c = torch.zeros_like(h)\n        out, [(h, c)] = rnn(x, [(h, c)])\n        assert isinstance(out, PackedSequence)\n        assert out.data.shape == (sum(lengths), hidden_size)\n        assert torch.equal(out.batch_sizes, torch.tensor([3, 2, 2, 1]))\n        assert torch.equal(out.sorted_indices, torch.tensor([2, 0, 1]))\n        assert torch.equal(out.unsorted_indices, torch.tensor([1, 2, 0]))\n        o, l = pad_packed_sequence(out)\n        assert torch.is_tensor(o)\n        assert o.shape == (4, 3, hidden_size)\n        assert torch.equal(l, torch.tensor([3, 1, 4]))\n        assert torch.allclose(o[3:, 0, ...], torch.tensor(0.0))\n        assert torch.allclose(o[1:, 1, ...], torch.tensor(0.0))\n        assert torch.allclose(o[4:, 2, ...], torch.tensor(0.0))\n\n\nclass TestInit(object):\n    def test_ortho_init(self):\n        # Linear\n        a = nn.Linear(2, 3)\n        ortho_init(a, weight_scale=1000., constant_bias=10.)\n        assert a.weight.max().item() > 30.\n        assert np.allclose(a.bias.detach().numpy(), 10.)\n        ortho_init(a, nonlinearity='relu')\n        \n        # Conv2d\n        a = nn.Conv2d(2, 3, 3)\n        ortho_init(a, weight_scale=1000., constant_bias=10.)\n        assert a.weight.max().item() > 50.\n        assert np.allclose(a.bias.detach().numpy(), 10.)\n        ortho_init(a, nonlinearity='relu')\n        \n        # LSTM\n        a = nn.LSTM(2, 3, 2)\n        ortho_init(a, weight_scale=1000., constant_bias=10.)\n        assert a.weight_hh_l0.max().item() > 50.\n        assert a.weight_hh_l1.max().item() > 50.\n        assert a.weight_ih_l0.max().item() > 50.\n        assert a.weight_ih_l1.max().item() > 50.\n        assert np.allclose(a.bias_hh_l0.detach().numpy(), 10.)\n        assert np.allclose(a.bias_hh_l1.detach().numpy(), 10.)\n        assert np.allclose(a.bias_ih_l0.detach().numpy(), 10.)\n        assert np.allclose(a.bias_ih_l1.detach().numpy(), 10.)\n        \n        # LSTMCell\n        a = nn.LSTMCell(3, 2)\n        ortho_init(a, weight_scale=1000., constant_bias=10.)\n        assert a.weight_hh.max().item() > 50.\n        assert a.weight_ih.max().item() > 50.\n        assert np.allclose(a.bias_hh.detach().numpy(), 10.)\n        assert np.allclose(a.bias_ih.detach().numpy(), 10.)\n\n\n@pytest.mark.parametrize('method', ['Adam', 'RMSprop', 'Adamax'])\n@pytest.mark.parametrize('N', [1, 10, 50, 100])\n@pytest.mark.parametrize('min_lr', [3e-4, 6e-5])\n@pytest.mark.parametrize('initial_lr', [1e-3, 7e-4])\ndef test_linear_lr_scheduler(method, N, min_lr, initial_lr):\n    net = nn.Linear(30, 16)\n    if method == 'Adam':\n        optimizer = optim.Adam(net.parameters(), lr=initial_lr)\n    elif method == 'RMSprop':\n        optimizer = optim.RMSprop(net.parameters(), lr=initial_lr)\n    elif method == 'Adamax':\n        optimizer = optim.Adamax(net.parameters(), lr=initial_lr)\n    lr_scheduler = linear_lr_scheduler(optimizer, N, min_lr)\n    assert lr_scheduler.base_lrs[0] == initial_lr\n    \n    optimizer.step()\n    for i in range(200):\n        lr_scheduler.step()\n        assert lr_scheduler.get_lr()[0] >= min_lr\n    assert lr_scheduler.get_lr()[0] == min_lr       \n    \n    \n@pytest.mark.parametrize('feature_dim', [5, 10, 30])\n@pytest.mark.parametrize('batch_size', [1, 16, 32])\n@pytest.mark.parametrize('num_action', [1, 4, 10])\ndef test_categorical_head(feature_dim, batch_size, num_action):\n    action_head = CategoricalHead(feature_dim, num_action, torch.device('cpu'))\n    assert isinstance(action_head, Module)\n    assert action_head.feature_dim == feature_dim\n    assert action_head.num_action == num_action\n    assert action_head.device.type == 'cpu'\n    dist = action_head(torch.randn(batch_size, feature_dim))\n    assert isinstance(dist, Categorical)\n    assert dist.batch_shape == (batch_size,)\n    assert dist.probs.shape == (batch_size, num_action)\n    x = dist.sample()\n    assert x.shape == (batch_size,)\n    \n    \n@pytest.mark.parametrize('batch_size', [1, 32])\n@pytest.mark.parametrize('feature_dim', [5, 20])\n@pytest.mark.parametrize('action_dim', [1, 4])\n@pytest.mark.parametrize('std0', [0.21, 0.5, 1.0])\ndef test_diag_gaussian_head(batch_size, feature_dim, action_dim, std0):\n    device = torch.device('cpu')\n    with pytest.raises(AssertionError):\n        DiagGaussianHead(feature_dim, action_dim, device, -0.5)\n    \n    def _basic_check(action_head):\n        assert action_head.feature_dim == feature_dim\n        assert action_head.action_dim == action_dim\n        assert action_head.device.type == 'cpu'\n        assert action_head.std0 == std0\n        assert isinstance(action_head.mean_head, nn.Linear)\n        assert isinstance(action_head.logstd_head, nn.Parameter)\n        \n    def _dist_check(action_dist):\n        assert isinstance(action_dist, Independent)\n        assert isinstance(action_dist.base_dist, Normal)\n        assert action_dist.batch_shape == (batch_size,)\n        action = action_dist.sample()\n        assert action.shape == (batch_size, action_dim)\n    \n    action_head = DiagGaussianHead(feature_dim, action_dim, device, std0)    \n    _basic_check(action_head)\n    action_dist = action_head(torch.randn(batch_size, feature_dim))\n    _dist_check(action_dist)\n    assert torch.allclose(action_dist.base_dist.stddev, torch.tensor(std0))\n"""
test/test_transform.py,0,"b'import numpy as np\n\nimport pytest\n\nfrom lagom.transform import interp_curves\nfrom lagom.transform import geometric_cumsum\nfrom lagom.transform import explained_variance\nfrom lagom.transform import LinearSchedule\nfrom lagom.transform import rank_transform\nfrom lagom.transform import PolyakAverage\nfrom lagom.transform import RunningMeanVar\nfrom lagom.transform import SumTree\nfrom lagom.transform import MinTree\nfrom lagom.transform import smooth_filter\n\n\ndef test_interp_curves():\n    # Make some inconsistent data\n    x1 = [4, 5, 7, 13, 20]\n    y1 = [0.25, 0.22, 0.53, 0.37, 0.55]\n    x2 = [2, 4, 6, 7, 9, 11, 15]\n    y2 = [0.03, 0.12, 0.4, 0.2, 0.18, 0.32, 0.39]\n    new_x, ys = interp_curves([x1, x2], [y1, y2])\n    \n    assert isinstance(new_x, (list, np.ndarray))\n    assert isinstance(ys, (list, np.ndarray))\n    assert len(new_x) == 10\n    assert len(ys) == 2\n    assert len(ys[0]) == 10\n    assert len(ys[1]) == 10\n    assert min(new_x) == 2\n    assert max(new_x) == 20\n    assert min(ys[0]) > 0 and min(ys[1]) > 0\n    assert max(ys[0]) < 0.6 and max(ys[1]) < 0.6\n\n\ndef test_geometric_cumsum():\n    assert np.allclose(geometric_cumsum(0.1, [1, 2, 3]), [1.23, 2.3, 3])\n    assert np.allclose(geometric_cumsum(0.1, [[1, 2, 3, 4], [5, 6, 7, 8]]), \n                       [[1.234, 2.34, 3.4, 4], [5.678, 6.78, 7.8, 8]])\n\n\ndef test_explained_variance():\n    assert np.isclose(explained_variance(y_true=[3, -0.5, 2, 7], y_pred=[2.5, 0.0, 2, 8]), 0.9571734666824341)\n    assert np.isclose(explained_variance(y_true=[[3, -0.5, 2, 7]], y_pred=[[2.5, 0.0, 2, 8]]), 0.9571734666824341)\n    assert np.isclose(explained_variance(y_true=[[0.5, 1], [-1, 1], [7, -6]], y_pred=[[0, 2], [-1, 2], [8, -5]]), \n                      0.9838709533214569)\n    assert np.isclose(explained_variance(y_true=[[0.5, 1], [-1, 10], [7, -6]], y_pred=[[0, 2], [-1, 0.00005], [8, -5]]), \n                      0.6704022586345673)\n\n\ndef test_linear_schedule():\n    with pytest.raises(AssertionError):\n        LinearSchedule(1.0, 0.1, 0, 0)\n    with pytest.raises(AssertionError):\n        LinearSchedule(1.0, 0.1, -1, 0)\n    with pytest.raises(AssertionError):\n        LinearSchedule(1.0, 0.1, 10, -1)\n    with pytest.raises(AssertionError):\n        LinearSchedule(1.0, 0.1, 10, 0)(-1)\n\n    # increasing: without warmup start\n    scheduler = LinearSchedule(initial=0.5, final=2.0, N=3, start=0)\n    assert scheduler(0) == 0.5\n    assert scheduler(1) == 1.0\n    assert scheduler(2) == 1.5\n    assert scheduler(3) == 2.0\n    assert all([scheduler(i) == 2.0] for i in [4, 5, 6, 7, 8])\n    assert all([scheduler(i) == scheduler.get_current() for i in range(10)])\n\n    # increasing: with warmup start\n    scheduler = LinearSchedule(initial=0.5, final=2.0, N=3, start=2)\n    assert all([scheduler(i) == 0.5] for i in [0, 1, 2])\n    assert scheduler(3) == 1.0\n    assert scheduler(4) == 1.5\n    assert scheduler(5) == 2.0\n    assert all([scheduler(i) == 2.0 for i in [6, 7, 8]])\n    assert all([scheduler(i) == scheduler.get_current() for i in range(10)])\n\n    # decreasing: without warmup start\n    scheduler = LinearSchedule(initial=1.0, final=0.1, N=3, start=0)\n    assert scheduler(0) == 1.0\n    assert scheduler(1) == 0.7\n    assert scheduler(2) == 0.4\n    assert scheduler(3) == 0.1\n    assert all([scheduler(i) == 0.1 for i in [4, 5, 6]])\n    assert all([scheduler(i) == scheduler.get_current() for i in range(10)])\n\n    # decreasing: with warmup start\n    scheduler = LinearSchedule(initial=1.0, final=0.1, N=3, start=2)\n    assert all([scheduler(i) for i in [0, 1, 2]])\n    assert scheduler(3) == 0.7\n    assert scheduler(4) == 0.4\n    assert scheduler(5) == 0.1\n    assert all([scheduler(i) == 0.1 for i in [6, 7, 8]])\n    assert all([scheduler(i) == scheduler.get_current() for i in range(10)])\n\n\ndef test_rank_transform():\n    with pytest.raises(AssertionError):\n        rank_transform(3)\n    with pytest.raises(AssertionError):\n        rank_transform([[1, 2, 3]])\n    \n    assert np.allclose(rank_transform([3, 14, 1], centered=True), [0, 0.5, -0.5])\n    assert np.allclose(rank_transform([3, 14, 1], centered=False), [1, 2, 0])\n\n\ndef test_polyak_average():\n    with pytest.raises(AssertionError):\n        PolyakAverage(alpha=-1.0)\n    with pytest.raises(AssertionError):\n        PolyakAverage(alpha=1.2)\n        \n    f = PolyakAverage(alpha=0.1)\n    x = f(0.5)\n    assert np.allclose(x, 0.5)\n    x = f(1.5)\n    assert np.allclose(x, 1.4)\n    assert np.allclose(x, f.get_current())\n    \n    f = PolyakAverage(alpha=0.1)\n    x = f(1.0)\n    assert np.allclose(x, 1.0)\n    x = f(2.0)\n    assert np.allclose(x, 1.9)\n    assert np.allclose(x, f.get_current())\n    \n    f = PolyakAverage(alpha=0.1)\n    x = f([0.5, 1.0])\n    assert np.allclose(x, [0.5, 1.0])\n    x = f([1.5, 2.0])\n    assert np.allclose(x, [1.4, 1.9])\n    assert np.allclose(x, f.get_current())\n\n\ndef test_running_mean_var():\n    with pytest.raises(AssertionError):\n        f = RunningMeanVar(shape=())\n        f(0.5)\n    with pytest.raises(AssertionError):\n        f = RunningMeanVar(shape=(1,))\n        f([0.5])\n    \n    # simple case:\n    f = RunningMeanVar(shape=())\n    x = [5.1, 4.3, 1.324]\n    f([x[0]])\n    f([x[1]])\n    f([x[2]])\n    assert np.allclose(np.mean(x), f.mean)\n    assert np.allclose(np.var(x), f.var)\n    \n    # random sampling\n    x = np.random.randn(1000)\n    xs = np.array_split(x, [1, 200, 500, 600, 900, 950])\n    assert np.allclose(np.concatenate(xs), x)\n    f = RunningMeanVar(shape=())\n    for x_part in xs:\n        f(x_part)\n    assert np.allclose(f.mean, x.mean())\n    assert np.allclose(f.var, x.var())\n    assert np.allclose(np.sqrt(f.var + 1e-8), x.std())\n    assert f.n == 1000\n\n    x = np.random.randn(1000, 32, 3)\n    xs = np.array_split(x, [1, 200, 500, 600, 900, 950])\n    assert np.allclose(np.concatenate(xs), x)\n    f = RunningMeanVar(shape=(32, 3))\n    for x_part in xs:\n        f(x_part)\n    assert np.allclose(f.mean, x.mean(0))\n    assert np.allclose(f.var, x.var(0))\n    assert np.allclose(np.sqrt(f.var + 1e-8), x.std(0))\n    assert f.n == 1000\n    \n    \ndef test_sum_tree():\n    # Naive test\n    tree = SumTree(4)\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert np.allclose(tree.sum(), 4.0)\n    assert np.allclose(tree.sum(0, 2), 0.0)\n    assert np.allclose(tree.sum(0, 3), 1.0)\n    assert np.allclose(tree.sum(2, 3), 1.0)\n    assert np.allclose(tree.sum(2, -1), 1.0)\n    assert np.allclose(tree.sum(2, 4), 4.0)\n    \n    del tree\n    \n    # overwritten same element\n    tree = SumTree(4)\n    tree[2] = 1.0\n    tree[2] = 3.0\n\n    assert np.allclose(tree.sum(), 3.0)\n    assert np.allclose(tree.sum(2, 3), 3.0)\n    assert np.allclose(tree.sum(2, -1), 3.0)\n    assert np.allclose(tree.sum(2, 4), 3.0)\n    assert np.allclose(tree.sum(1, 2), 0.0)\n    \n    del tree\n    \n    # prefixsum index: v1\n    tree = SumTree(4)\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert tree.find_prefixsum_index(0.0) == 2\n    assert tree.find_prefixsum_index(0.5) == 2\n    assert tree.find_prefixsum_index(0.99) == 2\n    assert tree.find_prefixsum_index(1.01) == 3\n    assert tree.find_prefixsum_index(3.00) == 3\n    assert tree.find_prefixsum_index(4.00) == 3\n\n    # prefixsum index: v2\n    tree = SumTree(4)\n    tree[0] = 0.5\n    tree[1] = 1.0\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert tree.find_prefixsum_index(0.00) == 0\n    assert tree.find_prefixsum_index(0.55) == 1\n    assert tree.find_prefixsum_index(0.99) == 1\n    assert tree.find_prefixsum_index(1.51) == 2\n    assert tree.find_prefixsum_index(3.00) == 3\n    assert tree.find_prefixsum_index(5.50) == 3\n\n    \ndef test_min_tree():\n    tree = MinTree(4)\n    tree[0] = 1.0\n    tree[2] = 0.5\n    tree[3] = 3.0\n\n    assert np.allclose(tree.min(), 0.5)\n    assert np.allclose(tree.min(0, 2), 1.0)\n    assert np.allclose(tree.min(0, 3), 0.5)\n    assert np.allclose(tree.min(0, -1), 0.5)\n    assert np.allclose(tree.min(2, 4), 0.5)\n    assert np.allclose(tree.min(3, 4), 3.0)\n\n    tree[2] = 0.7\n\n    assert np.allclose(tree.min(), 0.7)\n    assert np.allclose(tree.min(0, 2), 1.0)\n    assert np.allclose(tree.min(0, 3), 0.7)\n    assert np.allclose(tree.min(0, -1), 0.7)\n    assert np.allclose(tree.min(2, 4), 0.7)\n    assert np.allclose(tree.min(3, 4), 3.0)\n\n    tree[2] = 4.0\n\n    assert np.allclose(tree.min(), 1.0)\n    assert np.allclose(tree.min(0, 2), 1.0)\n    assert np.allclose(tree.min(0, 3), 1.0)\n    assert np.allclose(tree.min(0, -1), 1.0)\n    assert np.allclose(tree.min(2, 4), 3.0)\n    assert np.allclose(tree.min(2, 3), 4.0)\n    assert np.allclose(tree.min(2, -1), 4.0)\n    assert np.allclose(tree.min(3, 4), 3.0)\n    \n\ndef test_smooth_filter():\n    with pytest.raises(AssertionError):\n        smooth_filter([[1, 2, 3]], window_length=3, polyorder=2)\n    \n    x = np.linspace(0, 4*2*np.pi, num=100)\n    y = x*(np.sin(x) + np.random.random(100)*4)\n    out = smooth_filter(y, window_length=31, polyorder=10)\n    assert out.shape == (100,)\n'"
test/test_utils.py,9,"b'import os\nimport pytest\n\nimport numpy as np\nimport torch\n\nfrom lagom.utils import color_str\nfrom lagom.utils import IntervalConditioner\nfrom lagom.utils import NConditioner\nfrom lagom.utils import Seeder\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.utils import pickle_load\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import yaml_load\nfrom lagom.utils import yaml_dump\nfrom lagom.utils import timed\nfrom lagom.utils import ProcessMaster\nfrom lagom.utils import ProcessWorker\n\n\ndef test_color_str():\n    assert color_str(\'lagom\', \'green\', bold=True) == \'\\x1b[32m\\x1b[1mlagom\\x1b[0m\'\n    assert color_str(\'lagom\', \'white\') == \'\\x1b[37mlagom\\x1b[0m\'\n\n\ndef test_conditioner():\n    cond = IntervalConditioner(interval=4, mode=\'accumulative\')\n    assert cond.counter == 0\n    assert cond(0)\n    assert cond.counter == 0\n    assert not cond(2)\n    assert not cond(3)\n    assert cond(4)\n    assert cond.counter == 1\n    assert not cond(5)\n    assert cond(9)\n    assert cond.counter == 2\n    assert cond(12)\n    assert cond.counter == 3\n    del cond\n\n    cond = IntervalConditioner(interval=4, mode=\'incremental\')\n    assert cond.counter == 0\n    assert cond.total_n == 0\n    assert cond(0)\n    assert cond.counter == 0\n    assert cond.total_n == 0\n    assert not cond(3)\n    assert cond.counter == 0\n    assert cond.total_n == 3\n    assert cond(1)\n    assert cond.counter == 1\n    assert cond.total_n == 4\n    assert cond(4)\n    assert cond.counter == 2\n    assert cond.total_n == 8\n    assert not cond(1)\n    assert cond.counter == 2\n    assert cond.total_n == 9\n    del cond\n\n    cond = NConditioner(max_n=10, num_conditions=3, mode=\'accumulative\')\n    assert cond.counter == 0\n    assert cond(0)\n    assert cond.counter == 0\n    assert not cond(2)\n    assert not cond(3)\n    assert cond(4)\n    assert cond.counter == 1\n    assert not cond(5)\n    assert cond.counter == 1\n    assert cond(8)\n    assert cond.counter == 2\n    assert not cond(9)\n    assert cond.counter == 2\n    assert cond(10)\n    assert cond.counter == 3\n    assert not cond(15)\n    assert cond.counter == 3\n    assert not cond(20)\n    assert cond.counter == 3\n    del cond\n\n    cond = NConditioner(max_n=10, num_conditions=3, mode=\'incremental\')\n    assert cond.counter == 0\n    assert cond(0)\n    assert cond.counter == 0\n    assert not cond(2)\n    assert cond.counter == 0\n    assert cond.total_n == 2\n    assert not cond(1)\n    assert cond.counter == 0\n    assert cond.total_n == 3\n    assert cond(2)\n    assert cond.counter == 1\n    assert cond.total_n == 5\n    assert not cond(1)\n    assert cond.counter == 1\n    assert cond.total_n == 6\n    assert cond(2)\n    assert cond.counter == 2\n    assert cond.total_n == 8\n    assert cond(3)\n    assert cond.counter == 3\n    assert cond.total_n == 11\n    assert not cond(5)\n    assert cond.counter == 3\n    assert cond.total_n == 11\n    del cond\n\n\ndef test_seeder():\n    seeder = Seeder(init_seed=0)\n\n    assert seeder.rng.get_state()[1][0] == 0\n    assert np.random.get_state()[1][20] != seeder.rng.get_state()[1][20]\n\n    # Single list of seeds\n    seeds = seeder(size=1)\n    assert len(seeds) == 1\n    seeds = seeder(size=5)\n    assert len(seeds) == 5\n\n    # Batched seeds\n    seeds = seeder(size=[1, 3])\n    assert np.alltrue(np.array(seeds).shape == (1, 3))\n    seeds = seeder(size=[2, 3])\n    assert np.alltrue(np.array(seeds).shape == (2, 3))\n\n    \ndef test_tensorify():\n    # tensor\n    x = torch.tensor(2.43)\n    y = tensorify(x, \'cpu\')\n    assert torch.equal(x, y)\n    del x, y\n\n    x = torch.randn(10)\n    y = tensorify(x, \'cpu\')\n    assert torch.equal(x, y)\n    del x, y\n\n    x = torch.randn(10, 20, 30)\n    y = tensorify(x, \'cpu\')\n    assert torch.equal(x, y)\n    del x, y\n\n    # ndarray\n    x = np.array(2.43)\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y.item())\n    del x, y\n\n    x = np.random.randn(10)\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y)\n    del x, y\n\n    x = np.random.randn(10, 20, 30)\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y)\n    del x, y\n\n    # raw list\n    x = [2.43]\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y.item())\n    del x, y\n\n    x = [1, 2, 3, 4, 5, 6]\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y)\n    del x, y\n\n    x = [[1, 2], [3, 4], [5, 6]]\n    y = tensorify(x, \'cpu\')\n    assert np.allclose(x, y)\n    del x, y\n\n\ndef test_numpify():\n    # Tensor\n    x = torch.randn(5)\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    del x, y\n\n    x = torch.randn(5, 4)\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    del x, y\n\n    x = torch.randn(5, 4)\n    y = numpify(x, dtype=np.float16)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    assert y.dtype == np.float16\n    del x, y\n\n    # Array\n    x = np.random.randn(5)\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    del x, y\n\n    x = np.random.randn(5, 4)\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    del x, y\n\n    x = np.random.randn(5, 4)\n    y = numpify(x, dtype=np.float16)\n    assert isinstance(y, np.ndarray)\n    assert x.shape == y.shape\n    assert y.dtype == np.float16\n    del x, y\n\n    # List\n    x = [1, 2, 3, 4]\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert np.allclose(x, y)\n    del x, y\n\n    x = [[1.2, 2.3], [3.4, 4.5]]\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert np.allclose(x, y)\n    del x, y\n\n    # Tuple\n    x = (1, 2, 3, 4)\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert np.allclose(x, y)\n    del x, y\n\n    x = ((1.2, 2.3), (3.4, 4.5))\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    assert np.allclose(x, y)\n    del x, y\n\n    # Scalar\n    x = 1\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    del x, y\n\n    # Bool\n    x = True\n    y = numpify(x)\n    assert isinstance(y, np.ndarray)\n    del x, y\n\n\ndef test_pickle_yaml():\n    a = {\'one\': 1, \'two\': [2, 3]}\n    b = {\'three\': 3, \'four\': [4, 5]}\n    c = [a, b]\n\n    def check(x):\n        assert isinstance(x, list)\n        assert len(x) == 2\n        assert all([isinstance(i, dict) for i in x])\n        assert list(x[0].keys()) == [\'one\', \'two\']\n        assert list(x[1].keys()) == [\'three\', \'four\']\n        assert list(x[0].values()) == [1, [2, 3]]\n        assert list(x[1].values()) == [3, [4, 5]]\n\n    pickle_dump(c, \'.tmp_pickle\')\n    check(pickle_load(\'.tmp_pickle.pkl\'))\n    os.unlink(\'.tmp_pickle.pkl\')\n\n    yaml_dump(c, \'.tmp_yaml\')\n    check(yaml_load(\'.tmp_yaml.yml\'))\n    os.unlink(\'.tmp_yaml.yml\')\n\n\ndef test_timed():\n    with timed(\'red\', \'bold\'):\n        print(\'ok\')\n\n\ndef naive_primality(integer):\n    r""""""Naive way to test a prime by iterating over all preceding integers. """"""\n    prime = True\n    if integer <= 1:\n        prime = False\n    else:\n        for i in range(2, integer):\n            if integer % i == 0:\n                prime = False\n\n    return prime\n    \n    \nclass Worker(ProcessWorker):\n    def work(self, task_id, task):\n        return naive_primality(task)\n    \n    \nclass Master(ProcessMaster):\n    def make_tasks(self):\n        primes = [16127, 23251, 29611, 37199]\n        non_primes = [5853, 7179, 6957]\n        tasks = [primes[0], primes[1], non_primes[0], primes[2], non_primes[1], non_primes[2], primes[3]]\n        \n        return tasks\n\n\n@pytest.mark.parametrize(\'num_worker\', [1, 2, 3, 5, 7, 10])\ndef test_process_master_worker(num_worker):\n    def check(master):\n        assert all([not p.is_alive() for p in master.list_process])\n        assert all([conn.closed for conn in master.master_conns])\n        assert all([conn.closed for conn in master.worker_conns])\n    \n    master = Master(Worker, num_worker)\n    assert master.num_worker == num_worker\n    results = master()\n    assert results == [True, True, False, True, False, False, True]\n    check(master)\n'"
baselines/cem/__init__.py,0,b''
baselines/cem/agent.py,2,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gym.spaces as spaces\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.feature_network = MLP(config, env, device, **kwargs)\n        feature_dim = config['nn.sizes'][-1]\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.total_timestep = 0\n        \n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        features = self.feature_network(obs)\n        action_dist = self.action_head(features)\n        action = action_dist.sample()\n        out = {}\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        return out\n    \n    def learn(self, D, **kwargs):\n        pass\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if 'env.normalize_obs' in self.config and self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/cem/experiment.py,3,"b""from multiprocessing import Pool\nimport time\n\nimport numpy as np\nimport torch\nimport gym\nfrom lagom import Logger\nfrom lagom import EpisodeRunner\nfrom lagom.transform import describe\nfrom lagom.utils import CloudpickleWrapper\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import TimeStepEnv\n\nfrom lagom import CEM\nfrom baselines.cem.agent import Agent\n\n\nconfig = Config(\n    {'log.freq': 10, \n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['Acrobot-v1', 'BipedalWalker-v2', 'Pendulum-v0', 'LunarLanderContinuous-v2']), \n     \n     'nn.sizes': [64, 64],\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.generations': 500,  # total number of ES generations\n     'train.popsize': 32,\n     'train.worker_chunksize': 4,  # must be divisible by popsize\n     'train.mu0': 0.0,\n     'train.std0': 1.0,\n     'train.elite_ratio': 0.2,\n     'train.noise_scheduler_args': [0.01, 0.001, 400, 0]  # [initial, final, N, start]\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if config['env.clip_action'] and isinstance(env.action_space, gym.spaces.Box):\n        env = gym.wrappers.ClipAction(env)  # TODO: use tanh to squash policy output when RescaleAction wrapper merged in gym\n    env = TimeStepEnv(env)\n    return env\n\n\ndef fitness(data):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    config, seed, device, param = data\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    agent.from_vec(tensorify(param, 'cpu'))\n    runner = EpisodeRunner()\n    with torch.no_grad():\n        D = runner(agent, env, 10)\n    R = np.mean([sum(traj.rewards) for traj in D])\n    H = np.mean([traj.T for traj in D])\n    return R, H\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    \n    print('Initializing...')\n    agent = Agent(config, make_env(config, seed, 'eval'), device)\n    es = CEM([config['train.mu0']]*agent.num_params, config['train.std0'], \n             {'popsize': config['train.popsize'], \n              'seed': seed, \n              'elite_ratio': config['train.elite_ratio'], \n              'noise_scheduler_args': config['train.noise_scheduler_args']})\n    train_logs = []\n    checkpoint_count = 0\n    with Pool(processes=config['train.popsize']//config['train.worker_chunksize']) as pool:\n        print('Finish initialization. Training starts...')\n        for generation in range(config['train.generations']):\n            t0 = time.perf_counter()\n            solutions = es.ask()\n            data = [(config, seed, device, solution) for solution in solutions]\n            out = pool.map(CloudpickleWrapper(fitness), data, chunksize=config['train.worker_chunksize'])\n            Rs, Hs = zip(*out)\n            es.tell(solutions, [-R for R in Rs])\n            logger = Logger()\n            logger('generation', generation+1)\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            logger('Returns', describe(Rs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('Horizons', describe(Hs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('fbest', es.result.fbest)\n            train_logs.append(logger.logs)\n            if generation == 0 or (generation+1) % config['log.freq'] == 0:\n                logger.dump(keys=None, index=0, indent=0, border='-'*50)\n            if (generation+1) >= int(config['train.generations']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n                agent.from_vec(tensorify(es.result.xbest, 'cpu'))\n                agent.checkpoint(logdir, generation+1)\n                checkpoint_count += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    return None\n\n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=7,  # tune to fulfill computation power\n                   chunksize=1, \n                   use_gpu=False,\n                   gpu_ids=None)\n"""
baselines/cmaes/__init__.py,0,b''
baselines/cmaes/agent.py,2,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gym.spaces as spaces\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.feature_network = MLP(config, env, device, **kwargs)\n        feature_dim = config['nn.sizes'][-1]\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.total_timestep = 0\n        \n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        features = self.feature_network(obs)\n        action_dist = self.action_head(features)\n        action = action_dist.sample()\n        out = {}\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        return out\n    \n    def learn(self, D, **kwargs):\n        pass\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if 'env.normalize_obs' in self.config and self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/cmaes/experiment.py,3,"b""from multiprocessing import Pool\nimport time\n\nimport numpy as np\nimport torch\nimport gym\nfrom lagom import Logger\nfrom lagom import EpisodeRunner\nfrom lagom.transform import describe\nfrom lagom.utils import CloudpickleWrapper\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import TimeStepEnv\n\nfrom lagom import CMAES\nfrom baselines.cmaes.agent import Agent\n\n\nconfig = Config(\n    {'log.freq': 10, \n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['Acrobot-v1', 'BipedalWalker-v2', 'Pendulum-v0', 'LunarLanderContinuous-v2']), \n     \n     'nn.sizes': [64, 64],\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.generations': 500,  # total number of ES generations\n     'train.popsize': 32,\n     'train.worker_chunksize': 4,  # must be divisible by popsize\n     'train.mu0': 0.0,\n     'train.std0': 1.0,\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if config['env.clip_action'] and isinstance(env.action_space, gym.spaces.Box):\n        env = gym.wrappers.ClipAction(env)  # TODO: use tanh to squash policy output when RescaleAction wrapper merged in gym\n    env = TimeStepEnv(env)\n    return env\n\n\ndef fitness(data):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    config, seed, device, param = data\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    agent.from_vec(tensorify(param, 'cpu'))\n    runner = EpisodeRunner()\n    with torch.no_grad():\n        D = runner(agent, env, 10)\n    R = np.mean([sum(traj.rewards) for traj in D])\n    H = np.mean([traj.T for traj in D])\n    return R, H\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    \n    print('Initializing...')\n    agent = Agent(config, make_env(config, seed, 'eval'), device)\n    es = CMAES([config['train.mu0']]*agent.num_params, config['train.std0'], \n               {'popsize': config['train.popsize'], \n                'seed': seed})\n    train_logs = []\n    checkpoint_count = 0\n    with Pool(processes=config['train.popsize']//config['train.worker_chunksize']) as pool:\n        print('Finish initialization. Training starts...')\n        for generation in range(config['train.generations']):\n            t0 = time.perf_counter()\n            solutions = es.ask()\n            data = [(config, seed, device, solution) for solution in solutions]\n            out = pool.map(CloudpickleWrapper(fitness), data, chunksize=config['train.worker_chunksize'])\n            Rs, Hs = zip(*out)\n            es.tell(solutions, [-R for R in Rs])\n            logger = Logger()\n            logger('generation', generation+1)\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            logger('Returns', describe(Rs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('Horizons', describe(Hs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('fbest', es.result.fbest)\n            train_logs.append(logger.logs)\n            if generation == 0 or (generation+1) % config['log.freq'] == 0:\n                logger.dump(keys=None, index=0, indent=0, border='-'*50)\n            if (generation+1) >= int(config['train.generations']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n                agent.from_vec(tensorify(es.result.xbest, 'cpu'))\n                agent.checkpoint(logdir, generation+1)\n                checkpoint_count += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    return None\n\n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=12,  # tune to fulfill computation power\n                   chunksize=1, \n                   use_gpu=False,\n                   gpu_ids=None)\n"""
baselines/ddpg_td3/__init__.py,0,b''
baselines/ddpg_td3/ddpg_agent.py,10,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom gym.spaces import flatdim\nfrom lagom import BaseAgent\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.transform import describe\n\n\nclass Actor(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space), [400, 300])\n        self.action_head = nn.Linear(300, flatdim(env.action_space))\n        \n        assert np.unique(env.action_space.high).size == 1\n        assert -np.unique(env.action_space.low).item() == np.unique(env.action_space.high).item()\n        self.max_action = env.action_space.high[0]\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        x = self.max_action*torch.tanh(self.action_head(x))\n        return x\n\n\nclass Critic(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space) + flatdim(env.action_space), [400, 300])\n        self.Q_head = nn.Linear(300, 1)\n        \n        self.to(self.device)\n        \n    def forward(self, x, action):\n        x = torch.cat([x, action], dim=-1)\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        x = self.Q_head(x)\n        return x\n    \n    \nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.actor = Actor(config, env, device, **kwargs)\n        self.actor_target = Actor(config, env, device, **kwargs)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config['agent.actor.lr'])\n        \n        self.critic = Critic(config, env, device, **kwargs)\n        self.critic_target = Critic(config, env, device, **kwargs)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config['agent.critic.lr'])\n        \n        self.total_timestep = 0\n        \n    def polyak_update_target(self):\n        p = self.config['agent.polyak']\n        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n            target_param.data.copy_(p*target_param.data + (1 - p)*param.data)\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(p*target_param.data + (1 - p)*param.data)\n\n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        with torch.no_grad():\n            action = numpify(self.actor(obs).squeeze(0), 'float')\n        if kwargs['mode'] == 'train':\n            eps = np.random.normal(0.0, self.config['agent.action_noise'], size=action.shape)\n            action = np.clip(action + eps, self.env.action_space.low, self.env.action_space.high)\n        out = {}\n        out['raw_action'] = action\n        return out\n\n    def learn(self, D, **kwargs):\n        replay = kwargs['replay']\n        T = kwargs['T']\n        list_actor_loss = []\n        list_critic_loss = []\n        Q_vals = []\n        for i in range(T):\n            observations, actions, rewards, next_observations, masks = replay.sample(self.config['replay.batch_size'])\n            \n            Qs = self.critic(observations, actions)\n            with torch.no_grad():\n                next_Qs = self.critic_target(next_observations, self.actor_target(next_observations))\n                targets = rewards + self.config['agent.gamma']*masks*next_Qs    \n            critic_loss = F.mse_loss(Qs, targets.detach())\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            critic_grad_norm = nn.utils.clip_grad_norm_(self.critic.parameters(), self.config['agent.max_grad_norm'])\n            self.critic_optimizer.step()\n            \n            actor_loss = -self.critic(observations, self.actor(observations)).mean()\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            actor_loss.backward()\n            actor_grad_norm = nn.utils.clip_grad_norm_(self.actor.parameters(), self.config['agent.max_grad_norm'])\n            self.actor_optimizer.step()\n            \n            self.polyak_update_target()\n            \n            list_actor_loss.append(actor_loss)\n            list_critic_loss.append(critic_loss)\n            Q_vals.append(Qs)\n        self.total_timestep += T\n        \n        out = {}\n        out['actor_loss'] = torch.tensor(list_actor_loss).mean(0).item()\n        out['actor_grad_norm'] = actor_grad_norm\n        out['critic_loss'] = torch.tensor(list_critic_loss).mean(0).item()\n        out['critic_grad_norm'] = critic_grad_norm\n        describe_it = lambda x: describe(numpify(torch.cat(x), 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['Q'] = describe_it(Q_vals)\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n"""
baselines/ddpg_td3/engine.py,1,"b""import time\nfrom itertools import count\n\nimport torch\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\nfrom lagom.utils import color_str\n\n\nclass Engine(BaseEngine):\n    def train(self, n=None, **kwargs):\n        train_logs, eval_logs = [], []\n        checkpoint_count = 0\n        for iteration in count():\n            if self.agent.total_timestep >= self.config['train.timestep']:\n                break\n            t0 = time.perf_counter()\n            \n            if iteration < self.config['replay.init_trial']:\n                [traj] = self.runner(self.random_agent, self.env, 1)\n            else:\n                [traj] = self.runner(self.agent, self.env, 1, mode='train')\n            self.replay.add(traj)\n            # Number of gradient updates = collected episode length\n            out_agent = self.agent.learn(D=None, replay=self.replay, T=traj.T)\n            \n            logger = Logger()\n            logger('train_iteration', iteration+1)\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            [logger(key, value) for key, value in out_agent.items()]\n            logger('episode_return', sum(traj.rewards))\n            logger('episode_horizon', traj.T)\n            logger('accumulated_trained_timesteps', self.agent.total_timestep)\n            train_logs.append(logger.logs)\n            if iteration == 0 or (iteration+1) % self.config['log.freq'] == 0:\n                logger.dump(keys=None, index=0, indent=0, border='-'*50)\n            if self.agent.total_timestep >= int(self.config['train.timestep']*(checkpoint_count/(self.config['checkpoint.num'] - 1))):\n                self.agent.checkpoint(self.logdir, iteration + 1)\n                checkpoint_count += 1\n                \n            if self.agent.total_timestep >= int(self.config['train.timestep']*(len(eval_logs)/(self.config['eval.num'] - 1))):\n                eval_logs.append(self.eval(n=len(eval_logs)))\n        return train_logs, eval_logs\n\n    def eval(self, n=None, **kwargs):\n        t0 = time.perf_counter()\n        with torch.no_grad():\n            D = self.runner(self.agent, self.eval_env, 10, mode='eval')\n        \n        logger = Logger()\n        logger('eval_iteration', n+1)\n        logger('num_seconds', round(time.perf_counter() - t0, 1))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        logger('online_return', describe([sum(traj.rewards) for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe([traj.T for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_return', describe(self.eval_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(self.eval_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger.dump(keys=None, index=0, indent=0, border=color_str('+'*50, color='green'))\n        return logger.logs\n"""
baselines/ddpg_td3/experiment.py,0,"b""import os\nimport gym\n\nfrom lagom import EpisodeRunner\nfrom lagom import RandomAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import RecordEpisodeStatistics\nfrom lagom.envs import TimeStepEnv\n\nfrom baselines.ddpg_td3.ddpg_agent import Agent as DDPGAgent\nfrom baselines.ddpg_td3.td3_agent import Agent as TD3Agent\nfrom baselines.ddpg_td3.engine import Engine\nfrom baselines.ddpg_td3.replay_buffer import ReplayBuffer\n\n\nconfig = Config(\n    {'log.freq': 10,\n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3', 'Swimmer-v3']),\n     \n     'agent.gamma': 0.99,\n     'agent.polyak': 0.995,  # polyak averaging coefficient for targets update\n     'agent.actor.lr': 1e-3, \n     'agent.actor.use_lr_scheduler': False,\n     'agent.critic.lr': 1e-3,\n     'agent.critic.use_lr_scheduler': False,\n     'agent.action_noise': 0.1,\n     'agent.max_grad_norm': 999999,  # grad clipping by norm\n     \n     # TD3 hyperparams\n     'agent.use_td3': True,\n     'agent.target_noise': 0.2,\n     'agent.target_noise_clip': 0.5,\n     'agent.policy_delay': 2,\n     \n     'replay.capacity': 1000000, \n     'replay.init_trial': 10,  # number of random rollouts initially\n     'replay.batch_size': 100,\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'eval.num': 200\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if mode == 'eval':\n        env = RecordEpisodeStatistics(env, deque_size=100)\n    env = TimeStepEnv(env)\n    return env\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    env = make_env(config, seed, 'train')\n    eval_env = make_env(config, seed, 'eval')\n    random_agent = RandomAgent(config, env, device)\n    if config['agent.use_td3']:\n        agent = TD3Agent(config, env, device)\n    else:\n        agent = DDPGAgent(config, env, device)\n    runner = EpisodeRunner()\n    replay = ReplayBuffer(env, config['replay.capacity'], device)\n    engine = Engine(config, agent=agent, random_agent=random_agent, env=env, eval_env=eval_env, runner=runner, replay=replay, logdir=logdir)\n    \n    train_logs, eval_logs = engine.train()\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n    return None  \n    \n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[4153361530, 3503522377, 2876994566, 172236777, 3949341511], \n                   log_dir='logs/default',\n                   max_workers=os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=True,  # GPU much faster, note that performance differs between CPU/GPU\n                   gpu_ids=None)\n"""
baselines/ddpg_td3/replay_buffer.py,0,"b'import numpy as np\nfrom gym.spaces import flatdim\nfrom lagom.utils import tensorify\n\n\nclass ReplayBuffer(object):\n    def __init__(self, env, capacity, device):\n        self.env = env\n        self.capacity = capacity\n        self.device = device\n        \n        self.observations = np.zeros([capacity, flatdim(env.observation_space)], dtype=np.float32)\n        self.actions = np.zeros([capacity, flatdim(env.action_space)], dtype=np.float32)\n        self.rewards = np.zeros([capacity, 1], dtype=np.float32)\n        self.next_observations = np.zeros([capacity, flatdim(env.observation_space)], dtype=np.float32)\n        self.masks = np.zeros([capacity, 1], dtype=np.float32)\n        \n        self.size = 0\n        self.pointer = 0\n        \n    def __len__(self):\n        return self.size\n\n    def _add(self, observation, action, reward, next_observation, terminal):\n        self.observations[self.pointer] = observation\n        self.actions[self.pointer] = action\n        self.rewards[self.pointer] = reward\n        self.next_observations[self.pointer] = next_observation\n        self.masks[self.pointer] = 1. - terminal\n        \n        self.pointer = (self.pointer+1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n        \n    def add(self, traj):\n        for t in range(1, traj.T+1):\n            self._add(traj[t-1].observation, traj.actions[t-1], traj[t].reward, traj[t].observation, traj[t].terminal())\n\n    def sample(self, batch_size):\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return list(map(lambda x: tensorify(x, self.device), [self.observations[idx], \n                                                              self.actions[idx], \n                                                              self.rewards[idx], \n                                                              self.next_observations[idx], \n                                                              self.masks[idx]]))\n'"
baselines/ddpg_td3/td3_agent.py,14,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom gym.spaces import flatdim\nfrom lagom import BaseAgent\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.transform import describe\n\n\nclass Actor(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space), [400, 300])\n        self.action_head = nn.Linear(300, flatdim(env.action_space))\n        \n        assert np.unique(env.action_space.high).size == 1\n        assert -np.unique(env.action_space.low).item() == np.unique(env.action_space.high).item()\n        self.max_action = env.action_space.high[0]\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        x = self.max_action*torch.tanh(self.action_head(x))\n        return x\n\n\nclass Critic(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        # Q1\n        self.first_feature_layers = make_fc(flatdim(env.observation_space) + flatdim(env.action_space), [400, 300])\n        self.first_Q_head = nn.Linear(300, 1)\n        \n        # Q2\n        self.second_feature_layers = make_fc(flatdim(env.observation_space) + flatdim(env.action_space), [400, 300])\n        self.second_Q_head = nn.Linear(300, 1)\n        \n        self.to(self.device)\n        \n    def Q1(self, x, action):\n        x = torch.cat([x, action], dim=-1)\n        for layer in self.first_feature_layers:\n            x = F.relu(layer(x))\n        x = self.first_Q_head(x)\n        return x\n    \n    def Q2(self, x, action):\n        x = torch.cat([x, action], dim=-1)\n        for layer in self.second_feature_layers:\n            x = F.relu(layer(x))\n        x = self.second_Q_head(x)\n        return x\n        \n    def forward(self, x, action):\n        return self.Q1(x, action), self.Q2(x, action)\n    \n    \nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.actor = Actor(config, env, device, **kwargs)\n        self.actor_target = Actor(config, env, device, **kwargs)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config['agent.actor.lr'])\n        \n        self.critic = Critic(config, env, device, **kwargs)\n        self.critic_target = Critic(config, env, device, **kwargs)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config['agent.critic.lr'])\n        \n        self.max_action = env.action_space.high[0]\n        self.total_timestep = 0\n        \n    def polyak_update_target(self):\n        p = self.config['agent.polyak']\n        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n            target_param.data.copy_(p*target_param.data + (1 - p)*param.data)\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(p*target_param.data + (1 - p)*param.data)\n\n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        with torch.no_grad():\n            action = numpify(self.actor(obs).squeeze(0), 'float')\n        if kwargs['mode'] == 'train':\n            eps = np.random.normal(0.0, self.config['agent.action_noise'], size=action.shape)\n            action = np.clip(action + eps, self.env.action_space.low, self.env.action_space.high)\n        out = {}\n        out['raw_action'] = action\n        return out\n\n    def learn(self, D, **kwargs):\n        replay = kwargs['replay']\n        T = kwargs['T']\n        list_actor_loss = []\n        list_critic_loss = []\n        Q1_vals = []\n        Q2_vals = []\n        for i in range(T):\n            observations, actions, rewards, next_observations, masks = replay.sample(self.config['replay.batch_size'])\n            \n            Qs1, Qs2 = self.critic(observations, actions)\n            with torch.no_grad():\n                next_actions = self.actor_target(next_observations)\n                eps = torch.empty_like(next_actions).normal_(0.0, self.config['agent.target_noise'])\n                eps = eps.clamp(-self.config['agent.target_noise_clip'], self.config['agent.target_noise_clip'])\n                next_actions = torch.clamp(next_actions + eps, -self.max_action, self.max_action)\n                next_Qs1, next_Qs2 = self.critic_target(next_observations, next_actions)\n                next_Qs = torch.min(next_Qs1, next_Qs2)\n                targets = rewards + self.config['agent.gamma']*masks*next_Qs\n            critic_loss = F.mse_loss(Qs1, targets.detach()) + F.mse_loss(Qs2, targets.detach())\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            critic_grad_norm = nn.utils.clip_grad_norm_(self.critic.parameters(), self.config['agent.max_grad_norm'])\n            self.critic_optimizer.step()\n            \n            if i % self.config['agent.policy_delay'] == 0:\n                actor_loss = -self.critic.Q1(observations, self.actor(observations)).mean()\n                self.actor_optimizer.zero_grad()\n                self.critic_optimizer.zero_grad()\n                actor_loss.backward()\n                actor_grad_norm = nn.utils.clip_grad_norm_(self.actor.parameters(), self.config['agent.max_grad_norm'])\n                self.actor_optimizer.step()\n                \n                self.polyak_update_target()\n                list_actor_loss.append(actor_loss)\n            list_critic_loss.append(critic_loss)\n            Q1_vals.append(Qs1)\n            Q2_vals.append(Qs2)\n        self.total_timestep += T\n        \n        out = {}\n        out['actor_loss'] = torch.tensor(list_actor_loss).mean(0).item()\n        out['actor_grad_norm'] = actor_grad_norm\n        out['critic_loss'] = torch.tensor(list_critic_loss).mean(0).item()\n        out['critic_grad_norm'] = critic_grad_norm\n        describe_it = lambda x: describe(numpify(torch.cat(x), 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['Q1'] = describe_it(Q1_vals)\n        out['Q2'] = describe_it(Q2_vals)\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n"""
baselines/openaies/__init__.py,0,b''
baselines/openaies/agent.py,2,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gym.spaces as spaces\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.feature_network = MLP(config, env, device, **kwargs)\n        feature_dim = config['nn.sizes'][-1]\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.total_timestep = 0\n        \n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        features = self.feature_network(obs)\n        action_dist = self.action_head(features)\n        action = action_dist.sample()\n        out = {}\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        return out\n    \n    def learn(self, D, **kwargs):\n        pass\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if 'env.normalize_obs' in self.config and self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/openaies/experiment.py,3,"b""from multiprocessing import Pool\nimport time\n\nimport numpy as np\nimport torch\nimport gym\nfrom lagom import Logger\nfrom lagom import EpisodeRunner\nfrom lagom.transform import describe\nfrom lagom.utils import CloudpickleWrapper\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import TimeStepEnv\n\nfrom baselines.openaies.openaies import OpenAIES\nfrom baselines.openaies.agent import Agent\n\n\nconfig = Config(\n    {'log.freq': 10, \n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['Acrobot-v1', 'BipedalWalker-v2', 'Pendulum-v0', 'LunarLanderContinuous-v2']), \n     \n     'nn.sizes': [64, 64],\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.generations': 500,  # total number of ES generations\n     'train.popsize': 32,\n     'train.worker_chunksize': 4,  # must be divisible by popsize\n     'train.mu0': 0.0,\n     'train.std0': 1.0,\n     'train.lr': 1e-2,\n     'train.lr_decay': 1.0,\n     'train.min_lr': 1e-6,\n     'train.sigma_scheduler_args': [1.0, 0.01, 400, 0],\n     'train.antithetic': False,\n     'train.rank_transform': True\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if config['env.clip_action'] and isinstance(env.action_space, gym.spaces.Box):\n        env = gym.wrappers.ClipAction(env)  # TODO: use tanh to squash policy output when RescaleAction wrapper merged in gym\n    env = TimeStepEnv(env)\n    return env\n\n\ndef fitness(data):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    config, seed, device, param = data\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    agent.from_vec(tensorify(param, 'cpu'))\n    runner = EpisodeRunner()\n    with torch.no_grad():\n        D = runner(agent, env, 10)\n    R = np.mean([sum(traj.rewards) for traj in D])\n    H = np.mean([traj.T for traj in D])\n    return R, H\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    \n    print('Initializing...')\n    agent = Agent(config, make_env(config, seed, 'eval'), device)\n    es = OpenAIES([config['train.mu0']]*agent.num_params, config['train.std0'], \n                  {'popsize': config['train.popsize'], \n                   'seed': seed, \n                   'sigma_scheduler_args': config['train.sigma_scheduler_args'],\n                   'lr': config['train.lr'],\n                   'lr_decay': config['train.lr_decay'],\n                   'min_lr': config['train.min_lr'],\n                   'antithetic': config['train.antithetic'],\n                   'rank_transform': config['train.rank_transform']})\n    train_logs = []\n    checkpoint_count = 0\n    with Pool(processes=config['train.popsize']//config['train.worker_chunksize']) as pool:\n        print('Finish initialization. Training starts...')\n        for generation in range(config['train.generations']):\n            t0 = time.perf_counter()\n            solutions = es.ask()\n            data = [(config, seed, device, solution) for solution in solutions]\n            out = pool.map(CloudpickleWrapper(fitness), data, chunksize=config['train.worker_chunksize'])\n            Rs, Hs = zip(*out)\n            es.tell(solutions, [-R for R in Rs])\n            logger = Logger()\n            logger('generation', generation+1)\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            logger('Returns', describe(Rs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('Horizons', describe(Hs, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('fbest', es.result.fbest)\n            train_logs.append(logger.logs)\n            if generation == 0 or (generation+1) % config['log.freq'] == 0:\n                logger.dump(keys=None, index=0, indent=0, border='-'*50)\n            if (generation+1) >= int(config['train.generations']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n                agent.from_vec(tensorify(es.result.xbest, 'cpu'))\n                agent.checkpoint(logdir, generation+1)\n                checkpoint_count += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    return None\n\n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=7,  # tune to fulfill computation power\n                   chunksize=1, \n                   use_gpu=False,\n                   gpu_ids=None)\n"""
baselines/openaies/openaies.py,3,"b'from collections import namedtuple\n\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\n\nfrom lagom import BaseES\nfrom lagom.transform import rank_transform\nfrom lagom.transform import LinearSchedule\n\n\nclass OpenAIES(BaseES):\n    r""""""Implements OpenAI evolution strategies.\n    \n    .. note::\n    \n        In practice, the learning rate is better to be proportional to the batch size.\n        i.e. for larger batch size, use larger learning rate and vise versa. \n        \n    Args:\n        x0 (ndarray): initial mean\n        sigma0 (float): initial standard deviation\n        popsize (int): population size\n        sigma_scheduler_args (list): arguments for linear scheduling of standard deviation\n        lr (float): learning rate\n        lr_decay (float): learning rate decay\n        min_lr (float): minumum of learning rate\n        antithetic (bool): If True, then use antithetic sampling to generate population.\n        rank_transform (bool): If True, then use rank transformation of fitness (combat with outliers). \n        \n    """"""\n    def __init__(self, x0, sigma0, opts=None):\n        self.x0 = x0\n        self.sigma0 = sigma0\n        self.popsize = opts[\'popsize\']\n        self.sigma_scheduler = LinearSchedule(*opts[\'sigma_scheduler_args\'])\n        self.lr = opts[\'lr\']\n        self.lr_decay = opts[\'lr_decay\']\n        self.min_lr = opts[\'min_lr\']\n        self.antithetic = opts[\'antithetic\']\n        if self.antithetic:\n            assert self.popsize % 2 == 0, \'popsize must be even for antithetic sampling. \'\n        self.rank_transform = opts[\'rank_transform\']\n        \n        self.seed = opts[\'seed\'] if \'seed\' in opts else np.random.randint(1, 2**32)\n        self.np_random = np.random.RandomState(self.seed)\n        \n        self.iter = 0\n\n        # initialize mean and std\n        self.x = torch.from_numpy(np.asarray(x0)).float()\n        self.x.requires_grad = True\n        self.shape = tuple(self.x.shape)\n        if np.isscalar(sigma0):\n            self.sigma = np.full(self.shape, sigma0, dtype=np.float32)\n        else:\n            self.sigma = np.asarray(sigma0).astype(np.float32)\n        \n        self.optimizer = optim.Adam([self.x], lr=self.lr)\n        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.lr_decay)\n        \n        self.xbest = None\n        self.fbest = None\n\n    def ask(self):\n        if self.antithetic:\n            eps = self.np_random.randn(self.popsize//2, *self.shape)\n            eps = np.concatenate([eps, -eps], axis=0)\n        else:\n            eps = self.np_random.randn(self.popsize, *self.shape)\n        self.eps = eps\n        \n        solutions = self.x.detach().cpu().numpy() + self.eps*self.sigma\n        return solutions\n        \n    def tell(self, solutions, function_values):\n        solutions = np.asarray(solutions).astype(np.float32)\n        function_values = np.asarray(function_values).astype(np.float32)\n        if self.rank_transform:\n            original_function_values = np.copy(function_values)\n            function_values = rank_transform(function_values, centered=True)  # center: combat outliers\n        idx = np.argsort(function_values)\n        self.xbest = solutions[idx[0]]\n        if self.rank_transform:  # record original function values\n            self.fbest = original_function_values[idx[0]]\n        else:\n            self.fbest = function_values[idx[0]]\n        \n        # Enforce fitness as Gaussian distributed, also for centered ranks\n        F = (function_values - function_values.mean(-1))/(function_values.std(-1) + 1e-8)\n        # Compute gradient, F:[popsize], eps: [popsize, num_params]\n        self.optimizer.zero_grad()\n        grad = (1/self.sigma)*np.mean(np.expand_dims(F, 1)*self.eps, axis=0)\n        grad = torch.from_numpy(grad).float()\n        self.x.grad = grad\n        self.optimizer.step()\n        self.lr_scheduler.step()\n        \n        self.iter += 1\n        self.sigma = self.sigma_scheduler(self.iter)\n        \n    @property\n    def result(self):\n        OpenAIESResult = namedtuple(\'OpenAIESResult\', \n                                    [\'xbest\', \'fbest\', \'evals_best\', \'evaluations\', \'iterations\', \'xfavorite\', \'stds\'],\n                                    defaults=[None]*7)\n        result = OpenAIESResult(xbest=self.xbest, fbest=self.fbest, iterations=self.iter, xfavorite=self.x.detach().cpu().numpy(), stds=self.sigma)\n        return result\n        \n    def __repr__(self):\n        return f\'OpenAIES in dimension {len(self.x0)} (seed={self.seed})\'\n'"
baselines/ppo/__init__.py,0,b''
baselines/ppo/agent.py,15,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport gym.spaces as spaces\n\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.metric import bootstrapped_returns\nfrom lagom.metric import gae\nfrom lagom.transform import explained_variance as ev\nfrom lagom.transform import describe\n\nfrom torch.utils.data import DataLoader\nfrom baselines.ppo.dataset import Dataset\n\n\nclass Actor(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity='tanh', constant_bias=0.0)\n        \n        feature_dim = config['nn.sizes'][-1]\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer in self.feature_layers:\n            x = torch.tanh(layer(x))\n        action_dist = self.action_head(x)\n        return action_dist\n\n\nclass Critic(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n        \n        feature_dim = config['nn.sizes'][-1]\n        self.V_head = nn.Linear(feature_dim, 1)\n        ortho_init(self.V_head, weight_scale=1.0, constant_bias=0.0)\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        V = self.V_head(x)\n        return V\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.policy = Actor(config, env, device, **kwargs)\n        self.value = Critic(config, env, device, **kwargs)\n        \n        self.total_timestep = 0\n        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=config['agent.policy_lr'])\n        self.value_optimizer = optim.Adam(self.value.parameters(), lr=config['agent.value_lr'])\n        if config['agent.use_lr_scheduler']:\n            self.policy_lr_scheduler = linear_lr_scheduler(self.policy_optimizer, config['train.timestep'], min_lr=1e-8)\n        \n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        action_dist = self.policy(obs)\n        V = self.value(obs)\n        action = action_dist.sample()\n        out = {}\n        out['action_dist'] = action_dist\n        out['V'] = V\n        out['entropy'] = action_dist.entropy()\n        out['action'] = action\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        out['action_logprob'] = action_dist.log_prob(action.detach())\n        return out\n    \n    def learn_one_update(self, data):\n        data = [d.detach().to(self.device) for d in data]\n        observations, old_actions, old_logprobs, old_entropies, old_Vs, old_Qs, old_As = data\n        \n        action_dist = self.policy(observations)\n        logprobs = action_dist.log_prob(old_actions).squeeze()\n        entropies = action_dist.entropy().squeeze()\n        Vs = self.value(observations).squeeze()\n        assert all([x.ndim == 1 for x in [logprobs, entropies, Vs]])\n        \n        ratio = torch.exp(logprobs - old_logprobs)\n        eps = self.config['agent.clip_range']\n        policy_loss = -torch.min(ratio*old_As, \n                                 torch.clamp(ratio, 1.0 - eps, 1.0 + eps)*old_As)\n        policy_loss = policy_loss.mean(0)\n        \n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        policy_grad_norm = nn.utils.clip_grad_norm_(self.policy.parameters(), self.config['agent.max_grad_norm'])\n        self.policy_optimizer.step()\n        if self.config['agent.use_lr_scheduler']:\n            self.policy_lr_scheduler.step(self.total_timestep)\n        \n        clipped_Vs = old_Vs + torch.clamp(Vs - old_Vs, -eps, eps)\n        value_loss = torch.max(F.mse_loss(Vs, old_Qs, reduction='none'), \n                               F.mse_loss(clipped_Vs, old_Qs, reduction='none'))\n        value_loss = value_loss.mean(0)\n        \n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        value_grad_norm = nn.utils.clip_grad_norm_(self.value.parameters(), self.config['agent.max_grad_norm'])\n        self.value_optimizer.step()\n        \n        out = {}\n        out['policy_grad_norm'] = policy_grad_norm\n        out['value_grad_norm'] = value_grad_norm\n        out['policy_loss'] = policy_loss.item()\n        out['policy_entropy'] = entropies.mean().item()\n        out['value_loss'] = value_loss.item()\n        out['explained_variance'] = ev(y_true=numpify(old_Qs, 'float'), y_pred=numpify(Vs, 'float'))\n        out['approx_kl'] = (old_logprobs - logprobs).mean(0).item()\n        out['clip_frac'] = ((ratio < 1.0 - eps) | (ratio > 1.0 + eps)).float().mean(0).item()\n        return out\n        \n    def learn(self, D, **kwargs):\n        logprobs = [torch.cat(traj.get_infos('action_logprob')) for traj in D]\n        entropies = [torch.cat(traj.get_infos('entropy')) for traj in D]\n        Vs = [torch.cat(traj.get_infos('V')) for traj in D]\n        with torch.no_grad():\n            last_observations = tensorify([traj[-1].observation for traj in D], self.device)\n            last_Vs = self.value(last_observations).squeeze(-1)\n        Qs = [bootstrapped_returns(self.config['agent.gamma'], traj.rewards, last_V, traj.reach_terminal)\n              for traj, last_V in zip(D, last_Vs)]\n        As = [gae(self.config['agent.gamma'], self.config['agent.gae_lambda'], traj.rewards, V, last_V, traj.reach_terminal)\n              for traj, V, last_V in zip(D, Vs, last_Vs)]\n        \n        # Metrics -> Tensor, device\n        logprobs, entropies, Vs = map(lambda x: torch.cat(x).squeeze(), [logprobs, entropies, Vs])\n        Qs, As = map(lambda x: tensorify(np.concatenate(x).copy(), self.device), [Qs, As])\n        if self.config['agent.standardize_adv']:\n            As = (As - As.mean())/(As.std() + 1e-4)\n        assert all([x.ndim == 1 for x in [logprobs, entropies, Vs, Qs, As]])\n        \n        dataset = Dataset(D, logprobs, entropies, Vs, Qs, As)\n        dataloader = DataLoader(dataset, self.config['train.batch_size'], shuffle=True)\n        for epoch in range(self.config['train.num_epochs']):\n            logs = [self.learn_one_update(data) for data in dataloader]\n\n        self.total_timestep += sum([traj.T for traj in D])\n        out = {}\n        if self.config['agent.use_lr_scheduler']:\n            out['current_lr'] = self.policy_lr_scheduler.get_lr()\n        out['policy_grad_norm'] = np.mean([item['policy_grad_norm'] for item in logs])\n        out['value_grad_norm'] = np.mean([item['value_grad_norm'] for item in logs])\n        out['policy_loss'] = np.mean([item['policy_loss'] for item in logs])\n        out['policy_entropy'] = np.mean([item['policy_entropy'] for item in logs])\n        out['value_loss'] = np.mean([item['value_loss'] for item in logs])\n        out['explained_variance'] = np.mean([item['explained_variance'] for item in logs])\n        out['approx_kl'] = np.mean([item['approx_kl'] for item in logs])\n        out['clip_frac'] = np.mean([item['clip_frac'] for item in logs])\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/ppo/dataset.py,1,"b'import numpy as np\n\nfrom torch.utils import data\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, D, logprobs, entropies, Vs, Qs, As):\n        self.D = D\n        self.observations = np.concatenate([np.stack(traj.observations[:-1], 0) for traj in D], 0).astype(np.float32)\n        self.actions = np.concatenate([np.stack(traj.actions, 0) for traj in D], 0).astype(np.float32)\n        self.logprobs = logprobs\n        self.entropies = entropies\n        self.Vs = Vs\n        self.Qs = Qs\n        self.As = As\n        assert self.observations.shape[0] == len(self) and self.actions.shape[0] == len(self)\n        assert all([i.shape == (len(self),) for i in [self.logprobs, self.entropies, self.Vs, self.Qs, self.As]])\n\n    def __len__(self):\n        return sum([traj.T for traj in self.D])\n\n    def __getitem__(self, i):\n        D = (self.observations[i], self.actions[i], self.logprobs[i], \n             self.entropies[i], self.Vs[i], self.Qs[i], self.As[i])\n        return D\n'"
baselines/ppo/engine.py,0,"b""import time\n\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\n\n\nclass Engine(BaseEngine):        \n    def train(self, n=None, **kwargs):\n        self.agent.train()\n        t0 = time.perf_counter()\n        \n        D = self.runner(self.agent, self.env, self.config['train.timestep_per_iter'])\n        out_agent = self.agent.learn(D) \n        \n        logger = Logger()\n        logger('train_iteration', n+1)\n        logger('num_seconds', round(time.perf_counter() - t0, 1))\n        [logger(key, value) for key, value in out_agent.items()]\n        logger('num_trajectories', len(D))\n        logger('num_timesteps', sum([traj.T for traj in D]))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        logger('return', describe([sum(traj.rewards) for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        \n        E = [traj[-1].info['episode'] for traj in D if 'episode' in traj[-1].info]\n        logger('online_return', describe([e['return'] for e in E], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe([e['horizon'] for e in E], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_return', describe(self.env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(self.env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        return logger\n        \n    def eval(self, n=None, **kwargs):\n        pass\n"""
baselines/ppo/experiment.py,0,"b""import os\nfrom itertools import count\nimport gym\n\nfrom lagom import StepRunner\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import RecordEpisodeStatistics\nfrom lagom.envs import NormalizeObservation\nfrom lagom.envs import NormalizeReward\nfrom lagom.envs import TimeStepEnv\n\nfrom baselines.ppo.agent import Agent\nfrom baselines.ppo.engine import Engine\n\n\nconfig = Config(\n    {'log.freq': 10, \n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3', 'Swimmer-v3']), \n     'env.normalize_obs': True,\n     'env.normalize_reward': True,\n     \n     'nn.sizes': [64, 64],\n     \n     'agent.policy_lr': 3e-4,\n     'agent.use_lr_scheduler': True,\n     'agent.value_lr': 1e-3,\n     'agent.gamma': 0.99,\n     'agent.gae_lambda': 0.95,\n     'agent.standardize_adv': True,  # standardize advantage estimates\n     'agent.max_grad_norm': 0.5,  # grad clipping by norm\n     'agent.clip_range': 0.2,  # ratio clipping\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'train.timestep_per_iter': 2048,  # number of timesteps per iteration\n     'train.batch_size': 64,\n     'train.num_epochs': 10\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if config['env.clip_action'] and isinstance(env.action_space, gym.spaces.Box):\n        env = gym.wrappers.ClipAction(env)\n    if mode == 'train':\n        env = RecordEpisodeStatistics(env, deque_size=100)\n        if config['env.normalize_obs']:\n            env = NormalizeObservation(env, clip=5.)\n        if config['env.normalize_reward']:\n            env = NormalizeReward(env, clip=10., gamma=config['agent.gamma'])\n    env = TimeStepEnv(env)\n    return env\n    \n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    runner = StepRunner(reset_on_call=False)\n    engine = Engine(config, agent=agent, env=env, runner=runner)\n    train_logs = []\n    checkpoint_count = 0\n    for i in count():\n        if agent.total_timestep >= config['train.timestep']:\n            break\n        train_logger = engine.train(i)\n        train_logs.append(train_logger.logs)\n        if i == 0 or (i+1) % config['log.freq'] == 0:\n            train_logger.dump(keys=None, index=0, indent=0, border='-'*50)\n        if agent.total_timestep >= int(config['train.timestep']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n            agent.checkpoint(logdir, i + 1)\n            checkpoint_count += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    return None\n    \n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=False,  # CPU a bit faster\n                   gpu_ids=None)\n"""
baselines/sac/__init__.py,0,b''
baselines/sac/agent.py,24,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Independent\nfrom torch.distributions import Normal\nfrom torch.distributions import TransformedDistribution\nfrom torch.distributions import Transform\nfrom torch.distributions import constraints\n\nfrom gym.spaces import flatdim\nfrom lagom import BaseAgent\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.transform import describe\n\n\n# TODO: import from PyTorch when PR merged: https://github.com/pytorch/pytorch/pull/19785\nclass TanhTransform(Transform):\n    r""""""Transform via the mapping :math:`y = \\tanh(x)`.""""""\n    domain = constraints.real\n    codomain = constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    @staticmethod\n    def atanh(x):\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        return x.tanh()\n\n    def _inverse(self, y):\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2. * (np.log(2.) - x - F.softplus(-2. * x))\n    \n\nclass Actor(Module):\n    LOGSTD_MAX = 2\n    LOGSTD_MIN = -20\n\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space), [256, 256])\n        self.mean_head = nn.Linear(256, flatdim(env.action_space))\n        self.logstd_head = nn.Linear(256, flatdim(env.action_space))\n        \n        self.to(device)\n\n    def forward(self, x):\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        mean = self.mean_head(x)\n        logstd = self.logstd_head(x)\n        logstd = torch.tanh(logstd)\n        logstd = self.LOGSTD_MIN + 0.5*(self.LOGSTD_MAX - self.LOGSTD_MIN)*(1 + logstd)\n        std = torch.exp(logstd)\n        dist = TransformedDistribution(Independent(Normal(mean, std), 1), [TanhTransform(cache_size=1)])\n        return dist\n    \n    def mean_forward(self, x):\n        for layer in self.feature_layers:\n            x = F.relu(layer(x))\n        mean = self.mean_head(x)\n        return mean\n\n\nclass Critic(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        # Q1\n        self.first_feature_layers = make_fc(flatdim(env.observation_space) + flatdim(env.action_space), [256, 256])\n        self.first_Q_head = nn.Linear(256, 1)\n        \n        # Q2\n        self.second_feature_layers = make_fc(flatdim(env.observation_space) + flatdim(env.action_space), [256, 256])\n        self.second_Q_head = nn.Linear(256, 1)\n        \n        self.to(self.device)\n        \n    def Q1(self, x, action):\n        x = torch.cat([x, action], dim=-1)\n        for layer in self.first_feature_layers:\n            x = F.relu(layer(x))\n        x = self.first_Q_head(x)\n        return x\n    \n    def Q2(self, x, action):\n        x = torch.cat([x, action], dim=-1)\n        for layer in self.second_feature_layers:\n            x = F.relu(layer(x))\n        x = self.second_Q_head(x)\n        return x\n        \n    def forward(self, x, action):\n        return self.Q1(x, action), self.Q2(x, action)\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        self.actor = Actor(config, env, device, **kwargs)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config[\'agent.actor.lr\'])\n        \n        self.critic = Critic(config, env, device, **kwargs)\n        self.critic_target = Critic(config, env, device, **kwargs)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config[\'agent.critic.lr\'])\n        \n        self.target_entropy = -float(flatdim(env.action_space))\n        self.log_alpha = nn.Parameter(torch.tensor(np.log(config[\'agent.initial_temperature\'])).to(device))\n        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=config[\'agent.actor.lr\'])\n        \n        self.optimizer_zero_grad = lambda: [opt.zero_grad() for opt in [self.actor_optimizer, \n                                                                        self.critic_optimizer, \n                                                                        self.log_alpha_optimizer]]\n        self.total_timestep = 0\n        \n    @property\n    def alpha(self):\n        return self.log_alpha.exp()\n\n    def polyak_update_target(self):\n        p = self.config[\'agent.polyak\']\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(p*target_param.data + (1 - p)*param.data)\n\n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        with torch.no_grad():\n            if kwargs[\'mode\'] == \'train\':\n                action = numpify(self.actor(obs).sample(), \'float\')\n            elif kwargs[\'mode\'] == \'eval\':\n                action = numpify(torch.tanh(self.actor.mean_forward(obs)), \'float\')\n        out = {}\n        out[\'raw_action\'] = action.squeeze(0)\n        return out\n\n    def learn(self, D, **kwargs):\n        replay = kwargs[\'replay\']\n        T = kwargs[\'T\']\n        list_actor_loss = []\n        list_critic_loss = []\n        list_alpha_loss = []\n        Q1_vals = []\n        Q2_vals = []\n        logprob_vals = []\n        for i in range(T):\n            observations, actions, rewards, next_observations, masks = replay.sample(self.config[\'replay.batch_size\'])\n            \n            Qs1, Qs2 = self.critic(observations, actions)\n            with torch.no_grad():\n                action_dist = self.actor(next_observations)\n                next_actions = action_dist.rsample()\n                next_actions_logprob = action_dist.log_prob(next_actions).unsqueeze(-1)\n                next_Qs1, next_Qs2 = self.critic_target(next_observations, next_actions)\n                next_Qs = torch.min(next_Qs1, next_Qs2) - self.alpha.detach()*next_actions_logprob\n                targets = rewards + self.config[\'agent.gamma\']*masks*next_Qs\n            critic_loss = F.mse_loss(Qs1, targets.detach()) + F.mse_loss(Qs2, targets.detach())\n            self.optimizer_zero_grad()\n            critic_loss.backward()\n            critic_grad_norm = nn.utils.clip_grad_norm_(self.critic.parameters(), self.config[\'agent.max_grad_norm\'])\n            self.critic_optimizer.step()\n            \n            action_dist = self.actor(observations)\n            policy_actions = action_dist.rsample()\n            policy_actions_logprob = action_dist.log_prob(policy_actions).unsqueeze(-1)\n            actor_Qs1, actor_Qs2 = self.critic(observations, policy_actions)\n            actor_Qs = torch.min(actor_Qs1, actor_Qs2)\n            actor_loss = torch.mean(self.alpha.detach()*policy_actions_logprob - actor_Qs)\n            self.optimizer_zero_grad()\n            actor_loss.backward()\n            actor_grad_norm = nn.utils.clip_grad_norm_(self.actor.parameters(), self.config[\'agent.max_grad_norm\'])\n            self.actor_optimizer.step()\n            \n            alpha_loss = torch.mean(self.log_alpha*(-policy_actions_logprob - self.target_entropy).detach())\n            self.optimizer_zero_grad()\n            alpha_loss.backward()\n            self.log_alpha_optimizer.step()\n\n            self.polyak_update_target()\n            list_actor_loss.append(actor_loss)\n            list_critic_loss.append(critic_loss)\n            list_alpha_loss.append(alpha_loss)\n            Q1_vals.append(Qs1)\n            Q2_vals.append(Qs2)\n            logprob_vals.append(policy_actions_logprob)\n        self.total_timestep += T\n        \n        out = {}\n        out[\'actor_loss\'] = torch.tensor(list_actor_loss).mean(0).item()\n        out[\'actor_grad_norm\'] = actor_grad_norm\n        out[\'critic_loss\'] = torch.tensor(list_critic_loss).mean(0).item()\n        out[\'critic_grad_norm\'] = critic_grad_norm\n        describe_it = lambda x: describe(numpify(torch.cat(x), \'float\').squeeze(), axis=-1, repr_indent=1, repr_prefix=\'\\n\')\n        out[\'Q1\'] = describe_it(Q1_vals)\n        out[\'Q2\'] = describe_it(Q2_vals)\n        out[\'logprob\'] = describe_it(logprob_vals)\n        out[\'alpha_loss\'] = torch.tensor(list_alpha_loss).mean(0).item()\n        out[\'alpha\'] = self.alpha.item()\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f\'agent_{num_iter}.pth\')\n'"
baselines/sac/engine.py,1,"b""import time\nfrom itertools import count\n\nimport torch\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\nfrom lagom.utils import color_str\n\n\nclass Engine(BaseEngine):\n    def train(self, n=None, **kwargs):\n        train_logs, eval_logs = [], []\n        checkpoint_count = 0\n        for iteration in count():\n            if self.agent.total_timestep >= self.config['train.timestep']:\n                break\n            t0 = time.perf_counter()\n            \n            if iteration < self.config['replay.init_trial']:\n                [traj] = self.runner(self.random_agent, self.env, 1)\n            else:\n                [traj] = self.runner(self.agent, self.env, 1, mode='train')\n            self.replay.add(traj)\n            # Number of gradient updates = collected episode length\n            out_agent = self.agent.learn(D=None, replay=self.replay, T=traj.T)\n            \n            logger = Logger()\n            logger('train_iteration', iteration+1)\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            [logger(key, value) for key, value in out_agent.items()]\n            logger('episode_return', sum(traj.rewards))\n            logger('episode_horizon', traj.T)\n            logger('accumulated_trained_timesteps', self.agent.total_timestep)\n            train_logs.append(logger.logs)\n            if iteration == 0 or (iteration+1) % self.config['log.freq'] == 0:\n                logger.dump(keys=None, index=0, indent=0, border='-'*50)\n            if self.agent.total_timestep >= int(self.config['train.timestep']*(checkpoint_count/(self.config['checkpoint.num'] - 1))):\n                self.agent.checkpoint(self.logdir, iteration + 1)\n                checkpoint_count += 1\n                \n            if self.agent.total_timestep >= int(self.config['train.timestep']*(len(eval_logs)/(self.config['eval.num'] - 1))):\n                eval_logs.append(self.eval(n=len(eval_logs)))\n        return train_logs, eval_logs\n\n    def eval(self, n=None, **kwargs):\n        t0 = time.perf_counter()\n        with torch.no_grad():\n            D = self.runner(self.agent, self.eval_env, 10, mode='eval')\n        \n        logger = Logger()\n        logger('eval_iteration', n+1)\n        logger('num_seconds', round(time.perf_counter() - t0, 1))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        logger('online_return', describe([sum(traj.rewards) for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe([traj.T for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_return', describe(self.eval_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(self.eval_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger.dump(keys=None, index=0, indent=0, border=color_str('+'*50, color='green'))\n        return logger.logs\n"""
baselines/sac/experiment.py,0,"b""import os\nimport gym\n\nfrom lagom import EpisodeRunner\nfrom lagom import RandomAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import RecordEpisodeStatistics\nfrom lagom.envs import TimeStepEnv\n\n# TODO: replace it with official gym wrapper\nfrom .normalize_action import NormalizeAction\n\nfrom baselines.sac.agent import Agent\nfrom baselines.sac.engine import Engine\nfrom baselines.sac.replay_buffer import ReplayBuffer\n\n\nconfig = Config(\n    {'log.freq': 10,\n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3', 'Swimmer-v3']),\n     \n     'agent.gamma': 0.99,\n     'agent.polyak': 0.995,  # polyak averaging coefficient for targets update\n     'agent.actor.lr': 3e-4, \n     'agent.actor.use_lr_scheduler': False,\n     'agent.critic.lr': 3e-4,\n     'agent.critic.use_lr_scheduler': False,\n     'agent.initial_temperature': 1.0,\n     'agent.max_grad_norm': 999999,  # grad clipping by norm\n     \n     'replay.capacity': 1000000, \n     'replay.init_trial': 10,  # number of random rollouts initially\n     'replay.batch_size': 256,\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'eval.num': 200\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    env = NormalizeAction(env)  # TODO: use gym new wrapper RescaleAction when it's merged\n    if mode == 'eval':\n        env = RecordEpisodeStatistics(env, deque_size=100)\n    env = TimeStepEnv(env)\n    return env\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    env = make_env(config, seed, 'train')\n    eval_env = make_env(config, seed, 'eval')\n    random_agent = RandomAgent(config, env, device)\n    agent = Agent(config, env, device)\n    runner = EpisodeRunner()\n    replay = ReplayBuffer(env, config['replay.capacity'], device)\n    engine = Engine(config, agent=agent, random_agent=random_agent, env=env, eval_env=eval_env, runner=runner, replay=replay, logdir=logdir)\n    \n    train_logs, eval_logs = engine.train()\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n    return None  \n    \n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[4153361530, 3503522377, 2876994566, 172236777, 3949341511], \n                   log_dir='logs/default',\n                   max_workers=os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=True,  # GPU much faster, note that performance differs between CPU/GPU\n                   gpu_ids=None)\n"""
baselines/sac/normalize_action.py,0,"b'import gym\nfrom gym import spaces\nimport numpy as np\n\n\n# TODO: remove it after new wrapper merged into gym officially\nclass NormalizeAction(gym.ActionWrapper):\n    r""""""Rescale the continuous action space of the environment from [-1, 1]. """"""\n    def __init__(self, env):\n        assert isinstance(env.action_space, spaces.Box), \'expected Box action space.\'\n        super().__init__(env)\n\n    def action(self, action):\n        assert np.all(action >= -1.0) and np.all(action <= 1.0), \'expected range within [-1, 1], use tanh\'\n        low = self.env.action_space.low\n        high = self.env.action_space.high\n        action = low + (1.0 + action)*(high - low)/2.0\n        action = np.clip(action, low, high)\n        return action\n'"
baselines/sac/replay_buffer.py,0,"b'import numpy as np\nfrom gym.spaces import flatdim\nfrom lagom.utils import tensorify\n\n\nclass ReplayBuffer(object):\n    def __init__(self, env, capacity, device):\n        self.env = env\n        self.capacity = capacity\n        self.device = device\n        \n        self.observations = np.zeros([capacity, flatdim(env.observation_space)], dtype=np.float32)\n        self.actions = np.zeros([capacity, flatdim(env.action_space)], dtype=np.float32)\n        self.rewards = np.zeros([capacity, 1], dtype=np.float32)\n        self.next_observations = np.zeros([capacity, flatdim(env.observation_space)], dtype=np.float32)\n        self.masks = np.zeros([capacity, 1], dtype=np.float32)\n        \n        self.size = 0\n        self.pointer = 0\n        \n    def __len__(self):\n        return self.size\n\n    def _add(self, observation, action, reward, next_observation, terminal):\n        self.observations[self.pointer] = observation\n        self.actions[self.pointer] = action\n        self.rewards[self.pointer] = reward\n        self.next_observations[self.pointer] = next_observation\n        self.masks[self.pointer] = 1. - terminal\n        \n        self.pointer = (self.pointer+1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n        \n    def add(self, traj):\n        for t in range(1, traj.T+1):\n            self._add(traj[t-1].observation, traj.actions[t-1], traj[t].reward, traj[t].observation, traj[t].terminal())\n\n    def sample(self, batch_size):\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return list(map(lambda x: tensorify(x, self.device), [self.observations[idx], \n                                                              self.actions[idx], \n                                                              self.rewards[idx], \n                                                              self.next_observations[idx], \n                                                              self.masks[idx]]))\n'"
baselines/vpg/__init__.py,0,b''
baselines/vpg/agent.py,7,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport gym.spaces as spaces\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.metric import bootstrapped_returns\nfrom lagom.metric import gae\nfrom lagom.transform import explained_variance as ev\nfrom lagom.transform import describe\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(spaces.flatdim(env.observation_space), config['nn.sizes'])\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        feature_dim = config['nn.sizes'][-1]\n        self.feature_network = MLP(config, env, device, **kwargs)\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.V_head = nn.Linear(feature_dim, 1)\n        ortho_init(self.V_head, weight_scale=1.0, constant_bias=0.0)\n        self.V_head = self.V_head.to(device)  # reproducible between CPU/GPU, ortho_init behaves differently\n        \n        self.total_timestep = 0\n        self.optimizer = optim.Adam(self.parameters(), lr=config['agent.lr'])\n        if config['agent.use_lr_scheduler']:\n            self.lr_scheduler = linear_lr_scheduler(self.optimizer, config['train.timestep'], min_lr=1e-8)\n        \n    def choose_action(self, x, **kwargs):\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        features = self.feature_network(obs)\n        action_dist = self.action_head(features)\n        V = self.V_head(features)\n        action = action_dist.sample()\n        out = {}\n        out['action_dist'] = action_dist\n        out['V'] = V\n        out['entropy'] = action_dist.entropy()\n        out['action'] = action\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        out['action_logprob'] = action_dist.log_prob(action.detach())\n        return out\n    \n    def learn(self, D, **kwargs):\n        logprobs = [torch.cat(traj.get_infos('action_logprob')) for traj in D]\n        entropies = [torch.cat(traj.get_infos('entropy')) for traj in D]\n        Vs = [torch.cat(traj.get_infos('V')) for traj in D]\n        last_Vs = [traj.extra_info['last_info']['V'] for traj in D]\n        Qs = [bootstrapped_returns(self.config['agent.gamma'], traj.rewards, last_V, traj.reach_terminal)\n              for traj, last_V in zip(D, last_Vs)]\n        As = [gae(self.config['agent.gamma'], self.config['agent.gae_lambda'], traj.rewards, V, last_V, traj.reach_terminal)\n              for traj, V, last_V in zip(D, Vs, last_Vs)]\n        \n        # Metrics -> Tensor, device\n        logprobs, entropies, Vs = map(lambda x: torch.cat(x).squeeze(), [logprobs, entropies, Vs])\n        Qs, As = map(lambda x: tensorify(np.concatenate(x).copy(), self.device), [Qs, As])\n        if self.config['agent.standardize_adv']:\n            As = (As - As.mean())/(As.std() + 1e-4)\n        assert all([x.ndim == 1 for x in [logprobs, entropies, Vs, Qs, As]])\n        \n        # Loss\n        policy_loss = -logprobs*As.detach()\n        entropy_loss = -entropies\n        value_loss = F.mse_loss(Vs, Qs, reduction='none')\n        loss = policy_loss + self.config['agent.value_coef']*value_loss + self.config['agent.entropy_coef']*entropy_loss\n        loss = loss.mean()\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        grad_norm = nn.utils.clip_grad_norm_(self.parameters(), self.config['agent.max_grad_norm'])\n        self.optimizer.step()\n        if self.config['agent.use_lr_scheduler']:\n            self.lr_scheduler.step(self.total_timestep)\n        self.total_timestep += sum([traj.T for traj in D])\n        \n        out = {}\n        if self.config['agent.use_lr_scheduler']:\n            out['current_lr'] = self.lr_scheduler.get_lr()\n        out['loss'] = loss.item()\n        out['grad_norm'] = grad_norm\n        out['policy_loss'] = policy_loss.mean().item()\n        out['entropy_loss'] = entropy_loss.mean().item()\n        out['policy_entropy'] = -out['entropy_loss']\n        out['value_loss'] = value_loss.mean().item()\n        out['V'] = describe(numpify(Vs, 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['explained_variance'] = ev(y_true=numpify(Qs, 'float'), y_pred=numpify(Vs, 'float'))\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/vpg/agent_lstm.py,9,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport gym.spaces as spaces\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.networks import Module\nfrom lagom.networks import make_lnlstm\nfrom lagom.networks import ortho_init\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.metric import bootstrapped_returns\nfrom lagom.metric import gae\nfrom lagom.transform import explained_variance as ev\nfrom lagom.transform import describe\n\n\nclass FeatureNet(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.lstm = make_lnlstm(spaces.flatdim(env.observation_space), config['rnn.size'], num_layers=1)\n        \n        self.to(self.device)\n        \n    def forward(self, x, states):\n        return self.lstm(x, states)\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        feature_dim = config['rnn.size']\n        self.feature_network = FeatureNet(config, env, device, **kwargs)\n        if isinstance(env.action_space, spaces.Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, spaces.Box):\n            self.action_head = DiagGaussianHead(feature_dim, spaces.flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.V_head = nn.Linear(feature_dim, 1)\n        ortho_init(self.V_head, weight_scale=1.0, constant_bias=0.0)\n        self.V_head = self.V_head.to(device)  # reproducible between CPU/GPU, ortho_init behaves differently\n        \n        self.total_timestep = 0\n        self.optimizer = optim.Adam(self.parameters(), lr=config['agent.lr'])\n        if config['agent.use_lr_scheduler']:\n            self.lr_scheduler = linear_lr_scheduler(self.optimizer, config['train.timestep'], min_lr=1e-8)\n            \n        self.state = None\n        \n    def reset(self, batch_size):\n        h = torch.zeros(batch_size, self.config['rnn.size']).to(self.device)\n        c = torch.zeros_like(h)\n        return h, c\n        \n    def choose_action(self, x, **kwargs):\n        if x.first():\n            self.state = self.reset(1)\n        obs = tensorify(x.observation, self.device).unsqueeze(0)\n        obs = obs.unsqueeze(0)  # add seq_dim\n        features, [next_state] = self.feature_network(obs, [self.state])\n        if 'last_info' not in kwargs:\n            self.state = next_state\n        features = features.squeeze(0)  # squeeze seq_dim\n        action_dist = self.action_head(features)\n        V = self.V_head(features)\n        action = action_dist.sample()\n        out = {}\n        out['action_dist'] = action_dist\n        out['V'] = V\n        out['entropy'] = action_dist.entropy()\n        out['action'] = action\n        out['raw_action'] = numpify(action, self.env.action_space.dtype).squeeze(0)\n        out['action_logprob'] = action_dist.log_prob(action.detach())\n        return out\n    \n    def learn(self, D, **kwargs):\n        logprobs = [torch.cat(traj.get_infos('action_logprob')) for traj in D]\n        entropies = [torch.cat(traj.get_infos('entropy')) for traj in D]\n        Vs = [torch.cat(traj.get_infos('V')) for traj in D]\n        last_Vs = [traj.extra_info['last_info']['V'] for traj in D]\n        Qs = [bootstrapped_returns(self.config['agent.gamma'], traj.rewards, last_V, traj.reach_terminal)\n              for traj, last_V in zip(D, last_Vs)]\n        As = [gae(self.config['agent.gamma'], self.config['agent.gae_lambda'], traj.rewards, V, last_V, traj.reach_terminal)\n              for traj, V, last_V in zip(D, Vs, last_Vs)]\n        \n        # Metrics -> Tensor, device\n        logprobs, entropies, Vs = map(lambda x: torch.cat(x).squeeze(), [logprobs, entropies, Vs])\n        Qs, As = map(lambda x: tensorify(np.concatenate(x).copy(), self.device), [Qs, As])\n        if self.config['agent.standardize_adv']:\n            As = (As - As.mean())/(As.std() + 1e-4)\n        assert all([x.ndim == 1 for x in [logprobs, entropies, Vs, Qs, As]])\n        \n        # Loss\n        policy_loss = -logprobs*As.detach()\n        entropy_loss = -entropies\n        value_loss = F.mse_loss(Vs, Qs, reduction='none')\n        loss = policy_loss + self.config['agent.value_coef']*value_loss + self.config['agent.entropy_coef']*entropy_loss\n        loss = loss.mean()\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        grad_norm = nn.utils.clip_grad_norm_(self.parameters(), self.config['agent.max_grad_norm'])\n        self.optimizer.step()\n        if self.config['agent.use_lr_scheduler']:\n            self.lr_scheduler.step(self.total_timestep)\n        self.total_timestep += sum([traj.T for traj in D])\n        \n        out = {}\n        if self.config['agent.use_lr_scheduler']:\n            out['current_lr'] = self.lr_scheduler.get_lr()\n        out['loss'] = loss.item()\n        out['grad_norm'] = grad_norm\n        out['policy_loss'] = policy_loss.mean().item()\n        out['entropy_loss'] = entropy_loss.mean().item()\n        out['policy_entropy'] = -out['entropy_loss']\n        out['value_loss'] = value_loss.mean().item()\n        out['V'] = describe(numpify(Vs, 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['explained_variance'] = ev(y_true=numpify(Qs, 'float'), y_pred=numpify(Vs, 'float'))\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        if self.config['env.normalize_obs']:\n            moments = (self.env.obs_moments.mean, self.env.obs_moments.var)\n            pickle_dump(obj=moments, f=logdir/f'obs_moments_{num_iter}', ext='.pth')\n"""
baselines/vpg/engine.py,0,"b""import time\n\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\n\n\nclass Engine(BaseEngine):        \n    def train(self, n=None, **kwargs):\n        self.agent.train()\n        t0 = time.perf_counter()\n        \n        D = self.runner(self.agent, self.env, self.config['train.timestep_per_iter'])\n        out_agent = self.agent.learn(D) \n        \n        logger = Logger()\n        logger('train_iteration', n+1)\n        logger('num_seconds', round(time.perf_counter() - t0, 1))\n        [logger(key, value) for key, value in out_agent.items()]\n        logger('num_trajectories', len(D))\n        logger('num_timesteps', sum([traj.T for traj in D]))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        logger('return', describe([sum(traj.rewards) for traj in D], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        \n        E = [traj[-1].info['episode'] for traj in D if 'episode' in traj[-1].info]\n        logger('online_return', describe([e['return'] for e in E], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe([e['horizon'] for e in E], axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_return', describe(self.env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(self.env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        return logger\n        \n    def eval(self, n=None, **kwargs):\n        pass\n"""
baselines/vpg/experiment.py,0,"b""import os\nfrom itertools import count\nimport gym\n\nfrom lagom import StepRunner\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import RecordEpisodeStatistics\nfrom lagom.envs import NormalizeObservation\nfrom lagom.envs import NormalizeReward\nfrom lagom.envs import TimeStepEnv\n\nfrom baselines.vpg.agent import Agent\nfrom baselines.vpg.agent_lstm import Agent as LSTMAgent\nfrom baselines.vpg.engine import Engine\n\n\nconfig = Config(\n    {'log.freq': 10, \n     'checkpoint.num': 3,\n     \n     'env.id': Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3']), \n     'env.normalize_obs': True,\n     'env.normalize_reward': True,\n     \n     'use_lstm': Grid([True, False]),\n     'rnn.size': 128,\n     'nn.sizes': [64, 64],\n     \n     'agent.lr': 1e-3,\n     'agent.use_lr_scheduler': False,\n     'agent.gamma': 0.99,\n     'agent.gae_lambda': 0.97,\n     'agent.standardize_adv': True,  # standardize advantage estimates\n     'agent.max_grad_norm': 0.5,  # grad clipping by norm\n     'agent.entropy_coef': 0.01,\n     'agent.value_coef': 0.5,\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'train.timestep_per_iter': 1000,  # number of timesteps per iteration\n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    env = gym.make(config['env.id'])\n    env.seed(seed)\n    env.observation_space.seed(seed)\n    env.action_space.seed(seed)\n    if config['env.clip_action'] and isinstance(env.action_space, gym.spaces.Box):\n        env = gym.wrappers.ClipAction(env)\n    if mode == 'train':\n        env = RecordEpisodeStatistics(env, deque_size=100)\n        if config['env.normalize_obs']:\n            env = NormalizeObservation(env, clip=5.)\n        if config['env.normalize_reward']:\n            env = NormalizeReward(env, clip=10., gamma=config['agent.gamma'])\n    env = TimeStepEnv(env)\n    return env\n    \n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    env = make_env(config, seed, 'train')\n    if config['use_lstm']:\n        agent = LSTMAgent(config, env, device)\n    else:\n        agent = Agent(config, env, device)\n    runner = StepRunner(reset_on_call=False)\n    engine = Engine(config, agent=agent, env=env, runner=runner)\n    train_logs = []\n    checkpoint_count = 0\n    for i in count():\n        if agent.total_timestep >= config['train.timestep']:\n            break\n        train_logger = engine.train(i)\n        train_logs.append(train_logger.logs)\n        if i == 0 or (i+1) % config['log.freq'] == 0:\n            train_logger.dump(keys=None, index=0, indent=0, border='-'*50)\n        if agent.total_timestep >= int(config['train.timestep']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n            agent.checkpoint(logdir, i + 1)\n            checkpoint_count += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    return None\n    \n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=False,  # CPU a bit faster\n                   gpu_ids=None)\n"""
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nimport os\nimport sys\nimport mock\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n# Exclude some dependencies for Read the Docs to compile with\nMOCK_MODULES = [\'seaborn\', \n                \'matplotlib\',\n                \'matplotlib.pyplot\',\n                \'_tkinter\']\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = mock.Mock()\n\n# -- Project information -----------------------------------------------------\n\nproject = \'lagom\'\ncopyright = \'2018, Xingdong Zuo\'\nauthor = \'Xingdong Zuo\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\n# Read lagom version automatically\nwith open(\'../../lagom/version.py\', \'r\') as f:\n    version = f.read().split(""\'"")[-2]\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\'  # Support Google Style docstring, i.e. Args/Returns\n]\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_include_init_with_doc = False  # show docstring of __init__\nnapoleon_include_private_with_doc = False  # show docstring of _method\nnapoleon_include_special_with_doc = True  # show docstring of __method__\nnapoleon_use_ivar = True  # show argument dtype with link\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'ntemplates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'**.ipynb_checkpoints\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'lagomdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'lagom.tex\', \'lagom Documentation\',\n     \'Xingdong Zuo\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'lagom\', \'lagom Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'lagom\', \'lagom Documentation\',\n     author, \'lagom\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
examples/vae/__init__.py,0,b''
examples/vae/engine.py,5,"b'from time import perf_counter\n\nimport numpy as np\n\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.utils import color_str\n\nimport torch\nfrom torchvision.utils import save_image\n\nfrom model import vae_loss\n\n\nclass Engine(BaseEngine):\n    def train(self, n=None, **kwargs):\n        self.model.train()\n        \n        logger = Logger()\n        for i, (data, label) in enumerate(self.train_loader):\n            start_time = perf_counter()\n            data = data.to(self.model.device)\n            re_x, mu, logvar = self.model(data)\n            out = vae_loss(re_x, data, mu, logvar, \'BCE\')\n            loss = out[\'loss\']\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            \n            logger(\'epoch\', n)\n            self.model.total_iter += 1\n            logger(\'iteration\', self.model.total_iter)\n            logger(\'mini-batch\', i)\n            logger(\'train_loss\', out[\'loss\'].item())\n            logger(\'reconstruction_loss\', out[\'re_loss\'].item())\n            logger(\'KL_loss\', out[\'KL_loss\'].item())\n            logger(\'num_seconds\', round(perf_counter() - start_time, 1))\n            if i == 0 or (i+1) % self.config[\'log.freq\'] == 0:\n                logger.dump(keys=None, index=-1, indent=0, border=\'-\'*50)\n        mean_loss = np.mean([logger.logs[\'train_loss\']])\n        print(f\'====> Average loss: {mean_loss}\')\n        \n        # Use decoder to sample images from standard Gaussian noise\n        with torch.no_grad():  # fast, disable grad\n            z = torch.randn(64, self.config[\'nn.z_dim\']).to(self.model.device)\n            re_x = self.model.decode(z).cpu()\n            save_image(re_x.view(64, 1, 28, 28), f\'{kwargs[""logdir""]}/sample_{n}.png\')\n        return logger\n        \n    def eval(self, n=None, **kwargs):\n        self.model.eval()\n        \n        logger = Logger()\n        for i, (data, label) in enumerate(self.test_loader):\n            data = data.to(self.model.device)\n            with torch.no_grad():\n                re_x, mu, logvar = self.model(data)\n                out = vae_loss(re_x, data, mu, logvar, \'BCE\')\n                logger(\'eval_loss\', out[\'loss\'].item())\n        mean_loss = np.mean(logger.logs[\'eval_loss\'])\n        print(f\'====> Test set loss: {mean_loss}\')\n        \n        # Reconstruct some test images\n        data, label = next(iter(self.test_loader))  # get a random batch\n        data = data.to(self.model.device)\n        m = min(data.size(0), 8)  # number of images\n        D = data[:m]\n        with torch.no_grad():\n            re_x, _, _ = self.model(D)\n        compare_img = torch.cat([D.cpu(), re_x.cpu().view(-1, 1, 28, 28)])\n        save_image(compare_img, f\'{kwargs[""logdir""]}/reconstruction_{n}.png\', nrow=m)\n        return logger\n'"
examples/vae/experiment.py,2,"b""import os\nfrom pathlib import Path\nfrom itertools import count\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import Sample\nfrom lagom.experiment import run_experiment\n\nfrom engine import Engine\nfrom model import VAE\nfrom model import ConvVAE\n\n\nconfig = Config(\n    {'log.freq': 100, \n     \n     'nn.type': Grid(['VAE', 'ConvVAE']),\n     'nn.z_dim': 8,\n     \n     'lr': 1e-3,\n     \n     'train.num_epoch': 100,\n     'train.batch_size': 128, \n     'eval.batch_size': 128\n    })\n\n\ndef make_dataset(config):\n    train_dataset = datasets.MNIST('data/', \n                                   train=True, \n                                   download=True, \n                                   transform=transforms.ToTensor())\n    test_dataset = datasets.MNIST('data/', \n                                  train=False, \n                                  transform=transforms.ToTensor())\n    train_loader = DataLoader(train_dataset, \n                              batch_size=config['train.batch_size'], \n                              shuffle=True)\n    test_loader = DataLoader(test_dataset, \n                             batch_size=config['eval.batch_size'], \n                             shuffle=True)\n    return train_loader, test_loader\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n\n    train_loader, test_loader = make_dataset(config)\n    if config['nn.type'] == 'VAE':\n        model = VAE(config, device)\n    elif config['nn.type'] == 'ConvVAE':\n        model = ConvVAE(config, device)\n    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n    \n    engine = Engine(config, \n                    model=model, \n                    optimizer=optimizer,\n                    train_loader=train_loader, \n                    test_loader=test_loader)\n    \n    train_logs = []\n    eval_logs = []\n    for epoch in range(config['train.num_epoch']):\n        train_logger = engine.train(epoch, logdir=logdir)\n        train_logs.append(train_logger.logs)\n        eval_logger = engine.eval(epoch, logdir=logdir)\n        eval_logs.append(eval_logger.logs)\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n    pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n    return None\n\n\nif __name__ == '__main__':\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829], \n                   log_dir='logs/default',\n                   max_workers=os.cpu_count(),\n                   chunksize=1, \n                   use_gpu=True,  # GPU much faster\n                   gpu_ids=None)\n"""
examples/vae/model.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import make_cnn\nfrom lagom.networks import make_transposed_cnn\nfrom lagom.networks import ortho_init\n\n\nclass VAE(Module):\n    def __init__(self, config, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.device = device\n        \n        self.encoder = make_fc(784, [400])\n        for layer in self.encoder:\n            ortho_init(layer, nonlinearity=\'relu\', constant_bias=0.0)\n            \n        self.mean_head = nn.Linear(400, config[\'nn.z_dim\'])\n        ortho_init(self.mean_head, weight_scale=0.01, constant_bias=0.0)\n        self.logvar_head = nn.Linear(400, config[\'nn.z_dim\'])\n        ortho_init(self.logvar_head, weight_scale=0.01, constant_bias=0.0)\n        \n        self.decoder = make_fc(config[\'nn.z_dim\'], [400])\n        for layer in self.decoder:\n            ortho_init(layer, nonlinearity=\'relu\', constant_bias=0.0)\n        self.x_head = nn.Linear(400, 784)\n        ortho_init(self.x_head, nonlinearity=\'sigmoid\', constant_bias=0.0)\n        \n        self.to(device)\n        self.total_iter = 0\n\n    def encode(self, x):\n        for layer in self.encoder:\n            x = F.relu(layer(x))\n        mu = self.mean_head(x)\n        logvar = self.logvar_head(x)\n        return mu, logvar \n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5*logvar)\n        eps = torch.randn_like(std)\n        return mu + eps*std\n        \n    def decode(self, z):\n        for layer in self.decoder:\n            z = F.relu(layer(z))\n        re_x = torch.sigmoid(self.x_head(z))\n        return re_x\n        \n    def forward(self, x):\n        x = x.flatten(start_dim=1)\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        re_x = self.decode(z)\n        return re_x, mu, logvar\n\n        \nclass ConvVAE(Module):\n    def __init__(self, config, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.device = device\n        \n        self.encoder = make_cnn(input_channel=1, \n                                channels=[64, 64, 64], \n                                kernels=[4, 4, 4], \n                                strides=[2, 2, 1], \n                                paddings=[0, 0, 0])\n        for layer in self.encoder:\n            ortho_init(layer, nonlinearity=\'relu\', constant_bias=0.0)\n        \n        self.mean_head = nn.Linear(256, config[\'nn.z_dim\'])\n        ortho_init(self.mean_head, weight_scale=0.01, constant_bias=0.0)\n        self.logvar_head = nn.Linear(256, config[\'nn.z_dim\'])\n        ortho_init(self.logvar_head, weight_scale=0.01, constant_bias=0.0)\n        \n        self.decoder_fc = nn.Linear(config[\'nn.z_dim\'], 256)\n        ortho_init(self.decoder_fc, nonlinearity=\'relu\', constant_bias=0.0)\n        self.decoder = make_transposed_cnn(input_channel=64, \n                                           channels=[64, 64, 64], \n                                           kernels=[4, 4, 4], \n                                           strides=[2, 1, 1], \n                                           paddings=[0, 0, 0], \n                                           output_paddings=[0, 0, 0])\n        for layer in self.decoder:\n            ortho_init(layer, nonlinearity=\'relu\', constant_bias=0.0)\n        self.x_head = nn.Linear(9216, 784)\n        ortho_init(self.x_head, nonlinearity=\'sigmoid\', constant_bias=0.0)\n        \n        self.to(device)\n        self.total_iter = 0\n        \n    def encode(self, x):\n        for layer in self.encoder:\n            x = F.relu(layer(x))\n        # To shape [N, D]\n        x = x.flatten(start_dim=1)\n        mu = self.mean_head(x)\n        logvar = self.logvar_head(x)\n        return mu, logvar\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5*logvar)\n        eps = torch.randn_like(std)\n        return mu + eps*std\n    \n    def decode(self, z):\n        z = self.decoder_fc(z)\n        z = z.view(-1, 64, 2, 2)\n        for layer in self.decoder:\n            z = F.relu(layer(z))\n        z = z.flatten(start_dim=1)\n        re_x = torch.sigmoid(self.x_head(z))\n        return re_x\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        re_x = self.decode(z)\n        return re_x, mu, logvar\n        \n    \ndef vae_loss(re_x, x, mu, logvar, mode=\'BCE\'):\n    r""""""Calculate `VAE loss function`_. \n        \n    The VAE loss is the summation of reconstruction loss and KL loss. The KL loss\n    is presented in Appendix B. \n        \n    .. _VAE loss function:\n        https://arxiv.org/abs/1312.6114\n        \n    Args:\n        re_x (Tensor): reconstructed input returned from decoder\n        x (Tensor): ground-truth input\n        mu (Tensor): mean of the latent variable\n        logvar (Tensor): log-variance of the latent variable\n        mode (str): Type of reconstruction loss, supported [\'BCE\', \'MSE\']\n        \n    Returns:\n        dict: a dictionary of selected output such as loss, reconstruction loss and KL loss. \n    """"""\n    assert mode in [\'BCE\', \'MSE\'], f\'expected either BCE or MSE, got {mode}\'\n    \n    # shape [N, D]\n    x = x.view_as(re_x)\n    if mode == \'BCE\':\n        f = F.binary_cross_entropy\n    elif mode == \'MSE\':\n        f = F.mse_loss\n    re_loss = f(input=re_x, target=x, reduction=\'none\')\n    re_loss = re_loss.sum(1)\n    KL_loss = -0.5*torch.sum(1 + logvar - mu**2 - logvar.exp(), dim=1)\n    loss = re_loss + KL_loss\n    \n    out = {}\n    out[\'loss\'] = loss.mean()  # average over the batch\n    out[\'re_loss\'] = re_loss.mean()\n    out[\'KL_loss\'] = KL_loss.mean()\n    return out\n'"
lagom/envs/__init__.py,0,b'from .record_episode_statistics import RecordEpisodeStatistics\n\nfrom .normalize_observation import NormalizeObservation\nfrom .normalize_reward import NormalizeReward\n\nfrom .timestep_env import TimeStepEnv\n'
lagom/envs/normalize_observation.py,0,"b'import numpy as np\nimport gym\n\nfrom lagom.transform import RunningMeanVar\n\n\nclass NormalizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, clip=5., constant_moments=None):\n        super().__init__(env)\n        self.clip = clip\n        self.constant_moments = constant_moments\n        self.eps = 1e-8\n        if constant_moments is None:\n            self.obs_moments = RunningMeanVar(shape=env.observation_space.shape)\n        else:\n            self.constant_mean, self.constant_var = constant_moments\n            \n    def observation(self, observation):\n        if self.constant_moments is None:\n            self.obs_moments([observation])\n            mean = self.obs_moments.mean\n            std = np.sqrt(self.obs_moments.var + self.eps)\n        else:\n            mean = self.constant_mean\n            std = np.sqrt(self.constant_var + self.eps)\n        observation = np.clip((observation - mean)/std, -self.clip, self.clip)\n        return observation\n'"
lagom/envs/normalize_reward.py,0,"b""import numpy as np\nimport gym\n\nfrom lagom.transform import RunningMeanVar\n\n\nclass NormalizeReward(gym.RewardWrapper):\n    def __init__(self, env, clip=10., gamma=0.99, constant_var=None):\n        super().__init__(env)\n        self.clip = clip\n        assert gamma > 0.0 and gamma < 1.0, 'we do not allow discounted factor as 1.0. See docstring for details. '\n        self.gamma = gamma\n        self.constant_var = constant_var\n        self.eps = 1e-8\n        if constant_var is None:\n            self.reward_moments = RunningMeanVar(shape=())\n        \n        # Buffer to save discounted returns from each environment\n        self.all_returns = 0.0\n        \n    def reset(self):\n        # Reset returns buffer\n        self.all_returns = 0.0\n        return super().reset()\n    \n    def step(self, action):\n        observation, reward, done, info = super().step(action)\n        # Set discounted return buffer as zero if episode terminates\n        if done:\n            self.all_returns = 0.0\n        return observation, reward, done, info\n    \n    def reward(self, reward):\n        if self.constant_var is None:\n            self.all_returns = reward + self.gamma*self.all_returns\n            self.reward_moments([self.all_returns])\n            std = np.sqrt(self.reward_moments.var + self.eps)\n        else:\n            std = np.sqrt(self.constant_var + self.eps)\n        # Do NOT subtract from mean, but only divided by std\n        reward = np.clip(reward/std, -self.clip, self.clip)\n        return reward\n"""
lagom/envs/record_episode_statistics.py,0,"b""import time\nfrom collections import deque\n\nimport gym\n\n\nclass RecordEpisodeStatistics(gym.Wrapper):\n    def __init__(self, env, deque_size=100):\n        super().__init__(env)\n        self.t0 = time.perf_counter()\n        self.episode_return = 0.0\n        self.episode_horizon = 0\n        self.return_queue = deque(maxlen=deque_size)\n        self.horizon_queue = deque(maxlen=deque_size)\n        \n    def reset(self, **kwargs):\n        observation = super().reset(**kwargs)\n        self.episode_return = 0.0\n        self.episode_horizon = 0\n        return observation\n        \n    def step(self, action):\n        observation, reward, done, info = super().step(action)\n        self.episode_return += reward\n        self.episode_horizon += 1\n        if done:\n            info['episode'] = {'return': self.episode_return, \n                               'horizon': self.episode_horizon, \n                               'time': round(time.perf_counter() - self.t0, 4)}\n            self.return_queue.append(self.episode_return)\n            self.horizon_queue.append(self.episode_horizon)\n            self.episode_return = 0.0\n            self.episode_horizon = 0\n        return observation, reward, done, info\n"""
lagom/envs/timestep_env.py,0,"b'import gym\n\nfrom lagom.data import StepType\nfrom lagom.data import TimeStep\n\n\nclass TimeStepEnv(gym.Wrapper):\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        step_type = StepType.LAST if done else StepType.MID\n        timestep = TimeStep(step_type=step_type, observation=observation, reward=reward, done=done, info=info)\n        return timestep\n\n    def reset(self, **kwargs):\n        observation = self.env.reset(**kwargs)\n        return TimeStep(StepType.FIRST, observation=observation, reward=None, done=None, info=None)\n'"
lagom/experiment/__init__.py,0,b'from .config import Grid\nfrom .config import Sample\nfrom .config import Condition\nfrom .config import Config\n\nfrom .run_experiment import run_experiment\n'
lagom/experiment/config.py,0,"b'from itertools import product\n\n\nclass Grid(list):\n    r""""""A grid search over a list of values. """"""\n    def __init__(self, values):\n        super().__init__(values)\n\n\nclass Sample(object):\n    def __init__(self, f):\n        self.f = f\n        \n    def __call__(self):\n        return self.f()\n    \n    \nclass Condition(object):\n    def __init__(self, f):\n        assert callable(f)\n        self.f = f\n        \n    def __call__(self, config):\n        return self.f(config)\n\n\nclass Config(object):\n    r""""""Defines a set of configurations for the experiment. \n    \n    The configuration includes the following possible items:\n    \n    * Hyperparameters: learning rate, batch size etc.\n    \n    * Experiment settings: training iterations, logging directory, environment name etc.\n    \n    All items are stored in a dictionary. It is a good practice to semantically name each item\n    e.g. `network.lr` indicates the learning rate of the neural network. \n    \n    For hyperparameter search, we support both grid search (:class:`Grid`) and random search (:class:`Sample`).\n    \n    Call :meth:`make_configs` to generate a list of all configurations, each is assigned\n    with a unique ID. \n    \n    note::\n    \n        For random search over small positive float e.g. learning rate, it is recommended to\n        use log-uniform distribution, i.e.\n        .. math::\n            \\text{logU}(a, b) \\sim \\exp(U(\\log(a), \\log(b)))\n        \n        An example: `np.exp(np.random.uniform(low=np.log(low), high=np.log(high)))`\n            \n        Because direct uniform sampling is very `numerically unstable`_.\n        \n    .. warning::\n    \n        The random seeds should not be set here. Instead, it should be handled by\n        :class:`BaseExperimentMaster` and :class:`BaseExperimentWorker`.\n    \n    Example::\n    \n        >>> config = Config({\'log.dir\': \'some path\', \'network.lr\': Grid([1e-3, 5e-3]), \'env.id\': Grid([\'CartPole-v1\', \'Ant-v2\'])}, num_sample=1, keep_dict_order=False)\n        >>> import pandas as pd\n        >>> print(pd.DataFrame(config.make_configs()))\n               ID       env.id    log.dir  network.lr\n            0   0  CartPole-v1  some path       0.001\n            1   1       Ant-v2  some path       0.001\n            2   2  CartPole-v1  some path       0.005\n            3   3       Ant-v2  some path       0.005\n    \n    Args:\n        items (dict): a dictionary of all configuration items. \n        num_sample (int): number of samples for random configuration items. \n            If grid search is also provided, then the grid will be repeated :attr:`num_sample`\n            of times. \n        keep_dict_order (bool): if ``True``, then each generated configuration has the same\n            key ordering with :attr:`items`. \n            \n    .. _numerically unstable:\n            http://cs231n.github.io/neural-networks-3/#hyper\n    """"""\n    def __init__(self, items, num_sample=1, keep_dict_order=False):\n        assert isinstance(items, dict), f\'dict type expected, got {type(items)}\'\n        self.items = items\n        self.num_sample = num_sample\n        self.keep_dict_order = keep_dict_order\n        \n    def make_configs(self):\n        r""""""Generate a list of all possible combinations of configurations, including\n        grid search and random search. \n        \n        Returns:\n            list: a list of all possible configurations\n        """"""\n        keys_fixed = []\n        keys_grid = []\n        keys_sample = []\n        for key in self.items.keys():\n            x = self.items[key]\n            if isinstance(x, Grid):\n                keys_grid.append(key)\n            elif isinstance(x, Sample):\n                keys_sample.append(key)\n            else:\n                keys_fixed.append(key)\n        if len(keys_sample) == 0:  # if no random search defined, set num_sample=1 to avoid repetition\n            self.num_sample = 1\n                \n        product_grid = list(product(*[self.items[key] for key in keys_grid]))  # len >= 1, [()]\n        list_config = []\n        for n in range(len(product_grid)*self.num_sample):\n            x = {\'ID\': n}\n            x = {**x, **{key: self.items[key] for key in keys_fixed}}\n            \n            for idx, key in enumerate(keys_grid):\n                x[key] = product_grid[n % len(product_grid)][idx]\n            for key in keys_sample:\n                x[key] = self.items[key]()\n                \n            if self.keep_dict_order:\n                x = {**{\'ID\': x[\'ID\']}, **{key: x[key] for key in self.items.keys()}}\n                \n            for key, value in x.items():\n                if isinstance(value, Condition):\n                    x[key] = value(x)\n                \n            list_config.append(x)\n        return list_config\n'"
lagom/experiment/run_experiment.py,6,"b'import os\nfrom shutil import rmtree\nfrom shutil import copyfile\nfrom pathlib import Path\nimport inspect\nfrom itertools import product\nfrom concurrent.futures import ProcessPoolExecutor\n\nimport torch\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import yaml_dump\nfrom lagom.utils import ask_yes_or_no\nfrom lagom.utils import timeit\nfrom lagom.utils import color_str\nfrom lagom.utils import CloudpickleWrapper\n\n\n@timeit(color=\'green\', bold=True)\ndef run_experiment(run, config, seeds, log_dir, max_workers, chunksize=1, use_gpu=False, gpu_ids=None):\n    r""""""A convenient function to parallelize the experiment (master-worker pipeline). \n    \n    It is implemented by using `concurrent.futures.ProcessPoolExecutor`\n    \n    It automatically creates all subfolders for each pair of configuration and random seed\n    to store the loggings of the experiment. The root folder is given by the user.\n    Then all subfolders for each configuration are created with the name of their job IDs.\n    Under each configuration subfolder, a set subfolders are created for each\n    random seed (the random seed as folder name). Intuitively, an experiment could have \n    following directory structure::\n\n        - logs\n            - 0  # ID number\n                - 123  # random seed\n                - 345\n                - 567\n            - 1\n                - 123\n                - 345\n                - 567\n            - 2\n                - 123\n                - 345\n                - 567\n            - 3\n                - 123\n                - 345\n                - 567\n            - 4\n                - 123\n                - 345\n                - 567\n                \n    Args:\n        run (function): a function that defines an algorithm, it must take the \n            arguments `(config, seed, device, logdir)`\n        config (Config): a :class:`Config` object defining all configuration settings\n        seeds (list): a list of random seeds\n        log_dir (str): a string to indicate the path to store loggings.\n        max_workers (int): argument for ProcessPoolExecutor. if `None`, then all experiments run serially.\n        chunksize (int): argument for Executor.map()\n        use_gpu (bool): if `True`, then use CUDA. Otherwise, use CPU.\n        gpu_ids (list): if `None`, then use all available GPUs. Otherwise, only use the\n            GPU device defined in the list. \n    \n    """"""\n    configs = config.make_configs()\n    \n    # create logging dir\n    log_path = Path(log_dir)\n    if not log_path.exists():\n        log_path.mkdir(parents=True)\n    else:\n        msg = f""Logging directory \'{log_path.absolute()}\' already existed, do you want to clean it ?""\n        answer = ask_yes_or_no(msg)\n        if answer:\n            rmtree(log_path)\n            log_path.mkdir(parents=True)\n        else:  # back up\n            old_log_path = log_path.with_name(\'old_\' + log_path.name)\n            log_path.rename(old_log_path)\n            log_path.mkdir(parents=True)\n            print(f""The old logging directory is renamed to \'{old_log_path.absolute()}\'. "")\n            input(\'Please, press Enter to continue\\n>>> \')\n\n    # save source files\n    source_path = Path(log_path / \'source_files/\')\n    source_path.mkdir(parents=True)\n    [copyfile(s, source_path / s.name) for s in Path(inspect.getsourcefile(run)).parent.glob(\'*.py\')]\n    \n    # Create subfolders for each ID and subsubfolders for each random seed\n    for config in configs:\n        ID = config[\'ID\']\n        for seed in seeds:\n            p = log_path / f\'{ID}\' / f\'{seed}\'\n            p.mkdir(parents=True)\n        yaml_dump(obj=config, f=log_path / f\'{ID}\' / \'config\', ext=\'.yml\')\n        \n    pickle_dump(configs, log_path / \'configs\', ext=\'.pkl\')\n    \n    # Create unique id for each job\n    jobs = list(enumerate(product(configs, seeds)))\n    \n    def _run(job):\n        job_id, (config, seed) = job\n        # VERY IMPORTANT TO AVOID GETTING STUCK, oversubscription\n        # see following links\n        # https://github.com/pytorch/pytorch/issues/19163\n        # https://software.intel.com/en-us/intel-threading-building-blocks-openmp-or-native-threads\n        torch.set_num_threads(1)\n        if use_gpu:\n            num_gpu = torch.cuda.device_count()\n            if gpu_ids is None:  # use all GPUs\n                device_id = job_id % num_gpu\n            else:\n                assert all([i >= 0 and i < num_gpu for i in gpu_ids])\n                device_id = gpu_ids[job_id % len(gpu_ids)]\n            torch.cuda.set_device(device_id)\n            device = torch.device(f\'cuda:{device_id}\')\n        else:\n            device = torch.device(\'cpu\')\n            \n        print(f\'@ Experiment: ID: {config[""ID""]} ({len(configs)}), Seed: {seed}, Device: {device}, Job: {job_id} ({len(jobs)}), PID: {os.getpid()}\')\n        print(\'#\'*50)\n        [print(f\'# {key}: {value}\') for key, value in config.items()]\n        print(\'#\'*50)\n        \n        logdir = log_path / f\'{config[""ID""]}\' / f\'{seed}\'\n        result = run(config, seed, device, logdir)\n        # Release all un-freed GPU memory\n        if use_gpu:\n            torch.cuda.empty_cache()\n        return result\n    \n    if max_workers is None:\n        results = [_run(job) for job in jobs]\n    else:\n        with ProcessPoolExecutor(max_workers=min(max_workers, len(jobs))) as executor:\n            results = list(executor.map(CloudpickleWrapper(_run), jobs, chunksize=chunksize))\n    print(color_str(f\'\\nExperiment finished. Loggings are stored in {log_path.absolute()}. \', \'cyan\', bold=True))\n    return results\n'"
lagom/metric/__init__.py,0,b'from .returns import returns\nfrom .returns import bootstrapped_returns\n\nfrom .td import td0_target\nfrom .td import td0_error\n\nfrom .gae import gae\n\nfrom .vtrace import vtrace\n'
lagom/metric/gae.py,0,"b'import numpy as np\n\nfrom lagom.transform import geometric_cumsum\n\nfrom .td import td0_error\n\n\ndef gae(gamma, lam, rewards, Vs, last_V, reach_terminal):\n    r""""""Calculate the Generalized Advantage Estimation (GAE) of a batch of episodic transitions.\n    \n    Let :math:`\\delta_t` be the TD(0) error at time step :math:`t`, the GAE at time step :math:`t` is calculated\n    as follows\n    \n    .. math::\n        A_t^{\\mathrm{GAE}(\\gamma, \\lambda)} = \\sum_{k=0}^{\\infty}(\\gamma\\lambda)^k \\delta_{t + k}\n    \n    """"""\n    delta = td0_error(gamma, rewards, Vs, last_V, reach_terminal)\n    return geometric_cumsum(gamma*lam, delta)[0].astype(np.float32)\n'"
lagom/metric/returns.py,0,"b'import numpy as np\n\nfrom lagom.transform import geometric_cumsum\nfrom lagom.utils import numpify\n\n\ndef returns(gamma, rewards):\n    return geometric_cumsum(gamma, rewards)[0, :].astype(np.float32)\n\n\ndef bootstrapped_returns(gamma, rewards, last_V, reach_terminal):\n    r""""""Return (discounted) accumulated returns with bootstrapping for a \n    batch of episodic transitions. \n    \n    Formally, suppose we have all rewards :math:`(r_1, \\dots, r_T)`, it computes\n        \n    .. math::\n        Q_t = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{T - t} r_T + \\gamma^{T - t + 1} V(s_{T+1})\n        \n    .. note::\n\n        The state values for terminal states are masked out as zero !\n\n    """"""\n    last_V = numpify(last_V, np.float32).item()\n    \n    if reach_terminal:\n        out = geometric_cumsum(gamma, np.append(rewards, 0.0))\n    else:\n        out = geometric_cumsum(gamma, np.append(rewards, last_V))\n    return out[0, :-1].astype(np.float32)\n'"
lagom/metric/td.py,0,"b'import numpy as np\n\nfrom lagom.utils import numpify\n\n\ndef td0_target(gamma, rewards, Vs, last_V, reach_terminal):\n    r""""""Calculate TD(0) targets of a batch of episodic transitions. \n    \n    Let :math:`r_1, r_2, \\dots, r_T` be a list of rewards and let :math:`V(s_0), V(s_1), \\dots, V(s_{T-1}), V(s_{T})`\n    be a list of state values including a last state value. Let :math:`\\gamma` be a discounted factor, \n    the TD(0) targets are calculated as follows\n        \n    .. math::\n        r_t + \\gamma V(s_t), \\forall t = 1, 2, \\dots, T\n        \n    .. note::\n\n        The state values for terminal states are masked out as zero !\n    \n    """"""\n    rewards = numpify(rewards, np.float32)\n    Vs = numpify(Vs, np.float32)\n    last_V = numpify(last_V, np.float32)\n    \n    if reach_terminal:\n        Vs = np.append(Vs, 0.0)\n    else:\n        Vs = np.append(Vs, last_V)\n    out = rewards + gamma*Vs[1:]\n    return out.astype(np.float32)\n\n\ndef td0_error(gamma, rewards, Vs, last_V, reach_terminal):\n    r""""""Calculate TD(0) errors of a batch of episodic transitions. \n    \n    Let :math:`r_1, r_2, \\dots, r_T` be a list of rewards and let :math:`V(s_0), V(s_1), \\dots, V(s_{T-1}), V(s_{T})`\n    be a list of state values including a last state value. Let :math:`\\gamma` be a discounted factor, \n    the TD(0) errors are calculated as follows\n    \n    .. math::\n        \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\n        \n    .. note::\n\n        The state values for terminal states are masked out as zero !\n    \n    """"""\n    rewards = numpify(rewards, np.float32)\n    Vs = numpify(Vs, np.float32)\n    last_V = numpify(last_V, np.float32)\n    \n    if reach_terminal:\n        Vs = np.append(Vs, 0.0)\n    else:\n        Vs = np.append(Vs, last_V)\n    out = rewards + gamma*Vs[1:] - Vs[:-1]\n    return out.astype(np.float32)\n'"
lagom/metric/vtrace.py,0,"b'import numpy as np\n\nfrom lagom.utils import numpify\n\nfrom .td import td0_error\n\n\ndef vtrace(behavior_logprobs, target_logprobs, gamma, Rs, Vs, last_V, reach_terminal, clip_rho=1.0, clip_pg_rho=1.0):\n    behavior_logprobs = numpify(behavior_logprobs, np.float32)\n    target_logprobs = numpify(target_logprobs, np.float32)\n    Rs = numpify(Rs, np.float32)\n    Vs = numpify(Vs, np.float32)\n    last_V = numpify(last_V, np.float32)\n    assert all([item.ndim == 1 for item in [behavior_logprobs, target_logprobs, Rs, Vs]])\n    assert np.isscalar(gamma)\n\n    rhos = np.exp(target_logprobs - behavior_logprobs)\n    clipped_rhos = np.minimum(clip_rho, rhos)\n    cs = np.minimum(1.0, rhos)\n    deltas = clipped_rhos*td0_error(gamma, Rs, Vs, last_V, reach_terminal)\n\n    vs_minus_V = []\n    total = 0.0\n    for delta_t, c_t in zip(deltas[::-1], cs[::-1]):\n        total = delta_t + gamma*c_t*total\n        vs_minus_V.append(total)\n    vs_minus_V = np.asarray(vs_minus_V)[::-1]\n\n    vs = vs_minus_V + Vs\n    vs_next = np.append(vs[1:], (1. - reach_terminal)*last_V)\n    clipped_pg_rhos = np.minimum(clip_pg_rho, rhos)\n    As = clipped_pg_rhos*(Rs + gamma*vs_next - Vs)\n    return vs, As\n'"
lagom/networks/__init__.py,0,b'from .module import Module\n\nfrom .ln_rnn import LayerNormLSTMCell\nfrom .ln_rnn import LSTMLayer\nfrom .ln_rnn import StackedLSTM\nfrom .ln_rnn import make_lnlstm\n\nfrom .init import ortho_init\n\nfrom .lr_scheduler import linear_lr_scheduler\n\nfrom .make_blocks import make_fc\nfrom .make_blocks import make_cnn\nfrom .make_blocks import make_transposed_cnn\n\nfrom .categorical_head import CategoricalHead\nfrom .diag_gaussian_head import DiagGaussianHead\n\nfrom .mdn_head import MDNHead\n'
lagom/networks/categorical_head.py,6,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\nfrom .module import Module\nfrom .init import ortho_init\n\n\nclass CategoricalHead(Module):\n    r""""""Defines a module for a Categorical (discrete) action distribution. \n    \n    Example:\n    \n        >>> import torch\n        >>> action_head = CategoricalHead(30, 4, \'cpu\')\n        >>> action_head(torch.randn(2, 30))\n        Categorical(probs: torch.Size([2, 4]))\n        \n    Args:\n        feature_dim (int): number of input features\n        num_action (int): number of discrete actions\n        device (torch.device): PyTorch device\n        **kwargs: keyword arguments for more specifications.\n    \n    """"""\n    def __init__(self, feature_dim, num_action, device, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.feature_dim = feature_dim\n        self.num_action = num_action\n        self.device = device\n        \n        self.action_head = nn.Linear(self.feature_dim, self.num_action)\n        # weight_scale=0.01 -> uniformly distributed\n        ortho_init(self.action_head, weight_scale=0.01, constant_bias=0.0)\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        action_score = self.action_head(x)\n        action_prob = F.softmax(action_score, dim=-1)\n        action_dist = Categorical(probs=action_prob)\n        return action_dist\n'"
lagom/networks/diag_gaussian_head.py,9,"b'import math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Independent\nfrom torch.distributions import Normal\n\nfrom .module import Module\nfrom .init import ortho_init\n\n\nclass DiagGaussianHead(Module):\n    r""""""Defines a module for a diagonal Gaussian (continuous) action distribution which\n    the standard deviation is state independent. \n    \n    The network outputs the mean :math:`\\mu(x)` and the state independent logarithm of standard \n    deviation :math:`\\log\\sigma` (allowing to optimize in log-space, i.e. both negative and positive). \n    \n    The standard deviation is obtained by applying exponential function :math:`\\exp(x)`.\n    \n    Example:\n    \n        >>> import torch\n        >>> action_head = DiagGaussianHead(10, 4, \'cpu\', 0.45)\n        >>> action_dist = action_head(torch.randn(2, 10))\n        >>> action_dist.base_dist\n        Normal(loc: torch.Size([2, 4]), scale: torch.Size([2, 4]))\n        >>> action_dist.base_dist.stddev\n        tensor([[0.4500, 0.4500, 0.4500, 0.4500],\n                [0.4500, 0.4500, 0.4500, 0.4500]], grad_fn=<ExpBackward>)\n    \n    Args:\n        feature_dim (int): number of input features\n        action_dim (int): flat dimension of actions\n        device (torch.device): PyTorch device\n        std0 (float): initial standard deviation\n        **kwargs: keyword arguments for more specifications.\n    \n    """"""\n    def __init__(self, feature_dim, action_dim, device, std0, **kwargs):\n        super().__init__(**kwargs)\n        assert std0 > 0\n        self.feature_dim = feature_dim\n        self.action_dim = action_dim\n        self.device = device\n        self.std0 = std0\n        \n        self.mean_head = nn.Linear(self.feature_dim, self.action_dim)\n        # 0.01 -> almost zeros initially\n        ortho_init(self.mean_head, weight_scale=0.01, constant_bias=0.0)\n        self.logstd_head = nn.Parameter(torch.full((self.action_dim,), math.log(std0)))\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        mean = self.mean_head(x)\n        logstd = self.logstd_head.expand_as(mean)\n        std = torch.exp(logstd)\n        action_dist = Independent(Normal(loc=mean, scale=std), 1)\n        return action_dist\n'"
lagom/networks/init.py,1,"b'import torch.nn as nn\n\n\ndef ortho_init(module, nonlinearity=None, weight_scale=1.0, constant_bias=0.0):\n    r""""""Applies orthogonal initialization for the parameters of a given module.\n    \n    Args:\n        module (nn.Module): A module to apply orthogonal initialization over its parameters. \n        nonlinearity (str, optional): Nonlinearity followed by forward pass of the module. When nonlinearity\n            is not ``None``, the gain will be calculated and :attr:`weight_scale` will be ignored. \n            Default: ``None``\n        weight_scale (float, optional): Scaling factor to initialize the weight. Ignored when\n            :attr:`nonlinearity` is not ``None``. Default: 1.0\n        constant_bias (float, optional): Constant value to initialize the bias. Default: 0.0\n        \n    .. note::\n    \n        Currently, the only supported :attr:`module` are elementary neural network layers, e.g.\n        nn.Linear, nn.Conv2d, nn.LSTM. The submodules are not supported.\n    \n    Example::\n    \n        >>> a = nn.Linear(2, 3)\n        >>> ortho_init(a)\n    \n    """"""\n    if nonlinearity is not None:\n        gain = nn.init.calculate_gain(nonlinearity)\n    else:\n        gain = weight_scale\n        \n    if isinstance(module, (nn.RNNBase, nn.RNNCellBase)):\n        for name, param in module.named_parameters():\n            if \'weight_\' in name:\n                nn.init.orthogonal_(param, gain=gain)\n            elif \'bias_\' in name:\n                nn.init.constant_(param, constant_bias)\n    else:  # other modules with single .weight and .bias\n        nn.init.orthogonal_(module.weight, gain=gain)\n        nn.init.constant_(module.bias, constant_bias)\n'"
lagom/networks/ln_rnn.py,14,"b""from typing import List, Tuple\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import PackedSequence\nimport torch.jit as jit\n\n\nclass LayerNormLSTMCell(jit.ScriptModule):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight_ih = nn.Parameter(torch.randn(4*hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.randn(4*hidden_size, hidden_size))\n        # The layernorms provide learnable biases\n\n        self.layernorm_i = nn.LayerNorm(4*hidden_size)\n        self.layernorm_h = nn.LayerNorm(4*hidden_size)\n        self.layernorm_c = nn.LayerNorm(hidden_size)\n\n    @jit.script_method\n    def forward(self, input, state):\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n        hx, cx = state\n        igates = self.layernorm_i(torch.mm(input, self.weight_ih.t()))\n        hgates = self.layernorm_h(torch.mm(hx, self.weight_hh.t()))\n        gates = igates + hgates\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n\n        cy = self.layernorm_c(forgetgate*cx + ingate*cellgate)\n        hy = outgate*torch.tanh(cy)\n        return hy, (hy, cy)\n\n\nclass LSTMLayer(jit.ScriptModule):\n    def __init__(self, cell, *cell_args):\n        super().__init__()\n\n        self.cell = cell(*cell_args)\n\n    @jit.export\n    def forward_packed(self, input, state):\n        # type: (PackedSequence, Tuple[Tensor, Tensor]) -> Tuple[PackedSequence, Tuple[Tensor, Tensor]]\n        data, batch_sizes, sorted_indices, unsorted_indices = input\n        state = (state[0].index_select(0, sorted_indices), state[1].index_select(0, sorted_indices))\n        outputs = []\n        for batch_size, x in zip(batch_sizes, data.split(batch_sizes.numpy().tolist(), dim=0)):\n            assert batch_size == x.shape[0]\n            state = (state[0][:batch_size, ...], state[1][:batch_size, ...])\n            out, state = self.cell(x, state)\n            outputs += [out]\n        outputs = PackedSequence(torch.cat(outputs, 0), batch_sizes, sorted_indices, unsorted_indices)\n        return outputs, state\n\n    @jit.export\n    def forward_tensor(self, input, state):\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n        inputs = input.unbind(0)\n        outputs = []\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n            outputs += [out]\n        return torch.stack(outputs), state\n\n    @jit.ignore\n    def forward(self, input, state):\n        if isinstance(input, PackedSequence):\n            return self.forward_packed(input, state)\n        else:\n            return self.forward_tensor(input, state)\n\n\nclass StackedLSTM(jit.ScriptModule):\n    __constants__ = ['layers']  # Necessary for iterating through self.layers\n\n    def __init__(self, num_layers, layer, first_layer_args, other_layer_args):\n        super().__init__()\n        \n        self.layers = nn.ModuleList([layer(*first_layer_args)] + [layer(*other_layer_args) \n                                                                  for _ in range(num_layers - 1)])\n\n    @jit.export\n    def forward_packed(self, input, states):\n        # type: (PackedSequence, List[Tuple[Tensor, Tensor]]) -> Tuple[PackedSequence, List[Tuple[Tensor, Tensor]]]\n        # List[LSTMState]: One state per layer\n        output_states = jit.annotate(List[Tuple[Tensor, Tensor]], [])\n        output = input\n        # XXX: enumerate https://github.com/pytorch/pytorch/issues/14471\n        i = 0\n        for rnn_layer in self.layers:\n            state = states[i]\n            output, out_state = rnn_layer(output, state)\n            output_states += [out_state]\n            i += 1\n        return output, output_states\n        \n    @jit.export\n    def forward_tensor(self, input, states):\n        # type: (Tensor, List[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, List[Tuple[Tensor, Tensor]]]\n        # List[LSTMState]: One state per layer\n        output_states = jit.annotate(List[Tuple[Tensor, Tensor]], [])\n        output = input\n        # XXX: enumerate https://github.com/pytorch/pytorch/issues/14471\n        i = 0\n        for rnn_layer in self.layers:\n            state = states[i]\n            output, out_state = rnn_layer(output, state)\n            output_states += [out_state]\n            i += 1\n        return output, output_states\n    \n    @jit.ignore\n    def forward(self, input, states):\n        if isinstance(input, PackedSequence):\n            return self.forward_packed(input, states)\n        else:\n            return self.forward_tensor(input, states)\n\n\ndef make_lnlstm(input_size, hidden_size, num_layers=1):\n    return StackedLSTM(num_layers, \n                       LSTMLayer, \n                       first_layer_args=[LayerNormLSTMCell, input_size, hidden_size], \n                       other_layer_args=[LayerNormLSTMCell, hidden_size, hidden_size])\n"""
lagom/networks/lr_scheduler.py,1,"b'import torch.optim as optim\n\n\ndef linear_lr_scheduler(optimizer, N, min_lr):\n    r""""""Defines a linear learning rate scheduler. \n    \n    Args:\n        optimizer (Optimizer): optimizer\n        N (int): maximum bounds for the scheduling iteration\n            e.g. total number of epochs, iterations or time steps. \n        min_lr (float): lower bound of learning rate\n    """"""\n    initial_lr = optimizer.defaults[\'lr\']\n    f = lambda n: max(min_lr/initial_lr, 1 - n/N)\n    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=f)\n    return lr_scheduler\n'"
lagom/networks/make_blocks.py,1,"b'import torch.nn as nn\n\n\ndef make_fc(input_dim, hidden_sizes):\n    r""""""Returns a ModuleList of fully connected layers. \n    \n    .. note::\n    \n        All submodules can be automatically tracked because it uses nn.ModuleList. One can\n        use this function to generate parameters in :class:`BaseNetwork`. \n    \n    Example::\n    \n        >>> make_fc(3, [4, 5, 6])\n        ModuleList(\n          (0): Linear(in_features=3, out_features=4, bias=True)\n          (1): Linear(in_features=4, out_features=5, bias=True)\n          (2): Linear(in_features=5, out_features=6, bias=True)\n        )\n    \n    Args:\n        input_dim (int): input dimension in the first fully connected layer. \n        hidden_sizes (list): a list of hidden sizes, each for one fully connected layer. \n    \n    Returns:\n        nn.ModuleList: A ModuleList of fully connected layers.     \n    """"""\n    assert isinstance(hidden_sizes, list), f\'expected list, got {type(hidden_sizes)}\'\n    \n    hidden_sizes = [input_dim] + hidden_sizes\n    \n    fc = []\n    for in_features, out_features in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n        fc.append(nn.Linear(in_features=in_features, out_features=out_features))\n    \n    fc = nn.ModuleList(fc)\n    \n    return fc\n\n\ndef make_cnn(input_channel, channels, kernels, strides, paddings):\n    r""""""Returns a ModuleList of 2D convolution layers. \n    \n    .. note::\n    \n        All submodules can be automatically tracked because it uses nn.ModuleList. One can\n        use this function to generate parameters in :class:`BaseNetwork`. \n        \n    Example::\n    \n        >>> make_cnn(input_channel=3, channels=[16, 32], kernels=[4, 3], strides=[2, 1], paddings=[1, 0])\n        ModuleList(\n          (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n          (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n        )\n    \n    Args:\n        input_channel (int): input channel in the first convolution layer. \n        channels (list): a list of channels, each for one convolution layer.\n        kernels (list): a list of kernels, each for one convolution layer.\n        strides (list): a list of strides, each for one convolution layer. \n        paddings (list): a list of paddings, each for one convolution layer. \n    \n    Returns:\n        nn.ModuleList: A ModuleList of 2D convolution layers.\n    """"""\n    N = len(channels)\n    \n    for item in [channels, kernels, strides, paddings]:\n        assert isinstance(item, list), f\'expected as list, got {type(item)}\'\n        assert len(item) == N, f\'expected length {N}, got {len(item)}\'\n    \n    channels = [input_channel] + channels\n    \n    cnn = []\n    for i in range(N):\n        cnn.append(nn.Conv2d(in_channels=channels[i], \n                             out_channels=channels[i+1], \n                             kernel_size=kernels[i], \n                             stride=strides[i], \n                             padding=paddings[i], \n                             dilation=1, \n                             groups=1))\n    \n    cnn = nn.ModuleList(cnn)\n    \n    return cnn\n\n\ndef make_transposed_cnn(input_channel, channels, kernels, strides, paddings, output_paddings):\n    r""""""Returns a ModuleList of 2D transposed convolution layers. \n    \n    .. note::\n    \n        All submodules can be automatically tracked because it uses nn.ModuleList. One can\n        use this function to generate parameters in :class:`BaseNetwork`. \n        \n    Example::\n    \n        make_transposed_cnn(input_channel=3, \n                            channels=[16, 32], \n                            kernels=[4, 3], \n                            strides=[2, 1], \n                            paddings=[1, 0], \n                            output_paddings=[1, 0])\n        ModuleList(\n          (0): ConvTranspose2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): ConvTranspose2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n        )\n    \n    Args:\n        input_channel (int): input channel in the first transposed convolution layer. \n        channels (list): a list of channels, each for one transposed convolution layer.\n        kernels (list): a list of kernels, each for one transposed convolution layer.\n        strides (list): a list of strides, each for one transposed convolution layer. \n        paddings (list): a list of paddings, each for one transposed convolution layer. \n        output_paddings (list): a list of output paddings, each for one transposed convolution layer. \n    \n    Returns:\n        nn.ModuleList: A ModuleList of 2D transposed convolution layers.\n    """"""\n    N = len(channels)\n    \n    for item in [channels, kernels, strides, paddings, output_paddings]:\n        assert isinstance(item, list), f\'expected as list, got {type(item)}\'\n        assert len(item) == N, f\'expected length {N}, got {len(item)}\'\n    \n    channels = [input_channel] + channels\n    \n    transposed_cnn = []\n    for i in range(N):\n        transposed_cnn.append(nn.ConvTranspose2d(in_channels=channels[i], \n                                                 out_channels=channels[i+1], \n                                                 kernel_size=kernels[i], \n                                                 stride=strides[i], \n                                                 padding=paddings[i], \n                                                 output_padding=output_paddings[i],\n                                                 dilation=1, \n                                                 groups=1))\n    \n    transposed_cnn = nn.ModuleList(transposed_cnn)\n    \n    return transposed_cnn\n'"
lagom/networks/mdn_head.py,10,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom torch.distributions import Normal\n\nfrom lagom.networks import Module\nfrom lagom.networks import ortho_init\n\n\nclass MDNHead(Module):\n    def __init__(self, in_features, out_features, num_density, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_density = num_density\n        \n        self.pi_head = nn.Linear(in_features, out_features*num_density)\n        ortho_init(self.pi_head, weight_scale=0.01, constant_bias=0.0)\n        self.mean_head = nn.Linear(in_features, out_features*num_density)\n        ortho_init(self.mean_head, weight_scale=0.01, constant_bias=0.0)\n        self.logvar_head = nn.Linear(in_features, out_features*num_density)\n        ortho_init(self.logvar_head, weight_scale=0.01, constant_bias=0.0)\n        \n    def forward(self, x):\n        logit_pi = self.pi_head(x).view(-1, self.num_density, self.out_features)\n        mean = self.mean_head(x).view(-1, self.num_density, self.out_features)\n        logvar = self.logvar_head(x).view(-1, self.num_density, self.out_features)\n        std = torch.exp(0.5*logvar)\n        return logit_pi, mean, std\n        \n    def loss(self, logit_pi, mean, std, target):\n        r""""""Calculate the MDN loss function. \n        \n        The loss function (negative log-likelihood) is defined by:\n        \n        .. math::\n            L = -\\frac{1}{N}\\sum_{n=1}^{N}\\ln \\left( \\sum_{k=1}^{K}\\prod_{d=1}^{D} \\pi_{k}(x_{n, d})\n            \\mathcal{N}\\left( \\mu_k(x_{n, d}), \\sigma_k(x_{n,d}) \\right) \\right)\n            \n        For better numerical stability, we could use log-scale:\n        \n        .. math::\n            L = -\\frac{1}{N}\\sum_{n=1}^{N}\\ln \\left( \\sum_{k=1}^{K}\\exp \\left\\{ \\sum_{d=1}^{D} \n            \\ln\\pi_{k}(x_{n, d}) + \\ln\\mathcal{N}\\left( \\mu_k(x_{n, d}), \\sigma_k(x_{n,d}) \n            \\right) \\right\\} \\right) \n        \n        .. note::\n        \n            One should always use the second formula via log-sum-exp trick. The first formula\n            is numerically unstable resulting in +/- ``Inf`` and ``NaN`` error. \n        \n        The log-sum-exp trick is defined by\n        \n        .. math::\n            \\log\\sum_{i=1}^{N}\\exp(x_i) = a + \\log\\sum_{i=1}^{N}\\exp(x_i - a)\n            \n        where :math:`a = \\max_i(x_i)`\n        \n        Args:\n            logit_pi (Tensor): the logit of mixing coefficients, shape [N, K, D]\n            mean (Tensor): mean of Gaussian mixtures, shape [N, K, D]\n            std (Tensor): standard deviation of Gaussian mixtures, shape [N, K, D]\n            target (Tensor): target tensor, shape [N, D]\n\n        Returns:\n            Tensor: calculated loss\n        """"""\n        # target shape [N, D] to [N, 1, D]\n        target = target.unsqueeze(1)\n        \n        log_pi = F.log_softmax(logit_pi, dim=1)\n        \n        dist = Normal(mean, std)\n        log_probs = dist.log_prob(target)\n        \n        # [N, K, D] to [N, K]\n        joint_log_probs = torch.sum(log_pi + log_probs, dim=-1, keepdim=False)\n        # [N, K] to [N]\n        loss = torch.logsumexp(joint_log_probs, dim=-1, keepdim=False)\n        loss = -loss.mean(0)\n        \n        return loss\n    \n    def sample(self, logit_pi, mean, std, tau=1.0):\n        r""""""Sample from Gaussian mixtures using reparameterization trick.\n        \n        - Firstly sample categorically over mixing coefficients to determine a specific Gaussian\n        - Then sample from selected Gaussian distribution\n        \n        Args:\n            logit_pi (Tensor): the logit of mixing coefficients, shape [N, K, D]\n            mean (Tensor): mean of Gaussian mixtures, shape [N, K, D]\n            std (Tensor): standard deviation of Gaussian mixtures, shape [N, K, D]\n            tau (float): temperature during sampling, it controls uncertainty. \n                * If :math:`\\tau > 1`: increase uncertainty\n                * If :math:`\\tau < 1`: decrease uncertainty\n        \n        Returns:\n            Tensor: sampled data with shape [N, D]\n        """"""\n        N, K, D = logit_pi.shape\n        pi = F.softmax(logit_pi/tau, dim=1)\n        # [N, K, D] to [N*D, K]\n        pi = pi.permute(0, 2, 1).view(-1, K)\n        mean = mean.permute(0, 2, 1).view(-1, K)\n        std = std.permute(0, 2, 1).view(-1, K)\n        \n        pi_samples = Categorical(pi).sample()\n        \n        mean = mean[torch.arange(N*D), pi_samples]\n        std = std[torch.arange(N*D), pi_samples]\n        eps = torch.randn_like(std)\n        samples = mean + eps*std*np.sqrt(tau)\n        samples = samples.view(N, D)\n        \n        return samples\n'"
lagom/networks/module.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils import vector_to_parameters\nfrom torch.nn.utils import parameters_to_vector\n\n\nclass Module(nn.Module):\n    r""""""Wrap PyTorch nn.module to provide more helper functions. """"""\n    def __init__(self, **kwargs):\n        super().__init__()\n        \n        for key, val in kwargs.items():\n            self.__setattr__(key, val)\n        \n    @property\n    def num_params(self):\n        r""""""Returns the total number of parameters in the neural network. """"""\n        return sum(param.numel() for param in self.parameters())\n        \n    @property\n    def num_trainable_params(self):\n        r""""""Returns the total number of trainable parameters in the neural network.""""""\n        return sum(param.numel() for param in self.parameters() if param.requires_grad)\n    \n    @property\n    def num_untrainable_params(self):\n        r""""""Returns the total number of untrainable parameters in the neural network. """"""\n        return sum(param.numel() for param in self.parameters() if not param.requires_grad)\n    \n    def to_vec(self):\n        r""""""Returns the network parameters as a single flattened vector. """"""\n        return parameters_to_vector(parameters=self.parameters())\n    \n    def from_vec(self, x):\n        r""""""Set the network parameters from a single flattened vector.\n        \n        Args:\n            x (Tensor): A single flattened vector of the network parameters with consistent size.\n        """"""\n        vector_to_parameters(vec=x, parameters=self.parameters())\n    \n    def save(self, f):\n        r""""""Save the network parameters to a file. \n        \n        It complies with the `recommended approach for saving a model in PyTorch documentation`_. \n        \n        .. note::\n            It uses the highest pickle protocol to serialize the network parameters. \n        \n        Args:\n            f (str): file path. \n            \n        .. _recommended approach for saving a model in PyTorch documentation:\n            https://pytorch.org/docs/master/notes/serialization.html#best-practices\n        """"""\n        import pickle\n        torch.save(obj=self.state_dict(), f=f, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n        \n    def load(self, f):\n        r""""""Load the network parameters from a file. \n        \n        It complies with the `recommended approach for saving a model in PyTorch documentation`_. \n        \n        Args:\n            f (str): file path. \n            \n        .. _recommended approach for saving a model in PyTorch documentation:\n            https://pytorch.org/docs/master/notes/serialization.html#best-practices\n        """"""\n        self.load_state_dict(torch.load(f))\n'"
lagom/transform/__init__.py,0,b'from .describe import Describe\nfrom .describe import describe\nfrom .interp_curves import interp_curves\nfrom .geometric_cumsum import geometric_cumsum\nfrom .explained_variance import explained_variance\nfrom .linear_schedule import LinearSchedule\nfrom .rank_transform import rank_transform\nfrom .polyak_average import PolyakAverage\nfrom .running_mean_var import RunningMeanVar\nfrom .segment_tree import SegmentTree\nfrom .segment_tree import SumTree\nfrom .segment_tree import MinTree\nfrom .smooth_filter import smooth_filter\n'
lagom/transform/describe.py,0,"b""from dataclasses import dataclass\nimport numpy as np\n\n\n@dataclass\nclass Describe:\n    count: int\n    mean: float\n    std: float\n    min: float\n    max: float\n    repr_indent: int = 0\n    repr_prefix: str = None\n        \n    def __repr__(self):\n        s = ''\n        if self.repr_prefix is not None:\n            s += self.repr_prefix\n        ind = '\\t'*self.repr_indent\n        s += ind + f'count: {self.count}\\n'\n        s += ind + f'mean: {self.mean}\\n'\n        s += ind + f'std: {self.std}\\n'\n        s += ind + f'min: {self.min}\\n'\n        s += ind + f'max: {self.max}'\n        return s\n\n\ndef describe(x, axis=-1, repr_indent=0, repr_prefix=None):\n    if x is None or np.size(x) == 0:\n        return None\n    x = np.asarray(x)\n    count = x.shape[-1]\n    mean = x.mean(axis)\n    std = x.std(axis)\n    min = x.min(axis)\n    max = x.max(axis)\n    return Describe(count, mean, std, min, max, repr_indent, repr_prefix)\n"""
lagom/transform/explained_variance.py,0,"b'import numpy as np\nfrom sklearn.metrics import explained_variance_score\n\n\ndef explained_variance(y_true, y_pred, **kwargs):\n    r""""""Computes the explained variance regression score.\n    \n    It involves a fraction of variance that the prediction explains about the ground truth.\n   \n    Let :math:`\\hat{y}` be the predicted output and let :math:`y` be the ground truth output. Then the explained\n    variance is estimated as follows:\n   \n    .. math::\n        \\text{EV}(y, \\hat{y}) = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)}\n   \n    The best score is :math:`1.0`, and lower values are worse. A detailed interpretation is as following:\n   \n    * :math:`\\text{EV} = 1`: perfect prediction\n    * :math:`\\text{EV} = 0`: might as well have predicted zero\n    * :math:`\\text{EV} < 0`: worse than just predicting zero\n   \n    .. note::\n    \n        It calls the function from ``scikit-learn`` which handles exceptions better e.g. zero division, batch size.\n        \n    Example:\n    \n        >>> explained_variance(y_true=[3, -0.5, 2, 7], y_pred=[2.5, 0.0, 2, 8])\n        0.9571734475374732\n        \n        >>> explained_variance(y_true=[[3, -0.5, 2, 7]], y_pred=[[2.5, 0.0, 2, 8]])\n        0.9571734475374732\n        \n        >>> explained_variance(y_true=[[0.5, 1], [-1, 1], [7, -6]], y_pred=[[0, 2], [-1, 2], [8, -5]])\n        0.9838709677419355\n        \n        >>> explained_variance(y_true=[[0.5, 1], [-1, 10], [7, -6]], y_pred=[[0, 2], [-1, 0.00005], [8, -5]])\n        0.6704023148857179\n        \n    Args:\n        y_true (list): ground truth output\n        y_pred (list): predicted output\n        **kwargs: keyword arguments to specify the estimation of the explained variance. \n           \n    Returns:\n        float: estimated explained variance\n    """"""\n    y_true = np.squeeze(y_true)\n    y_pred = np.squeeze(y_pred)\n    assert y_true.shape == y_pred.shape\n    return explained_variance_score(y_true=y_true, y_pred=y_pred, **kwargs)\n'"
lagom/transform/geometric_cumsum.py,0,"b'import numpy as np\nfrom scipy.signal import lfilter\n\n\ndef geometric_cumsum(alpha, x):\n    r""""""Calculate future accumulated sums for each element in a list with an exponential factor. \n    \n    Given input data :math:`x_1, \\dots, x_n` and exponential factor :math:`\\alpha\\in [0, 1]`, it returns\n    an array :math:`y` with the same length and each element is calculated as following\n    \n    .. math::\n        y_i = x_i + \\alpha x_{i+1} + \\alpha^2 x_{i+2} + \\dots + \\alpha^{n-i-1}x_{n-1} + \\alpha^{n-i}x_{n}\n            \n    .. note::\n        To gain the optimal runtime speed, we use ``scipy.signal.lfilter``\n    \n    Example:\n    \n        >>> geometric_cumsum(0.1, [1, 2, 3, 4])\n        array([[1.234, 2.34 , 3.4  , 4.   ]])\n    \n    Args:\n        alpha (float): exponential factor between zero and one. \n        x (list): input data\n            \n    Returns:\n        ndarray: calculated data\n    """"""\n    x = np.asarray(x)\n    if x.ndim == 1:\n        x = np.expand_dims(x, 0)\n    assert x.ndim == 2 \n    return lfilter([1], [1, -alpha], x[:, ::-1], axis=1)[:, ::-1]\n'"
lagom/transform/interp_curves.py,0,"b'import numpy as np\n\n\ndef interp_curves(x, y):\n    r""""""Piecewise linear interpolation of a discrete set of data points and generate new :math:`x-y` values\n    from the interpolated line. \n    \n    It receives a batch of curves with :math:`x-y` values, a global min and max of the x-axis are \n    calculated over the entire batch and new x-axis values are generated to be applied to the interpolation\n    function. Each interpolated curve will share the same values in x-axis. \n    \n    .. note::\n    \n        This is useful for plotting a set of curves with uncertainty bands where each curve\n        has data points at different :math:`x` values. To generate such plot, we need the set of :math:`y` \n        values with consistent :math:`x` values. \n        \n    .. warning::\n    \n        Piecewise linear interpolation often can lead to more realistic uncertainty bands. Do not\n        use polynomial interpolation which the resulting curve can be extremely misleading. \n    \n    Example::\n    \n        >>> import matplotlib.pyplot as plt\n    \n        >>> x1 = [4, 5, 7, 13, 20]\n        >>> y1 = [0.25, 0.22, 0.53, 0.37, 0.55]\n        >>> x2 = [2, 4, 6, 7, 9, 11, 15]\n        >>> y2 = [0.03, 0.12, 0.4, 0.2, 0.18, 0.32, 0.39]\n        \n        >>> plt.scatter(x1, y1, c=\'blue\')\n        >>> plt.scatter(x2, y2, c=\'red\')\n        \n        >>> new_x, new_y = interp_curves([x1, x2], [y1, y2], num_point=100)\n        >>> plt.plot(new_x[0], new_y[0], \'blue\')\n        >>> plt.plot(new_x[1], new_y[1], \'red\')\n        \n    Args:\n            x (list): a batch of x values. \n            y (list): a batch of y values. \n            num_point (int): number of points to generate from the interpolated line. \n    \n    Returns:\n        tuple: a tuple of two lists. A list of interpolated x values (shared for the batch of curves)\n            and followed by a list of interpolated y values. \n    """"""\n    new_x = np.unique(np.hstack(x))\n    assert new_x.ndim == 1\n    ys = [np.interp(new_x, curve_x, curve_y) for curve_x, curve_y in zip(x, y)]\n    return new_x, ys\n'"
lagom/transform/linear_schedule.py,0,"b'class LinearSchedule(object):\n    r""""""A linear scheduling from an initial to a final value over a certain timesteps, then the final\n    value is fixed constantly afterwards. \n        \n    .. note::\n\n        This could be useful for following use cases:\n\n        * Decay of epsilon-greedy: initialized with :math:`1.0` and keep with :attr:`start` time steps, then linearly\n          decay to :attr:`final` over :attr:`N` time steps, and then fixed constantly as :attr:`final` afterwards.\n        * Beta parameter in prioritized experience replay. \n\n        Note that for learning rate decay, one should use PyTorch ``optim.lr_scheduler`` instead. \n    \n    Example:\n    \n        >>> scheduler = LinearSchedule(initial=1.0, final=0.1, N=3, start=0)\n        >>> [scheduler(i) for i in range(6)]\n        [1.0, 0.7, 0.4, 0.1, 0.1, 0.1]\n    \n    Args:\n        initial (float): initial value\n        final (float): final value\n        N (int): number of scheduling timesteps\n        start (int, optional): the timestep to start the scheduling. Default: 0\n        \n    """"""\n    def __init__(self, initial, final, N, start=0):\n        assert N > 0, f\'expected N as positive integer, got {N}\'\n        assert start >= 0, f\'expected start as non-negative integer, got {start}\'\n        \n        self.initial = initial\n        self.final = final\n        self.N = N\n        self.start = start\n        \n        self.x = None\n    \n    def __call__(self, x):\n        r""""""Returns the current value of the scheduling. \n        \n        Args:\n            x (int): the current timestep. \n            \n        Returns:\n            float: current value of the scheduling. \n        """"""\n        assert isinstance(x, int) and x >= 0, f\'expected as a non-negative integer, got {x}\'\n        \n        if x == 0 or x < self.start:\n            self.x = self.initial\n        elif x >= self.start + self.N:\n            self.x = self.final\n        else:  # scheduling over N steps\n            delta = self.final - self.initial\n            ratio = (x - self.start)/self.N\n            self.x = self.initial + ratio*delta\n        return self.x\n    \n    def get_current(self):\n        return self.x\n'"
lagom/transform/polyak_average.py,0,"b'import numpy as np\n\n\nclass PolyakAverage(object):\n    r""""""Keep a running average of a quantity via Polyak averaging. \n    \n    Compared with estimating mean, it is more sentitive to recent changes. \n    \n    Args:\n        alpha (float): factor to control the sensitivity to recent changes, in the range [0, 1].\n            Zero is most sensitive to recent change. \n    \n    """"""\n    def __init__(self, alpha):\n        assert alpha >= 0 and alpha <= 1\n        self.alpha = alpha\n        \n        self.x = None\n        \n    def __call__(self, x):\n        r""""""Update the estimate. \n        \n        Args:\n            x (object): additional data to update the estimation of running average. \n        """"""\n        x = np.asarray(x)\n        if self.x is None:\n            self.x = x\n        else:\n            self.x = self.alpha*self.x + (1 - self.alpha)*x\n        return self.x\n        \n    def get_current(self):\n        r""""""Return the current running average. """"""\n        return self.x\n'"
lagom/transform/rank_transform.py,0,"b'import numpy as np\n\n\ndef rank_transform(x, centered=True):\n    r""""""Rank transformation of a vector of values. The rank has the same dimensionality as the vector.\n    Each element in the rank indicates the index of the ascendingly sorted input.\n    i.e. ``ranks[i] = k``, it means i-th element in the input is :math:`k`-th smallest value. \n    \n    Rank transformation reduce sensitivity to outliers, e.g. in OpenAI ES, gradient computation\n    involves fitness values in the population, if there are outliers (too large fitness), it affects\n    the gradient too much. \n    \n    Note that a centered rank transformation to the range [-0.5, 0.5] is supported by an option. \n    \n    Example:\n    \n        >>> rank_transform([3, 14, 1], centered=True)\n        array([ 0. ,  0.5, -0.5])\n        \n        >>> rank_transform([3, 14, 1], centered=False)\n        array([1, 2, 0])\n        \n    Args:\n        x (list/ndarray): a vector of values.\n        centered (bool, optional): if ``True``, then centered the rank transformation \n            to :math:`[-0.5, 0.5]`. Defualt: ``True``\n\n    Returns:\n        ndarray: an numpy array of ranks of input data\n    """"""\n    x = np.asarray(x)\n    assert x.ndim == 1, \'must be one dimensional, i.e. a vector of scalar values\'\n    \n    ranks = np.empty(len(x), dtype=int)\n    ranks[x.argsort()] = np.arange(len(x))\n    if centered:\n        ranks = ranks/(ranks.size - 1) - 0.5\n    return ranks\n'"
lagom/transform/running_mean_var.py,0,"b'import numpy as np\n\n\nclass RunningMeanVar(object):\n    r""""""Estimates sample mean and variance by using `Chan\'s method`_. \n    \n    It supports for both scalar and multi-dimensional data, however, the input is\n    expected to be batched. The first dimension is always treated as batch dimension.\n    \n    .. note::\n    \n        For better precision, we handle the data with `np.float64`.\n    \n    .. warning::\n    \n        To use estimated moments for standardization, remember to keep the precision `np.float64`\n        and calculated as ..math:`\\frac{x - \\mu}{\\sqrt{\\sigma^2 + 10^{-8}}}`. \n    \n    Example:\n    \n        >>> f = RunningMeanVar(shape=())\n        >>> f([1, 2])\n        >>> f([3])\n        >>> f([4])\n        >>> f.mean\n        2.499937501562461\n        >>> f.var\n        1.2501499923440393\n    \n    .. _Chan\'s method:\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n        \n    """"""\n    def __init__(self, shape):\n        self.shape = shape\n        self.mean = np.zeros(shape, dtype=np.float64)\n        self.var = np.ones(shape, dtype=np.float64)\n        self.N = 1e-8  # numerical stability for variance term, and 1e-4 is for std\n        \n    def __call__(self, x):\n        r""""""Update the mean and variance given an additional batched data. \n        \n        Args:\n            x (object): additional batched data.\n        """"""\n        x = np.asarray(x, dtype=np.float64)\n        assert x.ndim == len(self.shape) + 1, f\'expected {len(self.shape) + 1}, got {x.ndim}\'\n        \n        batch_mean = x.mean(axis=0)\n        batch_var = x.var(axis=0)\n        batch_N = x.shape[0]\n        \n        new_N = self.N + batch_N\n        delta = batch_mean - self.mean\n        new_mean = self.mean + delta*(batch_N/new_N)\n        M_A = self.N*self.var\n        M_B = batch_N*batch_var\n        M_X = M_A + M_B + (delta**2)*((self.N*batch_N)/new_N)\n        new_var = M_X/new_N\n        \n        self.mean = new_mean\n        self.var = new_var\n        self.N = new_N\n    \n    @property\n    def n(self):\n        r""""""Returns the total number of samples so far. """"""\n        return int(self.N)\n'"
lagom/transform/segment_tree.py,0,"b'import operator\n\n\nclass SegmentTree(object):\n    r""""""Defines a segment tree data structure. \n    \n    It can be regarded as regular array, but with two major differences\n    \n    - Value modification is slower: O(ln(capacity)) instead of O(1)\n    - Efficient reduce operation over contiguous subarray: O(ln(segment size))\n    \n    Args:\n        capacity (int): total number of elements, it must be a power of two.\n        operation (lambda): binary operation forming a group, e.g. sum, min\n        identity_element (object): identity element in the group, e.g. 0 for sum\n    \n    """"""\n    def __init__(self, capacity, operation, identity_element):\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \'capacity must be positive and a power of 2.\'\n        self.capacity = capacity\n        self.operation = operation\n        self.value = [identity_element for _ in range(2*capacity)]\n        \n    def _reduce(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self.value[node]\n        mid = (node_start + node_end)//2\n        if end <= mid:  # go to left child\n            return self._reduce(start, end, 2*node, node_start, mid)\n        else:\n            if start >= mid + 1:  # go to right child\n                return self._reduce(start, end, 2*node + 1, mid + 1, node_end)\n            else:\n                return self.operation(self._reduce(start, mid, 2*node, node_start, mid), \n                                      self._reduce(mid + 1, end, 2*node + 1, mid + 1, node_end))\n            \n    def reduce(self, start=0, end=None):\n        r""""""Returns result of operation(A[start], operation(A[start+1], operation(... A[end - 1]))).\n        \n        Args:\n            start (int): start of segment\n            end (int): end of segment\n            \n        Returns:\n            object: result of reduce operation\n        """"""\n        if end is None:\n            end = self.capacity\n        if end < 0:\n            end += self.capacity\n        end -= 1\n        return self._reduce(start, end, 1, 0, self.capacity - 1)\n    \n    def __setitem__(self, index, value):\n        # index of leaf\n        index += self.capacity\n        self.value[index] = value\n        index //= 2\n        while index >= 1:\n            self.value[index] = self.operation(self.value[2*index], self.value[2*index + 1])\n            index //= 2\n            \n    def __getitem__(self, index):\n        assert 0 <= index < self.capacity\n        return self.value[index + self.capacity]\n\n\nclass SumTree(SegmentTree):\n    r""""""Defines the sum tree for storing replay priorities. \n    \n    Each leaf node contains priority value. Internal nodes maintain the sum of the priorities\n    of all leaf nodes in their subtrees. \n    \n    """"""\n    def __init__(self, capacity):\n        super().__init__(capacity, operator.add, 0.0)\n        \n    def sum(self, start=0, end=None):\n        r""""""Return A[start] + ... + A[end - 1]""""""\n        return super().reduce(start, end)\n    \n    def find_prefixsum_index(self, prefixsum):\n        r""""""Find the highest index `i` in the array such that\n        sum(A[0] + A[1] + ... + A[i - 1]) <= prefixsum\n        \n        if array values are probabilities, this function efficiently sample indices according\n        to the discrete probability. \n        \n        Args:\n            prefixsum (float): prefix sum. \n        \n        Returns:\n            int: highest index satisfying the prefixsum constraint\n        """"""\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        index = 1\n        while index < self.capacity:  # while non-leaf\n            if self.value[2*index] > prefixsum:\n                index = 2*index\n            else:\n                prefixsum -= self.value[2*index]\n                index = 2*index + 1\n        return index - self.capacity\n\n\nclass MinTree(SegmentTree):\n    def __init__(self, capacity):\n        super().__init__(capacity, min, float(\'inf\'))\n        \n    def min(self, start=0, end=None):\n        r""""""Returns min(A[start], ..., A[end])""""""\n        return super().reduce(start, end)\n'"
lagom/transform/smooth_filter.py,0,"b'import numpy as np\nfrom scipy.signal import savgol_filter\n\n\ndef smooth_filter(x, window_length, polyorder, **kwargs):\n    r""""""Smooth a sequence of noisy data points by applying `Savitzky\xe2\x80\x93Golay filter`_. It uses least\n    squares to fit a polynomial with a small sliding window and use this polynomial to estimate\n    the point in the center of the sliding window. \n    \n    This is useful when a curve is highly noisy, smoothing it out leads to better visualization quality.\n    \n    .. _Savitzky\xe2\x80\x93Golay filter:\n        https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n    \n    Example:\n    \n        >>> import matplotlib.pyplot as plt\n    \n        >>> x = np.linspace(0, 4*2*np.pi, num=100)\n        >>> y = x*(np.sin(x) + np.random.random(100)*4)\n        >>> y2 = smooth_filter(y, window_length=31, polyorder=10)\n        \n        >>> plt.plot(x, y)\n        >>> plt.plot(x, y2, \'red\')\n        \n    Args:\n        x (list): one-dimensional vector of scalar data points of a curve. \n        window_length (int): the length of the filter window\n        polyorder (int): the order of the polynomial used to fit the samples\n        \n    Returns:\n        ndarray: an numpy array of smoothed curve data\n    """"""\n    x = np.asarray(x)\n    assert x.ndim == 1, \'only a single vector of scalar values is supported\'\n    out = savgol_filter(x, window_length, polyorder, **kwargs)\n    return out\n'"
lagom/utils/__init__.py,0,b'from .seeding import set_global_seeds\nfrom .seeding import Seeder\n\nfrom .dtype import tensorify\nfrom .dtype import numpify\n\nfrom .multiprocessing import ProcessMaster\nfrom .multiprocessing import ProcessWorker\n\nfrom .colorize import color_str\n\nfrom .conditioner import IntervalConditioner\nfrom .conditioner import NConditioner\n\nfrom .timing import timed\nfrom .timing import timeit\n\nfrom .serialize import pickle_load\nfrom .serialize import pickle_dump\nfrom .serialize import yaml_load\nfrom .serialize import yaml_dump\nfrom .serialize import CloudpickleWrapper\n\nfrom .yes_no import ask_yes_or_no\n'
lagom/utils/colorize.py,0,"b'from colorama import Fore, Style\n\n\ndef color_str(string, color, bold=False):\n    r""""""Returns stylized string with coloring and bolding for printing.\n    \n    Example::\n    \n        >>> print(color_str(\'lagom\', \'green\', bold=True))\n        \n    Args:\n        string (str): input string\n        color (str): color name\n        bold (bool, optional): if ``True``, then the string is bolded. Default: ``False``\n    \n    Returns:\n        out: stylized string\n    \n    """"""\n    colors = {\'red\': Fore.RED, \'green\': Fore.GREEN, \'blue\': Fore.BLUE, \'cyan\': Fore.CYAN, \n              \'magenta\': Fore.MAGENTA, \'black\': Fore.BLACK, \'white\': Fore.WHITE}\n    style = colors[color]\n    if bold:\n        style += Style.BRIGHT\n    out = style + string + Style.RESET_ALL\n    return out\n'"
lagom/utils/conditioner.py,0,"b""class IntervalConditioner(object):\n    def __init__(self, interval, mode):\n        self.interval = interval\n        assert mode in ['accumulative', 'incremental']\n        self.mode = mode\n        self.counter = 0\n        if mode == 'incremental':\n            self.total_n = 0\n        \n    def __call__(self, n):\n        assert n >= 0\n        if n == 0:\n            return True\n        else:\n            if self.mode == 'accumulative':\n                check = n >= (self.counter+1)*self.interval\n            elif self.mode == 'incremental':\n                self.total_n += n\n                check = self.total_n >= (self.counter+1)*self.interval\n            if check:\n                self.counter += 1\n            return check\n\n\nclass NConditioner(IntervalConditioner):\n    def __init__(self, max_n, num_conditions, mode):\n        self.max_n = max_n\n        self.num_conditions = num_conditions\n        interval = max_n/num_conditions\n        super().__init__(interval, mode)\n        \n    def __call__(self, n):\n        if self.counter >= self.num_conditions:\n            return False\n        else:\n            return super().__call__(n)\n"""
lagom/utils/dtype.py,4,"b'import numpy as np\nimport torch\n\n\ndef tensorify(x, device):\n    if torch.is_tensor(x):\n        if str(x.device) != str(device):\n            x = x.to(device)\n        return x\n    elif isinstance(x, np.ndarray):\n        return torch.from_numpy(x).float().to(device)\n    else:\n        return torch.from_numpy(np.asarray(x)).float().to(device)\n    \n    \ndef numpify(x, dtype=None):\n    if torch.is_tensor(x):\n        x = x.detach().cpu().numpy()\n    else:\n        x = np.asarray(x)\n    if dtype is not None:\n        x = x.astype(dtype)\n    return x\n'"
lagom/utils/multiprocessing.py,0,"b'# Note that `__name__ == \'__main__\'` is only required for Windows compatibility\n# We don\'t use it because Ubuntu is expected. \n\nfrom abc import ABC\nfrom abc import abstractmethod\n\nfrom multiprocessing import Process\nfrom multiprocessing import Pipe\n\n\nclass ProcessWorker(ABC):\n    r""""""Base class for all workers implemented with Python multiprocessing.Process. \n    \n    It communicates with master via a Pipe connection. The worker is stand-by infinitely waiting for task\n    from master, working and sending back result. When it receives a ``close`` command, it breaks the infinite\n    loop and close the connection. \n        \n    """"""\n    def __init__(self, master_conn, worker_conn):\n        # Not used here. It\'s copied by forked process. \n        master_conn.close()\n        \n        while True:\n            job = worker_conn.recv()\n            \n            if job == \'close\':\n                worker_conn.send(\'confirmed\')\n                worker_conn.close()\n                break\n            else:\n                result = [[task_id, self.work(task_id, task)] for task_id, task in job]\n                worker_conn.send(result)\n                \n    @abstractmethod\n    def work(self, task_id, task):\n        r""""""Work on the given task and return the result. \n        \n        Args:\n            task_id (int): the task ID.\n            task (object): a given task. \n            \n        Returns:\n            object: working result.     \n        """"""\n        pass\n\n\nclass ProcessMaster(ABC):\n    r""""""Base class for all masters implemented with Python multiprocessing.Process. \n    \n    It creates a number of workers each with an individual Process. The communication between master\n    and each worker is via independent Pipe connection. The master assigns tasks to workers. When all\n    tasks are done, it stops all workers and terminate all processes. \n    \n    .. note::\n    \n        If there are more tasks than workers, then tasks will be splitted into chunks.\n        If there are less tasks than workers, then we reduce the number of workers to the number of tasks. \n    \n    """"""\n    def __init__(self, worker_class, num_worker):\n        self.worker_class = worker_class\n        self.num_worker = num_worker\n        \n    def __call__(self):\n        tasks = self.make_tasks()\n        if len(tasks) < self.num_worker:\n            self.num_worker = len(tasks)\n        \n        self.make_workers()\n        \n        results = self.assign_tasks(tasks)\n        \n        self.close()\n        \n        return results\n        \n    def make_workers(self):\n        self.master_conns, self.worker_conns = zip(*[Pipe() for _ in range(self.num_worker)])\n        \n        # daemonic process not allow to have children\n        self.list_process = [Process(target=self.worker_class, args=[master_conn, worker_conn], daemon=False)\n                             for master_conn, worker_conn in zip(self.master_conns, self.worker_conns)]\n        [process.start() for process in self.list_process]\n        \n        # Not used here. Already copied by forked process\n        [worker_conn.close() for worker_conn in self.worker_conns]\n        \n    @abstractmethod\n    def make_tasks(self):\n        r""""""Returns a list of tasks. \n        \n        Returns:\n            list: a list of tasks\n        """"""\n        pass\n        \n    def assign_tasks(self, tasks):\n        r""""""Assign a given list of tasks to the workers and return the received results. \n        \n        Args:\n            tasks (list): a list of tasks\n            \n        Returns:\n            object: received results\n        """"""\n        jobs = [[] for _ in range(self.num_worker)]\n        for task_id, task in enumerate(tasks):\n            jobs[task_id % self.num_worker].append([task_id, task])  # job = [task_id, task]\n            \n        [master_conn.send(job) for master_conn, job in zip(self.master_conns, jobs)]\n        \n        results = [None for _ in range(len(tasks))]\n        for master_conn in self.master_conns:\n            for task_id, result in master_conn.recv():\n                results[task_id] = result\n        \n        return results\n    \n    def close(self):\n        r""""""Defines everything required after finishing all the works, e.g. stop all workers, clean up. """"""\n        [master_conn.send(\'close\') for master_conn in self.master_conns]\n        assert all([master_conn.recv() == \'confirmed\' for master_conn in self.master_conns])\n        \n        [master_conn.close() for master_conn in self.master_conns]\n        assert all([master_conn.closed for master_conn in self.master_conns])\n        \n        [process.join() for process in self.list_process]\n'"
lagom/utils/seeding.py,2,"b'import random\nimport numpy as np\nimport torch\n\n\ndef set_global_seeds(seed):\n    r""""""Set the seed for generating random numbers.\n    \n    It sets the following dependencies with the given random seed:\n    \n    1. PyTorch\n    2. Numpy\n    3. Python random\n    \n    Args:\n        seed (int): a given seed.\n    """"""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\nclass Seeder(object):\n    r""""""A random seed generator. \n    \n    Given an initial seed, the seeder can be called continuously to sample a single\n    or a batch of random seeds. \n    \n    .. note::\n    \n        The seeder creates an independent RandomState to generate random\n        numbers. It does not affect the RandomState in ``np.random``. \n    \n    Example::\n    \n        >>> seeder = Seeder(init_seed=0)\n        >>> seeder(size=5)\n        [209652396, 398764591, 924231285, 1478610112, 441365315]\n        \n    """"""\n    def __init__(self, init_seed=0):\n        r""""""Initialize the seeder. \n        \n        Args:\n            init_seed (int, optional): Initial seed for generating random seeds. Default: ``0``.\n        """"""\n        assert isinstance(init_seed, int) and init_seed >= 0, f\'expected non-negative integer, got {init_seed}\'\n        \n        self.rng = np.random.RandomState(seed=init_seed)\n        \n        # Upper bound for sampling new random seeds\n        self.max = np.iinfo(np.int32).max\n        \n    def __call__(self, size=1):\n        r""""""Return the sampled random seeds according to the given size. \n        \n        Args:\n            size (int or list): The size of random seeds to sample. \n            \n        Returns:\n            list: a list of sampled random seeds.\n        """"""\n        seeds = self.rng.randint(low=0, high=self.max, size=size).tolist()\n        \n        return seeds\n'"
lagom/utils/serialize.py,0,"b'from pathlib import Path\n\nimport pickle\nimport cloudpickle\n\nimport yaml\n\n\ndef pickle_load(f):\n    r""""""Read a pickled data from a file. \n\n    Args:\n        f (str/Path): file path\n    """"""\n    if isinstance(f, Path):\n        f = f.as_posix()\n\n    with open(f, \'rb\') as file:\n        return cloudpickle.load(file)\n\n\ndef pickle_dump(obj, f, ext=\'.pkl\'):\n    r""""""Serialize an object using pickling and save in a file. \n    \n    .. note::\n    \n        It uses cloudpickle instead of pickle to support lambda\n        function and multiprocessing. By default, the highest\n        protocol is used. \n        \n    .. note::\n    \n        Except for pure array object, it is not recommended to use\n        ``np.save`` because it is often much slower. \n    \n    Args:\n        obj (object): a serializable object\n        f (str/Path): file path\n        ext (str, optional): file extension. Default: .pkl\n    """"""\n    if isinstance(f, Path):\n        f = f.as_posix()\n    \n    with open(f+ext, \'wb\') as file:\n        return cloudpickle.dump(obj=obj, file=file, protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef yaml_load(f):\n    r""""""Read the data from a YAML file. \n\n    Args:\n        f (str/Path): file path\n    """"""\n    if isinstance(f, Path):\n        f = f.as_posix()\n    \n    with open(f, \'r\') as file:\n        return yaml.load(file, Loader=yaml.FullLoader)\n\n\ndef yaml_dump(obj, f, ext=\'.yml\'):\n    r""""""Serialize a Python object using YAML and save in a file. \n    \n    .. note::\n    \n        YAML is recommended to use for a small dictionary and it is super\n        human-readable. e.g. configuration settings. For saving experiment\n        metrics, it is better to use :func:`pickle_dump`.\n        \n    .. note::\n    \n        Except for pure array object, it is not recommended to use\n        ``np.load`` because it is often much slower. \n        \n    Args:\n        obj (object): a serializable object\n        f (str/Path): file path\n        ext (str, optional): file extension. Default: .yml\n        \n    """"""\n    if isinstance(f, Path):\n        f = f.as_posix()\n    with open(f+ext, \'w\') as file:\n        return yaml.dump(obj, file, sort_keys=False)\n\n    \nclass CloudpickleWrapper(object):\n    r""""""Uses cloudpickle to serialize contents (multiprocessing uses pickle by default)\n    \n    This is useful when passing lambda definition through Process arguments.\n    """"""\n    def __init__(self, x):\n        self.x = x\n        \n    def __call__(self, *args, **kwargs):\n        return self.x(*args, **kwargs)\n    \n    def __getattr__(self, name):\n        return getattr(self.x, name)\n    \n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n    \n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n'"
lagom/utils/timing.py,0,"b'import functools\nfrom contextlib import contextmanager\n\nfrom time import perf_counter\nfrom datetime import timedelta\nfrom datetime import datetime\n\nfrom .colorize import color_str\n\n\n@contextmanager\ndef timed(color=\'green\', bold=False):\n    r""""""A decorator to print the total time of executing a body function. \n    \n    Args:\n        color (str, optional): color name. Default: \'green\'\n        bold (bool, optional): if ``True``, then the verbose is bolded. Default: ``False``\n    """"""\n    t = perf_counter()\n    yield\n    total_time = timedelta(seconds=round(perf_counter() - t))\n    timestamp = datetime.now().isoformat(\' \', \'seconds\')\n    print(color_str(string=f\'\\nTotal time: {total_time} at {timestamp}\', \n                    color=color, \n                    bold=bold))\n    \n    \ndef timeit(_func=None, *, color=\'green\', bold=False):\n    def decorator_timeit(f):\n        r""""""Print the runtime of the decorated function. """"""\n        @functools.wraps(f)\n        def wrapper_timeit(*args, **kwargs):\n            t = perf_counter()\n            out = f(*args, **kwargs)\n            total_time = timedelta(seconds=round(perf_counter() - t))\n            timestamp = datetime.now().isoformat(\' \', \'seconds\')\n            print(color_str(string=f\'\\nTotal time: {total_time} at {timestamp}\', \n                            color=color, \n                            bold=bold))\n            return out\n        return wrapper_timeit\n    if _func is None:\n        return decorator_timeit\n    else:\n        return decorator_timeit(_func)\n'"
lagom/utils/yes_no.py,0,"b'def ask_yes_or_no(msg):\n    r""""""Ask user to enter yes or no to a given message. \n    \n    Args:\n        msg (str): a message\n    """"""\n    print(msg)\n    \n    while True:\n        answer = str(input(\'>>> \')).lower().strip()\n        \n        if answer[0] == \'y\':\n            return True\n        elif answer[0] == \'n\':\n            return False\n        else:\n            print(""Please answer \'yes\' or \'no\':"")\n'"
lagom/vis/__init__.py,0,b'try:  # workaround on server without fake screen but still running other things well\n    from .image_viewer import ImageViewer\nexcept ImportError:\n    pass\n\nfrom .grid_image import GridImage\n\nfrom .utils import set_ticker\nfrom .utils import read_xy\n'
lagom/vis/grid_image.py,0,"b'import numpy as np\n\nfrom PIL import Image\n\n\nclass GridImage(object):\n    r""""""Generate a grid of images. The images can be iteratively added. \n    \n    Example::\n    \n        grid = GridImage(ncol=8, padding=5, pad_value=0)\n\n        a = np.random.randint(0, 255+1, size=[10, 3, 64, 64])\n        grid.add(a)\n        grid()\n    \n    Reference:\n    \n        * https://github.com/pytorch/vision/blob/master/torchvision/utils.py\n        \n        * https://github.com/facebookresearch/visdom/blob/master/py/visdom/__init__.py\n        \n    Args:\n        ncol (int, optional): Number of images to show in each row of the grid. \n            Final grid size is [N/ncol, ncol]. Default: 8. \n        padding (int, optional): Number of paddings. Default: 2.\n        pad_value (float, optional): Padding value in the range [0, 255]. \n            Black is 0 and white 255. Default: 0\n\n    """"""\n    def __init__(self, ncol=8, padding=2, pad_value=0):\n        self.ncol = ncol\n        self.padding = padding\n        self.pad_value = pad_value\n        \n        # Data buffer\n        self.x = None\n\n    def add(self, x):\n        r""""""Add a new data for making grid images. \n        \n        Args:\n            x (list/ndarray): a list or ndarray of images, with shape either [H, W], [C, H, W] or [N, C, H, W]\n        """"""\n        if not isinstance(x, (list, np.ndarray)):\n            raise TypeError(f\'list or ndarray expected, got {type(x)}\')\n\n        x = np.array(x)\n        assert x.ndim <= 4 or x.ndim >= 2, f\'either 2, 3, or 4 dimensions expected, got {x.ndim}\'\n\n        # Convert to shape [N, C, H, W]\n        if x.ndim == 2:  # Single image HxW -> [1, 1, H, W]\n            x = x.reshape([1, 1, *x.shape])\n        elif x.ndim == 3:  # Single image CxHxW -> [1, C, H, W]\n            x = x.reshape([1, *x.shape])\n\n        # Convert to RGB channels for single color channel\n        if x.shape[1] == 1:\n            x = np.concatenate([x]*3, axis=1)\n            \n        # Save to data buffer\n        if self.x is None:\n            self.x = x\n        else:  # concatenate with existing images in data buffer, along batch dimension N\n            self.x = np.concatenate([self.x, x], axis=0)\n        \n    def __call__(self, **kwargs):\n        r""""""Make grid of images. \n        \n        Args:\n            **kwargs: keyword aguments used to specify the grid of images. \n            \n        Returns:\n            Image: a grid of image with shape [H, W, C] and dtype ``np.uint8``\n        """"""\n        # Total number of images\n        N = self.x.shape[0]\n        # Number of images in one row\n        cols = min(N, self.ncol)\n        # Number of rows, at least one\n        rows = int(np.ceil(N/cols))\n        # Image height\n        img_H = self.x.shape[2]\n        # Image width\n        img_W = self.x.shape[3]\n        # Padded height\n        H = img_H + self.padding\n        # Padded width\n        W = img_W + self.padding\n        \n        # Create a grid\n        grid = np.full([3, rows*H + self.padding, cols*W + self.padding], float(self.pad_value))\n\n        n = 0\n        for row in range(rows):\n            for col in range(cols):\n                if n >= N:  # terminate when finish all images\n                    break\n                H_start = row*H + self.padding\n                H_end = H_start + img_H\n                W_start = col*W + self.padding\n                W_end = W_start + img_W\n\n                # Fill the image\n                grid[:, H_start:H_end, W_start:W_end] = self.x[n]\n\n                n += 1\n\n        # Enforce unit8 images in the range [0, 255]\n        if \'float\' in str(grid.dtype):\n            if grid.max() <= 1:  # value range in [0, 1]\n                grid *= 255.\n            grid = grid.astype(np.uint8)\n\n        # Convert to shape [H, W, C]\n        grid = np.transpose(grid, axes=[1, 2, 0])\n\n        img = Image.fromarray(grid)\n        \n        return img\n'"
lagom/vis/image_viewer.py,0,"b'import numpy as np\n\nimport pyglet\n\ntry:\n    import pyglet.gl as gl\nexcept Exception:\n    msg1 = \'1. make sure OpenGL is installed by running `sudo apt install python-opengl`. \\n\'\n    msg2 = \'2. if you are on a server, then create a fake screen with xvfb-run and make sure nvidia driver \'\n    msg3 = \'is installed with --no-opengl-files and cuda with --no-opengl-libs\'\n    raise ImportError(msg1+msg2+msg3) from None\n\n\nclass ImageViewer(pyglet.window.Window):\n    r""""""Display an image from an RGB array in an OpenGL window. \n    \n    Example::\n    \n        imageviewer = ImageViewer(max_width=500)\n        image = np.asarray(Image.open(\'x.jpg\'))\n        imageviewer(image)\n        \n    """"""\n    def __init__(self, max_width=500):\n        r""""""Initialize the OpenGL window. \n        \n        Args:\n            max_width (int): maximum width of the window. \n        """"""\n        self.max_width = max_width\n        self.closed = False\n        \n        # Create OpenGL window\n        super().__init__(visible=False, vsync=False, resizable=True)\n    \n    def __call__(self, x):\n        r""""""Create an image from the given RGB array and display to the window. \n        \n        Args:\n            x (ndarray): RGB array\n        """"""\n        assert isinstance(x, np.ndarray), f\'expected numpy array dtype, got {type(x)}\'\n        assert x.ndim == 3, f\'expected ndim=3, got {x.ndim}\'\n        assert x.shape[-1] == 3, f\'expected 3 color channel, got {x.shape[-1]}\'\n        \n        # Rescale the window according to the image and maximally allowed width\n        img_height, img_width, _ = x.shape\n        if img_width > self.max_width:  # too large, rescale\n            ratio = self.max_width/img_width\n            win_width = int(ratio*img_width)\n            win_height = int(ratio*img_height)\n        else:  # allowed range\n            win_width = img_width\n            win_height = img_height\n        # Resize the window\n        self.set_size(width=win_width, height=win_height)\n        \n        # Set window to be visible for first call\n        if not self.visible:\n            self.set_visible(True)\n        \n        # Create an image object\n        image = pyglet.image.ImageData(width=img_width, \n                                       height=img_height, \n                                       format=\'RGB\', \n                                       data=x.tobytes(), \n                                       pitch=img_width*-3)\n        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n        texture = image.get_texture()\n        # Resize the image texture as the window size\n        texture.width = self.width\n        texture.height = self.height\n        \n        # Clear the window and display the image\n        self.clear()\n        self.switch_to()\n        self.dispatch_events()\n        texture.blit(0, 0)  # draw image to active framebuffer displaying at lower-left corner\n        self.flip()  # filp the front and backend buffer\n\n    def close(self):\n        r""""""Close the Window. """"""\n        if not self.closed:\n            self.closed = True\n            super().close()\n'"
lagom/vis/utils.py,0,"b""from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom lagom.utils import pickle_load\nfrom lagom.utils import yaml_load\nfrom lagom.transform import interp_curves\nfrom lagom.transform import smooth_filter\n\n\ndef set_ticker(ax, axis='x', num=None, KM_format=False, integer=False):\n    if axis == 'x':\n        axis = ax.xaxis\n    elif axis == 'y':\n        axis = ax.yaxis\n    if num is not None:\n        axis.set_major_locator(plt.MaxNLocator(num))\n    if KM_format:\n        def tick_formatter(x, pos):\n            if abs(x) >= 0 and abs(x) < 1000:\n                return int(x) if integer else x\n            elif abs(x) >= 1000 and abs(x) < 1000000:\n                return f'{int(x/1000)}K' if integer else f'{x/1000}K'\n            elif abs(x) >= 1000000:\n                return f'{int(x/1000000)}M' if integer else f'{x/1000000}M'\n        axis.set_major_formatter(plt.FuncFormatter(tick_formatter))\n    return ax\n\n\ndef read_xy(log_folder, file_name, get_x, get_y, smooth_out=False, smooth_kws=None, point_step=1):\n    glob_dir = lambda x: [p for p in x.glob('*/') if p.is_dir() and str(p.name).isdigit()]\n    dfs = []\n    for id_folder in glob_dir(Path(log_folder)):\n        x = []\n        y = []\n        for seed_folder in glob_dir(id_folder):\n            logs = pickle_load(seed_folder / file_name)\n            x.append([get_x(log) for log in logs])\n            y.append([get_y(log) for log in logs])\n        new_x, ys = interp_curves(x, y)  # all seeds share same x values\n        \n        if smooth_out:\n            if smooth_kws is None:\n                smooth_kws = {'window_length': 51, 'polyorder': 3}\n            ys = [smooth_filter(y, **smooth_kws) for y in ys]\n        \n        if point_step > 1:\n            idx = np.arange(0, new_x.size, step=point_step)\n            new_x = new_x[idx, ...]\n            ys = [y[idx, ...] for y in ys]\n\n        df = pd.DataFrame({'x': np.tile(new_x, len(ys)), 'y': np.hstack(ys)})\n        config = yaml_load(id_folder / 'config.yml')\n        config = pd.DataFrame([config.values()], columns=config.keys())\n        config = config.applymap(lambda x: tuple(x) if isinstance(x, list) else x)\n        df = pd.concat([df, config], axis=1, ignore_index=False)\n        df = df.fillna(method='pad')  # padding all NaN configs\n        dfs.append(df)\n    dfs = pd.concat(dfs, axis=0, ignore_index=True)    \n    return dfs\n"""
legacy/a2c/__init__.py,0,b''
legacy/a2c/agent.py,16,"b'import numpy as np\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom lagom.networks import BaseNetwork\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.networks import linear_lr_scheduler\n\nfrom lagom.policies import BasePolicy\nfrom lagom.policies import CategoricalHead\nfrom lagom.policies import DiagGaussianHead\n\nfrom lagom.value_functions import StateValueHead\n\nfrom lagom.transform import ExplainedVariance\n\nfrom lagom.history.metrics import final_state_from_segment\nfrom lagom.history.metrics import terminal_state_from_segment\nfrom lagom.history.metrics import bootstrapped_returns_from_segment\nfrom lagom.history.metrics import gae_from_segment\n\nfrom lagom.agents import BaseAgent\n\n\nclass MLP(BaseNetwork):\n    def make_params(self, config):\n        self.feature_layers = make_fc(self.env_spec.observation_space.flat_dim, config[\'network.hidden_sizes\'])\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config[\'network.hidden_sizes\']])\n        \n    def init_params(self, config):\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity=\'leaky_relu\', constant_bias=0.0)\n\n    def reset(self, config, **kwargs):\n        pass\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.celu(layer(x)))\n            \n        return x\n    \n    \nclass Critic(BaseNetwork):\n    def make_params(self, config):\n        self.feature_layers = make_fc(self.env_spec.observation_space.flat_dim, config[\'network.hidden_sizes\'])\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config[\'network.hidden_sizes\']])\n        self.output_layer = StateValueHead(config, self.device, config[\'network.hidden_sizes\'][-1])\n        \n    def init_params(self, config):\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity=\'leaky_relu\', constant_bias=0.0)\n            \n        self.make_optimizer(config)\n\n    def make_optimizer(self, config, **kwargs):\n        self.optimizer = optim.Adam(self.parameters(), lr=config[\'algo.lr_V\'])\n        if config[\'algo.use_lr_scheduler\']:\n            if \'train.iter\' in config:\n                self.lr_scheduler = linear_lr_scheduler(self.optimizer, config[\'train.iter\'], \'iteration-based\')\n            elif \'train.timestep\' in config:\n                self.lr_scheduler = linear_lr_scheduler(self.optimizer, config[\'train.timestep\']+1, \'timestep-based\')\n        else:\n            self.lr_scheduler = None\n            \n    def optimizer_step(self, config, **kwargs):\n        if config[\'agent.max_grad_norm\'] is not None:\n            clip_grad_norm_(self.parameters(), config[\'agent.max_grad_norm\'])\n        \n        if self.lr_scheduler is not None:\n            if self.lr_scheduler.mode == \'iteration-based\':\n                self.lr_scheduler.step()\n            elif self.lr_scheduler.mode == \'timestep-based\':\n                self.lr_scheduler.step(kwargs[\'total_T\'])\n        \n        self.optimizer.step()\n            \n    def reset(self, config, **kwargs):\n        pass\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.celu(layer(x)))\n        x = self.output_layer(x)\n            \n        return x\n    \n    \nclass Policy(BasePolicy):\n    def make_networks(self, config):\n        self.feature_network = MLP(config, self.device, env_spec=self.env_spec)\n        feature_dim = config[\'network.hidden_sizes\'][-1]\n        \n        if self.env_spec.control_type == \'Discrete\':\n            self.action_head = CategoricalHead(config, self.device, feature_dim, self.env_spec)\n        elif self.env_spec.control_type == \'Continuous\':\n            self.action_head = DiagGaussianHead(config, \n                                                self.device, \n                                                feature_dim, \n                                                self.env_spec, \n                                                min_std=config[\'agent.min_std\'], \n                                                std_style=config[\'agent.std_style\'], \n                                                constant_std=config[\'agent.constant_std\'],\n                                                std_state_dependent=config[\'agent.std_state_dependent\'],\n                                                init_std=config[\'agent.init_std\'])\n        if not config[\'network.independent_V\']:\n            self.V_head = StateValueHead(config, self.device, feature_dim)\n    \n    def make_optimizer(self, config, **kwargs):\n        self.optimizer = optim.Adam(self.parameters(), lr=config[\'algo.lr\'])\n        if config[\'algo.use_lr_scheduler\']:\n            if \'train.iter\' in config:\n                self.lr_scheduler = linear_lr_scheduler(self.optimizer, config[\'train.iter\'], \'iteration-based\')\n            elif \'train.timestep\' in config:\n                self.lr_scheduler = linear_lr_scheduler(self.optimizer, config[\'train.timestep\']+1, \'timestep-based\')\n        else:\n            self.lr_scheduler = None\n            \n    def optimizer_step(self, config, **kwargs):\n        if config[\'agent.max_grad_norm\'] is not None:\n            clip_grad_norm_(self.parameters(), config[\'agent.max_grad_norm\'])\n        \n        if self.lr_scheduler is not None:\n            if self.lr_scheduler.mode == \'iteration-based\':\n                self.lr_scheduler.step()\n            elif self.lr_scheduler.mode == \'timestep-based\':\n                self.lr_scheduler.step(kwargs[\'total_T\'])\n        \n        self.optimizer.step()\n    \n    @property\n    def recurrent(self):\n        return False\n    \n    def reset(self, config, **kwargs):\n        pass\n\n    def __call__(self, x, out_keys=[\'action\', \'V\'], info={}, **kwargs):\n        out = {}\n        \n        features = self.feature_network(x)\n        action_dist = self.action_head(features)\n        \n        action = action_dist.sample().detach()  # TODO: detach is necessary or not ?\n        out[\'action\'] = action\n        \n        if \'V\' in out_keys:\n            V = self.V_head(features)\n            out[\'V\'] = V\n        if \'action_dist\' in out_keys:\n            out[\'action_dist\'] = action_dist\n        if \'action_logprob\' in out_keys:\n            out[\'action_logprob\'] = action_dist.log_prob(action)\n        if \'entropy\' in out_keys:\n            out[\'entropy\'] = action_dist.entropy()\n        if \'perplexity\' in out_keys:\n            out[\'perplexity\'] = action_dist.perplexity()\n        \n        return out\n\n\nclass Agent(BaseAgent):\n    r""""""Advantage Actor-Critic (A2C). """"""\n    def make_modules(self, config):\n        self.policy = Policy(config, self.env_spec, self.device)\n        if config[\'network.independent_V\']:\n            self.critic = Critic(config, self.device, env_spec=self.env_spec)\n        \n    def prepare(self, config, **kwargs):\n        self.total_T = 0\n\n    def reset(self, config, **kwargs):\n        pass\n\n    def choose_action(self, obs, info={}):\n        obs = torch.from_numpy(np.asarray(obs)).float().to(self.device)\n        \n        if self.training:\n            if self.config[\'network.independent_V\']:\n                out = self.policy(obs, out_keys=[\'action\', \'action_logprob\', \'entropy\'], info=info)\n                out[\'V\'] = self.critic(obs)\n            else:\n                out = self.policy(obs, out_keys=[\'action\', \'action_logprob\', \'V\', \'entropy\'], info=info)\n        else:\n            with torch.no_grad():\n                out = self.policy(obs, out_keys=[\'action\'], info=info)\n            \n        # sanity check for NaN\n        if torch.any(torch.isnan(out[\'action\'])):\n            raise ValueError(\'NaN!\')\n            \n        return out\n\n    def learn(self, D, info={}):\n        logprobs = torch.stack([info[\'action_logprob\'] for info in D.batch_infos], 1).squeeze(-1)\n        entropies = torch.stack([info[\'entropy\'] for info in D.batch_infos], 1).squeeze(-1)\n        all_Vs = torch.stack([info[\'V\'] for info in D.batch_infos], 1).squeeze(-1)\n        \n        last_states = torch.from_numpy(final_state_from_segment(D)).float().to(self.device)\n        with torch.no_grad():\n            if self.config[\'network.independent_V\']:\n                last_Vs = self.critic(last_states)\n            else:\n                last_Vs = self.policy(last_states, out_keys=[\'V\'])[\'V\']\n        Qs = bootstrapped_returns_from_segment(D, last_Vs, self.config[\'algo.gamma\'])\n        Qs = torch.from_numpy(Qs.copy()).float().to(self.device)\n        if self.config[\'agent.standardize_Q\']:\n            Qs = (Qs - Qs.mean(1, keepdim=True))/(Qs.std(1, keepdim=True) + 1e-8)\n        \n        As = gae_from_segment(D, all_Vs, last_Vs, self.config[\'algo.gamma\'], self.config[\'algo.gae_lambda\'])\n        As = torch.from_numpy(As.copy()).float().to(self.device)\n        if self.config[\'agent.standardize_adv\']:\n            As = (As - As.mean(1, keepdim=True))/(As.std(1, keepdim=True) + 1e-8)\n        \n        assert all([x.ndimension() == 2 for x in [logprobs, entropies, all_Vs, Qs, As]])\n        \n        policy_loss = -logprobs*As\n        policy_loss = policy_loss.mean()\n        entropy_loss = -entropies\n        entropy_loss = entropy_loss.mean()\n        value_loss = F.mse_loss(all_Vs, Qs, reduction=\'none\')\n        value_loss = value_loss.mean()\n        \n        entropy_coef = self.config[\'agent.entropy_coef\']\n        value_coef = self.config[\'agent.value_coef\']\n        loss = policy_loss + value_coef*value_loss + entropy_coef*entropy_loss\n        \n        if self.config[\'agent.fit_terminal_value\']:\n            terminal_states = terminal_state_from_segment(D)\n            if terminal_states is not None:\n                terminal_states = torch.from_numpy(terminal_states).float().to(self.device)\n                terminal_Vs = self.policy(terminal_states, out_keys=[\'V\'])[\'V\']\n                terminal_value_loss = F.mse_loss(terminal_Vs, torch.zeros_like(terminal_Vs))\n                terminal_value_loss_coef = self.config[\'agent.terminal_value_coef\']\n                loss += terminal_value_loss_coef*terminal_value_loss\n        \n        self.policy.optimizer.zero_grad()\n        if self.config[\'network.independent_V\']:\n            self.critic.optimizer.zero_grad()\n        loss.backward()\n        self.policy.optimizer_step(self.config, total_T=self.total_T)\n        if self.config[\'network.independent_V\']:\n            self.critic.optimizer_step(self.config, total_T=self.total_T)\n        \n        self.total_T += D.total_T\n        \n        out = {}\n        if self.policy.lr_scheduler is not None:\n            out[\'current_lr\'] = self.policy.lr_scheduler.get_lr()\n        out[\'loss\'] = loss.item()\n        out[\'policy_loss\'] = policy_loss.item()\n        out[\'entropy_loss\'] = entropy_loss.item()\n        out[\'policy_entropy\'] = -entropy_loss.item()\n        out[\'value_loss\'] = value_loss.item()\n        ev = ExplainedVariance()\n        ev = ev(y_true=Qs.detach().cpu().numpy().squeeze(), y_pred=all_Vs.detach().cpu().numpy().squeeze())\n        out[\'explained_variance\'] = ev\n        \n        return out\n\n    @property\n    def recurrent(self):\n        pass\n'"
legacy/a2c/algo.py,1,"b""from pathlib import Path\n\nfrom itertools import count\n\nimport numpy as np\nimport torch\n\nfrom lagom import Logger\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\n\nfrom lagom import BaseAlgorithm\n\nfrom lagom.envs import make_gym_env\nfrom lagom.envs import make_vec_env\nfrom lagom.envs import EnvSpec\nfrom lagom.envs.vec_env import SerialVecEnv\nfrom lagom.envs.vec_env import VecStandardize\nfrom lagom.envs.vec_env import VecClipAction\nfrom lagom.envs.wrappers import TimeAwareObservation\n\nfrom lagom.runner import RollingSegmentRunner\n\nfrom agent import Agent\nfrom engine import Engine\n\n\nclass Algorithm(BaseAlgorithm):\n    def __call__(self, config, seed, device):\n        set_global_seeds(seed)\n        logdir = Path(config['log.dir']) / str(config['ID']) / str(seed)\n\n        if config['env.time_aware_obs']:\n            kwargs = {'extra_wrapper': [TimeAwareObservation]}\n        else:\n            kwargs = {}\n        env = make_vec_env(SerialVecEnv, make_gym_env, config['env.id'], config['train.N'], seed, monitor=True, **kwargs)\n        if config['eval.independent']:\n            eval_env = make_vec_env(SerialVecEnv, make_gym_env, config['env.id'], config['eval.N'], seed)\n        if config['env.clip_action']:\n            env = VecClipAction(env)\n            if config['eval.independent']:\n                eval_env = VecClipAction(eval_env)\n        if config['env.standardize']:  # running averages of observation and reward\n            env = VecStandardize(venv=env, \n                                 use_obs=True, \n                                 use_reward=False,  # A2C specific \n                                 clip_obs=10., \n                                 clip_reward=10., \n                                 gamma=0.99, \n                                 eps=1e-8)    \n        env_spec = EnvSpec(env)\n        \n        agent = Agent(config, env_spec, device)\n        \n        runner = RollingSegmentRunner(config, agent, env)\n        \n        if config['eval.independent']:\n            engine = Engine(agent, runner, config, eval_env=eval_env)\n        else:\n            engine = Engine(agent, runner, config)\n        \n        train_logs = []\n        eval_logs = []\n        for i in count():\n            if 'train.iter' in config and i >= config['train.iter']:  # enough iterations\n                break\n            elif 'train.timestep' in config and agent.total_T >= config['train.timestep']:  # enough timesteps\n                break\n            \n            train_output = engine.train(i)\n            \n            if i == 0 or (i+1) % config['log.interval'] == 0:\n                train_log = engine.log_train(train_output)\n                train_logs.append(train_log)\n                \n                if config['eval.independent']:\n                    with torch.no_grad():  # disable grad, save memory\n                        eval_output = engine.eval(n=i)\n                    eval_log = engine.log_eval(eval_output)\n                    eval_logs.append(eval_log)\n\n        pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n        pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n        \n        return None\n"""
legacy/a2c/engine.py,0,"b""from time import time\nfrom itertools import chain\n\nimport numpy as np\nimport torch\n\nfrom lagom import Logger\nfrom lagom.utils import color_str\n\nfrom lagom.envs.vec_env import get_wrapper\nfrom lagom.envs.vec_env import VecStandardize\nfrom lagom.runner import EpisodeRunner\n\nfrom lagom.engine import BaseEngine\n\n\nclass Engine(BaseEngine):\n    def train(self, n):\n        self.agent.train()\n        \n        start_time = time()\n        \n        T = int(self.config['train.ratio_T']*self.runner.env.T)\n        D = self.runner(T)\n        \n        out_agent = self.agent.learn(D)\n        \n        train_output = {}\n        train_output['D'] = D\n        train_output['out_agent'] = out_agent\n        train_output['n'] = n\n        train_output['num_sec'] = time() - start_time\n        \n        return train_output\n\n    def log_train(self, train_output, **kwargs):\n        D = train_output['D']\n        out_agent = train_output['out_agent']\n        n = train_output['n']\n        num_sec = train_output['num_sec']\n        \n        logger = Logger()\n        logger('train_iteration', n+1)  # starts from 1\n        logger('num_seconds', round(num_sec, 1))\n        \n        [logger(key, value) for key, value in out_agent.items()]\n        \n        logger('num_segments', D.N)\n        logger('num_timesteps', D.total_T)\n        logger('accumulated_trained_timesteps', self.agent.total_T)\n        \n        monitor_env = get_wrapper(self.runner.env, 'VecMonitor')\n        infos = list(filter(lambda info: 'episode' in info, chain.from_iterable(D.infos)))\n        if len(infos) > 0:\n            online_returns = np.asarray([info['episode']['return'] for info in infos])\n            online_horizons = np.asarray([info['episode']['horizon'] for info in infos])\n            logger('online_N', len(infos))\n            logger('online_mean_return', online_returns.mean())\n            logger('online_std_return', online_returns.std())\n            logger('online_min_return', online_returns.min())\n            logger('online_max_return', online_returns.max())\n            logger('online_mean_horizon', online_horizons.mean())\n            logger('online_std_horizon', online_horizons.std())\n            logger('online_min_horizon', online_horizons.min())\n            logger('online_max_horizon', online_horizons.max())\n        running_returns = np.asarray(monitor_env.return_queue)\n        running_horizons = np.asarray(monitor_env.horizon_queue)\n        if running_returns.size > 0 and running_horizons.size > 0:\n            logger('running_queue', [len(monitor_env.return_queue), monitor_env.return_queue.maxlen])\n            logger('running_mean_return', running_returns.mean())\n            logger('running_std_return', running_returns.std())\n            logger('running_min_return', running_returns.min())\n            logger('running_max_return', running_returns.max())\n            logger('running_mean_horizon', running_horizons.mean())\n            logger('running_std_horizon', running_horizons.std())\n            logger('running_min_horizon', running_horizons.min())\n            logger('running_max_horizon', running_horizons.max())\n        \n        print('-'*50)\n        logger.dump(keys=None, index=None, indent=0)\n        print('-'*50)\n\n        return logger.logs\n        \n    def eval(self, n):\n        self.agent.eval()\n        \n        start_time = time()\n        \n        if self.config['env.standardize']:\n            eval_env = VecStandardize(venv=self.eval_env,\n                                      use_obs=True, \n                                      use_reward=False,  # do not process rewards, no training\n                                      clip_obs=self.runner.env.clip_obs, \n                                      clip_reward=self.runner.env.clip_reward, \n                                      gamma=self.runner.env.gamma, \n                                      eps=self.runner.env.eps, \n                                      constant_obs_mean=self.runner.env.obs_runningavg.mu,\n                                      constant_obs_std=self.runner.env.obs_runningavg.sigma)\n        eval_runner = EpisodeRunner(self.config, self.agent, eval_env)\n        T = eval_env.T\n        D = eval_runner(T)\n        \n        eval_output = {}\n        eval_output['D'] = D\n        eval_output['n'] = n\n        eval_output['T'] = T\n        eval_output['num_sec'] = time() - start_time\n        \n        return eval_output\n        \n    def log_eval(self, eval_output, **kwargs):\n        D = eval_output['D']\n        n = eval_output['n']\n        T = eval_output['T']\n        num_sec = eval_output['num_sec']\n        \n        logger = Logger()\n        \n        batch_returns = D.numpy_rewards.sum(1)\n        \n        logger('evaluation_iteration', n+1)\n        logger('num_seconds', round(num_sec, 1))\n        logger('num_trajectories', D.N)\n        logger('max_allowed_horizon', T)\n        logger('mean_horizon', D.Ts.mean())\n        logger('total_timesteps', D.total_T)\n        logger('accumulated_trained_timesteps', self.agent.total_T)\n        logger('mean_return', batch_returns.mean())\n        logger('std_return', batch_returns.std())\n        logger('min_return', batch_returns.min())\n        logger('max_return', batch_returns.max())\n        \n        print(color_str('+'*50, 'yellow', 'bold'))\n        logger.dump(keys=None, index=None, indent=0)\n        print(color_str('+'*50, 'yellow', 'bold'))\n\n        return logger.logs\n"""
legacy/a2c/experiment.py,0,"b""from lagom.experiment import Configurator\nfrom lagom.experiment import BaseExperimentWorker\nfrom lagom.experiment import BaseExperimentMaster\nfrom lagom.experiment import run_experiment\n\nfrom algo import Algorithm\n\n\nclass ExperimentWorker(BaseExperimentWorker):\n    def prepare(self):\n        pass\n        \n    def make_algo(self):\n        algo = Algorithm()\n        \n        return algo\n\n\nclass ExperimentMaster(BaseExperimentMaster):\n    def make_configs(self):\n        configurator = Configurator('grid')\n        \n        configurator.fixed('cuda', True)  # whether to use GPU\n        \n        configurator.fixed('env.id', 'HalfCheetah-v2')\n        configurator.fixed('env.standardize', True)  # whether to use VecStandardize\n        configurator.fixed('env.time_aware_obs', False)  # whether to append time step to observation\n        \n        configurator.fixed('network.recurrent', False)\n        configurator.fixed('network.hidden_sizes', [64, 64])  # TODO: [64, 64]\n        configurator.fixed('network.independent_V', False)  # share or not for params of policy and value network\n        \n        configurator.fixed('algo.lr', 7e-4)\n        configurator.fixed('algo.lr_V', 1e-3)\n        configurator.fixed('algo.use_lr_scheduler', True)\n        configurator.fixed('algo.gamma', 0.99)\n        configurator.fixed('algo.gae_lambda', 0.97)\n        \n        configurator.fixed('agent.standardize_Q', False)  # whether to standardize discounted returns\n        configurator.fixed('agent.standardize_adv', False)  # whether to standardize advantage estimates\n        configurator.fixed('agent.max_grad_norm', 0.5)  # grad clipping, set None to turn off\n        configurator.fixed('agent.entropy_coef', 0.01)\n        configurator.fixed('agent.value_coef', 0.5)\n        configurator.fixed('agent.fit_terminal_value', False)\n        configurator.fixed('agent.terminal_value_coef', 0.1)\n        # only for continuous control\n        configurator.fixed('env.clip_action', True)  # clip sampled action within valid bound before step()\n        configurator.fixed('agent.min_std', 1e-6)  # min threshould for std, avoid numerical instability\n        configurator.fixed('agent.std_style', 'exp')  # std parameterization, 'exp' or 'softplus'\n        configurator.fixed('agent.constant_std', None)  # constant std, set None to learn it\n        configurator.fixed('agent.std_state_dependent', False)  # whether to learn std with state dependency\n        configurator.fixed('agent.init_std', 0.5)  # initial std for state-independent std\n        \n        configurator.fixed('train.timestep', 1e6)  # either 'train.iter' or 'train.timestep'\n        configurator.fixed('train.N', 1)  # number of segments per training iteration\n        configurator.fixed('train.ratio_T', 0.005)  # percentage of max allowed horizon\n        configurator.fixed('eval.independent', False)\n        configurator.fixed('eval.N', 10)  # number of episodes to evaluate, do not specify T for complete episode\n        \n        configurator.fixed('log.interval', 1000)  # logging interval\n        configurator.fixed('log.dir', 'logs')  # logging directory\n        \n        list_config = configurator.make_configs()\n        \n        return list_config\n\n    def make_seeds(self):\n        list_seed = [1770966829, 1500925526, 2054191100]\n        \n        return list_seed\n    \n    def process_results(self, results):\n        assert all([result is None for result in results])\n\n        \nif __name__ == '__main__':\n    run_experiment(worker_class=ExperimentWorker, \n                   master_class=ExperimentMaster, \n                   num_worker=100)\n"""
legacy/impala/__init__.py,0,b''
legacy/impala/agent.py,6,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom gym.spaces import Discrete\nfrom gym.spaces import Box\nfrom gym.spaces import flatdim\n\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.envs.wrappers import get_wrapper\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.metric import vtrace\nfrom lagom.transform import explained_variance as ev\nfrom lagom.transform import describe\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space), config['nn.sizes'])\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        feature_dim = config['nn.sizes'][-1]\n        self.feature_network = MLP(config, env, device, **kwargs)\n        if isinstance(env.action_space, Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, Box):\n            self.action_head = DiagGaussianHead(feature_dim, flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.V_head = nn.Linear(feature_dim, 1)\n        ortho_init(self.V_head, weight_scale=1.0, constant_bias=0.0)\n        self.V_head = self.V_head.to(device)  # reproducible between CPU/GPU, ortho_init behaves differently\n        \n        self.register_buffer('total_timestep', torch.tensor(0))\n        #self.total_timestep = 0\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=config['agent.lr'])\n        if config['agent.use_lr_scheduler']:\n            self.lr_scheduler = linear_lr_scheduler(self.optimizer, config['train.timestep'], min_lr=1e-8)\n        self.gamma = config['agent.gamma']\n        self.clip_rho = config['agent.clip_rho']\n        self.clip_pg_rho = config['agent.clip_pg_rho']\n        \n    def choose_action(self, obs, **kwargs):\n        obs = tensorify(obs, self.device)\n        out = {}\n        features = self.feature_network(obs)\n        \n        action_dist = self.action_head(features)\n        out['action_dist'] = action_dist\n        out['entropy'] = action_dist.entropy()\n        \n        action = action_dist.sample()\n        out['action'] = action\n        out['raw_action'] = numpify(action, self.env.action_space.dtype)\n        out['action_logprob'] = action_dist.log_prob(action.detach())\n        \n        V = self.V_head(features)\n        out['V'] = V\n        return out\n    \n    def learn(self, D, **kwargs):\n        # Compute all metrics, D: list of Trajectory\n        Ts = [len(traj) for traj in D]\n        behavior_logprobs = [torch.cat(traj.get_all_info('action_logprob')) for traj in D]\n        out_agent = self.choose_action(np.concatenate([traj.numpy_observations[:-1] for traj in D], 0))\n        logprobs = out_agent['action_logprob'].squeeze()\n        entropies = out_agent['entropy'].squeeze()\n        Vs = out_agent['V'].squeeze()\n        with torch.no_grad():\n            last_observations = tensorify(np.concatenate([traj.last_observation for traj in D], 0), self.device)\n            last_Vs = self.V_head(self.feature_network(last_observations)).squeeze(-1)\n\n        vs, As = [], []\n        for traj, behavior_logprob, logprob, V, last_V in zip(D, behavior_logprobs, logprobs.detach().cpu().split(Ts), \n                                                              Vs.detach().cpu().split(Ts), last_Vs):\n            v, A = vtrace(behavior_logprob, logprob, self.gamma, traj.rewards, V, last_V, \n                          traj.reach_terminal, self.clip_rho, self.clip_pg_rho)\n            vs.append(v)\n            As.append(A)\n    \n        # Metrics -> Tensor, device\n        vs, As = map(lambda x: tensorify(np.concatenate(x).copy(), self.device), [vs, As])\n        if self.config['agent.standardize_adv']:\n            As = (As - As.mean())/(As.std() + 1e-8)\n        \n        assert all([x.ndimension() == 1 for x in [logprobs, entropies, Vs, vs, As]])\n        \n        # Loss\n        policy_loss = -logprobs*As\n        entropy_loss = -entropies\n        value_loss = F.mse_loss(Vs, vs, reduction='none')\n        \n        loss = policy_loss + self.config['agent.value_coef']*value_loss + self.config['agent.entropy_coef']*entropy_loss\n        loss = loss.mean()\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        grad_norm = nn.utils.clip_grad_norm_(self.parameters(), self.config['agent.max_grad_norm'])\n        self.optimizer.step()\n        if self.config['agent.use_lr_scheduler']:\n            self.lr_scheduler.step(self.total_timestep)\n        \n        self.total_timestep += sum([len(traj) for traj in D])\n        out = {}\n        if self.config['agent.use_lr_scheduler']:\n            out['current_lr'] = self.lr_scheduler.get_lr()\n        out['loss'] = loss.item()\n        out['grad_norm'] = grad_norm\n        out['policy_loss'] = policy_loss.mean().item()\n        out['entropy_loss'] = entropy_loss.mean().item()\n        out['policy_entropy'] = -entropy_loss.mean().item()\n        out['value_loss'] = value_loss.mean().item()\n        out['V'] = describe(numpify(Vs, 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['explained_variance'] = ev(y_true=numpify(vs, 'float'), y_pred=numpify(Vs, 'float'))\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        obs_env = get_wrapper(self.env, 'VecStandardizeObservation')\n        if obs_env is not None:\n            pickle_dump(obj=(obs_env.mean, obs_env.var), f=logdir/f'obs_moments_{num_iter}', ext='.pth')"""
legacy/impala/engine.py,0,"b""from time import perf_counter\nfrom itertools import chain\n\nimport numpy as np\nimport torch\n\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\nfrom lagom.utils import color_str\nfrom lagom.envs.wrappers import get_wrapper\n\n\nclass Engine(BaseEngine):        \n    def train(self, n=None, **kwargs):\n        self.agent.train()\n        start_time = perf_counter()\n        \n        D = kwargs['D']\n        out_agent = self.agent.learn(D) \n        \n        logger = Logger()\n        logger('train_iteration', n+1)\n        logger('num_seconds', round(perf_counter() - start_time, 1))\n        [logger(key, value) for key, value in out_agent.items()]\n        logger('num_trajectories', len(D))\n        logger('num_timesteps', sum([len(traj) for traj in D]))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        G = [traj.numpy_rewards.sum() for traj in D]\n        logger('return', describe(G, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        \n        infos = [info for info in chain.from_iterable([traj.infos for traj in D]) if 'episode' in info]\n        online_returns = [info['episode']['return'] for info in infos]\n        online_horizons = [info['episode']['horizon'] for info in infos]\n        logger('online_return', describe(online_returns, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe(online_horizons, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            \n        monitor_env = get_wrapper(self.env, 'VecMonitor')\n        logger('running_return', describe(monitor_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(monitor_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        return logger\n        \n    def eval(self, n=None, **kwargs):\n        pass\n"""
legacy/impala/experiment.py,9,"b""import time\nimport os\nfrom pathlib import Path\nfrom itertools import count\nfrom itertools import chain\n\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import ClipAction\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom lagom import Logger\nfrom lagom import EpisodeRunner\nfrom lagom.transform import describe\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.utils import color_str\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import Sample\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import make_vec_env\nfrom lagom.envs.wrappers import get_wrapper\nfrom lagom.envs.wrappers import VecMonitor\nfrom lagom.envs.wrappers import VecStandardizeObservation\nfrom lagom.envs.wrappers import VecStandardizeReward\nfrom lagom.envs.wrappers import VecStepInfo\n\nfrom baselines.impala.agent import Agent\nfrom baselines.impala.engine import Engine\n\n\nconfig = Config(\n    {'log.freq': 1, \n     'checkpoint.num': 3,\n     \n     'env.id': 'Hopper-v3',###Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3', 'Swimmer-v3']), \n     'env.standardize_obs': False,\n     'env.standardize_reward': False,\n     \n     'nn.sizes': [64, 64],\n     \n     'agent.lr': 7e-4,\n     'agent.use_lr_scheduler': False,\n     'agent.gamma': 0.99,\n     'agent.standardize_adv': True,  # standardize advantage estimates\n     'agent.max_grad_norm': 0.5,  # grad clipping by norm\n     'agent.entropy_coef': 0.01,\n     'agent.value_coef': 0.5,\n     'agent.clip_rho': 1.0,\n     'agent.clip_pg_rho': 1.0,\n     'agent.num_actors': 2,\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'train.timestep_per_iter': 200,\n     'train.batch_size': 64,  # number of timesteps per iteration\n     'eval.freq': 5000, \n     'eval.num_episode': 10\n     \n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    def _make_env():\n        env = gym.make(config['env.id'])\n        if config['env.clip_action'] and isinstance(env.action_space, Box):\n            env = ClipAction(env)\n        return env\n    env = make_vec_env(_make_env, 1, seed)  # single environment\n    env = VecMonitor(env)\n    if mode == 'train':\n        if config['env.standardize_obs']:\n            env = VecStandardizeObservation(env, clip=5.)\n        if config['env.standardize_reward']:\n            env = VecStandardizeReward(env, clip=10., gamma=config['agent.gamma'])\n        env = VecStepInfo(env)\n    return env\n\n\ndef actor(config, seed, make_env, learner_agent, runner, queue):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, torch.device('cpu'))\n    while learner_agent.total_timestep < config['train.timestep']:\n        \n        time.sleep(5.0)\n        t0 = time.perf_counter()\n        \n        \n        agent.load_state_dict(learner_agent.state_dict())  # copy to CPU by default\n        with torch.no_grad():\n            D = runner(agent, env, config['train.timestep_per_iter'])\n            [queue.put(traj) for traj in D]\n        \n        print(f'Actor #{os.getpid()}: collected {len(D)} trajectories, used {round(time.perf_counter() - t0, 1)} s, Queue size: {queue.qsize()}')\n        \n\n\ndef learner(config, logdir, agent, engine, queue):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    train_logs = []\n    checkpoint_count = 0\n    n = 0\n    while agent.total_timestep < config['train.timestep']:\n        D = []\n        while len(D) < config['train.batch_size']:\n            while queue.empty():\n                time.sleep(0.01)\n            D.append(queue.get_nowait())\n        train_logger = engine.train(n, D=D)\n        train_logs.append(train_logger.logs)\n        if n == 0 or (n+1) % config['log.freq'] == 0:\n            train_logger.dump(keys=None, index=0, indent=0, border='-'*50)\n        if agent.total_timestep >= int(config['train.timestep']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n            agent.checkpoint(logdir, n + 1)\n            checkpoint_count += 1\n        n += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n\n\ndef evaluator(config, logdir, seed, make_env, learner_agent):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    eval_logs = []\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, torch.device('cpu'))\n    runner = EpisodeRunner(reset_on_call=True)\n    evaluated_steps = config['eval.freq']\n    while learner_agent.total_timestep < config['train.timestep']:\n        if learner_agent.total_timestep < evaluated_steps:\n            time.sleep(1.0)\n        else:\n            t0 = time.perf_counter()\n            agent.load_state_dict(learner_agent.state_dict())  # copy to CPU by default\n            with torch.no_grad():\n                D = []\n                for _ in range(config['eval.num_episode']):\n                    D += runner(agent, env, env.spec.max_episode_steps)\n            logger = Logger()\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            logger('num_trajectories', len(D))\n            logger('num_timesteps', sum([len(traj) for traj in D]))\n            logger('accumulated_trained_timesteps', learner_agent.total_timestep)\n\n            infos = [info for info in chain.from_iterable([traj.infos for traj in D]) if 'episode' in info]\n            online_returns = [info['episode']['return'] for info in infos]\n            online_horizons = [info['episode']['horizon'] for info in infos]\n            logger('online_return', describe(online_returns, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('online_horizon', describe(online_horizons, axis=-1, repr_indent=1, repr_prefix='\\n'))\n\n            monitor_env = get_wrapper(env, 'VecMonitor')\n            logger('running_return', describe(monitor_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('running_horizon', describe(monitor_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger.dump(keys=None, index=0, indent=0, border=color_str('+'*50, color='green'))\n            eval_logs.append(logger.logs)\n            \n            evaluated_steps += config['eval.freq']\n    pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    queue = mp.Queue(maxsize=100)\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    agent.share_memory()\n    runner = EpisodeRunner(reset_on_call=False)\n    engine = Engine(config, agent=agent, env=env, runner=runner)\n    \n    learner_process = mp.Process(target=learner, args=(config, logdir, agent, engine, queue))\n    actor_processes = [mp.Process(target=actor, args=(config, seed, make_env, agent, runner, queue)) \n                       for _ in range(config['agent.num_actors'])]\n    evaluator_process = mp.Process(target=evaluator, args=(config, logdir, seed, make_env, agent))\n    \n    learner_process.start()\n    print('Learner started !')\n    [p.start() for p in actor_processes]\n    print('Actors started !')\n    evaluator_process.start()\n    print('Evaluator started !')\n    evaluator_process.join()\n    [p.join() for p in actor_processes]\n    learner_process.join()\n    return None\n    \n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')  # IMPORTANT for agent.share_memory()\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829],  ###[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=None, ########os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=True,  # IMPALA benefits from GPU\n                   gpu_ids=None)"""
legacy/impala/logs/default/source_files/__init__.py,0,b''
legacy/impala/logs/default/source_files/agent.py,6,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom gym.spaces import Discrete\nfrom gym.spaces import Box\nfrom gym.spaces import flatdim\n\nfrom lagom import BaseAgent\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import tensorify\nfrom lagom.utils import numpify\nfrom lagom.envs.wrappers import get_wrapper\nfrom lagom.networks import Module\nfrom lagom.networks import make_fc\nfrom lagom.networks import ortho_init\nfrom lagom.networks import CategoricalHead\nfrom lagom.networks import DiagGaussianHead\nfrom lagom.networks import linear_lr_scheduler\nfrom lagom.metric import vtrace\nfrom lagom.transform import explained_variance as ev\nfrom lagom.transform import describe\n\n\nclass MLP(Module):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.env = env\n        self.device = device\n        \n        self.feature_layers = make_fc(flatdim(env.observation_space), config['nn.sizes'])\n        for layer in self.feature_layers:\n            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for hidden_size in config['nn.sizes']])\n        \n        self.to(self.device)\n        \n    def forward(self, x):\n        for layer, layer_norm in zip(self.feature_layers, self.layer_norms):\n            x = layer_norm(F.relu(layer(x)))\n        return x\n\n\nclass Agent(BaseAgent):\n    def __init__(self, config, env, device, **kwargs):\n        super().__init__(config, env, device, **kwargs)\n        \n        feature_dim = config['nn.sizes'][-1]\n        self.feature_network = MLP(config, env, device, **kwargs)\n        if isinstance(env.action_space, Discrete):\n            self.action_head = CategoricalHead(feature_dim, env.action_space.n, device, **kwargs)\n        elif isinstance(env.action_space, Box):\n            self.action_head = DiagGaussianHead(feature_dim, flatdim(env.action_space), device, config['agent.std0'], **kwargs)\n        self.V_head = nn.Linear(feature_dim, 1)\n        ortho_init(self.V_head, weight_scale=1.0, constant_bias=0.0)\n        self.V_head = self.V_head.to(device)  # reproducible between CPU/GPU, ortho_init behaves differently\n        \n        self.register_buffer('total_timestep', torch.tensor(0))\n        #self.total_timestep = 0\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=config['agent.lr'])\n        if config['agent.use_lr_scheduler']:\n            self.lr_scheduler = linear_lr_scheduler(self.optimizer, config['train.timestep'], min_lr=1e-8)\n        self.gamma = config['agent.gamma']\n        self.clip_rho = config['agent.clip_rho']\n        self.clip_pg_rho = config['agent.clip_pg_rho']\n        \n    def choose_action(self, obs, **kwargs):\n        obs = tensorify(obs, self.device)\n        out = {}\n        features = self.feature_network(obs)\n        \n        action_dist = self.action_head(features)\n        out['action_dist'] = action_dist\n        out['entropy'] = action_dist.entropy()\n        \n        action = action_dist.sample()\n        out['action'] = action\n        out['raw_action'] = numpify(action, self.env.action_space.dtype)\n        out['action_logprob'] = action_dist.log_prob(action.detach())\n        \n        V = self.V_head(features)\n        out['V'] = V\n        return out\n    \n    def learn(self, D, **kwargs):\n        # Compute all metrics, D: list of Trajectory\n        Ts = [len(traj) for traj in D]\n        behavior_logprobs = [torch.cat(traj.get_all_info('action_logprob')) for traj in D]\n        out_agent = self.choose_action(np.concatenate([traj.numpy_observations[:-1] for traj in D], 0))\n        logprobs = out_agent['action_logprob'].squeeze()\n        entropies = out_agent['entropy'].squeeze()\n        Vs = out_agent['V'].squeeze()\n        with torch.no_grad():\n            last_observations = tensorify(np.concatenate([traj.last_observation for traj in D], 0), self.device)\n            last_Vs = self.V_head(self.feature_network(last_observations)).squeeze(-1)\n\n        vs, As = [], []\n        for traj, behavior_logprob, logprob, V, last_V in zip(D, behavior_logprobs, logprobs.detach().cpu().split(Ts), \n                                                              Vs.detach().cpu().split(Ts), last_Vs):\n            v, A = vtrace(behavior_logprob, logprob, self.gamma, traj.rewards, V, last_V, \n                          traj.reach_terminal, self.clip_rho, self.clip_pg_rho)\n            vs.append(v)\n            As.append(A)\n    \n        # Metrics -> Tensor, device\n        vs, As = map(lambda x: tensorify(np.concatenate(x).copy(), self.device), [vs, As])\n        if self.config['agent.standardize_adv']:\n            As = (As - As.mean())/(As.std() + 1e-8)\n        \n        assert all([x.ndimension() == 1 for x in [logprobs, entropies, Vs, vs, As]])\n        \n        # Loss\n        policy_loss = -logprobs*As\n        entropy_loss = -entropies\n        value_loss = F.mse_loss(Vs, vs, reduction='none')\n        \n        loss = policy_loss + self.config['agent.value_coef']*value_loss + self.config['agent.entropy_coef']*entropy_loss\n        loss = loss.mean()\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        grad_norm = nn.utils.clip_grad_norm_(self.parameters(), self.config['agent.max_grad_norm'])\n        self.optimizer.step()\n        if self.config['agent.use_lr_scheduler']:\n            self.lr_scheduler.step(self.total_timestep)\n        \n        self.total_timestep += sum([len(traj) for traj in D])\n        out = {}\n        if self.config['agent.use_lr_scheduler']:\n            out['current_lr'] = self.lr_scheduler.get_lr()\n        out['loss'] = loss.item()\n        out['grad_norm'] = grad_norm\n        out['policy_loss'] = policy_loss.mean().item()\n        out['entropy_loss'] = entropy_loss.mean().item()\n        out['policy_entropy'] = -entropy_loss.mean().item()\n        out['value_loss'] = value_loss.mean().item()\n        out['V'] = describe(numpify(Vs, 'float').squeeze(), axis=-1, repr_indent=1, repr_prefix='\\n')\n        out['explained_variance'] = ev(y_true=numpify(vs, 'float'), y_pred=numpify(Vs, 'float'))\n        return out\n    \n    def checkpoint(self, logdir, num_iter):\n        self.save(logdir/f'agent_{num_iter}.pth')\n        obs_env = get_wrapper(self.env, 'VecStandardizeObservation')\n        if obs_env is not None:\n            pickle_dump(obj=(obs_env.mean, obs_env.var), f=logdir/f'obs_moments_{num_iter}', ext='.pth')"""
legacy/impala/logs/default/source_files/engine.py,0,"b""from time import perf_counter\nfrom itertools import chain\n\nimport numpy as np\nimport torch\n\nfrom lagom import Logger\nfrom lagom import BaseEngine\nfrom lagom.transform import describe\nfrom lagom.utils import color_str\nfrom lagom.envs.wrappers import get_wrapper\n\n\nclass Engine(BaseEngine):        \n    def train(self, n=None, **kwargs):\n        self.agent.train()\n        start_time = perf_counter()\n        \n        D = kwargs['D']\n        out_agent = self.agent.learn(D) \n        \n        logger = Logger()\n        logger('train_iteration', n+1)\n        logger('num_seconds', round(perf_counter() - start_time, 1))\n        [logger(key, value) for key, value in out_agent.items()]\n        logger('num_trajectories', len(D))\n        logger('num_timesteps', sum([len(traj) for traj in D]))\n        logger('accumulated_trained_timesteps', self.agent.total_timestep)\n        G = [traj.numpy_rewards.sum() for traj in D]\n        logger('return', describe(G, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        \n        infos = [info for info in chain.from_iterable([traj.infos for traj in D]) if 'episode' in info]\n        online_returns = [info['episode']['return'] for info in infos]\n        online_horizons = [info['episode']['horizon'] for info in infos]\n        logger('online_return', describe(online_returns, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('online_horizon', describe(online_horizons, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            \n        monitor_env = get_wrapper(self.env, 'VecMonitor')\n        logger('running_return', describe(monitor_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        logger('running_horizon', describe(monitor_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n        return logger\n        \n    def eval(self, n=None, **kwargs):\n        pass\n"""
legacy/impala/logs/default/source_files/experiment.py,9,"b""import time\nimport os\nfrom pathlib import Path\nfrom itertools import count\nfrom itertools import chain\n\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import ClipAction\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom lagom import Logger\nfrom lagom import EpisodeRunner\nfrom lagom.transform import describe\nfrom lagom.utils import pickle_dump\nfrom lagom.utils import set_global_seeds\nfrom lagom.utils import color_str\nfrom lagom.experiment import Config\nfrom lagom.experiment import Grid\nfrom lagom.experiment import Sample\nfrom lagom.experiment import run_experiment\nfrom lagom.envs import make_vec_env\nfrom lagom.envs.wrappers import get_wrapper\nfrom lagom.envs.wrappers import VecMonitor\nfrom lagom.envs.wrappers import VecStandardizeObservation\nfrom lagom.envs.wrappers import VecStandardizeReward\nfrom lagom.envs.wrappers import VecStepInfo\n\nfrom baselines.impala.agent import Agent\nfrom baselines.impala.engine import Engine\n\n\nconfig = Config(\n    {'log.freq': 1, \n     'checkpoint.num': 3,\n     \n     'env.id': 'Hopper-v3',###Grid(['HalfCheetah-v3', 'Hopper-v3', 'Walker2d-v3', 'Swimmer-v3']), \n     'env.standardize_obs': False,\n     'env.standardize_reward': False,\n     \n     'nn.sizes': [64, 64],\n     \n     'agent.lr': 7e-4,\n     'agent.use_lr_scheduler': False,\n     'agent.gamma': 0.99,\n     'agent.standardize_adv': True,  # standardize advantage estimates\n     'agent.max_grad_norm': 0.5,  # grad clipping by norm\n     'agent.entropy_coef': 0.01,\n     'agent.value_coef': 0.5,\n     'agent.clip_rho': 1.0,\n     'agent.clip_pg_rho': 1.0,\n     'agent.num_actors': 2,\n     \n     # only for continuous control\n     'env.clip_action': True,  # clip action within valid bound before step()\n     'agent.std0': 0.6,  # initial std\n     \n     'train.timestep': int(1e6),  # total number of training (environmental) timesteps\n     'train.timestep_per_iter': 200,\n     'train.batch_size': 64,  # number of timesteps per iteration\n     'eval.freq': 5000, \n     'eval.num_episode': 10\n     \n    })\n\n\ndef make_env(config, seed, mode):\n    assert mode in ['train', 'eval']\n    def _make_env():\n        env = gym.make(config['env.id'])\n        if config['env.clip_action'] and isinstance(env.action_space, Box):\n            env = ClipAction(env)\n        return env\n    env = make_vec_env(_make_env, 1, seed)  # single environment\n    env = VecMonitor(env)\n    if mode == 'train':\n        if config['env.standardize_obs']:\n            env = VecStandardizeObservation(env, clip=5.)\n        if config['env.standardize_reward']:\n            env = VecStandardizeReward(env, clip=10., gamma=config['agent.gamma'])\n        env = VecStepInfo(env)\n    return env\n\n\ndef actor(config, seed, make_env, learner_agent, runner, queue):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, torch.device('cpu'))\n    while learner_agent.total_timestep < config['train.timestep']:\n        \n        time.sleep(5.0)\n        t0 = time.perf_counter()\n        \n        \n        agent.load_state_dict(learner_agent.state_dict())  # copy to CPU by default\n        with torch.no_grad():\n            D = runner(agent, env, config['train.timestep_per_iter'])\n            [queue.put(traj) for traj in D]\n        \n        print(f'Actor #{os.getpid()}: collected {len(D)} trajectories, used {round(time.perf_counter() - t0, 1)} s, Queue size: {queue.qsize()}')\n        \n\n\ndef learner(config, logdir, agent, engine, queue):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    train_logs = []\n    checkpoint_count = 0\n    n = 0\n    while agent.total_timestep < config['train.timestep']:\n        D = []\n        while len(D) < config['train.batch_size']:\n            while queue.empty():\n                time.sleep(0.01)\n            D.append(queue.get_nowait())\n        train_logger = engine.train(n, D=D)\n        train_logs.append(train_logger.logs)\n        if n == 0 or (n+1) % config['log.freq'] == 0:\n            train_logger.dump(keys=None, index=0, indent=0, border='-'*50)\n        if agent.total_timestep >= int(config['train.timestep']*(checkpoint_count/(config['checkpoint.num'] - 1))):\n            agent.checkpoint(logdir, n + 1)\n            checkpoint_count += 1\n        n += 1\n    pickle_dump(obj=train_logs, f=logdir/'train_logs', ext='.pkl')\n\n\ndef evaluator(config, logdir, seed, make_env, learner_agent):\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    eval_logs = []\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, torch.device('cpu'))\n    runner = EpisodeRunner(reset_on_call=True)\n    evaluated_steps = config['eval.freq']\n    while learner_agent.total_timestep < config['train.timestep']:\n        if learner_agent.total_timestep < evaluated_steps:\n            time.sleep(1.0)\n        else:\n            t0 = time.perf_counter()\n            agent.load_state_dict(learner_agent.state_dict())  # copy to CPU by default\n            with torch.no_grad():\n                D = []\n                for _ in range(config['eval.num_episode']):\n                    D += runner(agent, env, env.spec.max_episode_steps)\n            logger = Logger()\n            logger('num_seconds', round(time.perf_counter() - t0, 1))\n            logger('num_trajectories', len(D))\n            logger('num_timesteps', sum([len(traj) for traj in D]))\n            logger('accumulated_trained_timesteps', learner_agent.total_timestep)\n\n            infos = [info for info in chain.from_iterable([traj.infos for traj in D]) if 'episode' in info]\n            online_returns = [info['episode']['return'] for info in infos]\n            online_horizons = [info['episode']['horizon'] for info in infos]\n            logger('online_return', describe(online_returns, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('online_horizon', describe(online_horizons, axis=-1, repr_indent=1, repr_prefix='\\n'))\n\n            monitor_env = get_wrapper(env, 'VecMonitor')\n            logger('running_return', describe(monitor_env.return_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger('running_horizon', describe(monitor_env.horizon_queue, axis=-1, repr_indent=1, repr_prefix='\\n'))\n            logger.dump(keys=None, index=0, indent=0, border=color_str('+'*50, color='green'))\n            eval_logs.append(logger.logs)\n            \n            evaluated_steps += config['eval.freq']\n    pickle_dump(obj=eval_logs, f=logdir/'eval_logs', ext='.pkl')\n\n\ndef run(config, seed, device, logdir):\n    set_global_seeds(seed)\n    \n    queue = mp.Queue(maxsize=100)\n    env = make_env(config, seed, 'train')\n    agent = Agent(config, env, device)\n    agent.share_memory()\n    runner = EpisodeRunner(reset_on_call=False)\n    engine = Engine(config, agent=agent, env=env, runner=runner)\n    \n    learner_process = mp.Process(target=learner, args=(config, logdir, agent, engine, queue))\n    actor_processes = [mp.Process(target=actor, args=(config, seed, make_env, agent, runner, queue)) \n                       for _ in range(config['agent.num_actors'])]\n    evaluator_process = mp.Process(target=evaluator, args=(config, logdir, seed, make_env, agent))\n    \n    learner_process.start()\n    print('Learner started !')\n    [p.start() for p in actor_processes]\n    print('Actors started !')\n    evaluator_process.start()\n    print('Evaluator started !')\n    evaluator_process.join()\n    [p.join() for p in actor_processes]\n    learner_process.join()\n    return None\n    \n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')  # IMPORTANT for agent.share_memory()\n    torch.set_num_threads(1)  # VERY IMPORTANT TO AVOID GETTING STUCK\n    run_experiment(run=run, \n                   config=config, \n                   seeds=[1770966829],  ###[1770966829, 1500925526, 2054191100], \n                   log_dir='logs/default',\n                   max_workers=None, ########os.cpu_count(), \n                   chunksize=1, \n                   use_gpu=True,  # IMPALA benefits from GPU\n                   gpu_ids=None)"""
