file_path,api_count,code
backbone.py,1,"b'# Author: Zylo117\n\nimport math\n\nimport torch\nfrom torch import nn\n\nfrom efficientdet.model import BiFPN, Regressor, Classifier, EfficientNet\nfrom efficientdet.utils import Anchors\n\n\nclass EfficientDetBackbone(nn.Module):\n    def __init__(self, num_classes=80, compound_coef=0, load_weights=False, **kwargs):\n        super(EfficientDetBackbone, self).__init__()\n        self.compound_coef = compound_coef\n\n        self.backbone_compound_coef = [0, 1, 2, 3, 4, 5, 6, 6]\n        self.fpn_num_filters = [64, 88, 112, 160, 224, 288, 384, 384]\n        self.fpn_cell_repeats = [3, 4, 5, 6, 7, 7, 8, 8]\n        self.input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n        self.box_class_repeats = [3, 3, 3, 4, 4, 4, 5, 5]\n        self.anchor_scale = [4., 4., 4., 4., 4., 4., 4., 5.]\n        self.aspect_ratios = kwargs.get(\'ratios\', [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)])\n        self.num_scales = len(kwargs.get(\'scales\', [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]))\n        conv_channel_coef = {\n            # the channels of P3/P4/P5.\n            0: [40, 112, 320],\n            1: [40, 112, 320],\n            2: [48, 120, 352],\n            3: [48, 136, 384],\n            4: [56, 160, 448],\n            5: [64, 176, 512],\n            6: [72, 200, 576],\n            7: [72, 200, 576],\n        }\n\n        num_anchors = len(self.aspect_ratios) * self.num_scales\n\n        self.bifpn = nn.Sequential(\n            *[BiFPN(self.fpn_num_filters[self.compound_coef],\n                    conv_channel_coef[compound_coef],\n                    True if _ == 0 else False,\n                    attention=True if compound_coef < 6 else False)\n              for _ in range(self.fpn_cell_repeats[compound_coef])])\n\n        self.num_classes = num_classes\n        self.regressor = Regressor(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n                                   num_layers=self.box_class_repeats[self.compound_coef])\n        self.classifier = Classifier(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n                                     num_classes=num_classes,\n                                     num_layers=self.box_class_repeats[self.compound_coef])\n\n        self.anchors = Anchors(anchor_scale=self.anchor_scale[compound_coef], **kwargs)\n\n        self.backbone_net = EfficientNet(self.backbone_compound_coef[compound_coef], load_weights)\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def forward(self, inputs):\n        max_size = inputs.shape[-1]\n\n        _, p3, p4, p5 = self.backbone_net(inputs)\n\n        features = (p3, p4, p5)\n        features = self.bifpn(features)\n\n        regression = self.regressor(features)\n        classification = self.classifier(features)\n        anchors = self.anchors(inputs, inputs.dtype)\n\n        return features, regression, classification, anchors\n\n    def init_backbone(self, path):\n        state_dict = torch.load(path)\n        try:\n            ret = self.load_state_dict(state_dict, strict=False)\n            print(ret)\n        except RuntimeError as e:\n            print(\'Ignoring \' + str(e) + \'""\')\n'"
coco_eval.py,2,"b'# Author: Zylo117\n\n""""""\nCOCO-Style Evaluations\n\nput images here datasets/your_project_name/annotations/val_set_name/*.jpg\nput annotations here datasets/your_project_name/annotations/instances_{val_set_name}.json\nput weights here /path/to/your/weights/*.pth\nchange compound_coef\n\n""""""\n\nimport json\nimport os\n\nimport argparse\nimport torch\nimport yaml\nfrom tqdm import tqdm\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom backbone import EfficientDetBackbone\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess\n\nap = argparse.ArgumentParser()\nap.add_argument(\'-p\', \'--project\', type=str, default=\'coco\', help=\'project file that contains parameters\')\nap.add_argument(\'-c\', \'--compound_coef\', type=int, default=0, help=\'coefficients of efficientdet\')\nap.add_argument(\'-w\', \'--weights\', type=str, default=None, help=\'/path/to/weights\')\nap.add_argument(\'--nms_threshold\', type=float, default=0.5, help=\'nms threshold, don\\\'t change it if not for testing purposes\')\nap.add_argument(\'--cuda\', type=bool, default=True)\nap.add_argument(\'--device\', type=int, default=0)\nap.add_argument(\'--float16\', type=bool, default=False)\nap.add_argument(\'--override\', type=bool, default=True, help=\'override previous bbox results file if exists\')\nargs = ap.parse_args()\n\ncompound_coef = args.compound_coef\nnms_threshold = args.nms_threshold\nuse_cuda = args.cuda\ngpu = args.device\nuse_float16 = args.float16\noverride_prev_results = args.override\nproject_name = args.project\nweights_path = f\'weights/efficientdet-d{compound_coef}.pth\' if args.weights is None else args.weights\n\nprint(f\'running coco-style evaluation on project {project_name}, weights {weights_path}...\')\n\nparams = yaml.safe_load(open(f\'projects/{project_name}.yml\'))\nobj_list = params[\'obj_list\']\n\ninput_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n\n\ndef evaluate_coco(img_path, set_name, image_ids, coco, model, threshold=0.05):\n    results = []\n\n    regressBoxes = BBoxTransform()\n    clipBoxes = ClipBoxes()\n\n    for image_id in tqdm(image_ids):\n        image_info = coco.loadImgs(image_id)[0]\n        image_path = img_path + image_info[\'file_name\']\n\n        ori_imgs, framed_imgs, framed_metas = preprocess(image_path, max_size=input_sizes[compound_coef])\n        x = torch.from_numpy(framed_imgs[0])\n\n        if use_cuda:\n            x = x.cuda(gpu)\n            if use_float16:\n                x = x.half()\n            else:\n                x = x.float()\n        else:\n            x = x.float()\n\n        x = x.unsqueeze(0).permute(0, 3, 1, 2)\n        features, regression, classification, anchors = model(x)\n\n        preds = postprocess(x,\n                            anchors, regression, classification,\n                            regressBoxes, clipBoxes,\n                            threshold, nms_threshold)\n        \n        if not preds:\n            continue\n\n        preds = invert_affine(framed_metas, preds)[0]\n\n        scores = preds[\'scores\']\n        class_ids = preds[\'class_ids\']\n        rois = preds[\'rois\']\n\n        if rois.shape[0] > 0:\n            # x1,y1,x2,y2 -> x1,y1,w,h\n            rois[:, 2] -= rois[:, 0]\n            rois[:, 3] -= rois[:, 1]\n\n            bbox_score = scores\n\n            for roi_id in range(rois.shape[0]):\n                score = float(bbox_score[roi_id])\n                label = int(class_ids[roi_id])\n                box = rois[roi_id, :]\n\n                image_result = {\n                    \'image_id\': image_id,\n                    \'category_id\': label + 1,\n                    \'score\': float(score),\n                    \'bbox\': box.tolist(),\n                }\n\n                results.append(image_result)\n\n    if not len(results):\n        raise Exception(\'the model does not provide any valid output, check model architecture and the data input\')\n\n    # write output\n    filepath = f\'{set_name}_bbox_results.json\'\n    if os.path.exists(filepath):\n        os.remove(filepath)\n    json.dump(results, open(filepath, \'w\'), indent=4)\n\n\ndef _eval(coco_gt, image_ids, pred_json_path):\n    # load results in COCO evaluation tool\n    coco_pred = coco_gt.loadRes(pred_json_path)\n\n    # run COCO evaluation\n    print(\'BBox\')\n    coco_eval = COCOeval(coco_gt, coco_pred, \'bbox\')\n    coco_eval.params.imgIds = image_ids\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n\nif __name__ == \'__main__\':\n    SET_NAME = params[\'val_set\']\n    VAL_GT = f\'datasets/{params[""project_name""]}/annotations/instances_{SET_NAME}.json\'\n    VAL_IMGS = f\'datasets/{params[""project_name""]}/{SET_NAME}/\'\n    MAX_IMAGES = 10000\n    coco_gt = COCO(VAL_GT)\n    image_ids = coco_gt.getImgIds()[:MAX_IMAGES]\n    \n    if override_prev_results or not os.path.exists(f\'{SET_NAME}_bbox_results.json\'):\n        model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n                                     ratios=eval(params[\'anchors_ratios\']), scales=eval(params[\'anchors_scales\']))\n        model.load_state_dict(torch.load(weights_path, map_location=torch.device(\'cpu\')))\n        model.requires_grad_(False)\n        model.eval()\n\n        if use_cuda:\n            model.cuda(gpu)\n\n            if use_float16:\n                model.half()\n\n        evaluate_coco(VAL_IMGS, SET_NAME, image_ids, coco_gt, model)\n\n    _eval(coco_gt, image_ids, f\'{SET_NAME}_bbox_results.json\')\n'"
efficientdet_test.py,8,"b'# Author: Zylo117\n\n""""""\nSimple Inference Script of EfficientDet-Pytorch\n""""""\nimport time\nimport torch\nfrom torch.backends import cudnn\nfrom matplotlib import colors\n\nfrom backbone import EfficientDetBackbone\nimport cv2\nimport numpy as np\n\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box\n\ncompound_coef = 0\nforce_input_size = None  # set None to use default size\nimg_path = \'test/img.png\'\n\n# replace this part with your project\'s anchor config\nanchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\nanchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n\nthreshold = 0.2\niou_threshold = 0.2\n\nuse_cuda = True\nuse_float16 = False\ncudnn.fastest = True\ncudnn.benchmark = True\n\nobj_list = [\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\',\n            \'fire hydrant\', \'\', \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\',\n            \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'\', \'backpack\', \'umbrella\', \'\', \'\', \'handbag\', \'tie\',\n            \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\',\n            \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\',\n            \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\',\n            \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\', \'\', \'dining table\', \'\', \'\', \'toilet\', \'\', \'tv\',\n            \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\', \'sink\',\n            \'refrigerator\', \'\', \'book\', \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\',\n            \'toothbrush\']\n\n\ncolor_list = standard_to_bgr(STANDARD_COLORS)\n# tf bilinear interpolation is different from any other\'s, just make do\ninput_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\ninput_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\nori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n\nif use_cuda:\n    x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\nelse:\n    x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\nx = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n\nmodel = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n                             ratios=anchor_ratios, scales=anchor_scales)\nmodel.load_state_dict(torch.load(f\'weights/efficientdet-d{compound_coef}.pth\'))\nmodel.requires_grad_(False)\nmodel.eval()\n\nif use_cuda:\n    model = model.cuda()\nif use_float16:\n    model = model.half()\n\nwith torch.no_grad():\n    features, regression, classification, anchors = model(x)\n\n    regressBoxes = BBoxTransform()\n    clipBoxes = ClipBoxes()\n\n    out = postprocess(x,\n                      anchors, regression, classification,\n                      regressBoxes, clipBoxes,\n                      threshold, iou_threshold)\n\ndef display(preds, imgs, imshow=True, imwrite=False):\n    for i in range(len(imgs)):\n        if len(preds[i][\'rois\']) == 0:\n            continue\n\n        for j in range(len(preds[i][\'rois\'])):\n            x1, y1, x2, y2 = preds[i][\'rois\'][j].astype(np.int)\n            obj = obj_list[preds[i][\'class_ids\'][j]]\n            score = float(preds[i][\'scores\'][j])\n            plot_one_box(imgs[i], [x1, y1, x2, y2], label=obj,score=score,color=color_list[get_index_label(obj, obj_list)])\n\n\n        if imshow:\n            cv2.imshow(\'img\', imgs[i])\n            cv2.waitKey(0)\n\n        if imwrite:\n            cv2.imwrite(f\'test/img_inferred_d{compound_coef}_this_repo_{i}.jpg\', imgs[i])\n\n\nout = invert_affine(framed_metas, out)\ndisplay(out, ori_imgs, imshow=False, imwrite=True)\n\nprint(\'running speed test...\')\nwith torch.no_grad():\n    print(\'test1: model inferring and postprocessing\')\n    print(\'inferring image for 10 times...\')\n    t1 = time.time()\n    for _ in range(10):\n        _, regression, classification, anchors = model(x)\n\n        out = postprocess(x,\n                          anchors, regression, classification,\n                          regressBoxes, clipBoxes,\n                          threshold, iou_threshold)\n        out = invert_affine(framed_metas, out)\n\n    t2 = time.time()\n    tact_time = (t2 - t1) / 10\n    print(f\'{tact_time} seconds, {1 / tact_time} FPS, @batch_size 1\')\n\n    # uncomment this if you want a extreme fps test\n    # print(\'test2: model inferring only\')\n    # print(\'inferring images for batch_size 32 for 10 times...\')\n    # t1 = time.time()\n    # x = torch.cat([x] * 32, 0)\n    # for _ in range(10):\n    #     _, regression, classification, anchors = model(x)\n    #\n    # t2 = time.time()\n    # tact_time = (t2 - t1) / 10\n    # print(f\'{tact_time} seconds, {32 / tact_time} FPS, @batch_size 32\')\n'"
efficientdet_test_videos.py,6,"b'# Core Author: Zylo117\n# Script\'s Author: winter2897 \n\n""""""\nSimple Inference Script of EfficientDet-Pytorch for detecting objects on webcam\n""""""\nimport time\nimport torch\nimport cv2\nimport numpy as np\nfrom torch.backends import cudnn\nfrom backbone import EfficientDetBackbone\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess, preprocess_video\n\n# Video\'s path\nvideo_src = \'videotest.mp4\'  # set int to use webcam, set str to read from a video file\n\ncompound_coef = 0\nforce_input_size = None  # set None to use default size\n\nthreshold = 0.2\niou_threshold = 0.2\n\nuse_cuda = True\nuse_float16 = False\ncudnn.fastest = True\ncudnn.benchmark = True\n\nobj_list = [\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\',\n            \'fire hydrant\', \'\', \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\',\n            \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'\', \'backpack\', \'umbrella\', \'\', \'\', \'handbag\', \'tie\',\n            \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\',\n            \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\',\n            \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\',\n            \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\', \'\', \'dining table\', \'\', \'\', \'toilet\', \'\', \'tv\',\n            \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\', \'sink\',\n            \'refrigerator\', \'\', \'book\', \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\',\n            \'toothbrush\']\n\n# tf bilinear interpolation is different from any other\'s, just make do\ninput_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\ninput_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n\n# load model\nmodel = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list))\nmodel.load_state_dict(torch.load(f\'weights/efficientdet-d{compound_coef}.pth\'))\nmodel.requires_grad_(False)\nmodel.eval()\n\nif use_cuda:\n    model = model.cuda()\nif use_float16:\n    model = model.half()\n\n# function for display\ndef display(preds, imgs):\n    for i in range(len(imgs)):\n        if len(preds[i][\'rois\']) == 0:\n            return imgs[i]\n\n        for j in range(len(preds[i][\'rois\'])):\n            (x1, y1, x2, y2) = preds[i][\'rois\'][j].astype(np.int)\n            cv2.rectangle(imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 2)\n            obj = obj_list[preds[i][\'class_ids\'][j]]\n            score = float(preds[i][\'scores\'][j])\n\n            cv2.putText(imgs[i], \'{}, {:.3f}\'.format(obj, score),\n                        (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (255, 255, 0), 1)\n        \n        return imgs[i]\n# Box\nregressBoxes = BBoxTransform()\nclipBoxes = ClipBoxes()\n\n# Video capture\ncap = cv2.VideoCapture(video_src)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # frame preprocessing\n    ori_imgs, framed_imgs, framed_metas = preprocess_video(frame, max_size=input_size)\n\n    if use_cuda:\n        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n    else:\n        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\n    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n\n    # model predict\n    with torch.no_grad():\n        features, regression, classification, anchors = model(x)\n\n        out = postprocess(x,\n                        anchors, regression, classification,\n                        regressBoxes, clipBoxes,\n                        threshold, iou_threshold)\n\n    # result\n    out = invert_affine(framed_metas, out)\n    img_show = display(out, ori_imgs)\n\n    # show frame by frame\n    cv2.imshow(\'frame\',img_show)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'): \n        break\n\ncap.release()\ncv2.destroyAllWindows()\n\n\n\n\n\n'"
train.py,14,"b'# original author: signatrix\n# adapted from https://github.com/signatrix/efficientdet/blob/master/train.py\n# modified by Zylo117\n\nimport datetime\nimport os\nimport argparse\nimport traceback\n\nimport torch\nimport yaml\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom efficientdet.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater\nfrom backbone import EfficientDetBackbone\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\nfrom efficientdet.loss import FocalLoss\nfrom utils.sync_batchnorm import patch_replication_callback\nfrom utils.utils import replace_w_sync_bn, CustomDataParallel, get_last_weights, init_weights\n\n\nclass Params:\n    def __init__(self, project_file):\n        self.params = yaml.safe_load(open(project_file).read())\n\n    def __getattr__(self, item):\n        return self.params.get(item, None)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\'Yet Another EfficientDet Pytorch: SOTA object detection network - Zylo117\')\n    parser.add_argument(\'-p\', \'--project\', type=str, default=\'coco\', help=\'project file that contains parameters\')\n    parser.add_argument(\'-c\', \'--compound_coef\', type=int, default=0, help=\'coefficients of efficientdet\')\n    parser.add_argument(\'-n\', \'--num_workers\', type=int, default=12, help=\'num_workers of dataloader\')\n    parser.add_argument(\'--batch_size\', type=int, default=12, help=\'The number of images per batch among all devices\')\n    parser.add_argument(\'--head_only\', type=boolean_string, default=False,\n                        help=\'whether finetunes only the regressor and the classifier, \'\n                             \'useful in early stage convergence or small/easy dataset\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4)\n    parser.add_argument(\'--optim\', type=str, default=\'adamw\', help=\'select optimizer for training, \'\n                                                                   \'suggest using \\\'admaw\\\' until the\'\n                                                                   \' very final stage then switch to \\\'sgd\\\'\')\n    parser.add_argument(\'--num_epochs\', type=int, default=500)\n    parser.add_argument(\'--val_interval\', type=int, default=1, help=\'Number of epoches between valing phases\')\n    parser.add_argument(\'--save_interval\', type=int, default=500, help=\'Number of steps between saving\')\n    parser.add_argument(\'--es_min_delta\', type=float, default=0.0,\n                        help=\'Early stopping\\\'s parameter: minimum change loss to qualify as an improvement\')\n    parser.add_argument(\'--es_patience\', type=int, default=0,\n                        help=\'Early stopping\\\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\')\n    parser.add_argument(\'--data_path\', type=str, default=\'datasets/\', help=\'the root folder of dataset\')\n    parser.add_argument(\'--log_path\', type=str, default=\'logs/\')\n    parser.add_argument(\'-w\', \'--load_weights\', type=str, default=None,\n                        help=\'whether to load weights from a checkpoint, set None to initialize, set \\\'last\\\' to load last checkpoint\')\n    parser.add_argument(\'--saved_path\', type=str, default=\'logs/\')\n    parser.add_argument(\'--debug\', type=boolean_string, default=False, help=\'whether visualize the predicted boxes of training, \'\n                                                                  \'the output images will be in test/\')\n\n    args = parser.parse_args()\n    return args\n\n\ndef boolean_string(s):\n    if s not in {\'False\', \'True\'}:\n        raise ValueError(\'Not a valid boolean string\')\n    return s == \'True\'\n\n\nclass ModelWithLoss(nn.Module):\n    def __init__(self, model, debug=False):\n        super().__init__()\n        self.criterion = FocalLoss()\n        self.model = model\n        self.debug = debug\n\n    def forward(self, imgs, annotations, obj_list=None):\n        _, regression, classification, anchors = self.model(imgs)\n        if self.debug:\n            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations,\n                                                imgs=imgs, obj_list=obj_list)\n        else:\n            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations)\n        return cls_loss, reg_loss\n\n\ndef train(opt):\n    params = Params(f\'projects/{opt.project}.yml\')\n\n    if params.num_gpus == 0:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n    else:\n        torch.manual_seed(42)\n\n    opt.saved_path = opt.saved_path + f\'/{params.project_name}/\'\n    opt.log_path = opt.log_path + f\'/{params.project_name}/tensorboard/\'\n    os.makedirs(opt.log_path, exist_ok=True)\n    os.makedirs(opt.saved_path, exist_ok=True)\n\n    training_params = {\'batch_size\': opt.batch_size,\n                       \'shuffle\': True,\n                       \'drop_last\': True,\n                       \'collate_fn\': collater,\n                       \'num_workers\': opt.num_workers}\n\n    val_params = {\'batch_size\': opt.batch_size,\n                  \'shuffle\': False,\n                  \'drop_last\': True,\n                  \'collate_fn\': collater,\n                  \'num_workers\': opt.num_workers}\n\n    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n    training_set = CocoDataset(root_dir=os.path.join(opt.data_path, params.project_name), set=params.train_set,\n                               transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n                                                             Augmenter(),\n                                                             Resizer(input_sizes[opt.compound_coef])]))\n    training_generator = DataLoader(training_set, **training_params)\n\n    val_set = CocoDataset(root_dir=os.path.join(opt.data_path, params.project_name), set=params.val_set,\n                          transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n                                                        Resizer(input_sizes[opt.compound_coef])]))\n    val_generator = DataLoader(val_set, **val_params)\n\n    model = EfficientDetBackbone(num_classes=len(params.obj_list), compound_coef=opt.compound_coef,\n                                 ratios=eval(params.anchors_ratios), scales=eval(params.anchors_scales))\n\n    # load last weights\n    if opt.load_weights is not None:\n        if opt.load_weights.endswith(\'.pth\'):\n            weights_path = opt.load_weights\n        else:\n            weights_path = get_last_weights(opt.saved_path)\n        try:\n            last_step = int(os.path.basename(weights_path).split(\'_\')[-1].split(\'.\')[0])\n        except:\n            last_step = 0\n\n        try:\n            ret = model.load_state_dict(torch.load(weights_path), strict=False)\n        except RuntimeError as e:\n            print(f\'[Warning] Ignoring {e}\')\n            print(\n                \'[Warning] Don\\\'t panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.\')\n\n        print(f\'[Info] loaded weights: {os.path.basename(weights_path)}, resuming checkpoint from step: {last_step}\')\n    else:\n        last_step = 0\n        print(\'[Info] initializing weights...\')\n        init_weights(model)\n\n    # freeze backbone if train head_only\n    if opt.head_only:\n        def freeze_backbone(m):\n            classname = m.__class__.__name__\n            for ntl in [\'EfficientNet\', \'BiFPN\']:\n                if ntl in classname:\n                    for param in m.parameters():\n                        param.requires_grad = False\n\n        model.apply(freeze_backbone)\n        print(\'[Info] freezed backbone\')\n\n    # https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n    # apply sync_bn when using multiple gpu and batch_size per gpu is lower than 4\n    #  useful when gpu memory is limited.\n    # because when bn is disable, the training will be very unstable or slow to converge,\n    # apply sync_bn can solve it,\n    # by packing all mini-batch across all gpus as one batch and normalize, then send it back to all gpus.\n    # but it would also slow down the training by a little bit.\n    if params.num_gpus > 1 and opt.batch_size // params.num_gpus < 4:\n        model.apply(replace_w_sync_bn)\n        use_sync_bn = True\n    else:\n        use_sync_bn = False\n\n    writer = SummaryWriter(opt.log_path + f\'/{datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")}/\')\n\n    # warp the model with loss function, to reduce the memory usage on gpu0 and speedup\n    model = ModelWithLoss(model, debug=opt.debug)\n\n    if params.num_gpus > 0:\n        model = model.cuda()\n        if params.num_gpus > 1:\n            model = CustomDataParallel(model, params.num_gpus)\n            if use_sync_bn:\n                patch_replication_callback(model)\n\n    if opt.optim == \'adamw\':\n        optimizer = torch.optim.AdamW(model.parameters(), opt.lr)\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), opt.lr, momentum=0.9, nesterov=True)\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n\n    epoch = 0\n    best_loss = 1e5\n    best_epoch = 0\n    step = max(0, last_step)\n    model.train()\n\n    num_iter_per_epoch = len(training_generator)\n\n    try:\n        for epoch in range(opt.num_epochs):\n            last_epoch = step // num_iter_per_epoch\n            if epoch < last_epoch:\n                continue\n\n            epoch_loss = []\n            progress_bar = tqdm(training_generator)\n            for iter, data in enumerate(progress_bar):\n                if iter < step - last_epoch * num_iter_per_epoch:\n                    progress_bar.update()\n                    continue\n                try:\n                    imgs = data[\'img\']\n                    annot = data[\'annot\']\n\n                    if params.num_gpus == 1:\n                        # if only one gpu, just send it to cuda:0\n                        # elif multiple gpus, send it to multiple gpus in CustomDataParallel, not here\n                        imgs = imgs.cuda()\n                        annot = annot.cuda()\n\n                    optimizer.zero_grad()\n                    cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n                    cls_loss = cls_loss.mean()\n                    reg_loss = reg_loss.mean()\n\n                    loss = cls_loss + reg_loss\n                    if loss == 0 or not torch.isfinite(loss):\n                        continue\n\n                    loss.backward()\n                    # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n                    optimizer.step()\n\n                    epoch_loss.append(float(loss))\n\n                    progress_bar.set_description(\n                        \'Step: {}. Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Total loss: {:.5f}\'.format(\n                            step, epoch, opt.num_epochs, iter + 1, num_iter_per_epoch, cls_loss.item(),\n                            reg_loss.item(), loss.item()))\n                    writer.add_scalars(\'Loss\', {\'train\': loss}, step)\n                    writer.add_scalars(\'Regression_loss\', {\'train\': reg_loss}, step)\n                    writer.add_scalars(\'Classfication_loss\', {\'train\': cls_loss}, step)\n\n                    # log learning_rate\n                    current_lr = optimizer.param_groups[0][\'lr\']\n                    writer.add_scalar(\'learning_rate\', current_lr, step)\n\n                    step += 1\n\n                    if step % opt.save_interval == 0 and step > 0:\n                        save_checkpoint(model, f\'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth\')\n                        print(\'checkpoint...\')\n\n                except Exception as e:\n                    print(\'[Error]\', traceback.format_exc())\n                    print(e)\n                    continue\n            scheduler.step(np.mean(epoch_loss))\n\n            if epoch % opt.val_interval == 0:\n                model.eval()\n                loss_regression_ls = []\n                loss_classification_ls = []\n                for iter, data in enumerate(val_generator):\n                    with torch.no_grad():\n                        imgs = data[\'img\']\n                        annot = data[\'annot\']\n\n                        if params.num_gpus == 1:\n                            imgs = imgs.cuda()\n                            annot = annot.cuda()\n\n                        cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n                        cls_loss = cls_loss.mean()\n                        reg_loss = reg_loss.mean()\n\n                        loss = cls_loss + reg_loss\n                        if loss == 0 or not torch.isfinite(loss):\n                            continue\n\n                        loss_classification_ls.append(cls_loss.item())\n                        loss_regression_ls.append(reg_loss.item())\n\n                cls_loss = np.mean(loss_classification_ls)\n                reg_loss = np.mean(loss_regression_ls)\n                loss = cls_loss + reg_loss\n\n                print(\n                    \'Val. Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}\'.format(\n                        epoch, opt.num_epochs, cls_loss, reg_loss, loss))\n                writer.add_scalars(\'Loss\', {\'val\': loss}, step)\n                writer.add_scalars(\'Regression_loss\', {\'val\': reg_loss}, step)\n                writer.add_scalars(\'Classfication_loss\', {\'val\': cls_loss}, step)\n\n                if loss + opt.es_min_delta < best_loss:\n                    best_loss = loss\n                    best_epoch = epoch\n\n                    save_checkpoint(model, f\'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth\')\n\n                model.train()\n                           \n                # Early stopping\n                if epoch - best_epoch > opt.es_patience > 0:\n                    print(\'[Info] Stop training at epoch {}. The lowest loss achieved is {}\'.format(epoch, best_loss))\n                    break\n    except KeyboardInterrupt:\n        save_checkpoint(model, f\'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth\')\n        writer.close()\n    writer.close()\n\n\ndef save_checkpoint(model, name):\n    if isinstance(model, CustomDataParallel):\n        torch.save(model.module.model.state_dict(), os.path.join(opt.saved_path, name))\n    else:\n        torch.save(model.model.state_dict(), os.path.join(opt.saved_path, name))\n\n\nif __name__ == \'__main__\':\n    opt = get_args()\n    train(opt)\n'"
efficientdet/config.py,0,"b'COCO_CLASSES = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n                ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n                ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n                ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n                ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n                ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n                ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n                ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n                ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n                ""teddy bear"", ""hair drier"", ""toothbrush""]\n\ncolors = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),\n          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),\n          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),\n          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),\n          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),\n          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),\n          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),\n          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),\n          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),\n          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),\n          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),\n          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),\n          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),\n          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),\n          (2, 20, 184), (122, 37, 185)]\n'"
efficientdet/dataset.py,5,"b'import os\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom pycocotools.coco import COCO\nimport cv2\n\n\nclass CocoDataset(Dataset):\n    def __init__(self, root_dir, set=\'train2017\', transform=None):\n\n        self.root_dir = root_dir\n        self.set_name = set\n        self.transform = transform\n\n        self.coco = COCO(os.path.join(self.root_dir, \'annotations\', \'instances_\' + self.set_name + \'.json\'))\n        self.image_ids = self.coco.getImgIds()\n\n        self.load_classes()\n\n    def load_classes(self):\n\n        # load class names (name -> label)\n        categories = self.coco.loadCats(self.coco.getCatIds())\n        categories.sort(key=lambda x: x[\'id\'])\n\n        self.classes = {}\n        for c in categories:\n            self.classes[c[\'name\']] = len(self.classes)\n\n        # also load the reverse (label -> name)\n        self.labels = {}\n        for key, value in self.classes.items():\n            self.labels[value] = key\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n    def load_image(self, image_index):\n        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n        path = os.path.join(self.root_dir, self.set_name, image_info[\'file_name\'])\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        return img.astype(np.float32) / 255.\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n        annotations = np.zeros((0, 5))\n\n        # some images appear to miss annotations\n        if len(annotations_ids) == 0:\n            return annotations\n\n        # parse annotations\n        coco_annotations = self.coco.loadAnns(annotations_ids)\n        for idx, a in enumerate(coco_annotations):\n\n            # some annotations have basically no width / height, skip them\n            if a[\'bbox\'][2] < 1 or a[\'bbox\'][3] < 1:\n                continue\n\n            annotation = np.zeros((1, 5))\n            annotation[0, :4] = a[\'bbox\']\n            annotation[0, 4] = a[\'category_id\'] - 1\n            annotations = np.append(annotations, annotation, axis=0)\n\n        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n\n        return annotations\n\n\ndef collater(data):\n    imgs = [s[\'img\'] for s in data]\n    annots = [s[\'annot\'] for s in data]\n    scales = [s[\'scale\'] for s in data]\n\n    imgs = torch.from_numpy(np.stack(imgs, axis=0))\n\n    max_num_annots = max(annot.shape[0] for annot in annots)\n\n    if max_num_annots > 0:\n\n        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n\n        for idx, annot in enumerate(annots):\n            if annot.shape[0] > 0:\n                annot_padded[idx, :annot.shape[0], :] = annot\n    else:\n        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n\n    imgs = imgs.permute(0, 3, 1, 2)\n\n    return {\'img\': imgs, \'annot\': annot_padded, \'scale\': scales}\n\n\nclass Resizer(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n    \n    def __init__(self, img_size=512):\n        self.img_size = img_size\n\n    def __call__(self, sample):\n        image, annots = sample[\'img\'], sample[\'annot\']\n        height, width, _ = image.shape\n        if height > width:\n            scale = self.img_size / height\n            resized_height = self.img_size\n            resized_width = int(width * scale)\n        else:\n            scale = self.img_size / width\n            resized_height = int(height * scale)\n            resized_width = self.img_size\n\n        image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n\n        new_image = np.zeros((self.img_size, self.img_size, 3))\n        new_image[0:resized_height, 0:resized_width] = image\n\n        annots[:, :4] *= scale\n\n        return {\'img\': torch.from_numpy(new_image).to(torch.float32), \'annot\': torch.from_numpy(annots), \'scale\': scale}\n\n\nclass Augmenter(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, flip_x=0.5):\n        if np.random.rand() < flip_x:\n            image, annots = sample[\'img\'], sample[\'annot\']\n            image = image[:, ::-1, :]\n\n            rows, cols, channels = image.shape\n\n            x1 = annots[:, 0].copy()\n            x2 = annots[:, 2].copy()\n\n            x_tmp = x1.copy()\n\n            annots[:, 0] = cols - x2\n            annots[:, 2] = cols - x_tmp\n\n            sample = {\'img\': image, \'annot\': annots}\n\n        return sample\n\n\nclass Normalizer(object):\n\n    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n        self.mean = np.array([[mean]])\n        self.std = np.array([[std]])\n\n    def __call__(self, sample):\n        image, annots = sample[\'img\'], sample[\'annot\']\n\n        return {\'img\': ((image.astype(np.float32) - self.mean) / self.std), \'annot\': annots}\n'"
efficientdet/loss.py,47,"b""import torch\nimport torch.nn as nn\nimport cv2\nimport numpy as np\n\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import postprocess, invert_affine, display\n\n\ndef calc_iou(a, b):\n    # a(anchor) [boxes, (y1, x1, y2, x2)]\n    # b(gt, coco-style) [boxes, (x1, y1, x2, y2)]\n\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n    iw = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 0])\n    ih = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 1])\n    iw = torch.clamp(iw, min=0)\n    ih = torch.clamp(ih, min=0)\n    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n    ua = torch.clamp(ua, min=1e-8)\n    intersection = iw * ih\n    IoU = intersection / ua\n\n    return IoU\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, classifications, regressions, anchors, annotations, **kwargs):\n        alpha = 0.25\n        gamma = 2.0\n        batch_size = classifications.shape[0]\n        classification_losses = []\n        regression_losses = []\n\n        anchor = anchors[0, :, :]  # assuming all image sizes are the same, which it is\n        dtype = anchors.dtype\n\n        anchor_widths = anchor[:, 3] - anchor[:, 1]\n        anchor_heights = anchor[:, 2] - anchor[:, 0]\n        anchor_ctr_x = anchor[:, 1] + 0.5 * anchor_widths\n        anchor_ctr_y = anchor[:, 0] + 0.5 * anchor_heights\n\n        for j in range(batch_size):\n\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n\n            bbox_annotation = annotations[j]\n            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n\n            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n            \n            if bbox_annotation.shape[0] == 0:\n                if torch.cuda.is_available():\n                    \n                    alpha_factor = torch.ones_like(classification) * alpha\n                    alpha_factor = alpha_factor.cuda()\n                    alpha_factor = 1. - alpha_factor\n                    focal_weight = classification\n                    focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n                    \n                    bce = -(torch.log(1.0 - classification))\n                    \n                    cls_loss = focal_weight * bce\n                    \n                    regression_losses.append(torch.tensor(0).to(dtype).cuda())\n                    classification_losses.append(cls_loss.sum())\n                else:\n                    \n                    alpha_factor = torch.ones_like(classification) * alpha\n                    alpha_factor = 1. - alpha_factor\n                    focal_weight = classification\n                    focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n                    \n                    bce = -(torch.log(1.0 - classification))\n                    \n                    cls_loss = focal_weight * bce\n                    \n                    regression_losses.append(torch.tensor(0).to(dtype))\n                    classification_losses.append(cls_loss.sum())\n\n                continue\n                \n            IoU = calc_iou(anchor[:, :], bbox_annotation[:, :4])\n\n            IoU_max, IoU_argmax = torch.max(IoU, dim=1)\n\n            # compute the loss for classification\n            targets = torch.ones_like(classification) * -1\n            if torch.cuda.is_available():\n                targets = targets.cuda()\n\n            targets[torch.lt(IoU_max, 0.4), :] = 0\n\n            positive_indices = torch.ge(IoU_max, 0.5)\n\n            num_positive_anchors = positive_indices.sum()\n\n            assigned_annotations = bbox_annotation[IoU_argmax, :]\n\n            targets[positive_indices, :] = 0\n            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n\n            alpha_factor = torch.ones_like(targets) * alpha\n            if torch.cuda.is_available():\n                alpha_factor = alpha_factor.cuda()\n\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n\n            cls_loss = focal_weight * bce\n\n            zeros = torch.zeros_like(cls_loss)\n            if torch.cuda.is_available():\n                zeros = zeros.cuda()\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n\n            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.to(dtype), min=1.0))\n\n            if positive_indices.sum() > 0:\n                assigned_annotations = assigned_annotations[positive_indices, :]\n\n                anchor_widths_pi = anchor_widths[positive_indices]\n                anchor_heights_pi = anchor_heights[positive_indices]\n                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n\n                gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n                gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths\n                gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights\n\n                # efficientdet style\n                gt_widths = torch.clamp(gt_widths, min=1)\n                gt_heights = torch.clamp(gt_heights, min=1)\n\n                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n\n                targets = torch.stack((targets_dy, targets_dx, targets_dh, targets_dw))\n                targets = targets.t()\n\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 / 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 / 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                if torch.cuda.is_available():\n                    regression_losses.append(torch.tensor(0).to(dtype).cuda())\n                else:\n                    regression_losses.append(torch.tensor(0).to(dtype))\n\n        # debug\n        imgs = kwargs.get('imgs', None)\n        if imgs is not None:\n            regressBoxes = BBoxTransform()\n            clipBoxes = ClipBoxes()\n            obj_list = kwargs.get('obj_list', None)\n            out = postprocess(imgs.detach(),\n                              torch.stack([anchors[0]] * imgs.shape[0], 0).detach(), regressions.detach(), classifications.detach(),\n                              regressBoxes, clipBoxes,\n                              0.5, 0.3)\n            imgs = imgs.permute(0, 2, 3, 1).cpu().numpy()\n            imgs = ((imgs * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255).astype(np.uint8)\n            imgs = [cv2.cvtColor(img, cv2.COLOR_RGB2BGR) for img in imgs]\n            display(out, imgs, obj_list, imshow=False, imwrite=True)\n\n        return torch.stack(classification_losses).mean(dim=0, keepdim=True), \\\n               torch.stack(regression_losses).mean(dim=0, keepdim=True)\n"""
efficientdet/model.py,19,"b'import torch.nn as nn\nimport torch\nfrom torchvision.ops.boxes import nms as nms_torch\n\nfrom efficientnet import EfficientNet as EffNet\nfrom efficientnet.utils import MemoryEfficientSwish, Swish\nfrom efficientnet.utils_extra import Conv2dStaticSamePadding, MaxPool2dStaticSamePadding\n\n\ndef nms(dets, thresh):\n    return nms_torch(dets[:, :4], dets[:, 4], thresh)\n\n\nclass SeparableConvBlock(nn.Module):\n    """"""\n    created by Zylo117\n    """"""\n\n    def __init__(self, in_channels, out_channels=None, norm=True, activation=False, onnx_export=False):\n        super(SeparableConvBlock, self).__init__()\n        if out_channels is None:\n            out_channels = in_channels\n\n        # Q: whether separate conv\n        #  share bias between depthwise_conv and pointwise_conv\n        #  or just pointwise_conv apply bias.\n        # A: Confirmed, just pointwise_conv applies bias, depthwise_conv has no bias.\n\n        self.depthwise_conv = Conv2dStaticSamePadding(in_channels, in_channels,\n                                                      kernel_size=3, stride=1, groups=in_channels, bias=False)\n        self.pointwise_conv = Conv2dStaticSamePadding(in_channels, out_channels, kernel_size=1, stride=1)\n\n        self.norm = norm\n        if self.norm:\n            # Warning: pytorch momentum is different from tensorflow\'s, momentum_pytorch = 1 - momentum_tensorflow\n            self.bn = nn.BatchNorm2d(num_features=out_channels, momentum=0.01, eps=1e-3)\n\n        self.activation = activation\n        if self.activation:\n            self.swish = MemoryEfficientSwish() if not onnx_export else Swish()\n\n    def forward(self, x):\n        x = self.depthwise_conv(x)\n        x = self.pointwise_conv(x)\n\n        if self.norm:\n            x = self.bn(x)\n\n        if self.activation:\n            x = self.swish(x)\n\n        return x\n\n\nclass BiFPN(nn.Module):\n    """"""\n    modified by Zylo117\n    """"""\n\n    def __init__(self, num_channels, conv_channels, first_time=False, epsilon=1e-4, onnx_export=False, attention=True):\n        """"""\n\n        Args:\n            num_channels:\n            conv_channels:\n            first_time: whether the input comes directly from the efficientnet,\n                        if True, downchannel it first, and downsample P5 to generate P6 then P7\n            epsilon: epsilon of fast weighted attention sum of BiFPN, not the BN\'s epsilon\n            onnx_export: if True, use Swish instead of MemoryEfficientSwish\n        """"""\n        super(BiFPN, self).__init__()\n        self.epsilon = epsilon\n        # Conv layers\n        self.conv6_up = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv5_up = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv4_up = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv3_up = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv4_down = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv5_down = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv6_down = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n        self.conv7_down = SeparableConvBlock(num_channels, onnx_export=onnx_export)\n\n        # Feature scaling layers\n        self.p6_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p5_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p4_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p3_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n\n        self.p4_downsample = MaxPool2dStaticSamePadding(3, 2)\n        self.p5_downsample = MaxPool2dStaticSamePadding(3, 2)\n        self.p6_downsample = MaxPool2dStaticSamePadding(3, 2)\n        self.p7_downsample = MaxPool2dStaticSamePadding(3, 2)\n\n        self.swish = MemoryEfficientSwish() if not onnx_export else Swish()\n\n        self.first_time = first_time\n        if self.first_time:\n            self.p5_down_channel = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[2], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n            )\n            self.p4_down_channel = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[1], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n            )\n            self.p3_down_channel = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[0], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n            )\n\n            self.p5_to_p6 = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[2], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n                MaxPool2dStaticSamePadding(3, 2)\n            )\n            self.p6_to_p7 = nn.Sequential(\n                MaxPool2dStaticSamePadding(3, 2)\n            )\n\n            self.p4_down_channel_2 = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[1], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n            )\n            self.p5_down_channel_2 = nn.Sequential(\n                Conv2dStaticSamePadding(conv_channels[2], num_channels, 1),\n                nn.BatchNorm2d(num_channels, momentum=0.01, eps=1e-3),\n            )\n\n        # Weight\n        self.p6_w1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.p6_w1_relu = nn.ReLU()\n        self.p5_w1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.p5_w1_relu = nn.ReLU()\n        self.p4_w1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.p4_w1_relu = nn.ReLU()\n        self.p3_w1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.p3_w1_relu = nn.ReLU()\n\n        self.p4_w2 = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)\n        self.p4_w2_relu = nn.ReLU()\n        self.p5_w2 = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)\n        self.p5_w2_relu = nn.ReLU()\n        self.p6_w2 = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)\n        self.p6_w2_relu = nn.ReLU()\n        self.p7_w2 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.p7_w2_relu = nn.ReLU()\n\n        self.attention = attention\n\n    def forward(self, inputs):\n        """"""\n        illustration of a minimal bifpn unit\n            P7_0 -------------------------> P7_2 -------->\n               |-------------|                \xe2\x86\x91\n                             \xe2\x86\x93                |\n            P6_0 ---------> P6_1 ---------> P6_2 -------->\n               |-------------|--------------\xe2\x86\x91 \xe2\x86\x91\n                             \xe2\x86\x93                |\n            P5_0 ---------> P5_1 ---------> P5_2 -------->\n               |-------------|--------------\xe2\x86\x91 \xe2\x86\x91\n                             \xe2\x86\x93                |\n            P4_0 ---------> P4_1 ---------> P4_2 -------->\n               |-------------|--------------\xe2\x86\x91 \xe2\x86\x91\n                             |--------------\xe2\x86\x93 |\n            P3_0 -------------------------> P3_2 -------->\n        """"""\n\n        # downsample channels using same-padding conv2d to target phase\'s if not the same\n        # judge: same phase as target,\n        # if same, pass;\n        # elif earlier phase, downsample to target phase\'s by pooling\n        # elif later phase, upsample to target phase\'s by nearest interpolation\n\n        if self.attention:\n            p3_out, p4_out, p5_out, p6_out, p7_out = self._forward_fast_attention(inputs)\n        else:\n            p3_out, p4_out, p5_out, p6_out, p7_out = self._forward(inputs)\n\n        return p3_out, p4_out, p5_out, p6_out, p7_out\n\n    def _forward_fast_attention(self, inputs):\n        if self.first_time:\n            p3, p4, p5 = inputs\n\n            p6_in = self.p5_to_p6(p5)\n            p7_in = self.p6_to_p7(p6_in)\n\n            p3_in = self.p3_down_channel(p3)\n            p4_in = self.p4_down_channel(p4)\n            p5_in = self.p5_down_channel(p5)\n\n        else:\n            # P3_0, P4_0, P5_0, P6_0 and P7_0\n            p3_in, p4_in, p5_in, p6_in, p7_in = inputs\n\n        # P7_0 to P7_2\n\n        # Weights for P6_0 and P7_0 to P6_1\n        p6_w1 = self.p6_w1_relu(self.p6_w1)\n        weight = p6_w1 / (torch.sum(p6_w1, dim=0) + self.epsilon)\n        # Connections for P6_0 and P7_0 to P6_1 respectively\n        p6_up = self.conv6_up(self.swish(weight[0] * p6_in + weight[1] * self.p6_upsample(p7_in)))\n\n        # Weights for P5_0 and P6_1 to P5_1\n        p5_w1 = self.p5_w1_relu(self.p5_w1)\n        weight = p5_w1 / (torch.sum(p5_w1, dim=0) + self.epsilon)\n        # Connections for P5_0 and P6_1 to P5_1 respectively\n        p5_up = self.conv5_up(self.swish(weight[0] * p5_in + weight[1] * self.p5_upsample(p6_up)))\n\n        # Weights for P4_0 and P5_1 to P4_1\n        p4_w1 = self.p4_w1_relu(self.p4_w1)\n        weight = p4_w1 / (torch.sum(p4_w1, dim=0) + self.epsilon)\n        # Connections for P4_0 and P5_1 to P4_1 respectively\n        p4_up = self.conv4_up(self.swish(weight[0] * p4_in + weight[1] * self.p4_upsample(p5_up)))\n\n        # Weights for P3_0 and P4_1 to P3_2\n        p3_w1 = self.p3_w1_relu(self.p3_w1)\n        weight = p3_w1 / (torch.sum(p3_w1, dim=0) + self.epsilon)\n        # Connections for P3_0 and P4_1 to P3_2 respectively\n        p3_out = self.conv3_up(self.swish(weight[0] * p3_in + weight[1] * self.p3_upsample(p4_up)))\n\n        if self.first_time:\n            p4_in = self.p4_down_channel_2(p4)\n            p5_in = self.p5_down_channel_2(p5)\n\n        # Weights for P4_0, P4_1 and P3_2 to P4_2\n        p4_w2 = self.p4_w2_relu(self.p4_w2)\n        weight = p4_w2 / (torch.sum(p4_w2, dim=0) + self.epsilon)\n        # Connections for P4_0, P4_1 and P3_2 to P4_2 respectively\n        p4_out = self.conv4_down(\n            self.swish(weight[0] * p4_in + weight[1] * p4_up + weight[2] * self.p4_downsample(p3_out)))\n\n        # Weights for P5_0, P5_1 and P4_2 to P5_2\n        p5_w2 = self.p5_w2_relu(self.p5_w2)\n        weight = p5_w2 / (torch.sum(p5_w2, dim=0) + self.epsilon)\n        # Connections for P5_0, P5_1 and P4_2 to P5_2 respectively\n        p5_out = self.conv5_down(\n            self.swish(weight[0] * p5_in + weight[1] * p5_up + weight[2] * self.p5_downsample(p4_out)))\n\n        # Weights for P6_0, P6_1 and P5_2 to P6_2\n        p6_w2 = self.p6_w2_relu(self.p6_w2)\n        weight = p6_w2 / (torch.sum(p6_w2, dim=0) + self.epsilon)\n        # Connections for P6_0, P6_1 and P5_2 to P6_2 respectively\n        p6_out = self.conv6_down(\n            self.swish(weight[0] * p6_in + weight[1] * p6_up + weight[2] * self.p6_downsample(p5_out)))\n\n        # Weights for P7_0 and P6_2 to P7_2\n        p7_w2 = self.p7_w2_relu(self.p7_w2)\n        weight = p7_w2 / (torch.sum(p7_w2, dim=0) + self.epsilon)\n        # Connections for P7_0 and P6_2 to P7_2\n        p7_out = self.conv7_down(self.swish(weight[0] * p7_in + weight[1] * self.p7_downsample(p6_out)))\n\n        return p3_out, p4_out, p5_out, p6_out, p7_out\n\n    def _forward(self, inputs):\n        if self.first_time:\n            p3, p4, p5 = inputs\n\n            p6_in = self.p5_to_p6(p5)\n            p7_in = self.p6_to_p7(p6_in)\n\n            p3_in = self.p3_down_channel(p3)\n            p4_in = self.p4_down_channel(p4)\n            p5_in = self.p5_down_channel(p5)\n\n        else:\n            # P3_0, P4_0, P5_0, P6_0 and P7_0\n            p3_in, p4_in, p5_in, p6_in, p7_in = inputs\n\n        # P7_0 to P7_2\n\n        # Connections for P6_0 and P7_0 to P6_1 respectively\n        p6_up = self.conv6_up(self.swish(p6_in + self.p6_upsample(p7_in)))\n\n        # Connections for P5_0 and P6_1 to P5_1 respectively\n        p5_up = self.conv5_up(self.swish(p5_in + self.p5_upsample(p6_up)))\n\n        # Connections for P4_0 and P5_1 to P4_1 respectively\n        p4_up = self.conv4_up(self.swish(p4_in + self.p4_upsample(p5_up)))\n\n        # Connections for P3_0 and P4_1 to P3_2 respectively\n        p3_out = self.conv3_up(self.swish(p3_in + self.p3_upsample(p4_up)))\n\n        if self.first_time:\n            p4_in = self.p4_down_channel_2(p4)\n            p5_in = self.p5_down_channel_2(p5)\n\n        # Connections for P4_0, P4_1 and P3_2 to P4_2 respectively\n        p4_out = self.conv4_down(\n            self.swish(p4_in + p4_up + self.p4_downsample(p3_out)))\n\n        # Connections for P5_0, P5_1 and P4_2 to P5_2 respectively\n        p5_out = self.conv5_down(\n            self.swish(p5_in + p5_up + self.p5_downsample(p4_out)))\n\n        # Connections for P6_0, P6_1 and P5_2 to P6_2 respectively\n        p6_out = self.conv6_down(\n            self.swish(p6_in + p6_up + self.p6_downsample(p5_out)))\n\n        # Connections for P7_0 and P6_2 to P7_2\n        p7_out = self.conv7_down(self.swish(p7_in + self.p7_downsample(p6_out)))\n\n        return p3_out, p4_out, p5_out, p6_out, p7_out\n\n\nclass Regressor(nn.Module):\n    """"""\n    modified by Zylo117\n    """"""\n\n    def __init__(self, in_channels, num_anchors, num_layers, onnx_export=False):\n        super(Regressor, self).__init__()\n        self.num_layers = num_layers\n        self.num_layers = num_layers\n\n        self.conv_list = nn.ModuleList(\n            [SeparableConvBlock(in_channels, in_channels, norm=False, activation=False) for i in range(num_layers)])\n        self.bn_list = nn.ModuleList(\n            [nn.ModuleList([nn.BatchNorm2d(in_channels, momentum=0.01, eps=1e-3) for i in range(num_layers)]) for j in\n             range(5)])\n        self.header = SeparableConvBlock(in_channels, num_anchors * 4, norm=False, activation=False)\n        self.swish = MemoryEfficientSwish() if not onnx_export else Swish()\n\n    def forward(self, inputs):\n        feats = []\n        for feat, bn_list in zip(inputs, self.bn_list):\n            for i, bn, conv in zip(range(self.num_layers), bn_list, self.conv_list):\n                feat = conv(feat)\n                feat = bn(feat)\n                feat = self.swish(feat)\n            feat = self.header(feat)\n\n            feat = feat.permute(0, 2, 3, 1)\n            feat = feat.contiguous().view(feat.shape[0], -1, 4)\n\n            feats.append(feat)\n\n        feats = torch.cat(feats, dim=1)\n\n        return feats\n\n\nclass Classifier(nn.Module):\n    """"""\n    modified by Zylo117\n    """"""\n\n    def __init__(self, in_channels, num_anchors, num_classes, num_layers, onnx_export=False):\n        super(Classifier, self).__init__()\n        self.num_anchors = num_anchors\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.conv_list = nn.ModuleList(\n            [SeparableConvBlock(in_channels, in_channels, norm=False, activation=False) for i in range(num_layers)])\n        self.bn_list = nn.ModuleList(\n            [nn.ModuleList([nn.BatchNorm2d(in_channels, momentum=0.01, eps=1e-3) for i in range(num_layers)]) for j in\n             range(5)])\n        self.header = SeparableConvBlock(in_channels, num_anchors * num_classes, norm=False, activation=False)\n        self.swish = MemoryEfficientSwish() if not onnx_export else Swish()\n\n    def forward(self, inputs):\n        feats = []\n        for feat, bn_list in zip(inputs, self.bn_list):\n            for i, bn, conv in zip(range(self.num_layers), bn_list, self.conv_list):\n                feat = conv(feat)\n                feat = bn(feat)\n                feat = self.swish(feat)\n            feat = self.header(feat)\n\n            feat = feat.permute(0, 2, 3, 1)\n            feat = feat.contiguous().view(feat.shape[0], feat.shape[1], feat.shape[2], self.num_anchors,\n                                          self.num_classes)\n            feat = feat.contiguous().view(feat.shape[0], -1, self.num_classes)\n\n            feats.append(feat)\n\n        feats = torch.cat(feats, dim=1)\n        feats = feats.sigmoid()\n\n        return feats\n\n\nclass EfficientNet(nn.Module):\n    """"""\n    modified by Zylo117\n    """"""\n\n    def __init__(self, compound_coef, load_weights=False):\n        super(EfficientNet, self).__init__()\n        model = EffNet.from_pretrained(f\'efficientnet-b{compound_coef}\', load_weights)\n        del model._conv_head\n        del model._bn1\n        del model._avg_pooling\n        del model._dropout\n        del model._fc\n        self.model = model\n\n    def forward(self, x):\n        x = self.model._conv_stem(x)\n        x = self.model._bn0(x)\n        x = self.model._swish(x)\n        feature_maps = []\n\n        # TODO: temporarily storing extra tensor last_x and del it later might not be a good idea,\n        #  try recording stride changing when creating efficientnet,\n        #  and then apply it here.\n        last_x = None\n        for idx, block in enumerate(self.model._blocks):\n            drop_connect_rate = self.model._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self.model._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n\n            if block._depthwise_conv.stride == [2, 2]:\n                feature_maps.append(last_x)\n            elif idx == len(self.model._blocks) - 1:\n                feature_maps.append(x)\n            last_x = x\n        del last_x\n        return feature_maps[1:]\n\n\nif __name__ == \'__main__\':\n    from tensorboardX import SummaryWriter\n\n\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n'"
efficientdet/utils.py,9,"b'import itertools\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass BBoxTransform(nn.Module):\n    def forward(self, anchors, regression):\n        """"""\n        decode_box_outputs adapted from https://github.com/google/automl/blob/master/efficientdet/anchors.py\n\n        Args:\n            anchors: [batchsize, boxes, (y1, x1, y2, x2)]\n            regression: [batchsize, boxes, (dy, dx, dh, dw)]\n\n        Returns:\n\n        """"""\n        y_centers_a = (anchors[..., 0] + anchors[..., 2]) / 2\n        x_centers_a = (anchors[..., 1] + anchors[..., 3]) / 2\n        ha = anchors[..., 2] - anchors[..., 0]\n        wa = anchors[..., 3] - anchors[..., 1]\n\n        w = regression[..., 3].exp() * wa\n        h = regression[..., 2].exp() * ha\n\n        y_centers = regression[..., 0] * ha + y_centers_a\n        x_centers = regression[..., 1] * wa + x_centers_a\n\n        ymin = y_centers - h / 2.\n        xmin = x_centers - w / 2.\n        ymax = y_centers + h / 2.\n        xmax = x_centers + w / 2.\n\n        return torch.stack([xmin, ymin, xmax, ymax], dim=2)\n\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width - 1)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height - 1)\n\n        return boxes\n\n\nclass Anchors(nn.Module):\n    """"""\n    adapted and modified from https://github.com/google/automl/blob/master/efficientdet/anchors.py by Zylo117\n    """"""\n\n    def __init__(self, anchor_scale=4., pyramid_levels=None, **kwargs):\n        super().__init__()\n        self.anchor_scale = anchor_scale\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n\n        self.strides = kwargs.get(\'strides\', [2 ** x for x in self.pyramid_levels])\n        self.scales = np.array(kwargs.get(\'scales\', [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]))\n        self.ratios = kwargs.get(\'ratios\', [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)])\n\n        self.last_anchors = {}\n        self.last_shape = None\n\n    def forward(self, image, dtype=torch.float32):\n        """"""Generates multiscale anchor boxes.\n\n        Args:\n          image_size: integer number of input image size. The input image has the\n            same dimension for width and height. The image_size should be divided by\n            the largest feature stride 2^max_level.\n          anchor_scale: float number representing the scale of size of the base\n            anchor to the feature stride 2^level.\n          anchor_configs: a dictionary with keys as the levels of anchors and\n            values as a list of anchor configuration.\n\n        Returns:\n          anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all\n            feature levels.\n        Raises:\n          ValueError: input size must be the multiple of largest feature stride.\n        """"""\n        image_shape = image.shape[2:]\n\n        if image_shape == self.last_shape and image.device in self.last_anchors:\n            return self.last_anchors[image.device]\n\n        if self.last_shape is None or self.last_shape != image_shape:\n            self.last_shape = image_shape\n\n        if dtype == torch.float16:\n            dtype = np.float16\n        else:\n            dtype = np.float32\n\n        boxes_all = []\n        for stride in self.strides:\n            boxes_level = []\n            for scale, ratio in itertools.product(self.scales, self.ratios):\n                if image_shape[1] % stride != 0:\n                    raise ValueError(\'input size must be divided by the stride.\')\n                base_anchor_size = self.anchor_scale * stride * scale\n                anchor_size_x_2 = base_anchor_size * ratio[0] / 2.0\n                anchor_size_y_2 = base_anchor_size * ratio[1] / 2.0\n\n                x = np.arange(stride / 2, image_shape[1], stride)\n                y = np.arange(stride / 2, image_shape[0], stride)\n                xv, yv = np.meshgrid(x, y)\n                xv = xv.reshape(-1)\n                yv = yv.reshape(-1)\n\n                # y1,x1,y2,x2\n                boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n                                   yv + anchor_size_y_2, xv + anchor_size_x_2))\n                boxes = np.swapaxes(boxes, 0, 1)\n                boxes_level.append(np.expand_dims(boxes, axis=1))\n            # concat anchors on the same level to the reshape NxAx4\n            boxes_level = np.concatenate(boxes_level, axis=1)\n            boxes_all.append(boxes_level.reshape([-1, 4]))\n\n        anchor_boxes = np.vstack(boxes_all)\n\n        anchor_boxes = torch.from_numpy(anchor_boxes.astype(dtype)).to(image.device)\n        anchor_boxes = anchor_boxes.unsqueeze(0)\n\n        # save it for later use to reduce overhead\n        self.last_anchors[image.device] = anchor_boxes\n        return anchor_boxes\n'"
efficientnet/__init__.py,0,"b'__version__ = ""0.6.1""\nfrom .model import EfficientNet\nfrom .utils import (\n    GlobalParams,\n    BlockArgs,\n    BlockDecoder,\n    efficientnet,\n    get_model_params,\n)\n\n'"
efficientnet/model.py,2,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import (\n    round_filters,\n    round_repeats,\n    drop_connect,\n    get_same_padding_conv2d,\n    get_model_params,\n    efficientnet_params,\n    load_pretrained_weights,\n    Swish,\n    MemoryEfficientSwish,\n)\n\nclass MBConvBlock(nn.Module):\n    """"""\n    Mobile Inverted Residual Bottleneck Block\n\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    """"""\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n        self._swish = MemoryEfficientSwish()\n\n    def forward(self, inputs, drop_connect_rate=None):\n        """"""\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        """"""\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._expand_conv(inputs)\n            x = self._bn0(x)\n            x = self._swish(x)\n\n        x = self._depthwise_conv(x)\n        x = self._bn1(x)\n        x = self._swish(x)\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_reduce(x_squeezed)\n            x_squeezed = self._swish(x_squeezed)\n            x_squeezed = self._se_expand(x_squeezed)\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._project_conv(x)\n        x = self._bn2(x)\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n    def set_swish(self, memory_efficient=True):\n        """"""Sets swish function as memory efficient (for training) or standard (for export)""""""\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n\n\nclass EfficientNet(nn.Module):\n    """"""\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n\n    Example:\n        model = EfficientNet.from_pretrained(\'efficientnet-b0\')\n\n    """"""\n\n    def __init__(self, blocks_args=None, global_params=None):\n        super().__init__()\n        assert isinstance(blocks_args, list), \'blocks_args should be a list\'\n        assert len(blocks_args) > 0, \'block args must be greater than 0\'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n\n        # Head\n        in_channels = block_args.output_filters  # output of final block\n        out_channels = round_filters(1280, self._global_params)\n        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Final linear layer\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n        self._swish = MemoryEfficientSwish()\n\n    def set_swish(self, memory_efficient=True):\n        """"""Sets swish function as memory efficient (for training) or standard (for export)""""""\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n        for block in self._blocks:\n            block.set_swish(memory_efficient)\n\n\n    def extract_features(self, inputs):\n        """""" Returns output of the final convolution layer """"""\n\n        # Stem\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n        # Head\n        x = self._swish(self._bn1(self._conv_head(x)))\n\n        return x\n\n    def forward(self, inputs):\n        """""" Calls extract_features to extract features, applies final linear layer, and returns logits. """"""\n        bs = inputs.size(0)\n        # Convolution layers\n        x = self.extract_features(inputs)\n\n        # Pooling and final linear layer\n        x = self._avg_pooling(x)\n        x = x.view(bs, -1)\n        x = self._dropout(x)\n        x = self._fc(x)\n        return x\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return cls(blocks_args, global_params)\n\n    @classmethod\n    def from_pretrained(cls, model_name, load_weights=True, advprop=True, num_classes=1000, in_channels=3):\n        model = cls.from_name(model_name, override_params={\'num_classes\': num_classes})\n        if load_weights:\n            load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000), advprop=advprop)\n        if in_channels != 3:\n            Conv2d = get_same_padding_conv2d(image_size = model._global_params.image_size)\n            out_channels = round_filters(32, model._global_params)\n            model._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name):\n        """""" Validates model name. """"""\n        valid_models = [\'efficientnet-b\'+str(i) for i in range(9)]\n        if model_name not in valid_models:\n            raise ValueError(\'model_name should be one of: \' + \', \'.join(valid_models))\n'"
efficientnet/utils.py,10,"b'""""""\nThis file contains helper functions for building the model and for loading model parameters.\nThese helper functions are built to mirror those in the official TensorFlow implementation.\n""""""\n\nimport re\nimport math\nimport collections\nfrom functools import partial\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\nfrom .utils_extra import Conv2dStaticSamePadding\n\n########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\n\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\',\n    \'depth_divisor\', \'min_depth\', \'drop_connect_rate\', \'image_size\'])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'stride\', \'se_ratio\'])\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    """""" Calculate and round number of filters based on depth multiplier. """"""\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """""" Round number of filters based on depth multiplier. """"""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    """""" Drop connect. """"""\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    """""" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. """"""\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow, for a dynamic image size """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Identity(nn.Module):\n    def __init__(self, ):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    """""" Map EfficientNet model name to parameter coefficients. """"""\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n        \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n        \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        """""" Gets a block through a string notation of arguments. """"""\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert ((\'s\' in options and len(options[\'s\']) == 1) or\n                (len(options[\'s\']) == 2 and options[\'s\'][0] == options[\'s\'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=[int(options[\'s\'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        """"""\n        Decodes a list of string notations to specify blocks inside the network.\n\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        """"""\n        Encodes a list of BlockArgs to a list of strings.\n\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    """""" Creates a efficientnet model. """"""\n\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s11_e6_i192_o320_se0.25\',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format=\'channels_last\',  # removed, this is always true in PyTorch\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    """""" Get the block args and global params for a given model """"""\n    if model_name.startswith(\'efficientnet\'):\n        w, d, s, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    \'efficientnet-b0\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b0-355c32eb.pth\',\n    \'efficientnet-b1\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b1-f1951068.pth\',\n    \'efficientnet-b2\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b2-8bb594d6.pth\',\n    \'efficientnet-b3\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b3-5fb5a3c3.pth\',\n    \'efficientnet-b4\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b4-6ed6700e.pth\',\n    \'efficientnet-b5\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b5-b6417697.pth\',\n    \'efficientnet-b6\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b6-c76e70fd.pth\',\n    \'efficientnet-b7\': \'https://publicmodels.blob.core.windows.net/container/aa/efficientnet-b7-dcc49843.pth\',\n}\n\nurl_map_advprop = {\n    \'efficientnet-b0\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b0-b64d5a18.pth\',\n    \'efficientnet-b1\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b1-0f3ce85a.pth\',\n    \'efficientnet-b2\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b2-6e9d97e5.pth\',\n    \'efficientnet-b3\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b3-cdd7c0f4.pth\',\n    \'efficientnet-b4\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b4-44fb3a87.pth\',\n    \'efficientnet-b5\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b5-86493f6b.pth\',\n    \'efficientnet-b6\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b6-ac80338e.pth\',\n    \'efficientnet-b7\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b7-4652b6dd.pth\',\n    \'efficientnet-b8\': \'https://publicmodels.blob.core.windows.net/container/advprop/efficientnet-b8-22a8fe65.pth\',\n}\n\n\ndef load_pretrained_weights(model, model_name, load_fc=True, advprop=False):\n    """""" Loads pretrained weights, and downloads if loading for the first time. """"""\n    # AutoAugment or Advprop (different preprocessing)\n    url_map_ = url_map_advprop if advprop else url_map\n    state_dict = model_zoo.load_url(url_map_[model_name], map_location=torch.device(\'cpu\'))\n    # state_dict = torch.load(\'../../weights/backbone_efficientnetb0.pth\')\n    if load_fc:\n        ret = model.load_state_dict(state_dict, strict=False)\n        print(ret)\n    else:\n        state_dict.pop(\'_fc.weight\')\n        state_dict.pop(\'_fc.bias\')\n        res = model.load_state_dict(state_dict, strict=False)\n        assert set(res.missing_keys) == set([\'_fc.weight\', \'_fc.bias\']), \'issue loading pretrained weights\'\n    print(\'Loaded pretrained weights for {}\'.format(model_name))\n'"
efficientnet/utils_extra.py,1,"b'# Author: Zylo117\n\nimport math\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Conv2dStaticSamePadding(nn.Module):\n    """"""\n    created by Zylo117\n    The real keras/tensorflow conv2d with same padding\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True, groups=1, dilation=1, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride,\n                              bias=bias, groups=groups)\n        self.stride = self.conv.stride\n        self.kernel_size = self.conv.kernel_size\n        self.dilation = self.conv.dilation\n\n        if isinstance(self.stride, int):\n            self.stride = [self.stride] * 2\n        elif len(self.stride) == 1:\n            self.stride = [self.stride[0]] * 2\n\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = [self.kernel_size] * 2\n        elif len(self.kernel_size) == 1:\n            self.kernel_size = [self.kernel_size[0]] * 2\n\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        \n        extra_h = (math.ceil(w / self.stride[1]) - 1) * self.stride[1] - w + self.kernel_size[1]\n        extra_v = (math.ceil(h / self.stride[0]) - 1) * self.stride[0] - h + self.kernel_size[0]\n        \n        left = extra_h // 2\n        right = extra_h - left\n        top = extra_v // 2\n        bottom = extra_v - top\n\n        x = F.pad(x, [left, right, top, bottom])\n\n        x = self.conv(x)\n        return x\n\n\nclass MaxPool2dStaticSamePadding(nn.Module):\n    """"""\n    created by Zylo117\n    The real keras/tensorflow MaxPool2d with same padding\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.pool = nn.MaxPool2d(*args, **kwargs)\n        self.stride = self.pool.stride\n        self.kernel_size = self.pool.kernel_size\n\n        if isinstance(self.stride, int):\n            self.stride = [self.stride] * 2\n        elif len(self.stride) == 1:\n            self.stride = [self.stride[0]] * 2\n\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = [self.kernel_size] * 2\n        elif len(self.kernel_size) == 1:\n            self.kernel_size = [self.kernel_size[0]] * 2\n\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        \n        extra_h = (math.ceil(w / self.stride[1]) - 1) * self.stride[1] - w + self.kernel_size[1]\n        extra_v = (math.ceil(h / self.stride[0]) - 1) * self.stride[0] - h + self.kernel_size[0]\n\n        left = extra_h // 2\n        right = extra_h - left\n        top = extra_v // 2\n        bottom = extra_v - top\n\n        x = F.pad(x, [left, right, top, bottom])\n\n        x = self.pool(x)\n        return x\n'"
utils/utils.py,4,"b'# Author: Zylo117\n\nimport os\n\nimport cv2\nimport numpy as np\nimport torch\nfrom glob import glob\nfrom torch import nn\nfrom torchvision.ops import nms\nfrom torchvision.ops.boxes import batched_nms\nfrom typing import Union\nimport uuid\n\nfrom utils.sync_batchnorm import SynchronizedBatchNorm2d\n\nfrom torch.nn.init import _calculate_fan_in_and_fan_out, _no_grad_normal_\nimport math\nimport webcolors\n\ndef invert_affine(metas: Union[float, list, tuple], preds):\n    for i in range(len(preds)):\n        if len(preds[i][\'rois\']) == 0:\n            continue\n        else:\n            if metas is float:\n                preds[i][\'rois\'][:, [0, 2]] = preds[i][\'rois\'][:, [0, 2]] / metas\n                preds[i][\'rois\'][:, [1, 3]] = preds[i][\'rois\'][:, [1, 3]] / metas\n            else:\n                new_w, new_h, old_w, old_h, padding_w, padding_h = metas[i]\n                preds[i][\'rois\'][:, [0, 2]] = preds[i][\'rois\'][:, [0, 2]] / (new_w / old_w)\n                preds[i][\'rois\'][:, [1, 3]] = preds[i][\'rois\'][:, [1, 3]] / (new_h / old_h)\n    return preds\n\n\ndef aspectaware_resize_padding(image, width, height, interpolation=None, means=None):\n    old_h, old_w, c = image.shape\n    if old_w > old_h:\n        new_w = width\n        new_h = int(width / old_w * old_h)\n    else:\n        new_w = int(height / old_h * old_w)\n        new_h = height\n\n    canvas = np.zeros((height, height, c), np.float32)\n    if means is not None:\n        canvas[...] = means\n\n    if new_w != old_w or new_h != old_h:\n        if interpolation is None:\n            image = cv2.resize(image, (new_w, new_h))\n        else:\n            image = cv2.resize(image, (new_w, new_h), interpolation=interpolation)\n\n    padding_h = height - new_h\n    padding_w = width - new_w\n\n    if c > 1:\n        canvas[:new_h, :new_w] = image\n    else:\n        if len(image.shape) == 2:\n            canvas[:new_h, :new_w, 0] = image\n        else:\n            canvas[:new_h, :new_w] = image\n\n    return canvas, new_w, new_h, old_w, old_h, padding_w, padding_h,\n\n\ndef preprocess(*image_path, max_size=512, mean=(0.406, 0.456, 0.485), std=(0.225, 0.224, 0.229)):\n    ori_imgs = [cv2.imread(img_path) for img_path in image_path]\n    normalized_imgs = [(img / 255 - mean) / std for img in ori_imgs]\n    imgs_meta = [aspectaware_resize_padding(img[..., ::-1], max_size, max_size,\n                                            means=None) for img in normalized_imgs]\n    framed_imgs = [img_meta[0] for img_meta in imgs_meta]\n    framed_metas = [img_meta[1:] for img_meta in imgs_meta]\n\n    return ori_imgs, framed_imgs, framed_metas\n\n\ndef preprocess_video(*frame_from_video, max_size=512, mean=(0.406, 0.456, 0.485), std=(0.225, 0.224, 0.229)):\n    ori_imgs = frame_from_video\n    normalized_imgs = [(img / 255 - mean) / std for img in ori_imgs]\n    imgs_meta = [aspectaware_resize_padding(img[..., ::-1], max_size, max_size,\n                                            means=None) for img in normalized_imgs]\n    framed_imgs = [img_meta[0] for img_meta in imgs_meta]\n    framed_metas = [img_meta[1:] for img_meta in imgs_meta]\n\n    return ori_imgs, framed_imgs, framed_metas\n\n\ndef postprocess(x, anchors, regression, classification, regressBoxes, clipBoxes, threshold, iou_threshold):\n    transformed_anchors = regressBoxes(anchors, regression)\n    transformed_anchors = clipBoxes(transformed_anchors, x)\n    scores = torch.max(classification, dim=2, keepdim=True)[0]\n    scores_over_thresh = (scores > threshold)[:, :, 0]\n    out = []\n    for i in range(x.shape[0]):\n        if scores_over_thresh[i].sum() == 0:\n            out.append({\n                \'rois\': np.array(()),\n                \'class_ids\': np.array(()),\n                \'scores\': np.array(()),\n            })\n            continue\n\n        classification_per = classification[i, scores_over_thresh[i, :], ...].permute(1, 0)\n        transformed_anchors_per = transformed_anchors[i, scores_over_thresh[i, :], ...]\n        scores_per = scores[i, scores_over_thresh[i, :], ...]\n        scores_, classes_ = classification_per.max(dim=0)\n        anchors_nms_idx = batched_nms(transformed_anchors_per, scores_per[:, 0], classes_, iou_threshold=iou_threshold)\n\n        if anchors_nms_idx.shape[0] != 0:\n            classes_ = classes_[anchors_nms_idx]\n            scores_ = scores_[anchors_nms_idx]\n            boxes_ = transformed_anchors_per[anchors_nms_idx, :]\n\n            out.append({\n                \'rois\': boxes_.cpu().numpy(),\n                \'class_ids\': classes_.cpu().numpy(),\n                \'scores\': scores_.cpu().numpy(),\n            })\n        else:\n            out.append({\n                \'rois\': np.array(()),\n                \'class_ids\': np.array(()),\n                \'scores\': np.array(()),\n            })\n\n    return out\n\n\ndef display(preds, imgs, obj_list, imshow=True, imwrite=False):\n    for i in range(len(imgs)):\n        if len(preds[i][\'rois\']) == 0:\n            continue\n\n        for j in range(len(preds[i][\'rois\'])):\n            (x1, y1, x2, y2) = preds[i][\'rois\'][j].astype(np.int)\n            cv2.rectangle(imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 2)\n            obj = obj_list[preds[i][\'class_ids\'][j]]\n            score = float(preds[i][\'scores\'][j])\n\n            cv2.putText(imgs[i], \'{}, {:.3f}\'.format(obj, score),\n                        (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (255, 255, 0), 1)\n        if imshow:\n            cv2.imshow(\'img\', imgs[i])\n            cv2.waitKey(0)\n\n        if imwrite:\n            os.makedirs(\'test/\', exist_ok=True)\n            cv2.imwrite(f\'test/{uuid.uuid4().hex}.jpg\', imgs[i])\n\n\ndef replace_w_sync_bn(m):\n    for var_name in dir(m):\n        target_attr = getattr(m, var_name)\n        if type(target_attr) == torch.nn.BatchNorm2d:\n            num_features = target_attr.num_features\n            eps = target_attr.eps\n            momentum = target_attr.momentum\n            affine = target_attr.affine\n\n            # get parameters\n            running_mean = target_attr.running_mean\n            running_var = target_attr.running_var\n            if affine:\n                weight = target_attr.weight\n                bias = target_attr.bias\n\n            setattr(m, var_name,\n                    SynchronizedBatchNorm2d(num_features, eps, momentum, affine))\n\n            target_attr = getattr(m, var_name)\n            # set parameters\n            target_attr.running_mean = running_mean\n            target_attr.running_var = running_var\n            if affine:\n                target_attr.weight = weight\n                target_attr.bias = bias\n\n    for var_name, children in m.named_children():\n        replace_w_sync_bn(children)\n\n\nclass CustomDataParallel(nn.DataParallel):\n    """"""\n    force splitting data to all gpus instead of sending all data to cuda:0 and then moving around.\n    """"""\n\n    def __init__(self, module, num_gpus):\n        super().__init__(module)\n        self.num_gpus = num_gpus\n\n    def scatter(self, inputs, kwargs, device_ids):\n        # More like scatter and data prep at the same time. The point is we prep the data in such a way\n        # that no scatter is necessary, and there\'s no need to shuffle stuff around different GPUs.\n        devices = [\'cuda:\' + str(x) for x in range(self.num_gpus)]\n        splits = inputs[0].shape[0] // self.num_gpus\n\n        if splits == 0:\n            raise Exception(\'Batchsize must be greater than num_gpus.\')\n\n        return [(inputs[0][splits * device_idx: splits * (device_idx + 1)].to(f\'cuda:{device_idx}\', non_blocking=True),\n                 inputs[1][splits * device_idx: splits * (device_idx + 1)].to(f\'cuda:{device_idx}\', non_blocking=True))\n                for device_idx in range(len(devices))], \\\n               [kwargs] * len(devices)\n\n\ndef get_last_weights(weights_path):\n    weights_path = glob(weights_path + f\'/*.pth\')\n    weights_path = sorted(weights_path,\n                          key=lambda x: int(x.rsplit(\'_\')[-1].rsplit(\'.\')[0]),\n                          reverse=True)[0]\n    print(f\'using weights {weights_path}\')\n    return weights_path\n\n\ndef init_weights(model):\n    for name, module in model.named_modules():\n        is_conv_layer = isinstance(module, nn.Conv2d)\n\n        if is_conv_layer:\n            if ""conv_list"" or ""header"" in name:\n                variance_scaling_(module.weight.data)\n            else:\n                nn.init.kaiming_uniform_(module.weight.data)\n\n            if module.bias is not None:\n                if ""classifier.header"" in name:\n                    bias_value = -np.log((1 - 0.01) / 0.01)\n                    torch.nn.init.constant_(module.bias, bias_value)\n                else:\n                    module.bias.data.zero_()\n\n\ndef variance_scaling_(tensor, gain=1.):\n    # type: (Tensor, float) -> Tensor\n    r""""""\n    initializer for SeparableConv in Regressor/Classifier\n    reference: https://keras.io/zh/initializers/  VarianceScaling\n    """"""\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = math.sqrt(gain / float(fan_in))\n\n    return _no_grad_normal_(tensor, 0., std)\n\nSTANDARD_COLORS = [\n    \'LawnGreen\', \'Chartreuse\', \'Aqua\',\'Beige\', \'Azure\',\'BlanchedAlmond\',\'Bisque\',\n    \'Aquamarine\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'AliceBlue\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\'\n]\n\ndef from_colorname_to_bgr(color):\n    rgb_color=webcolors.name_to_rgb(color)\n    result=(rgb_color.blue,rgb_color.green,rgb_color.red)\n    return result\n\ndef standard_to_bgr(list_color_name):\n    standard= []\n    for i in range(len(list_color_name)-36): #-36 used to match the len(obj_list)\n        standard.append(from_colorname_to_bgr(list_color_name[i]))\n    return standard\n\ndef get_index_label(label, obj_list):\n    index = int(obj_list.index(label))\n    return index\n\ndef plot_one_box(img, coord, label=None, score=None, color=None, line_thickness=None):\n    tl = line_thickness or int(round(0.001 * max(img.shape[0:2])))  # line thickness\n    color = color\n    c1, c2 = (int(coord[0]), int(coord[1])), (int(coord[2]), int(coord[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl)\n    if label:\n        tf = max(tl - 2, 1)  # font thickness\n        s_size = cv2.getTextSize(str(\'{:.0%}\'.format(score)),0, fontScale=float(tl) / 3, thickness=tf)[0]\n        t_size = cv2.getTextSize(label, 0, fontScale=float(tl) / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0]+s_size[0]+15, c1[1] - t_size[1] -3\n        cv2.rectangle(img, c1, c2 , color, -1)  # filled\n        cv2.putText(img, \'{}: {:.0%}\'.format(label, score), (c1[0],c1[1] - 2), 0, float(tl) / 3, [0, 0, 0], thickness=tf, lineType=cv2.FONT_HERSHEY_SIMPLEX)\n'"
utils/sync_batchnorm/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .batchnorm import patch_sync_batchnorm, convert_model\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
utils/sync_batchnorm/batchnorm.py,16,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\nimport contextlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\ntry:\n    from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\nexcept ImportError:\n    ReduceAddCoalesced = Broadcast = None\n\ntry:\n    from jactorch.parallel.comm import SyncMaster\n    from jactorch.parallel.data_parallel import JacDataParallel as DataParallelWithCallback\nexcept ImportError:\n    from .comm import SyncMaster\n    from .replicate import DataParallelWithCallback\n\n__all__ = [\n    \'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\',\n    \'patch_sync_batchnorm\', \'convert_model\'\n]\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dimensions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        assert ReduceAddCoalesced is not None, \'Can not use Synchronized Batch Normalization without CUDA support.\'\n\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n\n        # Always using same ""device order"" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        if hasattr(torch, \'no_grad\'):\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n        else:\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n\n\n@contextlib.contextmanager\ndef patch_sync_batchnorm():\n    import torch.nn as nn\n\n    backup = nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d\n\n    nn.BatchNorm1d = SynchronizedBatchNorm1d\n    nn.BatchNorm2d = SynchronizedBatchNorm2d\n    nn.BatchNorm3d = SynchronizedBatchNorm3d\n\n    yield\n\n    nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d = backup\n\n\ndef convert_model(module):\n    """"""Traverse the input module and its child recursively\n       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n       to SynchronizedBatchNorm*N*d\n\n    Args:\n        module: the input module needs to be convert to SyncBN model\n\n    Examples:\n        >>> import torch.nn as nn\n        >>> import torchvision\n        >>> # m is a standard pytorch model\n        >>> m = torchvision.models.resnet18(True)\n        >>> m = nn.DataParallel(m)\n        >>> # after convert, m is using SyncBN\n        >>> m = convert_model(m)\n    """"""\n    if isinstance(module, torch.nn.DataParallel):\n        mod = module.module\n        mod = convert_model(mod)\n        mod = DataParallelWithCallback(mod, device_ids=module.device_ids)\n        return mod\n\n    mod = module\n    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n                                        torch.nn.modules.batchnorm.BatchNorm2d,\n                                        torch.nn.modules.batchnorm.BatchNorm3d],\n                                       [SynchronizedBatchNorm1d,\n                                        SynchronizedBatchNorm2d,\n                                        SynchronizedBatchNorm3d]):\n        if isinstance(module, pth_module):\n            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)\n            mod.running_mean = module.running_mean\n            mod.running_var = module.running_var\n            if module.affine:\n                mod.weight.data = module.weight.data.clone().detach()\n                mod.bias.data = module.bias.data.clone().detach()\n\n    for name, child in module.named_children():\n        mod.add_module(name, convert_model(child))\n\n    return mod\n'"
utils/sync_batchnorm/batchnorm_reimpl.py,6,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : batchnorm_reimpl.py\n# Author : acgtyrant\n# Date   : 11/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n__all__ = [\'BatchNorm2dReimpl\']\n\n\nclass BatchNorm2dReimpl(nn.Module):\n    """"""\n    A re-implementation of batch normalization, used for testing the numerical\n    stability.\n\n    Author: acgtyrant\n    See also:\n    https://github.com/vacancy/Synchronized-BatchNorm-PyTorch/issues/14\n    """"""\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super().__init__()\n\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.weight = nn.Parameter(torch.empty(num_features))\n        self.bias = nn.Parameter(torch.empty(num_features))\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        init.uniform_(self.weight)\n        init.zeros_(self.bias)\n\n    def forward(self, input_):\n        batchsize, channels, height, width = input_.size()\n        numel = batchsize * height * width\n        input_ = input_.permute(1, 0, 2, 3).contiguous().view(channels, numel)\n        sum_ = input_.sum(1)\n        sum_of_square = input_.pow(2).sum(1)\n        mean = sum_ / numel\n        sumvar = sum_of_square - sum_ * mean\n\n        self.running_mean = (\n                (1 - self.momentum) * self.running_mean\n                + self.momentum * mean.detach()\n        )\n        unbias_var = sumvar / (numel - 1)\n        self.running_var = (\n                (1 - self.momentum) * self.running_var\n                + self.momentum * unbias_var.detach()\n        )\n\n        bias_var = sumvar / numel\n        inv_std = 1 / (bias_var + self.eps).pow(0.5)\n        output = (\n                (input_ - mean.unsqueeze(1)) * inv_std.unsqueeze(1) *\n                self.weight.unsqueeze(1) + self.bias.unsqueeze(1))\n\n        return output.view(channels, batchsize, height, width).permute(1, 0, 2, 3).contiguous()\n\n'"
utils/sync_batchnorm/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {\'master_callback\': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'master_callback\'])\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
utils/sync_batchnorm/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
utils/sync_batchnorm/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\nimport torch\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, x, y):\n        adiff = float((x - y).abs().max())\n        if (y == 0).all():\n            rdiff = 'NaN'\n        else:\n            rdiff = float((adiff / y).abs().max())\n\n        message = (\n            'Tensor close check failed\\n'\n            'adiff={}\\n'\n            'rdiff={}\\n'\n        ).format(adiff, rdiff)\n        self.assertTrue(torch.allclose(x, y), message)\n\n"""
